{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTM\n",
    "author : nakamuraleon  \n",
    "email : nakamuraleon0552@gmail.com  \n",
    "\n",
    "### 1. Table of contents\n",
    "\n",
    "### 2. Dataset\n",
    "\n",
    "### 3. Reference\n",
    "https://qiita.com/m3yrin/items/3a8157f65eb9862ac21e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import grid_graph\n",
    "import gensim\n",
    "import glob\n",
    "import trainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, 3. Loading File (Load dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfile(period):\n",
    "    f = open('./Dataset/'+str(period)+'_docsfile.txt', 'rb')\n",
    "    dicts = pickle.load(f)\n",
    "    return dicts\n",
    "\n",
    "def splitdata(dicts):\n",
    "    test_valid_size = int(len(dicts) * 0.1)\n",
    "    test_data  = dicts[:test_valid_size]\n",
    "    valid_data = dicts[test_valid_size : test_valid_size*2]\n",
    "    train_data = dicts[test_valid_size*2 :]\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 2000\n",
    "dicts = loadfile(period)\n",
    "dicts = dicts[:500]\n",
    "bow_vocab = gensim.corpora.Dictionary(dicts)\n",
    "bow_vocab_size = len(bow_vocab)\n",
    "train_data, valid_data, test_data = splitdata(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:199: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  torch.nn.init.kaiming_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 1000 \n",
    "topic_num = 15\n",
    "batch_size = 32\n",
    "ntm_model = trainer.NTMEstimator(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num, l1_strength=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:211: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.data = np.array(data)\n",
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:212: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 2188.0782\n",
      "Test epoch : 1 Average loss: 2495.7283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/08dvs7wn6t5_jn5sxxsly14c0000gn/T/ipykernel_7796/2372893990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, valid_data, bow_vocab, batch_size, n_epoch)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_test_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mpp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PP(train) = %.3f, PP(valid) = %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py\u001b[0m in \u001b[0;36mcompute_perplexity\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdata_bow_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bow_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m#loss += loss_function(recon_batch, data_bow, mu, logvar).detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/ntm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/ntm.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ntm_model, z_train, z_valid = ntm_model.fit(train_data, valid_data, bow_vocab, batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vocab = gensim.corpora.Dictionary(dicts)\n",
    "bow_vocab_size = len(bow_vocab)\n",
    "hidden_dim = 1000\n",
    "topic_num = 15\n",
    "batch_size = 32\n",
    "ntm_model = trainer.NTMEstimator(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num, l1_strength=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:170: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  torch.nn.init.kaiming_uniform(m.weight)\n",
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:179: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.data = np.array(data)\n",
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n",
      "/Users/nakamurareon/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1759e-11, 1.0140e-18, 2.0393e-19, 1.0418e-24, 1.2098e-15, 9.9998e-01,\n",
      "         2.8157e-23, 4.8728e-29, 6.0457e-20, 8.8956e-16, 8.0873e-18, 7.6977e-14,\n",
      "         1.9210e-05, 5.1432e-17, 2.1127e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0425, 0.0611, 0.0674, 0.0385, 0.0333, 0.0478, 0.0371, 0.1147, 0.1018,\n",
      "         0.0984, 0.0464, 0.0936, 0.0469, 0.1000, 0.0704]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 2101.4947\n",
      "Test epoch : 1 Average loss: 1719.2348\n",
      "PP(train) = 20830.188, PP(valid) = 19495.021\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0420e-20, 1.3733e-43, 4.1421e-17, 7.0960e-27, 1.8266e-03, 1.1954e-11,\n",
      "         2.4577e-30, 0.0000e+00, 2.4632e-25, 1.8964e-25, 4.1880e-32, 4.5594e-19,\n",
      "         5.1087e-05, 9.4366e-31, 9.9812e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0602, 0.0669, 0.1076, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 2058.0023\n",
      "Test epoch : 2 Average loss: 1717.0748\n",
      "PP(train) = 20445.207, PP(valid) = 19022.256\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3682e-42, 0.0000e+00, 2.6625e-43, 0.0000e+00, 1.6135e-12, 9.9987e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6587e-36, 8.1008e-17,\n",
      "         1.3472e-04, 4.0600e-41, 9.5433e-21]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0425, 0.0611, 0.0674, 0.0385, 0.0333, 0.0478, 0.0371, 0.1147, 0.1018,\n",
      "         0.0984, 0.0464, 0.0936, 0.0469, 0.1000, 0.0705]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1991.7610\n",
      "Test epoch : 3 Average loss: 1716.3109\n",
      "PP(train) = 19532.725, PP(valid) = 18374.672\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0681e-28, 0.0000e+00, 8.1275e-43, 0.0000e+00, 4.5363e-01, 5.4607e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5274e-43, 2.6790e-27, 2.9890e-04,\n",
      "         5.0620e-08, 0.0000e+00, 2.6031e-31]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0449, 0.0564, 0.0562, 0.0395, 0.0555, 0.0569, 0.0618, 0.1152, 0.0725,\n",
      "         0.0994, 0.0566, 0.0789, 0.0477, 0.0830, 0.0755]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1954.3791\n",
      "Test epoch : 4 Average loss: 1718.2908\n",
      "PP(train) = 18585.238, PP(valid) = 17807.355\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2425e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9137e-01, 1.7741e-17,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0656e-33, 8.3965e-03, 4.3145e-19,\n",
      "         5.1855e-17, 0.0000e+00, 2.3573e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0483, 0.0422, 0.0381, 0.0957, 0.0657, 0.1058, 0.1082, 0.0451,\n",
      "         0.0933, 0.0675, 0.0600, 0.0455, 0.0624, 0.0769]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1936.9195\n",
      "Test epoch : 5 Average loss: 1724.6527\n",
      "PP(train) = 17851.922, PP(valid) = 17864.287\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.8726e-01, 0.0000e+00, 4.7644e-44, 0.0000e+00, 1.4797e-10, 5.1951e-14,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2750e-41, 1.2740e-02, 3.1097e-11,\n",
      "         1.2205e-21, 0.0000e+00, 8.4709e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0912, 0.0660, 0.0299, 0.1016, 0.0439, 0.0453, 0.0375, 0.1025, 0.0680,\n",
      "         0.0767, 0.0775, 0.1095, 0.0679, 0.0482, 0.0344]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1932.0570\n",
      "Test epoch : 6 Average loss: 1725.5029\n",
      "PP(train) = 17248.998, PP(valid) = 17726.939\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0554e-14, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9998e-01, 6.0367e-25,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8455e-37, 3.1135e-22, 5.7946e-16,\n",
      "         1.7314e-05, 0.0000e+00, 8.6400e-16]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1926.3991\n",
      "Test epoch : 7 Average loss: 1721.7955\n",
      "PP(train) = 16795.553, PP(valid) = 17436.537\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0061e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1793e-01, 1.9275e-13,\n",
      "         3.1666e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3730e-07, 1.1830e-13,\n",
      "         1.6611e-02, 0.0000e+00, 5.6546e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0642, 0.0471, 0.0725, 0.0458, 0.0741, 0.0677, 0.1081, 0.1004, 0.0541,\n",
      "         0.0688, 0.0651, 0.0585, 0.0498, 0.0683, 0.0556]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1921.0188\n",
      "Test epoch : 8 Average loss: 1720.6809\n",
      "PP(train) = 16527.105, PP(valid) = 17588.014\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9677e-10, 0.0000e+00, 2.8026e-44, 0.0000e+00, 9.9314e-01, 5.9192e-15,\n",
      "         1.4015e-40, 0.0000e+00, 0.0000e+00, 5.2829e-43, 6.5858e-10, 2.4978e-09,\n",
      "         6.8571e-03, 0.0000e+00, 1.4347e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0423, 0.0382, 0.0955, 0.0657, 0.1062, 0.1082, 0.0452,\n",
      "         0.0940, 0.0671, 0.0601, 0.0457, 0.0621, 0.0769]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1915.9267\n",
      "Test epoch : 9 Average loss: 1716.4574\n",
      "PP(train) = 16289.788, PP(valid) = 17434.773\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0520e-15, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2201e-03, 2.4839e-19,\n",
      "         1.2336e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5070e-21, 3.5341e-13,\n",
      "         5.2291e-02, 0.0000e+00, 9.4449e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0798, 0.0458, 0.1028, 0.0509, 0.0593, 0.0671, 0.1042, 0.0921, 0.0603,\n",
      "         0.0543, 0.0615, 0.0557, 0.0516, 0.0708, 0.0439]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1912.3042\n",
      "Test epoch : 10 Average loss: 1713.3371\n",
      "PP(train) = 16089.124, PP(valid) = 17318.045\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.5993e-17, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5156e-01, 2.1121e-19,\n",
      "         8.0461e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4406e-13, 2.6221e-14,\n",
      "         1.9143e-01, 0.0000e+00, 5.7010e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0478, 0.0527, 0.0505, 0.0430, 0.0808, 0.0667, 0.0952, 0.1028, 0.0512,\n",
      "         0.0924, 0.0638, 0.0594, 0.0489, 0.0640, 0.0807]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1910.5552\n",
      "Test epoch : 11 Average loss: 1712.9490\n",
      "PP(train) = 15989.591, PP(valid) = 17410.000\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5938e-18, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9970e-01, 3.0007e-21,\n",
      "         9.3796e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9200e-18, 1.6201e-10,\n",
      "         2.9878e-04, 0.0000e+00, 1.5456e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00006\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1909.0962\n",
      "Test epoch : 12 Average loss: 1711.0834\n",
      "PP(train) = 15823.288, PP(valid) = 17245.445\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9240e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4359e-08, 1.6717e-23,\n",
      "         3.2181e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1931e-28, 2.0909e-21,\n",
      "         5.5265e-01, 0.0000e+00, 4.4735e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0625, 0.0604, 0.0897, 0.0570, 0.0500, 0.0672, 0.0752, 0.0867, 0.0690,\n",
      "         0.0740, 0.0552, 0.0554, 0.0563, 0.0688, 0.0727]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00012\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1907.2707\n",
      "Test epoch : 13 Average loss: 1710.7424\n",
      "PP(train) = 15696.229, PP(valid) = 17154.998\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.1334e-23, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2835e-01, 1.5487e-28,\n",
      "         1.1940e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5209e-25, 2.8444e-16,\n",
      "         1.6455e-01, 0.0000e+00, 1.0710e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0492, 0.0520, 0.0521, 0.0431, 0.0809, 0.0668, 0.0971, 0.1030, 0.0513,\n",
      "         0.0899, 0.0642, 0.0594, 0.0489, 0.0645, 0.0776]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00021\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1905.0743\n",
      "Test epoch : 14 Average loss: 1711.7523\n",
      "PP(train) = 15566.596, PP(valid) = 17248.539\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3002e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7547e-08, 8.1672e-27,\n",
      "         3.7956e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8643e-25, 3.6607e-18,\n",
      "         2.6131e-08, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00038\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1902.7668\n",
      "Test epoch : 15 Average loss: 1709.7150\n",
      "PP(train) = 15332.947, PP(valid) = 16978.971\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7266e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5320e-11, 5.5057e-35,\n",
      "         9.8700e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7732e-37, 2.2473e-31,\n",
      "         2.5931e-01, 0.0000e+00, 7.4069e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0726, 0.0516, 0.0979, 0.0536, 0.0555, 0.0675, 0.0915, 0.0902, 0.0641,\n",
      "         0.0620, 0.0591, 0.0559, 0.0538, 0.0703, 0.0543]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00068\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1900.4382\n",
      "Test epoch : 16 Average loss: 1709.6600\n",
      "PP(train) = 15140.922, PP(valid) = 16975.975\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6211e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8672e-02, 3.5865e-33,\n",
      "         1.1550e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5034e-36, 5.8138e-21,\n",
      "         2.6967e-05, 0.0000e+00, 9.3130e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0788, 0.0448, 0.0985, 0.0495, 0.0624, 0.0672, 0.1080, 0.0938, 0.0585,\n",
      "         0.0547, 0.0626, 0.0562, 0.0509, 0.0706, 0.0434]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00122\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1897.7465\n",
      "Test epoch : 17 Average loss: 1709.6625\n",
      "PP(train) = 14967.579, PP(valid) = 17019.094\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7515e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8397e-12, 6.8782e-32,\n",
      "         3.7050e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0831e-35, 6.7943e-28,\n",
      "         1.8778e-02, 0.0000e+00, 9.8122e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0811, 0.0449, 0.1039, 0.0505, 0.0598, 0.0670, 0.1063, 0.0922, 0.0597,\n",
      "         0.0530, 0.0618, 0.0556, 0.0512, 0.0708, 0.0423]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00219\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1895.3601\n",
      "Test epoch : 18 Average loss: 1707.4363\n",
      "PP(train) = 14701.714, PP(valid) = 16771.863\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2166e-12, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4920e-39,\n",
      "         2.4903e-11, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00395\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1892.0874\n",
      "Test epoch : 19 Average loss: 1707.2936\n",
      "PP(train) = 14489.999, PP(valid) = 16792.398\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3050e-11, 3.0651e-39,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4013e-45, 9.8139e-36,\n",
      "         5.1962e-10, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00711\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1888.7631\n",
      "Test epoch : 20 Average loss: 1706.0420\n",
      "PP(train) = 14242.773, PP(valid) = 16676.805\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0323e-11, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8278e-36,\n",
      "         1.6947e-02, 0.0000e+00, 9.8305e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0811, 0.0448, 0.1039, 0.0504, 0.0598, 0.0670, 0.1065, 0.0922, 0.0597,\n",
      "         0.0529, 0.0618, 0.0556, 0.0512, 0.0708, 0.0422]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.01281\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1885.8359\n",
      "Test epoch : 21 Average loss: 1704.4707\n",
      "PP(train) = 13938.165, PP(valid) = 16510.975\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6934e-16, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8055e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.02305\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1880.8666\n",
      "Test epoch : 22 Average loss: 1704.2910\n",
      "PP(train) = 13678.657, PP(valid) = 16519.227\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3313e-17, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.7455e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.04150\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1877.1095\n",
      "Test epoch : 23 Average loss: 1701.6236\n",
      "PP(train) = 13330.747, PP(valid) = 16242.552\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.6132e-16, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.6259e-09, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.07473\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1871.7318\n",
      "Test epoch : 24 Average loss: 1701.5578\n",
      "PP(train) = 13011.266, PP(valid) = 16254.235\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3458e-28, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6402e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.13455\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1867.0996\n",
      "Test epoch : 25 Average loss: 1698.8779\n",
      "PP(train) = 12639.498, PP(valid) = 16013.519\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0576e-16, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8410e-09, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.24223\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1861.9121\n",
      "Test epoch : 26 Average loss: 1696.8432\n",
      "PP(train) = 12272.278, PP(valid) = 15817.661\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1004e-20, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.1961e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.43609\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1855.4301\n",
      "Test epoch : 27 Average loss: 1695.3453\n",
      "PP(train) = 11874.525, PP(valid) = 15679.666\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5171e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.5102e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.78519\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1849.0216\n",
      "Test epoch : 28 Average loss: 1693.4998\n",
      "PP(train) = 11448.701, PP(valid) = 15512.622\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2603e-15, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1563e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 1.41372\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1841.7444\n",
      "Test epoch : 29 Average loss: 1690.2756\n",
      "PP(train) = 11042.518, PP(valid) = 15237.918\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7814e-29, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.5637e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 2.54544\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1834.2807\n",
      "Test epoch : 30 Average loss: 1688.7900\n",
      "PP(train) = 10599.947, PP(valid) = 15095.722\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6534e-27, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.0276e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 4.58280\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1826.8731\n",
      "Test epoch : 31 Average loss: 1685.4627\n",
      "PP(train) = 10168.596, PP(valid) = 14805.678\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5383e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6581e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 8.25125\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1819.3389\n",
      "Test epoch : 32 Average loss: 1681.8975\n",
      "PP(train) = 9789.102, PP(valid) = 14519.961\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3770e-27, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.5783e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 14.85641\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1811.5040\n",
      "Test epoch : 33 Average loss: 1679.9506\n",
      "PP(train) = 9375.649, PP(valid) = 14319.901\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2842e-29, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.6507e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 26.74652\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1803.3906\n",
      "Test epoch : 34 Average loss: 1677.1236\n",
      "PP(train) = 9024.741, PP(valid) = 14104.724\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1936e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7210e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 48.15530\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1796.5294\n",
      "Test epoch : 35 Average loss: 1674.4018\n",
      "PP(train) = 8680.160, PP(valid) = 13863.026\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9994e-26, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.3696e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 86.69641\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1789.4618\n",
      "Test epoch : 36 Average loss: 1670.8920\n",
      "PP(train) = 8371.889, PP(valid) = 13617.457\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2786e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1412e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 156.09438\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1783.2331\n",
      "Test epoch : 37 Average loss: 1668.4740\n",
      "PP(train) = 8075.081, PP(valid) = 13413.537\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9707e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7916e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 281.02982\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1778.1406\n",
      "Test epoch : 38 Average loss: 1666.2186\n",
      "PP(train) = 7816.246, PP(valid) = 13253.258\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3896e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.4500e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 505.94394\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1774.3486\n",
      "Test epoch : 39 Average loss: 1664.0371\n",
      "PP(train) = 7569.144, PP(valid) = 13049.229\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3456e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2023e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 910.97644\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1771.0354\n",
      "Test epoch : 40 Average loss: 1661.3160\n",
      "PP(train) = 7355.713, PP(valid) = 12877.352\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5932e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6712e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 1640.24817\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1772.7461\n",
      "Test epoch : 41 Average loss: 1660.5707\n",
      "PP(train) = 7158.698, PP(valid) = 12833.854\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3693e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8124e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 2953.11621\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1780.5304\n",
      "Test epoch : 42 Average loss: 1658.1051\n",
      "PP(train) = 6972.018, PP(valid) = 12645.503\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8382e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.5093e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 5316.18652\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1797.4161\n",
      "Test epoch : 43 Average loss: 1657.5754\n",
      "PP(train) = 6815.937, PP(valid) = 12635.027\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8067e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.8915e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 9569.68750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1833.8361\n",
      "Test epoch : 44 Average loss: 1657.6178\n",
      "PP(train) = 6679.115, PP(valid) = 12643.767\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0722e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.5413e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 17223.79688\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1902.7788\n",
      "Test epoch : 45 Average loss: 1653.2791\n",
      "PP(train) = 6542.993, PP(valid) = 12300.939\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4795e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.8281e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.003, l1 strength = 30993.64258\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 2026.6666\n",
      "Test epoch : 46 Average loss: 1653.4344\n",
      "PP(train) = 6419.488, PP(valid) = 12317.702\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4723e-38, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.2826e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.005, l1 strength = 55737.91016\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 2253.1480\n",
      "Test epoch : 47 Average loss: 1651.7803\n",
      "PP(train) = 6304.787, PP(valid) = 12204.433\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2204e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.7959e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.008, l1 strength = 100130.45312\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 2654.3393\n",
      "Test epoch : 48 Average loss: 1645.2766\n",
      "PP(train) = 6219.039, PP(valid) = 11783.297\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6681e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.013, l1 strength = 179471.18750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 3356.3447\n",
      "Test epoch : 49 Average loss: 1644.0855\n",
      "PP(train) = 6119.650, PP(valid) = 11715.785\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.1900e-36, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.0856e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.020, l1 strength = 320575.68750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 4558.4498\n",
      "Test epoch : 50 Average loss: 1639.1436\n",
      "PP(train) = 6045.090, PP(valid) = 11386.842\n",
      "Writing to ./topicwords/0-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:         modulus \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 18.3313 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                      m       tf                      mm                                                                 m      kgf cm                          \n",
      "\n",
      "===== # 2, Topic : 14, p : 17.4352 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                        \n",
      "\n",
      "===== # 3, Topic : 14, p : 18.5964 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                   \n",
      "\n",
      "===== # 4, Topic : 12, p : 13.5700 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                    )                                                                                                                                                       \n",
      "\n",
      "===== # 5, Topic : 14, p : 12.2896 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                          \n",
      "\n",
      "===== # 6, Topic : 14, p : 15.7209 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                    \n",
      "\n",
      "===== # 7, Topic : 14, p : 17.7473 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                              \n",
      "\n",
      "===== # 8, Topic : 14, p : 15.9815 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                               \n",
      "\n",
      "===== # 9, Topic : 14, p : 17.8182 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                         mm           \n",
      "\n",
      "===== # 10, Topic : 14, p : 17.5189 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                 \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1809e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0939e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.029, l1 strength = 569729.68750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 5962.1815\n",
      "Test epoch : 1 Average loss: 1191.4539\n",
      "PP(train) = 3691.721, PP(valid) = 3850.420\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.3856e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.041, l1 strength = 1006166.81250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 9279.7498\n",
      "Test epoch : 2 Average loss: 1180.9759\n",
      "PP(train) = 3546.057, PP(valid) = 3693.820\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6052e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5003e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.056, l1 strength = 1762721.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 14652.9801\n",
      "Test epoch : 3 Average loss: 1179.1876\n",
      "PP(train) = 3377.658, PP(valid) = 3587.145\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5390e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.072, l1 strength = 3057251.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 23214.6508\n",
      "Test epoch : 4 Average loss: 1172.4840\n",
      "PP(train) = 3261.032, PP(valid) = 3471.989\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0790e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7428e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.090, l1 strength = 5242904.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 36636.7195\n",
      "Test epoch : 5 Average loss: 1170.7272\n",
      "PP(train) = 3199.507, PP(valid) = 3421.341\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0039e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.3250e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.110, l1 strength = 8879121.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 57297.3547\n",
      "Test epoch : 6 Average loss: 1170.5021\n",
      "PP(train) = 3155.331, PP(valid) = 3391.418\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.3693e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.132, l1 strength = 14831203.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 88462.0234\n",
      "Test epoch : 7 Average loss: 1168.6116\n",
      "PP(train) = 3147.729, PP(valid) = 3377.153\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6052e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.8572e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.155, l1 strength = 24398830.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 134406.5656\n",
      "Test epoch : 8 Average loss: 1169.1614\n",
      "PP(train) = 3114.715, PP(valid) = 3367.228\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0065e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6563e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.178, l1 strength = 39496500.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 200571.3031\n",
      "Test epoch : 9 Average loss: 1168.3104\n",
      "PP(train) = 3124.604, PP(valid) = 3370.846\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.1210e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.199, l1 strength = 62925176.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 293757.1312\n",
      "Test epoch : 10 Average loss: 1168.5010\n",
      "PP(train) = 3123.687, PP(valid) = 3376.982\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.3055e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.221, l1 strength = 98784704.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 422515.8187\n",
      "Test epoch : 11 Average loss: 1168.7874\n",
      "PP(train) = 3143.943, PP(valid) = 3397.637\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6520e-33, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.244, l1 strength = 152766640.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 596317.3875\n",
      "Test epoch : 12 Average loss: 1169.4292\n",
      "PP(train) = 3175.650, PP(valid) = 3429.975\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.7947e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.267, l1 strength = 232463008.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 824542.5750\n",
      "Test epoch : 13 Average loss: 1171.7021\n",
      "PP(train) = 3184.014, PP(valid) = 3455.130\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6744e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.289, l1 strength = 348290688.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1117128.1250\n",
      "Test epoch : 14 Average loss: 1172.4016\n",
      "PP(train) = 3228.599, PP(valid) = 3494.538\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0256e-36, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.312, l1 strength = 513951200.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1482708.4000\n",
      "Test epoch : 15 Average loss: 1173.9201\n",
      "PP(train) = 3281.472, PP(valid) = 3546.416\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9236e-44, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.334, l1 strength = 746026816.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1924364.8500\n",
      "Test epoch : 16 Average loss: 1176.4342\n",
      "PP(train) = 3304.353, PP(valid) = 3585.606\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.9567e-43, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.358, l1 strength = 1066925184.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 2445156.4000\n",
      "Test epoch : 17 Average loss: 1178.5924\n",
      "PP(train) = 3356.005, PP(valid) = 3643.260\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.383, l1 strength = 1500606976.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 3034558.2500\n",
      "Test epoch : 18 Average loss: 1181.0197\n",
      "PP(train) = 3426.418, PP(valid) = 3711.745\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.411, l1 strength = 2074655616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 3675308.7000\n",
      "Test epoch : 19 Average loss: 1183.8939\n",
      "PP(train) = 3492.959, PP(valid) = 3781.629\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.445, l1 strength = 2812308736.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 4329778.5500\n",
      "Test epoch : 20 Average loss: 1187.6315\n",
      "PP(train) = 3543.924, PP(valid) = 3841.020\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.469, l1 strength = 3722843136.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 4936755.6000\n",
      "Test epoch : 21 Average loss: 1191.0399\n",
      "PP(train) = 3646.917, PP(valid) = 3944.299\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.496, l1 strength = 4847098880.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 5484389.7000\n",
      "Test epoch : 22 Average loss: 1195.0349\n",
      "PP(train) = 3745.258, PP(valid) = 4044.186\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.522, l1 strength = 6193662976.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 5921829.3000\n",
      "Test epoch : 23 Average loss: 1199.8309\n",
      "PP(train) = 3843.611, PP(valid) = 4147.489\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.548, l1 strength = 7777174528.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 6217343.8000\n",
      "Test epoch : 24 Average loss: 1205.5751\n",
      "PP(train) = 3986.913, PP(valid) = 4299.753\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.572, l1 strength = 9590194176.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 6342585.4000\n",
      "Test epoch : 25 Average loss: 1212.2033\n",
      "PP(train) = 4159.689, PP(valid) = 4485.142\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.605, l1 strength = 11628182528.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 6286984.6000\n",
      "Test epoch : 26 Average loss: 1219.7102\n",
      "PP(train) = 4313.225, PP(valid) = 4650.401\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.623, l1 strength = 13782609920.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 6021768.6000\n",
      "Test epoch : 27 Average loss: 1227.9625\n",
      "PP(train) = 4527.015, PP(valid) = 4876.071\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.640, l1 strength = 16133785600.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 5634001.8000\n",
      "Test epoch : 28 Average loss: 1236.6383\n",
      "PP(train) = 4805.577, PP(valid) = 5169.146\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.671, l1 strength = 18660048896.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 5146683.7000\n",
      "Test epoch : 29 Average loss: 1245.9209\n",
      "PP(train) = 5073.465, PP(valid) = 5451.735\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.701, l1 strength = 21120004096.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 4570977.5000\n",
      "Test epoch : 30 Average loss: 1256.1041\n",
      "PP(train) = 5383.941, PP(valid) = 5784.367\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.719, l1 strength = 23419119616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 3955105.4000\n",
      "Test epoch : 31 Average loss: 1266.3172\n",
      "PP(train) = 5789.340, PP(valid) = 6205.140\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.741, l1 strength = 25641385984.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 3376672.9500\n",
      "Test epoch : 32 Average loss: 1276.0822\n",
      "PP(train) = 6267.415, PP(valid) = 6696.744\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.776, l1 strength = 27657787392.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 2840127.9000\n",
      "Test epoch : 33 Average loss: 1285.5446\n",
      "PP(train) = 6802.644, PP(valid) = 7233.494\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.805, l1 strength = 29118582784.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 2330700.1500\n",
      "Test epoch : 34 Average loss: 1294.4546\n",
      "PP(train) = 7346.683, PP(valid) = 7778.181\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.843, l1 strength = 30036213760.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1895993.5000\n",
      "Test epoch : 35 Average loss: 1302.0822\n",
      "PP(train) = 7901.883, PP(valid) = 8333.398\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.862, l1 strength = 30189176832.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1518521.0000\n",
      "Test epoch : 36 Average loss: 1308.2620\n",
      "PP(train) = 8456.940, PP(valid) = 8882.014\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.886, l1 strength = 29937135616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1207429.5250\n",
      "Test epoch : 37 Average loss: 1313.4686\n",
      "PP(train) = 8985.015, PP(valid) = 9405.302\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.906, l1 strength = 29197051904.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 966058.8875\n",
      "Test epoch : 38 Average loss: 1317.7053\n",
      "PP(train) = 9454.980, PP(valid) = 9871.242\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.915, l1 strength = 28084510720.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 772603.8125\n",
      "Test epoch : 39 Average loss: 1320.9455\n",
      "PP(train) = 9841.614, PP(valid) = 10252.601\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.945, l1 strength = 26849177600.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 619414.9875\n",
      "Test epoch : 40 Average loss: 1323.4844\n",
      "PP(train) = 10155.512, PP(valid) = 10561.895\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.961, l1 strength = 25143134208.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 500576.4188\n",
      "Test epoch : 41 Average loss: 1325.3848\n",
      "PP(train) = 10410.868, PP(valid) = 10814.684\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.965, l1 strength = 23288903680.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 403472.6688\n",
      "Test epoch : 42 Average loss: 1326.7063\n",
      "PP(train) = 10609.403, PP(valid) = 11013.312\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.973, l1 strength = 21511426048.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 322612.7062\n",
      "Test epoch : 43 Average loss: 1327.6624\n",
      "PP(train) = 10764.993, PP(valid) = 11167.432\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.983, l1 strength = 19756115968.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 256673.7594\n",
      "Test epoch : 44 Average loss: 1328.2255\n",
      "PP(train) = 10869.898, PP(valid) = 11270.399\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.987, l1 strength = 18011334656.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 207613.5719\n",
      "Test epoch : 45 Average loss: 1328.6509\n",
      "PP(train) = 10958.768, PP(valid) = 11355.198\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6726e-42, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.992, l1 strength = 16380030976.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 166663.3406\n",
      "Test epoch : 46 Average loss: 1328.7734\n",
      "PP(train) = 11018.796, PP(valid) = 11414.012\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 14842259456.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 137352.0719\n",
      "Test epoch : 47 Average loss: 1328.7972\n",
      "PP(train) = 11054.013, PP(valid) = 11448.909\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7998e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 13427365888.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 111994.4406\n",
      "Test epoch : 48 Average loss: 1328.6748\n",
      "PP(train) = 11065.921, PP(valid) = 11462.183\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.7698e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 12137481216.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 90792.7125\n",
      "Test epoch : 49 Average loss: 1328.5350\n",
      "PP(train) = 11063.916, PP(valid) = 11461.854\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 10964709376.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 74566.7906\n",
      "Test epoch : 50 Average loss: 1328.2069\n",
      "PP(train) = 11060.129, PP(valid) = 11460.128\n",
      "Writing to ./topicwords/1-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 22.0825 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                   t                      c                                                            h                               /                                                                                      \n",
      "\n",
      "===== # 2, Topic : 14, p : 23.1633 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                               B                        A                                                                                 A)(B                                                    \n",
      "\n",
      "===== # 3, Topic : 14, p : 22.8644 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                            Pretensioned Spun High Strength Concrete Piles                               \n",
      "\n",
      "===== # 4, Topic : 14, p : 23.9521 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                     ]   CT L                                                                         [                             \n",
      "\n",
      "===== # 5, Topic : 14, p : 18.0947 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                            GlobalPositioning System            \n",
      "\n",
      "===== # 6, Topic : 14, p : 19.0435 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                               Real Time Kinematic GPS  Global Navigation Satellite System                                             \n",
      "\n",
      "===== # 7, Topic : 14, p : 20.5914 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                             \n",
      "\n",
      "===== # 8, Topic : 14, p : 21.4089 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                 \n",
      "\n",
      "===== # 9, Topic : 14, p : 20.2750 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                  \n",
      "\n",
      "===== # 10, Topic : 14, p : 22.8340 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                                      \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1044e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 9898746880.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 60619.3805\n",
      "Test epoch : 1 Average loss: 1347.7310\n",
      "PP(train) = 11725.834, PP(valid) = 11786.882\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3961e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.4984e-39, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 8933801984.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 49814.5477\n",
      "Test epoch : 2 Average loss: 1347.0616\n",
      "PP(train) = 11705.276, PP(valid) = 11768.473\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4574e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 8060758016.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 41039.3406\n",
      "Test epoch : 3 Average loss: 1346.4330\n",
      "PP(train) = 11665.059, PP(valid) = 11732.165\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2612e-44, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.1550e-37, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 7271508992.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 34051.9039\n",
      "Test epoch : 4 Average loss: 1345.8435\n",
      "PP(train) = 11612.923, PP(valid) = 11685.534\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8653e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 6558798336.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 28020.8840\n",
      "Test epoch : 5 Average loss: 1345.1701\n",
      "PP(train) = 11560.006, PP(valid) = 11638.135\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0145e-38, 0.0000e+00,\n",
      "         0.0000e+00, 1.0761e-41, 1.4300e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0194e-33, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 5915181568.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 23288.0949\n",
      "Test epoch : 6 Average loss: 1344.5181\n",
      "PP(train) = 11499.146, PP(valid) = 11583.368\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0538e-29, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.5405e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 5334408192.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 19651.6848\n",
      "Test epoch : 7 Average loss: 1343.7681\n",
      "PP(train) = 11442.382, PP(valid) = 11532.599\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.1520e-42, 0.0000e+00, 6.6636e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.4161e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 4810295808.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 16695.9195\n",
      "Test epoch : 8 Average loss: 1342.9147\n",
      "PP(train) = 11385.322, PP(valid) = 11480.671\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6052e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0391e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 4337363968.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 14080.2271\n",
      "Test epoch : 9 Average loss: 1342.0971\n",
      "PP(train) = 11325.393, PP(valid) = 11425.927\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3022e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.8709e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1748e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 3910855680.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 11907.7191\n",
      "Test epoch : 10 Average loss: 1341.3303\n",
      "PP(train) = 11259.710, PP(valid) = 11365.188\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8755e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 3526202368.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 10248.5584\n",
      "Test epoch : 11 Average loss: 1340.5592\n",
      "PP(train) = 11192.626, PP(valid) = 11303.413\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2540e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8841e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 3179322112.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 8762.0396\n",
      "Test epoch : 12 Average loss: 1339.7377\n",
      "PP(train) = 11128.329, PP(valid) = 11244.397\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.1494e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 2866472960.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 7493.7900\n",
      "Test epoch : 13 Average loss: 1338.9060\n",
      "PP(train) = 11064.391, PP(valid) = 11185.370\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7431e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.4492e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.9929e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 2584366848.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 6487.4632\n",
      "Test epoch : 14 Average loss: 1338.1080\n",
      "PP(train) = 10998.879, PP(valid) = 11124.753\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6309e-38, 0.0000e+00, 5.1624e-22, 0.0000e+00,\n",
      "         0.0000e+00, 5.8159e-40, 5.7857e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.8900e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2329992960.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 5656.4725\n",
      "Test epoch : 15 Average loss: 1337.3157\n",
      "PP(train) = 10932.640, PP(valid) = 11063.441\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1158e-31, 0.0000e+00, 2.2500e-30, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0639e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5204e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2100634112.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 4990.1344\n",
      "Test epoch : 16 Average loss: 1336.5554\n",
      "PP(train) = 10864.831, PP(valid) = 11000.380\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2612e-44, 0.0000e+00, 1.2415e-38, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.7664e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1893827456.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 4431.8427\n",
      "Test epoch : 17 Average loss: 1335.7543\n",
      "PP(train) = 10799.224, PP(valid) = 10939.419\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8882e-27, 0.0000e+00, 2.3871e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.4178e-21, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.2954e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1707371648.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 3956.3056\n",
      "Test epoch : 18 Average loss: 1334.9572\n",
      "PP(train) = 10733.275, PP(valid) = 10878.205\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2675e-42, 0.0000e+00,\n",
      "         0.0000e+00, 1.7607e-41, 3.9965e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0459e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1539265024.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 3547.0721\n",
      "Test epoch : 19 Average loss: 1334.1174\n",
      "PP(train) = 10668.778, PP(valid) = 10818.294\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0898e-33, 0.0000e+00, 3.6873e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.0845e-32, 5.2576e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.5557e-12, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1387695232.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 3208.0313\n",
      "Test epoch : 20 Average loss: 1333.2349\n",
      "PP(train) = 10605.974, PP(valid) = 10759.820\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.8212e-30, 0.0000e+00, 1.1820e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.5320e-35, 1.1166e-23, 4.2894e-42, 0.0000e+00, 0.0000e+00,\n",
      "         5.6717e-01, 5.0447e-44, 4.3283e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0620, 0.0609, 0.0893, 0.0571, 0.0497, 0.0671, 0.0744, 0.0864, 0.0692,\n",
      "         0.0746, 0.0550, 0.0553, 0.0564, 0.0687, 0.0738]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1251046912.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 2927.5840\n",
      "Test epoch : 21 Average loss: 1332.3958\n",
      "PP(train) = 10542.241, PP(valid) = 10700.479\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1457e-22, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.7099e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0237e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1127848576.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 2687.3851\n",
      "Test epoch : 22 Average loss: 1331.5555\n",
      "PP(train) = 10478.462, PP(valid) = 10641.099\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0197e-28, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.1968e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1016782272.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 2488.9457\n",
      "Test epoch : 23 Average loss: 1330.7169\n",
      "PP(train) = 10415.168, PP(valid) = 10581.575\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.2706e-42, 0.0000e+00, 4.0221e-40, 0.0000e+00,\n",
      "         0.0000e+00, 1.5993e-36, 8.9683e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1730e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 916653376.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 2318.9877\n",
      "Test epoch : 24 Average loss: 1329.9246\n",
      "PP(train) = 10350.510, PP(valid) = 10521.100\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2500e-42, 0.0000e+00, 8.2275e-33, 0.0000e+00,\n",
      "         0.0000e+00, 4.0918e-43, 4.0186e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.3710e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 826382592.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 2172.8865\n",
      "Test epoch : 25 Average loss: 1329.1346\n",
      "PP(train) = 10286.713, PP(valid) = 10461.298\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2992e-42, 0.0000e+00, 1.0821e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.1062e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8205e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 744995584.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 2049.9332\n",
      "Test epoch : 26 Average loss: 1328.2989\n",
      "PP(train) = 10224.772, PP(valid) = 10403.117\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2111e-38, 0.0000e+00, 6.9601e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.2144e-39, 3.6884e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.7413e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 671622208.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1945.9314\n",
      "Test epoch : 27 Average loss: 1327.5064\n",
      "PP(train) = 10161.800, PP(valid) = 10344.036\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.5451e-40, 0.0000e+00, 8.5391e-30, 0.0000e+00,\n",
      "         0.0000e+00, 3.5639e-30, 2.5786e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3116e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 605473664.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1855.9654\n",
      "Test epoch : 28 Average loss: 1326.6923\n",
      "PP(train) = 10099.851, PP(valid) = 10285.938\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8768e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.5666e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.2180e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 545838656.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1779.5522\n",
      "Test epoch : 29 Average loss: 1325.8623\n",
      "PP(train) = 10039.312, PP(valid) = 10229.229\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2972e-34, 0.0000e+00, 2.2400e-19, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.2019e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9635e-15, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 492077280.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1713.5242\n",
      "Test epoch : 30 Average loss: 1325.0373\n",
      "PP(train) = 9978.633, PP(valid) = 10172.106\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.8669e-36, 0.0000e+00, 7.0704e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.0187e-42, 2.0115e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.9697e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 443609824.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1656.0240\n",
      "Test epoch : 31 Average loss: 1324.2143\n",
      "PP(train) = 9918.633, PP(valid) = 10115.640\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8026e-43, 0.0000e+00, 3.5916e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.3461e-41, 3.3004e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.8055e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 399916192.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1606.9162\n",
      "Test epoch : 32 Average loss: 1323.4065\n",
      "PP(train) = 9858.777, PP(valid) = 10059.381\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.4806e-41, 0.0000e+00, 7.8869e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 2.0985e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6204e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 360524288.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1564.2543\n",
      "Test epoch : 33 Average loss: 1322.6303\n",
      "PP(train) = 9798.777, PP(valid) = 10002.958\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0638e-44, 0.0000e+00, 9.6755e-25, 0.0000e+00,\n",
      "         0.0000e+00, 5.0033e-38, 2.6566e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5526e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 325012480.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1526.9400\n",
      "Test epoch : 34 Average loss: 1321.8589\n",
      "PP(train) = 9739.097, PP(valid) = 9946.520\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.7610e-35, 0.0000e+00, 2.1535e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8007e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 292998624.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1494.4554\n",
      "Test epoch : 35 Average loss: 1321.0294\n",
      "PP(train) = 9682.051, PP(valid) = 9892.673\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.0593e-32, 0.0000e+00, 3.2133e-32, 0.0000e+00,\n",
      "         0.0000e+00, 7.4411e-40, 2.1934e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.8170e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 264138128.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1466.3688\n",
      "Test epoch : 36 Average loss: 1320.1977\n",
      "PP(train) = 9625.376, PP(valid) = 9839.043\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1827e-18, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.4790e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.3810e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 238120416.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1441.7290\n",
      "Test epoch : 37 Average loss: 1319.3892\n",
      "PP(train) = 9568.559, PP(valid) = 9785.424\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7399e-28, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.6950e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7457e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 214665456.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1420.0832\n",
      "Test epoch : 38 Average loss: 1318.5966\n",
      "PP(train) = 9511.305, PP(valid) = 9731.048\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.3809e-38, 0.0000e+00, 8.2911e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.4512e-25, 3.6758e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7451e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 193520816.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1401.2268\n",
      "Test epoch : 39 Average loss: 1317.8196\n",
      "PP(train) = 9454.250, PP(valid) = 9677.075\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6493e-34, 0.0000e+00, 6.1745e-23, 0.0000e+00,\n",
      "         0.0000e+00, 4.6798e-27, 4.2710e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9022e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 174458928.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1384.4280\n",
      "Test epoch : 40 Average loss: 1317.0402\n",
      "PP(train) = 9397.913, PP(valid) = 9623.642\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.1763e-40, 0.0000e+00, 1.4868e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0928e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2847e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 157274656.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1369.3602\n",
      "Test epoch : 41 Average loss: 1316.2247\n",
      "PP(train) = 9343.979, PP(valid) = 9572.857\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.8942e-41, 0.0000e+00, 3.4979e-32, 0.0000e+00,\n",
      "         0.0000e+00, 4.7363e-26, 2.0941e-24, 9.8091e-45, 0.0000e+00, 0.0000e+00,\n",
      "         5.8704e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 141783040.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1356.3272\n",
      "Test epoch : 42 Average loss: 1315.4516\n",
      "PP(train) = 9288.371, PP(valid) = 9520.099\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.0829e-44, 0.0000e+00, 1.5612e-31, 0.0000e+00,\n",
      "         0.0000e+00, 2.2342e-36, 8.9543e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2763e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 127817352.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1344.4615\n",
      "Test epoch : 43 Average loss: 1314.7049\n",
      "PP(train) = 9232.917, PP(valid) = 9467.531\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.4078e-45, 0.0000e+00, 4.0217e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.1528e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 115227288.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1334.1940\n",
      "Test epoch : 44 Average loss: 1313.9187\n",
      "PP(train) = 9179.239, PP(valid) = 9416.699\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4677e-42, 0.0000e+00, 1.3492e-34, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 3.0561e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.3751e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 103877352.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1324.8000\n",
      "Test epoch : 45 Average loss: 1313.1246\n",
      "PP(train) = 9126.221, PP(valid) = 9366.507\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4975e-39, 0.0000e+00, 3.4222e-31, 0.0000e+00,\n",
      "         0.0000e+00, 6.4509e-32, 8.7468e-38, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         6.0530e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 93645392.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1316.4671\n",
      "Test epoch : 46 Average loss: 1312.3752\n",
      "PP(train) = 9072.704, PP(valid) = 9315.756\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2262e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1033e-38, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 84421280.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1308.8594\n",
      "Test epoch : 47 Average loss: 1311.6106\n",
      "PP(train) = 9019.459, PP(valid) = 9265.254\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7637e-37, 0.0000e+00, 1.3273e-25, 0.0000e+00,\n",
      "         0.0000e+00, 2.6377e-35, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.4254e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 76105744.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1302.0609\n",
      "Test epoch : 48 Average loss: 1310.8418\n",
      "PP(train) = 8967.440, PP(valid) = 9216.168\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8516e-39, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 7.5390e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8605e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 68609112.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1296.1288\n",
      "Test epoch : 49 Average loss: 1310.0829\n",
      "PP(train) = 8915.841, PP(valid) = 9167.141\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.5036e-31, 0.0000e+00, 1.8754e-19, 0.0000e+00,\n",
      "         0.0000e+00, 9.2792e-38, 3.1118e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.4930e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 61850920.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1290.5800\n",
      "Test epoch : 50 Average loss: 1309.3146\n",
      "PP(train) = 8864.835, PP(valid) = 9118.635\n",
      "Writing to ./topicwords/2-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 11.0400 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                   portland cement                                                                                                     high early strength heat                                                                   normal ultra moderate low sulfate resisting           light burned magnesia                                       \n",
      "\n",
      "===== # 2, Topic : 14, p : 15.0329 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                              \n",
      "\n",
      "===== # 3, Topic : 14, p : 14.5552 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                      b    a                         a)(b                                            c                  d                                               a)(d a)(c                                                                                            \n",
      "\n",
      "===== # 4, Topic : 14, p : 13.8147 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                               \n",
      "\n",
      "===== # 5, Topic : 14, p : 12.9823 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                     \n",
      "\n",
      "===== # 6, Topic : 14, p : 15.2124 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                 )                                                                            \n",
      "\n",
      "===== # 7, Topic : 14, p : 12.8808 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                \n",
      "\n",
      "===== # 8, Topic : 14, p : 13.0293 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                              i ii                                                                             iii                    \n",
      "\n",
      "===== # 9, Topic : 14, p : 12.6260 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                             \n",
      "\n",
      "===== # 10, Topic : 14, p : 13.1264 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4018e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4666e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9213e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 55758432.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1113.3629\n",
      "Test epoch : 1 Average loss: 1040.5516\n",
      "PP(train) = 9326.139, PP(valid) = 9396.067\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2025e-39, 0.0000e+00, 1.4312e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4416e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9970e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 50266068.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1108.9252\n",
      "Test epoch : 2 Average loss: 1039.8685\n",
      "PP(train) = 9283.979, PP(valid) = 9358.105\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1247e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.1642e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6737e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 45314720.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1104.7792\n",
      "Test epoch : 3 Average loss: 1039.2618\n",
      "PP(train) = 9238.836, PP(valid) = 9319.079\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.0110e-39, 0.0000e+00, 6.5244e-33, 0.0000e+00,\n",
      "         0.0000e+00, 2.2042e-42, 1.5865e-15, 5.3508e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.5143e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 40851092.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1101.1774\n",
      "Test epoch : 4 Average loss: 1038.7344\n",
      "PP(train) = 9189.198, PP(valid) = 9276.978\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5137e-36, 1.1771e-43, 6.8363e-30, 2.8026e-45,\n",
      "         0.0000e+00, 3.6921e-33, 6.6323e-33, 1.0229e-43, 0.0000e+00, 5.4934e-40,\n",
      "         3.5591e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 36827144.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1098.0652\n",
      "Test epoch : 5 Average loss: 1038.2565\n",
      "PP(train) = 9136.418, PP(valid) = 9232.389\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1461e-24, 0.0000e+00, 1.3822e-32, 0.0000e+00,\n",
      "         0.0000e+00, 9.1949e-33, 1.8835e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.3346e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 33199566.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1094.8295\n",
      "Test epoch : 6 Average loss: 1037.7920\n",
      "PP(train) = 9084.106, PP(valid) = 9188.549\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2163e-36, 0.0000e+00, 8.1490e-29, 0.0000e+00,\n",
      "         0.0000e+00, 4.2642e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0168e-11, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 29929316.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1092.0337\n",
      "Test epoch : 7 Average loss: 1037.2927\n",
      "PP(train) = 9033.206, PP(valid) = 9145.947\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.7514e-26, 0.0000e+00, 1.4420e-37, 0.0000e+00,\n",
      "         0.0000e+00, 1.3000e-30, 4.5718e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.9338e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 26981194.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1089.5112\n",
      "Test epoch : 8 Average loss: 1036.7802\n",
      "PP(train) = 8983.216, PP(valid) = 9104.245\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.2884e-37, 0.0000e+00, 1.6890e-20, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.4927e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2453e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 24323470.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1087.0796\n",
      "Test epoch : 9 Average loss: 1036.2472\n",
      "PP(train) = 8935.121, PP(valid) = 9064.091\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.2397e-39, 0.0000e+00, 2.7350e-30, 0.0000e+00,\n",
      "         0.0000e+00, 4.5003e-35, 4.5250e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.6173e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 21927540.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1084.8997\n",
      "Test epoch : 10 Average loss: 1035.7117\n",
      "PP(train) = 8887.716, PP(valid) = 9024.397\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.3149e-32, 0.0000e+00, 1.0267e-27, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0308e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.4255e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 19767616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1082.7629\n",
      "Test epoch : 11 Average loss: 1035.1723\n",
      "PP(train) = 8841.220, PP(valid) = 8985.546\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.5464e-41, 0.0000e+00, 1.0001e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.1034e-39, 7.6711e-28, 1.5471e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.4607e-07, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 17820450.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1080.8744\n",
      "Test epoch : 12 Average loss: 1034.6363\n",
      "PP(train) = 8794.724, PP(valid) = 8946.547\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6648e-38, 0.0000e+00, 1.8911e-24, 0.0000e+00,\n",
      "         0.0000e+00, 3.2835e-38, 5.6177e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0044e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 16065086.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1079.1114\n",
      "Test epoch : 13 Average loss: 1034.1310\n",
      "PP(train) = 8747.655, PP(valid) = 8907.062\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, 1.6689e-42, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.8319e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.1722e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 14482630.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1077.5869\n",
      "Test epoch : 14 Average loss: 1033.6425\n",
      "PP(train) = 8699.627, PP(valid) = 8866.441\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.2164e-29, 6.7262e-44, 3.8335e-20, 0.0000e+00,\n",
      "         0.0000e+00, 2.1845e-28, 1.4404e-29, 9.8091e-45, 0.0000e+00, 0.0000e+00,\n",
      "         1.6418e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 13056050.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1076.0246\n",
      "Test epoch : 15 Average loss: 1033.1623\n",
      "PP(train) = 8652.035, PP(valid) = 8826.283\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.5526e-34, 0.0000e+00, 1.8146e-19, 0.0000e+00,\n",
      "         0.0000e+00, 7.0065e-45, 4.5369e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.6406e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 11769992.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1074.6062\n",
      "Test epoch : 16 Average loss: 1032.6893\n",
      "PP(train) = 8604.868, PP(valid) = 8786.391\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.4008e-41, 0.0000e+00, 7.6506e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.6854e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8639e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 10610615.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1073.1935\n",
      "Test epoch : 17 Average loss: 1032.1839\n",
      "PP(train) = 8559.713, PP(valid) = 8748.197\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2339e-27, 2.9660e-34, 1.3445e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.2278e-39, 1.6342e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.0425e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 9565440.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1071.9678\n",
      "Test epoch : 18 Average loss: 1031.6701\n",
      "PP(train) = 8514.876, PP(valid) = 8710.270\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.2320e-26, 3.5796e-41, 1.5135e-06, 1.9945e-40,\n",
      "         2.8026e-45, 1.3851e-28, 3.4205e-16, 1.5465e-40, 5.0574e-35, 1.9648e-36,\n",
      "         5.1981e-01, 6.2965e-39, 4.8019e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0636, 0.0594, 0.0907, 0.0567, 0.0506, 0.0673, 0.0769, 0.0871, 0.0684,\n",
      "         0.0726, 0.0557, 0.0555, 0.0561, 0.0690, 0.0704]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 8623217.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1070.9102\n",
      "Test epoch : 19 Average loss: 1031.1667\n",
      "PP(train) = 8470.654, PP(valid) = 8672.754\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7757e-26, 1.5770e-39, 6.8066e-22, 0.0000e+00,\n",
      "         0.0000e+00, 8.1213e-33, 1.8935e-26, 1.5728e-37, 0.0000e+00, 0.0000e+00,\n",
      "         7.3987e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7773806.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1069.6264\n",
      "Test epoch : 20 Average loss: 1030.6626\n",
      "PP(train) = 8426.581, PP(valid) = 8635.252\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.8282e-44, 0.0000e+00, 3.4857e-30, 0.0000e+00,\n",
      "         0.0000e+00, 2.9916e-41, 2.1242e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9971e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7008064.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1068.4507\n",
      "Test epoch : 21 Average loss: 1030.1687\n",
      "PP(train) = 8382.434, PP(valid) = 8597.617\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.1012e-40, 0.0000e+00, 2.6806e-20, 0.0000e+00,\n",
      "         0.0000e+00, 8.9311e-35, 2.7017e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2027e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6317750.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1067.5108\n",
      "Test epoch : 22 Average loss: 1029.6635\n",
      "PP(train) = 8339.241, PP(valid) = 8560.872\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2872e-40, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0249e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0991e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5695434.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1066.4610\n",
      "Test epoch : 23 Average loss: 1029.1624\n",
      "PP(train) = 8296.058, PP(valid) = 8524.050\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6062e-36, 2.9574e-32, 1.9670e-16, 0.0000e+00,\n",
      "         0.0000e+00, 4.4775e-37, 2.9501e-20, 3.6174e-35, 0.0000e+00, 5.6052e-44,\n",
      "         1.9344e-11, 8.7124e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5134417.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1065.5935\n",
      "Test epoch : 24 Average loss: 1028.6832\n",
      "PP(train) = 8252.351, PP(valid) = 8486.632\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.4203e-33, 0.0000e+00, 8.4046e-20, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.8310e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.4089e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4628663.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1064.5226\n",
      "Test epoch : 25 Average loss: 1028.2119\n",
      "PP(train) = 8209.288, PP(valid) = 8449.798\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.8468e-27, 0.0000e+00, 7.1011e-19, 0.0000e+00,\n",
      "         0.0000e+00, 1.1706e-27, 1.5278e-21, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.6707e-26, 1.3705e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4172726.75000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1063.7013\n",
      "Test epoch : 26 Average loss: 1027.7301\n",
      "PP(train) = 8166.411, PP(valid) = 8412.995\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.5532e-41, 0.0000e+00, 1.0192e-26, 0.0000e+00,\n",
      "         0.0000e+00, 2.9427e-44, 3.5356e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0949e-33, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3761701.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1062.7419\n",
      "Test epoch : 27 Average loss: 1027.2338\n",
      "PP(train) = 8124.650, PP(valid) = 8377.191\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8026e-45, 0.0000e+00, 1.3704e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.0774e-34, 9.4200e-29, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2875e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3391163.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1061.9443\n",
      "Test epoch : 28 Average loss: 1026.7458\n",
      "PP(train) = 8083.040, PP(valid) = 8341.461\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.3438e-31, 0.0000e+00, 9.6267e-20, 1.0496e-42,\n",
      "         0.0000e+00, 8.4585e-35, 3.3241e-35, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         7.9493e-11, 1.2444e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3057124.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1061.1535\n",
      "Test epoch : 29 Average loss: 1026.2439\n",
      "PP(train) = 8042.349, PP(valid) = 8306.587\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4902e-30, 3.6434e-44, 4.2632e-30, 0.0000e+00,\n",
      "         0.0000e+00, 1.9772e-37, 1.0145e-35, 0.0000e+00, 0.0000e+00, 7.7975e-39,\n",
      "         7.0251e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2755988.75000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1060.5193\n",
      "Test epoch : 30 Average loss: 1025.7667\n",
      "PP(train) = 8001.046, PP(valid) = 8271.000\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0940e-40, 0.0000e+00, 6.8076e-26, 0.0000e+00,\n",
      "         0.0000e+00, 4.2039e-45, 2.8537e-24, 2.8026e-45, 0.0000e+00, 0.0000e+00,\n",
      "         8.8459e-12, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2484516.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1059.4622\n",
      "Test epoch : 31 Average loss: 1025.2800\n",
      "PP(train) = 7960.426, PP(valid) = 8235.910\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.9466e-43, 0.0000e+00, 5.6955e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.3748e-33, 3.6402e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9577e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2239784.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1058.8300\n",
      "Test epoch : 32 Average loss: 1024.7796\n",
      "PP(train) = 7920.436, PP(valid) = 8201.432\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 2.5424e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.6859e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2804e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2019159.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1058.0931\n",
      "Test epoch : 33 Average loss: 1024.2884\n",
      "PP(train) = 7880.706, PP(valid) = 8167.183\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.5464e-39, 0.0000e+00, 1.3463e-40, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.8757e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.9838e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1820266.37500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1057.2083\n",
      "Test epoch : 34 Average loss: 1023.7913\n",
      "PP(train) = 7841.901, PP(valid) = 8133.697\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7728e-36, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.2250e-27, 1.5669e-41, 0.0000e+00, 0.0000e+00,\n",
      "         5.4638e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1640965.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1056.5887\n",
      "Test epoch : 35 Average loss: 1023.3058\n",
      "PP(train) = 7802.602, PP(valid) = 8099.746\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7344e-36, 0.0000e+00, 7.5710e-32, 0.0000e+00,\n",
      "         0.0000e+00, 2.4063e-29, 7.7068e-28, 0.0000e+00, 0.0000e+00, 2.3738e-42,\n",
      "         5.3983e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1479325.37500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1055.9440\n",
      "Test epoch : 36 Average loss: 1022.8402\n",
      "PP(train) = 7763.130, PP(valid) = 8065.453\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1334e-13, 1.1996e-29, 1.4571e-10, 1.5554e-42,\n",
      "         0.0000e+00, 3.0251e-38, 1.3858e-33, 1.5982e-38, 0.0000e+00, 1.4013e-45,\n",
      "         3.9753e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1333607.62500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1055.2942\n",
      "Test epoch : 37 Average loss: 1022.3667\n",
      "PP(train) = 7723.777, PP(valid) = 8031.220\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.3168e-35, 1.9618e-44, 1.4542e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.4153e-33, 2.5514e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6164e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1202243.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1054.5850\n",
      "Test epoch : 38 Average loss: 1021.8990\n",
      "PP(train) = 7684.660, PP(valid) = 7997.232\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.4592e-39, 0.0000e+00, 4.9883e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.2877e-40, 7.1564e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1893e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1083819.12500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1053.9218\n",
      "Test epoch : 39 Average loss: 1021.4286\n",
      "PP(train) = 7646.441, PP(valid) = 7963.957\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.0166e-43, 0.0000e+00, 2.1074e-32, 0.0000e+00, 2.4722e-14, 2.8026e-45,\n",
      "         0.0000e+00, 7.0127e-34, 2.8698e-20, 9.2598e-42, 0.0000e+00, 0.0000e+00,\n",
      "         1.1627e-09, 9.9226e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 977059.87500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1053.3307\n",
      "Test epoch : 40 Average loss: 1020.9626\n",
      "PP(train) = 7608.325, PP(valid) = 7930.881\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0601e-31, 0.0000e+00, 7.7071e-19, 0.0000e+00,\n",
      "         0.0000e+00, 1.3089e-29, 1.2654e-19, 1.6956e-43, 0.0000e+00, 5.6052e-45,\n",
      "         8.5673e-16, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 880816.75000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1052.7229\n",
      "Test epoch : 41 Average loss: 1020.4954\n",
      "PP(train) = 7570.154, PP(valid) = 7897.574\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2612e-44, 0.0000e+00, 1.7846e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.4946e-39, 2.4531e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0136e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 794053.81250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1051.9710\n",
      "Test epoch : 42 Average loss: 1020.0217\n",
      "PP(train) = 7532.533, PP(valid) = 7864.774\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.8437e-34, 0.0000e+00, 1.1210e-44, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.8079e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1314e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 715837.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1051.3343\n",
      "Test epoch : 43 Average loss: 1019.5415\n",
      "PP(train) = 7496.122, PP(valid) = 7832.984\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.9727e-26, 4.4490e-35, 1.6500e-20, 1.1980e-41,\n",
      "         0.0000e+00, 9.4950e-27, 7.4965e-22, 6.7725e-42, 0.0000e+00, 3.8886e-42,\n",
      "         1.1801e-09, 8.0575e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 645325.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1050.7035\n",
      "Test epoch : 44 Average loss: 1019.0622\n",
      "PP(train) = 7459.396, PP(valid) = 7800.980\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.2599e-43, 1.6686e-26, 3.6968e-28, 4.3260e-14, 2.8026e-45,\n",
      "         1.4013e-45, 3.5161e-26, 1.3338e-18, 1.4401e-35, 0.0000e+00, 1.1403e-38,\n",
      "         8.5371e-04, 2.4271e-32, 9.9915e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 581758.87500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1050.2476\n",
      "Test epoch : 45 Average loss: 1018.6085\n",
      "PP(train) = 7422.262, PP(valid) = 7768.507\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6199e-30, 4.0194e-40, 3.8053e-16, 0.0000e+00,\n",
      "         0.0000e+00, 9.4410e-34, 9.3939e-27, 1.8693e-42, 0.0000e+00, 0.0000e+00,\n",
      "         1.7584e-12, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 524454.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1049.4820\n",
      "Test epoch : 46 Average loss: 1018.1637\n",
      "PP(train) = 7385.066, PP(valid) = 7735.940\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7606e-40, 0.0000e+00, 7.0065e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.1923e-37, 0.0000e+00, 0.0000e+00, 3.3631e-44,\n",
      "         8.7984e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 472793.81250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1049.1428\n",
      "Test epoch : 47 Average loss: 1017.7146\n",
      "PP(train) = 7348.358, PP(valid) = 7703.790\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3355e-38, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.2459e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.5222e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 426222.28125\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1048.2135\n",
      "Test epoch : 48 Average loss: 1017.2350\n",
      "PP(train) = 7312.997, PP(valid) = 7672.804\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0377e-30, 0.0000e+00, 2.3930e-29, 0.0000e+00,\n",
      "         0.0000e+00, 4.8899e-38, 2.1571e-38, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         8.2508e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 384238.18750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1047.6744\n",
      "Test epoch : 49 Average loss: 1016.7495\n",
      "PP(train) = 7278.133, PP(valid) = 7642.278\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8026e-44, 0.0000e+00, 6.5096e-32, 0.0000e+00,\n",
      "         0.0000e+00, 5.6052e-44, 1.2170e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.9062e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 346389.65625\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1047.1801\n",
      "Test epoch : 50 Average loss: 1016.2896\n",
      "PP(train) = 7242.405, PP(valid) = 7610.923\n",
      "Writing to ./topicwords/3-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 12.7804 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                   \n",
      "\n",
      "===== # 2, Topic : 14, p : 10.2380 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                 \n",
      "\n",
      "===== # 3, Topic : 14, p : 11.7243 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                               \n",
      "\n",
      "===== # 4, Topic : 14, p : 11.4109 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                 \n",
      "\n",
      "===== # 5, Topic : 14, p : 13.6796 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                         \n",
      "\n",
      "===== # 6, Topic : 14, p : 13.0241 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                            sec    - cm                                               Sand Compaction Pile                        sec                                           \n",
      "\n",
      "===== # 7, Topic : 14, p : 12.8237 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                         Autoclaved light weight concrete                           \n",
      "\n",
      "===== # 8, Topic : 14, p : 12.4037 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                        \n",
      "\n",
      "===== # 9, Topic : 14, p : 13.0495 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                     \n",
      "\n",
      "===== # 10, Topic : 14, p : 12.1954 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                       \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.1272e-36, 9.8622e-37, 3.1893e-26, 0.0000e+00,\n",
      "         0.0000e+00, 4.0504e-27, 9.3212e-25, 4.3160e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.5671e-09, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 312269.31250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1163.8823\n",
      "Test epoch : 1 Average loss: 1223.5025\n",
      "PP(train) = 7310.320, PP(valid) = 7378.892\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0346e-22, 0.0000e+00, 1.2755e-25, 0.0000e+00,\n",
      "         0.0000e+00, 3.7835e-44, 4.0219e-18, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6231e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 281509.90625\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1162.9532\n",
      "Test epoch : 2 Average loss: 1222.8771\n",
      "PP(train) = 7272.693, PP(valid) = 7345.331\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.4848e-29, 2.1510e-42, 1.4648e-16, 0.0000e+00,\n",
      "         0.0000e+00, 6.6447e-28, 4.2165e-26, 8.1840e-29, 0.0000e+00, 1.4013e-45,\n",
      "         2.8672e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 253780.39062\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1162.4774\n",
      "Test epoch : 3 Average loss: 1222.2336\n",
      "PP(train) = 7232.825, PP(valid) = 7310.898\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.0835e-31, 0.0000e+00, 1.7417e-37, 0.0000e+00,\n",
      "         0.0000e+00, 1.7236e-36, 3.0268e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6530e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 228782.31250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1161.8604\n",
      "Test epoch : 4 Average loss: 1221.5761\n",
      "PP(train) = 7191.501, PP(valid) = 7275.489\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8511e-31, 7.0065e-45, 2.5956e-25, 0.0000e+00,\n",
      "         0.0000e+00, 4.1926e-36, 1.0901e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.5904e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 206246.60938\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1160.9727\n",
      "Test epoch : 5 Average loss: 1220.9037\n",
      "PP(train) = 7150.142, PP(valid) = 7240.333\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1036e-38, 0.0000e+00, 1.8430e-33, 0.0000e+00,\n",
      "         0.0000e+00, 9.7482e-33, 7.0065e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.3511e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 185930.73438\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1160.1352\n",
      "Test epoch : 6 Average loss: 1220.2338\n",
      "PP(train) = 7109.040, PP(valid) = 7205.520\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0197e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.7061e-30, 1.8964e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6314e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 167616.03125\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1159.3454\n",
      "Test epoch : 7 Average loss: 1219.5733\n",
      "PP(train) = 7067.959, PP(valid) = 7170.696\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4204e-41, 0.0000e+00,\n",
      "         0.0000e+00, 9.1488e-38, 3.3586e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0611e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 151105.37500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1158.5958\n",
      "Test epoch : 8 Average loss: 1218.8954\n",
      "PP(train) = 7028.027, PP(valid) = 7136.864\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.3297e-39, 0.0000e+00, 2.4543e-34, 0.0000e+00,\n",
      "         0.0000e+00, 5.7373e-33, 9.3331e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6885e-15, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 136221.06250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1157.7117\n",
      "Test epoch : 9 Average loss: 1218.2131\n",
      "PP(train) = 6989.307, PP(valid) = 7104.133\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6444e-32, 1.4013e-45, 1.3349e-33, 0.0000e+00,\n",
      "         0.0000e+00, 6.8777e-21, 1.1537e-29, 1.6873e-32, 0.0000e+00, 0.0000e+00,\n",
      "         2.9829e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 122802.90625\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1157.0260\n",
      "Test epoch : 10 Average loss: 1217.5506\n",
      "PP(train) = 6949.772, PP(valid) = 7070.555\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2012e-25, 2.4383e-43, 2.2807e-15, 0.0000e+00,\n",
      "         0.0000e+00, 3.4065e-08, 1.8229e-28, 7.6776e-34, 1.4013e-45, 4.7662e-33,\n",
      "         8.8370e-08, 7.0394e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 110706.47656\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1156.4185\n",
      "Test epoch : 11 Average loss: 1216.9285\n",
      "PP(train) = 6908.669, PP(valid) = 7035.598\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1538e-35, 0.0000e+00, 5.8546e-37, 0.0000e+00,\n",
      "         0.0000e+00, 3.0406e-30, 6.5827e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3435e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 99801.57812\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1155.5543\n",
      "Test epoch : 12 Average loss: 1216.3098\n",
      "PP(train) = 6867.809, PP(valid) = 7000.754\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.9271e-37, 0.0000e+00, 1.1886e-30, 0.0000e+00,\n",
      "         0.0000e+00, 4.6407e-27, 6.2639e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6969e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 89970.84375\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1154.7768\n",
      "Test epoch : 13 Average loss: 1215.6547\n",
      "PP(train) = 6828.702, PP(valid) = 6967.358\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.9735e-40, 0.0000e+00, 2.7413e-27, 0.0000e+00,\n",
      "         0.0000e+00, 5.3975e-33, 1.0346e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0022e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 81108.46094\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1154.0697\n",
      "Test epoch : 14 Average loss: 1214.9868\n",
      "PP(train) = 6790.917, PP(valid) = 6935.173\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4477e-39, 0.0000e+00,\n",
      "         0.0000e+00, 6.1540e-37, 2.1019e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3515e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 73119.04688\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1153.1220\n",
      "Test epoch : 15 Average loss: 1214.2997\n",
      "PP(train) = 6754.412, PP(valid) = 6904.079\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.0858e-36, 7.0065e-45, 3.8734e-25, 0.0000e+00,\n",
      "         0.0000e+00, 3.9618e-33, 8.2016e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5920e-24, 1.4013e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 65916.61719\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1152.7651\n",
      "Test epoch : 16 Average loss: 1213.6414\n",
      "PP(train) = 6717.212, PP(valid) = 6872.238\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8481e-30, 0.0000e+00,\n",
      "         0.0000e+00, 7.6389e-35, 6.4092e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.9024e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 59423.64453\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1151.7668\n",
      "Test epoch : 17 Average loss: 1213.0187\n",
      "PP(train) = 6679.032, PP(valid) = 6839.436\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.7688e-24, 3.0604e-42, 5.2514e-35, 0.0000e+00,\n",
      "         0.0000e+00, 4.4312e-22, 1.1890e-41, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         5.1144e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 53570.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1151.2044\n",
      "Test epoch : 18 Average loss: 1212.4019\n",
      "PP(train) = 6641.050, PP(valid) = 6806.823\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7347e-40, 0.0000e+00, 6.5768e-37, 0.0000e+00,\n",
      "         0.0000e+00, 5.4739e-32, 1.1181e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2705e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 48293.42969\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1150.1665\n",
      "Test epoch : 19 Average loss: 1211.7669\n",
      "PP(train) = 6604.298, PP(valid) = 6775.237\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1718e-32, 0.0000e+00, 2.4012e-40, 0.0000e+00,\n",
      "         0.0000e+00, 3.4245e-40, 8.7016e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.6445e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 43536.39062\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1149.6737\n",
      "Test epoch : 20 Average loss: 1211.1183\n",
      "PP(train) = 6568.626, PP(valid) = 6744.486\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2768e-39, 0.0000e+00, 2.0726e-30, 0.0000e+00,\n",
      "         0.0000e+00, 5.8932e-23, 3.6691e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.5287e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 39247.93359\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1148.8669\n",
      "Test epoch : 21 Average loss: 1210.4655\n",
      "PP(train) = 6533.758, PP(valid) = 6714.471\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3960e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.2304e-31, 2.0347e-42, 0.0000e+00, 0.0000e+00,\n",
      "         1.3513e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 35381.90234\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1148.0925\n",
      "Test epoch : 22 Average loss: 1209.8157\n",
      "PP(train) = 6498.895, PP(valid) = 6684.385\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4119e-33, 0.0000e+00, 2.0405e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.8625e-34, 6.3240e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3658e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 31896.68555\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1147.3571\n",
      "Test epoch : 23 Average loss: 1209.2005\n",
      "PP(train) = 6462.912, PP(valid) = 6653.272\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1968e-33, 8.4078e-45, 1.2249e-26, 0.0000e+00,\n",
      "         0.0000e+00, 2.0897e-36, 1.9267e-35, 6.4988e-40, 0.0000e+00, 0.0000e+00,\n",
      "         3.4336e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 28754.77148\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1146.7751\n",
      "Test epoch : 24 Average loss: 1208.5990\n",
      "PP(train) = 6426.906, PP(valid) = 6622.134\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, 7.7751e-30, 0.0000e+00,\n",
      "         0.0000e+00, 1.3198e-35, 1.2120e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.5636e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 25922.34570\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1146.0256\n",
      "Test epoch : 25 Average loss: 1207.9911\n",
      "PP(train) = 6391.778, PP(valid) = 6591.688\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0393e-37, 4.5122e-43, 4.0446e-32, 0.0000e+00,\n",
      "         0.0000e+00, 8.3420e-33, 7.3341e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2963e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 23368.92188\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1145.3200\n",
      "Test epoch : 26 Average loss: 1207.3663\n",
      "PP(train) = 6357.728, PP(valid) = 6562.196\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7685e-40, 0.0000e+00, 2.5453e-19, 0.0000e+00,\n",
      "         0.0000e+00, 8.3895e-26, 3.0817e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.2979e-03, 2.8673e-33, 9.9270e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0446, 0.1042, 0.0503, 0.0600, 0.0669, 0.1071, 0.0923, 0.0595,\n",
      "         0.0526, 0.0619, 0.0556, 0.0511, 0.0708, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 21067.01758\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1144.6782\n",
      "Test epoch : 27 Average loss: 1206.7434\n",
      "PP(train) = 6323.870, PP(valid) = 6532.830\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.6070e-35, 6.1697e-39, 1.2730e-31, 0.0000e+00,\n",
      "         0.0000e+00, 2.7361e-20, 2.9165e-21, 6.4018e-33, 0.0000e+00, 0.0000e+00,\n",
      "         3.0442e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 18991.85742\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1143.9979\n",
      "Test epoch : 28 Average loss: 1206.1221\n",
      "PP(train) = 6290.487, PP(valid) = 6503.843\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2340e-37, 2.8026e-45, 1.6367e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.3418e-36, 1.2267e-29, 6.0256e-44, 0.0000e+00, 0.0000e+00,\n",
      "         5.6587e-13, 1.1598e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 17121.10547\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1143.3160\n",
      "Test epoch : 29 Average loss: 1205.5093\n",
      "PP(train) = 6257.185, PP(valid) = 6474.904\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0597e-40, 0.0000e+00, 5.8368e-30, 0.0000e+00,\n",
      "         0.0000e+00, 1.0233e-22, 5.8030e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.1389e-36, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 15434.62793\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1142.4066\n",
      "Test epoch : 30 Average loss: 1204.8954\n",
      "PP(train) = 6224.326, PP(valid) = 6446.285\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.8874e-36, 0.0000e+00, 1.6576e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.0172e-39, 3.7849e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.2766e-24, 2.5307e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 13914.27344\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1141.7689\n",
      "Test epoch : 31 Average loss: 1204.2654\n",
      "PP(train) = 6192.241, PP(valid) = 6418.313\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.3290e-39, 0.0000e+00, 4.3657e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.8124e-30, 1.0463e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.4458e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 12543.67871\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1141.0462\n",
      "Test epoch : 32 Average loss: 1203.6551\n",
      "PP(train) = 6159.864, PP(valid) = 6390.035\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.3160e-38, 0.0000e+00, 1.3172e-42, 0.0000e+00,\n",
      "         0.0000e+00, 1.1102e-29, 4.5253e-35, 1.5414e-44, 0.0000e+00, 0.0000e+00,\n",
      "         1.2134e-34, 2.2070e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 11308.09082\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1140.6108\n",
      "Test epoch : 33 Average loss: 1203.0739\n",
      "PP(train) = 6126.406, PP(valid) = 6360.729\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8427e-37, 0.0000e+00,\n",
      "         0.0000e+00, 2.1875e-40, 1.0317e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8310e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 10194.21191\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1139.6436\n",
      "Test epoch : 34 Average loss: 1202.4945\n",
      "PP(train) = 6093.438, PP(valid) = 6331.854\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1263e-39, 0.0000e+00, 4.1779e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 2.4264e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6107e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 9190.05371\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1138.8080\n",
      "Test epoch : 35 Average loss: 1201.8650\n",
      "PP(train) = 6062.734, PP(valid) = 6305.027\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4443e-41, 1.4235e-19, 0.0000e+00,\n",
      "         0.0000e+00, 9.0064e-26, 1.8097e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3680e-25, 2.8026e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 8284.80762\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1138.4679\n",
      "Test epoch : 36 Average loss: 1201.2408\n",
      "PP(train) = 6032.363, PP(valid) = 6278.404\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2093e-31, 0.0000e+00, 7.8285e-24, 0.0000e+00,\n",
      "         0.0000e+00, 6.9392e-26, 2.8920e-29, 3.4584e-42, 0.0000e+00, 9.1785e-43,\n",
      "         1.2191e-24, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7468.73096\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1137.8532\n",
      "Test epoch : 37 Average loss: 1200.6479\n",
      "PP(train) = 6001.268, PP(valid) = 6251.083\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.2083e-33, 0.0000e+00, 6.4068e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.0806e-33, 5.2283e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1813e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6733.04004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1137.0040\n",
      "Test epoch : 38 Average loss: 1200.0547\n",
      "PP(train) = 5970.505, PP(valid) = 6224.100\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5862e-37, 0.0000e+00, 3.3058e-33, 0.0000e+00,\n",
      "         0.0000e+00, 1.8302e-26, 1.3572e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2809e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6069.81641\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1136.1772\n",
      "Test epoch : 39 Average loss: 1199.4544\n",
      "PP(train) = 5940.392, PP(valid) = 6197.606\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5821e-36, 1.4013e-45, 4.3878e-29, 0.0000e+00,\n",
      "         0.0000e+00, 1.8719e-28, 8.7411e-36, 3.0381e-40, 0.0000e+00, 0.0000e+00,\n",
      "         3.4035e-22, 5.7061e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5471.92236\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1135.7667\n",
      "Test epoch : 40 Average loss: 1198.8695\n",
      "PP(train) = 5909.766, PP(valid) = 6170.601\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0934e-32, 3.9334e-42, 1.5701e-18, 0.0000e+00,\n",
      "         0.0000e+00, 4.2824e-34, 7.3188e-22, 2.0456e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.2596e-22, 5.8127e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4932.92285\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1135.0438\n",
      "Test epoch : 41 Average loss: 1198.2958\n",
      "PP(train) = 5879.073, PP(valid) = 6143.491\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1935e-33, 0.0000e+00, 1.1001e-30, 0.0000e+00,\n",
      "         0.0000e+00, 4.5531e-26, 1.2987e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.1182e-31, 5.0447e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4447.01611\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1134.2674\n",
      "Test epoch : 42 Average loss: 1197.7165\n",
      "PP(train) = 5849.042, PP(valid) = 6116.949\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6879e-41, 0.0000e+00, 2.0030e-31, 0.0000e+00,\n",
      "         0.0000e+00, 3.2111e-32, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.6319e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4008.97241\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1133.6509\n",
      "Test epoch : 43 Average loss: 1197.1181\n",
      "PP(train) = 5820.213, PP(valid) = 6091.556\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1210e-44, 0.0000e+00, 1.0171e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.4230e-24, 2.3009e-42, 4.8485e-43, 0.0000e+00, 0.0000e+00,\n",
      "         2.6502e-32, 6.7893e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3614.07739\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1133.0803\n",
      "Test epoch : 44 Average loss: 1196.5298\n",
      "PP(train) = 5791.399, PP(valid) = 6066.088\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.0638e-43, 2.5697e-25, 0.0000e+00, 2.4509e-20, 0.0000e+00,\n",
      "         0.0000e+00, 2.5599e-23, 1.1062e-20, 1.7937e-40, 0.0000e+00, 6.2330e-42,\n",
      "         7.5958e-08, 5.5057e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3258.08057\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1132.4278\n",
      "Test epoch : 45 Average loss: 1195.9450\n",
      "PP(train) = 5762.899, PP(valid) = 6040.900\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.7006e-25, 0.0000e+00, 9.7075e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.7048e-27, 4.4226e-22, 1.0005e-42, 0.0000e+00, 0.0000e+00,\n",
      "         3.5767e-27, 2.8727e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2937.15039\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1131.7857\n",
      "Test epoch : 46 Average loss: 1195.3654\n",
      "PP(train) = 5734.287, PP(valid) = 6015.553\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.1563e-37, 2.7225e-38, 3.5313e-31, 0.0000e+00,\n",
      "         0.0000e+00, 4.6455e-19, 2.4120e-29, 1.8037e-40, 0.0000e+00, 0.0000e+00,\n",
      "         6.9118e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2647.83276\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1131.0377\n",
      "Test epoch : 47 Average loss: 1194.8008\n",
      "PP(train) = 5705.001, PP(valid) = 5989.616\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0923e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.3507e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.2034e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2387.01367\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1130.2635\n",
      "Test epoch : 48 Average loss: 1194.2172\n",
      "PP(train) = 5676.584, PP(valid) = 5964.433\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7187e-36, 1.1897e-39, 4.6286e-22, 0.0000e+00,\n",
      "         0.0000e+00, 2.2448e-23, 5.2992e-11, 3.4682e-42, 0.0000e+00, 8.1107e-42,\n",
      "         1.6931e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2151.88599\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1129.7702\n",
      "Test epoch : 49 Average loss: 1193.6442\n",
      "PP(train) = 5648.681, PP(valid) = 5939.678\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.6841e-34, 0.0000e+00, 2.9853e-35, 0.0000e+00,\n",
      "         0.0000e+00, 2.2055e-27, 1.4237e-23, 8.5186e-41, 0.0000e+00, 0.0000e+00,\n",
      "         7.1978e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1939.91919\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1129.1041\n",
      "Test epoch : 50 Average loss: 1193.0720\n",
      "PP(train) = 5621.145, PP(valid) = 5915.292\n",
      "Writing to ./topicwords/4-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 10.7438 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "===== # 2, Topic : 14, p : 12.5608 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                   ]                                                                 tan                                                [       \n",
      "\n",
      "===== # 3, Topic : 14, p : 11.3715 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                     \n",
      "\n",
      "===== # 4, Topic : 14, p : 12.0034 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                              )          ,        a                            B            a)(c   http://kotobank word/%E                    E  c                 b                         HYPERLINK jp              a),(b                                           A                          \n",
      "\n",
      "===== # 5, Topic : 14, p : 12.1383 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                       \n",
      "\n",
      "===== # 6, Topic : 14, p : 11.9535 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                            mm                                                        \n",
      "\n",
      "===== # 7, Topic : 14, p : 12.3813 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                    IIIIII                                                                \n",
      "\n",
      "===== # 8, Topic : 14, p : 11.2464 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                     \n",
      "\n",
      "===== # 9, Topic : 14, p : 12.0628 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                Y                                                                                                                                            \n",
      "\n",
      "===== # 10, Topic : 14, p : 11.4821 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                    b             \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.5915e-26, 1.5692e-37, 8.5865e-20, 0.0000e+00,\n",
      "         0.0000e+00, 6.7502e-21, 1.6676e-29, 9.7060e-40, 1.3387e-40, 3.4332e-43,\n",
      "         8.0822e-14, 1.2470e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1748.83167\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1149.9243\n",
      "Test epoch : 1 Average loss: 1156.1174\n",
      "PP(train) = 5939.491, PP(valid) = 5876.007\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4347e-26, 1.1953e-39, 9.1561e-24, 0.0000e+00,\n",
      "         0.0000e+00, 4.0807e-19, 5.5691e-35, 5.0867e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.2300e-18, 9.8091e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1576.56677\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1149.2495\n",
      "Test epoch : 2 Average loss: 1155.5561\n",
      "PP(train) = 5912.246, PP(valid) = 5851.525\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5961e-42, 7.0065e-45, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1986e-34, 4.0966e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0199e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1421.27051\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1148.5109\n",
      "Test epoch : 3 Average loss: 1154.9800\n",
      "PP(train) = 5884.206, PP(valid) = 5827.239\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2251e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.4153e-43, 2.8320e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3739e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1281.27136\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1147.8102\n",
      "Test epoch : 4 Average loss: 1154.4122\n",
      "PP(train) = 5855.952, PP(valid) = 5803.303\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 2.9006e-38, 1.1542e-34, 5.6199e-33, 0.0000e+00,\n",
      "         0.0000e+00, 5.9624e-32, 4.1019e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.7060e-34, 1.2705e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1155.06250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1147.4173\n",
      "Test epoch : 5 Average loss: 1153.8571\n",
      "PP(train) = 5826.655, PP(valid) = 5778.562\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0439e-33, 0.0000e+00, 1.2769e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.3423e-33, 3.5093e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.2745e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1041.28564\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1146.6109\n",
      "Test epoch : 6 Average loss: 1153.3076\n",
      "PP(train) = 5797.356, PP(valid) = 5753.898\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.7458e-18, 8.5185e-42, 4.0301e-31, 0.0000e+00,\n",
      "         0.0000e+00, 9.2799e-25, 2.6383e-27, 3.1215e-34, 0.0000e+00, 0.0000e+00,\n",
      "         1.9349e-15, 2.7544e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 938.71606\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1146.0107\n",
      "Test epoch : 7 Average loss: 1152.7554\n",
      "PP(train) = 5768.546, PP(valid) = 5729.662\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2192e-34, 0.0000e+00, 1.8597e-31, 0.0000e+00,\n",
      "         0.0000e+00, 2.2943e-32, 2.5226e-33, 1.9989e-39, 0.0000e+00, 0.0000e+00,\n",
      "         1.4690e-19, 4.5220e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 846.24988\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1145.3019\n",
      "Test epoch : 8 Average loss: 1152.1948\n",
      "PP(train) = 5740.464, PP(valid) = 5706.230\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.2512e-42, 3.9862e-40, 1.2939e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.5543e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6434e-44, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 762.89191\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1144.5267\n",
      "Test epoch : 9 Average loss: 1151.6284\n",
      "PP(train) = 5713.119, PP(valid) = 5683.469\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.8106e-31, 1.2546e-38, 2.0133e-26, 0.0000e+00,\n",
      "         0.0000e+00, 7.1550e-35, 9.0453e-30, 3.5104e-38, 0.0000e+00, 0.0000e+00,\n",
      "         1.1305e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 687.74493\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1143.9054\n",
      "Test epoch : 10 Average loss: 1151.0732\n",
      "PP(train) = 5685.345, PP(valid) = 5660.231\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1019e-43, 2.0665e-32, 0.0000e+00,\n",
      "         0.0000e+00, 5.8632e-28, 1.0840e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.5333e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 620.00012\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1143.3221\n",
      "Test epoch : 11 Average loss: 1150.5224\n",
      "PP(train) = 5657.776, PP(valid) = 5637.041\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.1850e-31, 2.2041e-31, 3.6217e-24, 0.0000e+00,\n",
      "         0.0000e+00, 6.1134e-29, 4.8825e-35, 1.4610e-38, 0.0000e+00, 0.0000e+00,\n",
      "         2.9188e-34, 2.2421e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 558.92834\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1142.7250\n",
      "Test epoch : 12 Average loss: 1149.9838\n",
      "PP(train) = 5629.987, PP(valid) = 5613.640\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8252e-36, 1.4013e-45, 3.2109e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.4144e-23, 6.5109e-41, 8.7119e-42, 0.0000e+00, 0.0000e+00,\n",
      "         9.5580e-25, 1.4599e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 503.87231\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1142.0698\n",
      "Test epoch : 13 Average loss: 1149.4580\n",
      "PP(train) = 5601.837, PP(valid) = 5589.830\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.8251e-41, 1.1153e-22, 4.4793e-38, 8.6991e-23, 0.0000e+00,\n",
      "         0.0000e+00, 1.1470e-31, 9.9678e-18, 0.0000e+00, 4.2039e-45, 0.0000e+00,\n",
      "         1.3572e-16, 6.7014e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 454.23947\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1141.4811\n",
      "Test epoch : 14 Average loss: 1148.9327\n",
      "PP(train) = 5574.131, PP(valid) = 5566.359\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0727e-29, 2.8630e-37, 7.2441e-23, 0.0000e+00,\n",
      "         0.0000e+00, 2.5794e-29, 1.0775e-31, 7.2998e-41, 0.0000e+00, 0.0000e+00,\n",
      "         1.0124e-20, 3.2880e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 409.49561\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1140.7793\n",
      "Test epoch : 15 Average loss: 1148.3874\n",
      "PP(train) = 5547.525, PP(valid) = 5543.971\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0041e-34, 0.0000e+00,\n",
      "         0.0000e+00, 6.1685e-37, 2.0237e-25, 1.0454e-42, 0.0000e+00, 0.0000e+00,\n",
      "         6.3386e-20, 4.2039e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 369.15915\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1140.2664\n",
      "Test epoch : 16 Average loss: 1147.8279\n",
      "PP(train) = 5522.157, PP(valid) = 5522.694\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7040e-42, 0.0000e+00, 6.6007e-27, 0.0000e+00,\n",
      "         0.0000e+00, 2.4645e-23, 9.5062e-22, 2.6014e-34, 0.0000e+00, 0.0000e+00,\n",
      "         2.0287e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 332.79593\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1139.3046\n",
      "Test epoch : 17 Average loss: 1147.2789\n",
      "PP(train) = 5496.595, PP(valid) = 5501.152\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4545e-42, 2.7470e-37, 1.0168e-29, 0.0000e+00,\n",
      "         0.0000e+00, 5.7269e-24, 6.4443e-26, 4.5228e-35, 0.0000e+00, 0.0000e+00,\n",
      "         8.9534e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 300.01459\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1138.8852\n",
      "Test epoch : 18 Average loss: 1146.7360\n",
      "PP(train) = 5470.928, PP(valid) = 5479.460\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.4389e-24, 1.8780e-41, 4.6046e-20, 0.0000e+00,\n",
      "         0.0000e+00, 3.4362e-22, 2.4002e-17, 4.9551e-34, 0.0000e+00, 7.0690e-37,\n",
      "         2.6660e-16, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 270.46231\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1138.2028\n",
      "Test epoch : 19 Average loss: 1146.2021\n",
      "PP(train) = 5444.928, PP(valid) = 5457.384\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2307e-41, 9.9488e-28, 1.2326e-33, 4.0989e-18, 0.0000e+00,\n",
      "         0.0000e+00, 3.6236e-33, 3.7072e-33, 1.3658e-40, 0.0000e+00, 7.4549e-43,\n",
      "         1.2527e-18, 3.6135e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 243.82101\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1137.7246\n",
      "Test epoch : 20 Average loss: 1145.6765\n",
      "PP(train) = 5419.032, PP(valid) = 5435.242\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7295e-39, 5.0852e-22, 1.8233e-33, 9.7137e-19, 0.0000e+00,\n",
      "         4.1823e-36, 1.3163e-20, 8.8946e-15, 1.3909e-32, 0.0000e+00, 0.0000e+00,\n",
      "         2.1960e-10, 8.0591e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 219.80396\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1137.0021\n",
      "Test epoch : 21 Average loss: 1145.1530\n",
      "PP(train) = 5393.454, PP(valid) = 5413.433\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6845e-36, 2.6625e-44, 2.3274e-40, 0.0000e+00,\n",
      "         0.0000e+00, 4.7994e-35, 4.6301e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.7531e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 198.15265\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1136.3790\n",
      "Test epoch : 22 Average loss: 1144.6312\n",
      "PP(train) = 5368.079, PP(valid) = 5391.788\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.9569e-30, 6.0299e-32, 4.4602e-17, 0.0000e+00,\n",
      "         0.0000e+00, 4.1153e-15, 2.2091e-28, 2.4806e-28, 0.0000e+00, 0.0000e+00,\n",
      "         6.9658e-21, 3.2139e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 178.63405\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1135.8427\n",
      "Test epoch : 23 Average loss: 1144.1094\n",
      "PP(train) = 5343.047, PP(valid) = 5370.424\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.8399e-34, 9.8091e-45, 1.9122e-18, 0.0000e+00,\n",
      "         0.0000e+00, 2.8785e-23, 4.3893e-27, 5.0010e-38, 0.0000e+00, 4.2179e-43,\n",
      "         9.7318e-22, 1.2378e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 161.03809\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1135.0782\n",
      "Test epoch : 24 Average loss: 1143.5831\n",
      "PP(train) = 5318.675, PP(valid) = 5349.691\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4187e-32, 2.5442e-40, 3.5842e-22, 0.0000e+00,\n",
      "         0.0000e+00, 6.5332e-18, 1.7790e-24, 4.1598e-37, 0.0000e+00, 0.0000e+00,\n",
      "         3.2226e-22, 7.9085e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 145.17538\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1134.5399\n",
      "Test epoch : 25 Average loss: 1143.0590\n",
      "PP(train) = 5294.389, PP(valid) = 5328.972\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 2.5658e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.8110e-38, 5.0261e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8842e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 130.87520\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1133.8009\n",
      "Test epoch : 26 Average loss: 1142.5367\n",
      "PP(train) = 5270.520, PP(valid) = 5308.682\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3660e-39, 2.0874e-32, 1.1699e-35, 1.0510e-18, 0.0000e+00,\n",
      "         0.0000e+00, 8.0417e-12, 5.8799e-21, 5.4126e-34, 0.0000e+00, 2.3822e-44,\n",
      "         3.4943e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 117.98362\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1133.3289\n",
      "Test epoch : 27 Average loss: 1142.0172\n",
      "PP(train) = 5246.549, PP(valid) = 5288.182\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4920e-38, 9.4269e-40, 1.2051e-26, 0.0000e+00,\n",
      "         0.0000e+00, 8.4845e-22, 1.2331e-19, 8.0898e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.3311e-18, 5.4651e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 106.36190\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1132.6846\n",
      "Test epoch : 28 Average loss: 1141.5030\n",
      "PP(train) = 5222.388, PP(valid) = 5267.520\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1705e-27, 0.0000e+00,\n",
      "         0.0000e+00, 3.9113e-36, 2.6092e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.3230e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 95.88496\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1131.9944\n",
      "Test epoch : 29 Average loss: 1140.9806\n",
      "PP(train) = 5198.898, PP(valid) = 5247.424\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1463e-31, 1.4321e-39, 6.2012e-29, 0.0000e+00,\n",
      "         0.0000e+00, 6.2432e-39, 3.9306e-29, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5374e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 86.44002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1131.3888\n",
      "Test epoch : 30 Average loss: 1140.4576\n",
      "PP(train) = 5176.077, PP(valid) = 5227.999\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.9041e-38, 3.1962e-23, 2.2452e-26, 3.1957e-24, 0.0000e+00,\n",
      "         0.0000e+00, 2.5008e-15, 8.8883e-23, 3.9429e-27, 4.7364e-43, 0.0000e+00,\n",
      "         7.8494e-21, 3.0791e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 77.92543\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1130.9290\n",
      "Test epoch : 31 Average loss: 1139.9466\n",
      "PP(train) = 5152.913, PP(valid) = 5208.158\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.9492e-44, 7.5494e-23, 1.8813e-36, 1.3201e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.0786e-21, 9.6179e-27, 2.8136e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.4848e-16, 5.1150e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 70.24956\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1130.2272\n",
      "Test epoch : 32 Average loss: 1139.4502\n",
      "PP(train) = 5128.821, PP(valid) = 5187.215\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.9162e-20, 5.7283e-36, 1.2671e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.0955e-16, 7.7094e-16, 2.4949e-35, 0.0000e+00, 3.9797e-43,\n",
      "         1.2958e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 63.32978\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1129.7436\n",
      "Test epoch : 33 Average loss: 1138.9561\n",
      "PP(train) = 5105.188, PP(valid) = 5166.824\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4836e-41, 4.4438e-33, 0.0000e+00,\n",
      "         0.0000e+00, 6.9240e-27, 3.4252e-28, 1.4314e-40, 0.0000e+00, 1.5274e-43,\n",
      "         1.8087e-18, 3.9797e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 57.09161\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1129.0583\n",
      "Test epoch : 34 Average loss: 1138.4540\n",
      "PP(train) = 5082.385, PP(valid) = 5147.196\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.3855e-37, 0.0000e+00, 8.1181e-31, 0.0000e+00,\n",
      "         0.0000e+00, 3.4976e-32, 1.6753e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.2467e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 51.46793\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1128.4623\n",
      "Test epoch : 35 Average loss: 1137.9429\n",
      "PP(train) = 5060.403, PP(valid) = 5128.327\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1126e-41, 1.1730e-21, 0.0000e+00, 2.2165e-13, 6.4880e-43,\n",
      "         0.0000e+00, 1.6053e-15, 8.3583e-18, 4.0322e-32, 0.0000e+00, 7.1707e-41,\n",
      "         3.2520e-14, 1.6453e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 46.39819\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1127.9863\n",
      "Test epoch : 36 Average loss: 1137.4326\n",
      "PP(train) = 5038.512, PP(valid) = 5109.455\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0263e-22, 1.6036e-37, 2.0334e-17, 0.0000e+00,\n",
      "         0.0000e+00, 1.0186e-19, 5.5474e-21, 8.6079e-36, 0.0000e+00, 1.3311e-41,\n",
      "         3.3831e-18, 1.6913e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 41.82784\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1127.3349\n",
      "Test epoch : 37 Average loss: 1136.9319\n",
      "PP(train) = 5016.428, PP(valid) = 5090.472\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2543e-40, 2.6322e-35, 4.0605e-30, 6.0987e-12, 0.0000e+00,\n",
      "         0.0000e+00, 1.7255e-16, 1.0412e-22, 1.9688e-41, 4.2039e-45, 0.0000e+00,\n",
      "         2.8085e-22, 1.8466e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 37.70768\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1126.7031\n",
      "Test epoch : 38 Average loss: 1136.4305\n",
      "PP(train) = 4994.737, PP(valid) = 5071.820\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1644e-28, 5.7958e-34, 2.1109e-34, 0.0000e+00,\n",
      "         0.0000e+00, 5.3749e-17, 3.0902e-30, 3.4661e-35, 0.0000e+00, 8.2249e-39,\n",
      "         2.8003e-22, 2.8005e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 33.99337\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1126.1168\n",
      "Test epoch : 39 Average loss: 1135.9287\n",
      "PP(train) = 4972.869, PP(valid) = 5052.910\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3428e-35, 9.5114e-21, 4.8307e-39, 9.0594e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.5849e-16, 3.7393e-19, 2.7688e-30, 0.0000e+00, 2.7620e-42,\n",
      "         4.3060e-18, 3.4700e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 30.64492\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1125.6030\n",
      "Test epoch : 40 Average loss: 1135.4401\n",
      "PP(train) = 4950.539, PP(valid) = 5033.517\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4287e-41, 0.0000e+00, 5.3719e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.5598e-28, 1.2101e-31, 1.0229e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.1321e-26, 2.4024e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 27.62631\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1125.0986\n",
      "Test epoch : 41 Average loss: 1134.9550\n",
      "PP(train) = 4928.395, PP(valid) = 5014.300\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4821e-33, 6.6744e-40, 4.0337e-25, 0.0000e+00,\n",
      "         0.0000e+00, 4.9048e-27, 9.9018e-37, 1.7252e-40, 0.0000e+00, 0.0000e+00,\n",
      "         3.6662e-21, 2.7465e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 24.90504\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1124.4622\n",
      "Test epoch : 42 Average loss: 1134.4648\n",
      "PP(train) = 4907.163, PP(valid) = 4995.871\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.0958e-36, 3.1371e-30, 3.4124e-41, 1.2073e-24, 0.0000e+00,\n",
      "         0.0000e+00, 3.1213e-23, 8.7199e-27, 1.2661e-34, 0.0000e+00, 2.2421e-44,\n",
      "         2.2849e-08, 4.2126e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 22.45183\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1123.9603\n",
      "Test epoch : 43 Average loss: 1133.9641\n",
      "PP(train) = 4886.769, PP(valid) = 4978.278\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7804e-27, 8.7675e-40, 1.8202e-26, 0.0000e+00,\n",
      "         0.0000e+00, 3.4924e-20, 3.2360e-30, 0.0000e+00, 2.6260e-41, 0.0000e+00,\n",
      "         8.0560e-26, 1.6272e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 20.24026\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1123.3179\n",
      "Test epoch : 44 Average loss: 1133.4711\n",
      "PP(train) = 4866.505, PP(valid) = 4960.799\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-44, 4.0186e-25, 5.6239e-37, 2.8409e-32, 0.0000e+00,\n",
      "         0.0000e+00, 2.6622e-17, 1.2827e-21, 4.6243e-44, 1.3593e-43, 1.4503e-42,\n",
      "         7.8909e-22, 1.1911e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 18.24663\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1122.8259\n",
      "Test epoch : 45 Average loss: 1132.9844\n",
      "PP(train) = 4845.591, PP(valid) = 4942.661\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2281e-43, 4.2806e-23, 3.1228e-34, 1.6280e-17, 0.0000e+00,\n",
      "         2.3724e-42, 2.7934e-07, 2.4658e-14, 5.5733e-23, 0.0000e+00, 2.2701e-43,\n",
      "         5.9248e-26, 1.7315e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 16.44942\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1122.2560\n",
      "Test epoch : 46 Average loss: 1132.5049\n",
      "PP(train) = 4824.565, PP(valid) = 4924.354\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.6572e-38, 0.0000e+00, 1.7068e-22, 0.0000e+00,\n",
      "         0.0000e+00, 1.3065e-27, 2.0223e-26, 4.1521e-40, 0.0000e+00, 1.4013e-45,\n",
      "         1.8370e-28, 8.1945e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 14.82931\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1121.5306\n",
      "Test epoch : 47 Average loss: 1132.0244\n",
      "PP(train) = 4803.813, PP(valid) = 4906.269\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1123e-42, 2.5263e-37, 4.1234e-36, 9.1780e-33, 0.0000e+00,\n",
      "         0.0000e+00, 2.1369e-22, 2.0349e-29, 1.5695e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.9758e-24, 2.1356e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 13.36876\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1120.9866\n",
      "Test epoch : 48 Average loss: 1131.5387\n",
      "PP(train) = 4783.693, PP(valid) = 4888.843\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2612e-44, 5.0767e-29, 1.1210e-44, 3.0654e-28, 0.0000e+00,\n",
      "         0.0000e+00, 6.5210e-20, 2.1543e-23, 3.0818e-39, 0.0000e+00, 0.0000e+00,\n",
      "         7.4162e-19, 4.0152e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 12.05206\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1120.3822\n",
      "Test epoch : 49 Average loss: 1131.0532\n",
      "PP(train) = 4764.016, PP(valid) = 4871.855\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.7386e-38, 1.7065e-18, 2.5047e-26, 1.9055e-27, 0.0000e+00,\n",
      "         2.0739e-43, 4.4991e-08, 3.6034e-36, 1.3263e-40, 0.0000e+00, 9.8846e-39,\n",
      "         1.3649e-24, 1.7849e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 10.86504\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1120.0119\n",
      "Test epoch : 50 Average loss: 1130.5794\n",
      "PP(train) = 4744.018, PP(valid) = 4854.486\n",
      "Writing to ./topicwords/5-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 10.7826 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                    /                                                                                                    )                                  )                                                           \n",
      "\n",
      "===== # 2, Topic : 14, p : 10.4457 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                           \n",
      "\n",
      "===== # 3, Topic : 14, p : 11.7238 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                             mm                                                                                           \n",
      "\n",
      "===== # 4, Topic : 14, p : 10.6353 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                \n",
      "\n",
      "===== # 5, Topic : 14, p : 10.3108 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                      s\n",
      "\n",
      "===== # 6, Topic : 14, p : 12.8190 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                          \n",
      "\n",
      "===== # 7, Topic : 14, p : 11.6060 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                   c d                      ELCENTRO TAFT HACHINOHE     c d                            RESP         cMu         bMu           Masing                                       (      m    RC        IVA   cMu              ELCENTRO TAFT HACHINOHE    bMu                    SHAKE                      (e                         (   a b    )(                  F T m  kN   New         \n",
      "\n",
      "===== # 8, Topic : 14, p : 10.2942 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                        \n",
      "\n",
      "===== # 9, Topic : 14, p : 10.1551 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                             \n",
      "\n",
      "===== # 10, Topic : 14, p : 11.4787 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                              \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.3938e-37, 5.9401e-42, 1.2422e-26, 0.0000e+00,\n",
      "         0.0000e+00, 5.5057e-38, 6.4187e-21, 1.8108e-38, 0.0000e+00, 0.0000e+00,\n",
      "         5.0135e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 9.79494\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1155.7562\n",
      "Test epoch : 1 Average loss: 1085.2525\n",
      "PP(train) = 4883.764, PP(valid) = 4840.363\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.2246e-29, 7.9975e-38, 3.1753e-26, 0.0000e+00,\n",
      "         0.0000e+00, 7.3928e-37, 1.2471e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8966e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 8.83023\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1155.2938\n",
      "Test epoch : 2 Average loss: 1084.7866\n",
      "PP(train) = 4862.845, PP(valid) = 4822.692\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.7829e-32, 1.4928e-40, 1.8089e-34, 0.0000e+00,\n",
      "         0.0000e+00, 2.8466e-27, 9.5656e-37, 1.3238e-41, 0.0000e+00, 0.0000e+00,\n",
      "         9.6942e-28, 2.3822e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7.96055\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1154.7136\n",
      "Test epoch : 3 Average loss: 1084.3150\n",
      "PP(train) = 4840.871, PP(valid) = 4804.867\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2762e-35, 1.4151e-36, 7.7303e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.2108e-25, 1.1283e-27, 5.4138e-40, 0.0000e+00, 0.0000e+00,\n",
      "         6.4213e-31, 4.2040e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7.17653\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1154.1032\n",
      "Test epoch : 4 Average loss: 1083.8441\n",
      "PP(train) = 4818.537, PP(valid) = 4787.166\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3687e-42, 2.2708e-19, 9.8841e-30, 1.3074e-11, 0.0000e+00,\n",
      "         0.0000e+00, 1.9884e-09, 2.3944e-22, 5.9389e-25, 4.2039e-45, 5.1848e-44,\n",
      "         5.7788e-09, 2.3994e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6.46973\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1153.5677\n",
      "Test epoch : 5 Average loss: 1083.3713\n",
      "PP(train) = 4796.278, PP(valid) = 4769.776\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4572e-37, 5.3429e-21, 8.3991e-30, 8.4434e-05, 0.0000e+00,\n",
      "         0.0000e+00, 5.7088e-09, 1.7831e-22, 3.9969e-23, 1.3725e-37, 6.9287e-32,\n",
      "         2.4887e-16, 1.4340e-30, 9.9992e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5.83253\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1152.8920\n",
      "Test epoch : 6 Average loss: 1082.9090\n",
      "PP(train) = 4773.362, PP(valid) = 4751.974\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.1001e-40, 4.0200e-28, 6.5167e-34, 5.8083e-21, 0.0000e+00,\n",
      "         0.0000e+00, 4.2799e-32, 1.5432e-22, 4.9373e-41, 0.0000e+00, 0.0000e+00,\n",
      "         6.4401e-18, 1.3217e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5.25810\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1152.2358\n",
      "Test epoch : 7 Average loss: 1082.4391\n",
      "PP(train) = 4751.153, PP(valid) = 4734.802\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6167e-36, 0.0000e+00, 1.6045e-25, 0.0000e+00,\n",
      "         0.0000e+00, 4.9092e-24, 8.5222e-30, 6.8699e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.0877e-27, 8.5050e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4.74024\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1151.5405\n",
      "Test epoch : 8 Average loss: 1081.9646\n",
      "PP(train) = 4729.467, PP(valid) = 4718.002\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5600e-40, 2.4474e-17, 7.5645e-33, 5.0040e-09, 0.0000e+00,\n",
      "         0.0000e+00, 3.8576e-20, 2.0066e-16, 1.2221e-32, 0.0000e+00, 4.4554e-38,\n",
      "         2.8810e-10, 1.0238e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4.27338\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1150.8149\n",
      "Test epoch : 9 Average loss: 1081.4953\n",
      "PP(train) = 4707.643, PP(valid) = 4701.137\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2240e-37, 2.3989e-26, 9.0787e-38, 1.2161e-18, 0.0000e+00,\n",
      "         0.0000e+00, 1.6262e-07, 2.3683e-17, 1.8345e-34, 1.5210e-41, 3.0548e-43,\n",
      "         8.5035e-16, 1.5310e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3.85250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1150.2756\n",
      "Test epoch : 10 Average loss: 1081.0310\n",
      "PP(train) = 4685.650, PP(valid) = 4684.015\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5414e-42, 2.9996e-29, 4.1899e-43, 2.9019e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.6755e-25, 4.0162e-34, 1.2014e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.1326e-27, 2.8026e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3.47308\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1149.6345\n",
      "Test epoch : 11 Average loss: 1080.5712\n",
      "PP(train) = 4663.819, PP(valid) = 4666.960\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4682e-28, 1.0706e-37, 7.9953e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.0415e-28, 9.3356e-30, 1.9864e-35, 0.0000e+00, 0.0000e+00,\n",
      "         2.2210e-30, 3.9236e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3.13108\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1149.0393\n",
      "Test epoch : 12 Average loss: 1080.1128\n",
      "PP(train) = 4642.133, PP(valid) = 4650.024\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.6066e-32, 2.2615e-17, 1.0628e-37, 2.8751e-21, 0.0000e+00,\n",
      "         0.0000e+00, 3.3146e-17, 1.4112e-17, 4.7544e-34, 0.0000e+00, 2.0145e-39,\n",
      "         1.4615e-17, 2.6181e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.82278\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1148.4788\n",
      "Test epoch : 13 Average loss: 1079.6555\n",
      "PP(train) = 4620.689, PP(valid) = 4633.239\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0636e-42, 1.6007e-18, 1.8292e-38, 1.7696e-23, 0.0000e+00,\n",
      "         0.0000e+00, 1.0170e-12, 2.2366e-26, 9.9728e-35, 0.0000e+00, 1.9732e-40,\n",
      "         1.4953e-24, 6.7889e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.54486\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1147.7426\n",
      "Test epoch : 14 Average loss: 1079.1978\n",
      "PP(train) = 4599.722, PP(valid) = 4616.778\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.6129e-37, 2.7865e-33, 5.9238e-28, 0.0000e+00,\n",
      "         0.0000e+00, 9.0843e-15, 1.3849e-24, 2.5004e-28, 0.0000e+00, 0.0000e+00,\n",
      "         1.6284e-14, 1.1797e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.29432\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1147.1279\n",
      "Test epoch : 15 Average loss: 1078.7419\n",
      "PP(train) = 4579.077, PP(valid) = 4600.639\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0949e-26, 1.7207e-36, 1.0812e-21, 0.0000e+00,\n",
      "         0.0000e+00, 2.3094e-28, 1.9000e-24, 1.6408e-39, 0.0000e+00, 2.3730e-41,\n",
      "         8.3211e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.06845\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1146.5418\n",
      "Test epoch : 16 Average loss: 1078.2945\n",
      "PP(train) = 4558.024, PP(valid) = 4584.044\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 4.1884e-37, 2.0417e-32, 7.4306e-31, 0.0000e+00,\n",
      "         0.0000e+00, 3.2518e-17, 1.7565e-21, 1.2900e-36, 0.0000e+00, 0.0000e+00,\n",
      "         9.1029e-26, 1.4707e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1.86482\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1145.8664\n",
      "Test epoch : 17 Average loss: 1077.8433\n",
      "PP(train) = 4537.579, PP(valid) = 4567.972\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0334e-16, 6.7188e-22, 6.2267e-16, 0.0000e+00,\n",
      "         0.0000e+00, 1.0864e-12, 1.4808e-14, 9.9518e-33, 0.0000e+00, 7.3875e-27,\n",
      "         1.3265e-14, 5.4810e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.68124\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1145.2484\n",
      "Test epoch : 18 Average loss: 1077.3897\n",
      "PP(train) = 4517.363, PP(valid) = 4552.060\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2855e-42, 2.6831e-27, 2.2507e-38, 0.0000e+00,\n",
      "         0.0000e+00, 1.6973e-26, 1.6679e-22, 7.3503e-39, 0.0000e+00, 0.0000e+00,\n",
      "         1.7282e-25, 2.6625e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.51575\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1144.6268\n",
      "Test epoch : 19 Average loss: 1076.9374\n",
      "PP(train) = 4497.658, PP(valid) = 4536.588\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.8736e-37, 3.8665e-21, 1.3817e-42, 5.8213e-22, 0.0000e+00,\n",
      "         0.0000e+00, 8.1622e-19, 4.8266e-27, 8.1709e-36, 0.0000e+00, 0.0000e+00,\n",
      "         1.8065e-18, 3.2193e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.36655\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1144.0285\n",
      "Test epoch : 20 Average loss: 1076.4900\n",
      "PP(train) = 4478.082, PP(valid) = 4521.165\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.9745e-24, 3.3619e-35, 1.8859e-10, 0.0000e+00,\n",
      "         0.0000e+00, 1.5993e-11, 3.8960e-28, 3.9330e-32, 5.6052e-45, 4.4097e-41,\n",
      "         4.6910e-17, 5.2838e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.23205\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1143.4240\n",
      "Test epoch : 21 Average loss: 1076.0445\n",
      "PP(train) = 4458.569, PP(valid) = 4505.750\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1911e-43, 4.3535e-31, 9.0804e-43, 3.0690e-19, 0.0000e+00,\n",
      "         4.4421e-43, 7.7938e-07, 1.3228e-25, 3.1222e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.0697e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.11080\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1142.9165\n",
      "Test epoch : 22 Average loss: 1075.6062\n",
      "PP(train) = 4438.583, PP(valid) = 4489.867\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.0638e-44, 9.1476e-34, 4.4634e-35, 4.8900e-12, 0.0000e+00,\n",
      "         0.0000e+00, 1.2163e-24, 1.8727e-26, 2.3822e-44, 3.3103e-40, 1.4446e-38,\n",
      "         3.6765e-23, 4.2844e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.00149\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1142.3791\n",
      "Test epoch : 23 Average loss: 1075.1681\n",
      "PP(train) = 4418.863, PP(valid) = 4474.167\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.1213e-36, 1.7256e-27, 2.6016e-26, 1.0179e-19, 0.0000e+00,\n",
      "         1.6675e-43, 4.3137e-21, 3.5196e-22, 4.7604e-33, 0.0000e+00, 0.0000e+00,\n",
      "         2.9189e-20, 1.6135e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.90294\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1141.7398\n",
      "Test epoch : 24 Average loss: 1074.7330\n",
      "PP(train) = 4399.533, PP(valid) = 4458.743\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6287e-36, 4.5566e-28, 1.4154e-39, 8.5278e-27, 0.0000e+00,\n",
      "         0.0000e+00, 3.7381e-12, 1.0187e-16, 5.3353e-33, 3.4920e-42, 3.5803e-39,\n",
      "         1.4065e-26, 1.6989e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.81410\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1141.1952\n",
      "Test epoch : 25 Average loss: 1074.3019\n",
      "PP(train) = 4380.358, PP(valid) = 4443.436\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.3822e-44, 5.2240e-35, 2.5526e-37, 3.8765e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.9306e-28, 3.5047e-26, 1.5094e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.2637e-25, 6.3988e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.73400\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1140.5179\n",
      "Test epoch : 26 Average loss: 1073.8691\n",
      "PP(train) = 4361.326, PP(valid) = 4428.156\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1210e-44, 4.1444e-19, 6.7341e-39, 3.7882e-25, 0.0000e+00,\n",
      "         0.0000e+00, 3.1362e-20, 1.3077e-31, 2.2427e-31, 0.0000e+00, 0.0000e+00,\n",
      "         2.6433e-27, 3.7196e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.66179\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1139.8973\n",
      "Test epoch : 27 Average loss: 1073.4330\n",
      "PP(train) = 4342.926, PP(valid) = 4413.493\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.1657e-44, 4.2104e-37, 8.6347e-37, 6.4882e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.2407e-26, 5.1589e-20, 1.6447e-34, 3.0418e-41, 5.8611e-41,\n",
      "         1.2333e-36, 7.6055e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.59670\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1139.3321\n",
      "Test epoch : 28 Average loss: 1072.9973\n",
      "PP(train) = 4324.815, PP(valid) = 4399.092\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3142e-38, 2.0939e-19, 3.3155e-34, 1.9555e-13, 0.0000e+00,\n",
      "         0.0000e+00, 1.6636e-07, 7.5164e-15, 5.4568e-37, 4.0638e-44, 2.3140e-40,\n",
      "         5.3995e-22, 8.4035e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.53800\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1138.7525\n",
      "Test epoch : 29 Average loss: 1072.5664\n",
      "PP(train) = 4306.730, PP(valid) = 4384.659\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5749e-41, 8.1170e-24, 1.4162e-33, 7.7196e-14, 0.0000e+00,\n",
      "         0.0000e+00, 3.1182e-22, 5.5800e-20, 3.1194e-31, 0.0000e+00, 2.1019e-44,\n",
      "         1.5306e-21, 5.5448e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.48509\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1138.2408\n",
      "Test epoch : 30 Average loss: 1072.1408\n",
      "PP(train) = 4288.328, PP(valid) = 4369.936\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.4466e-40, 1.0060e-25, 1.2137e-24, 1.7945e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.0752e-13, 2.2320e-28, 1.3539e-33, 0.0000e+00, 8.4078e-45,\n",
      "         3.4275e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.43738\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1137.6000\n",
      "Test epoch : 31 Average loss: 1071.7176\n",
      "PP(train) = 4269.968, PP(valid) = 4355.166\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.8506e-41, 6.4189e-21, 0.0000e+00, 1.2482e-34, 0.0000e+00,\n",
      "         0.0000e+00, 2.1796e-25, 7.1454e-22, 0.0000e+00, 0.0000e+00, 5.4651e-44,\n",
      "         2.1451e-33, 4.6849e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.39437\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1136.9479\n",
      "Test epoch : 32 Average loss: 1071.2914\n",
      "PP(train) = 4252.088, PP(valid) = 4340.814\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.4959e-32, 1.7062e-39, 2.6671e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.3560e-25, 1.6563e-29, 8.3202e-35, 1.4013e-45, 0.0000e+00,\n",
      "         7.9091e-33, 5.2375e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.35559\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1136.4076\n",
      "Test epoch : 33 Average loss: 1070.8718\n",
      "PP(train) = 4234.243, PP(valid) = 4326.407\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, 5.7796e-37, 0.0000e+00,\n",
      "         0.0000e+00, 1.2986e-30, 8.8233e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.4162e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.32063\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1135.8099\n",
      "Test epoch : 34 Average loss: 1070.4494\n",
      "PP(train) = 4216.677, PP(valid) = 4312.274\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.2809e-40, 7.7201e-18, 1.3902e-35, 1.1477e-18, 0.0000e+00,\n",
      "         8.4078e-45, 2.3199e-22, 1.2862e-20, 2.1035e-23, 0.0000e+00, 4.5278e-39,\n",
      "         5.7315e-13, 5.8000e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.28911\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1135.3746\n",
      "Test epoch : 35 Average loss: 1070.0292\n",
      "PP(train) = 4199.457, PP(valid) = 4298.425\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4412e-38, 2.6024e-20, 6.1237e-43, 2.8767e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.5931e-24, 3.2672e-11, 9.8722e-35, 0.0000e+00, 0.0000e+00,\n",
      "         6.0339e-15, 3.9626e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.26068\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1134.7239\n",
      "Test epoch : 36 Average loss: 1069.6165\n",
      "PP(train) = 4181.892, PP(valid) = 4284.249\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0106e-24, 0.0000e+00, 9.4149e-31, 0.0000e+00,\n",
      "         0.0000e+00, 4.8714e-19, 8.6341e-25, 9.1986e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.2630e-24, 3.8491e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.23506\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1134.1271\n",
      "Test epoch : 37 Average loss: 1069.1984\n",
      "PP(train) = 4164.802, PP(valid) = 4270.424\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.4007e-38, 2.1094e-29, 2.8640e-35, 0.0000e+00,\n",
      "         0.0000e+00, 2.8981e-25, 1.5487e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.7062e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.21196\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1133.4927\n",
      "Test epoch : 38 Average loss: 1068.7818\n",
      "PP(train) = 4148.074, PP(valid) = 4257.002\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3870e-39, 4.9404e-22, 3.1707e-32, 7.0983e-07, 0.0000e+00,\n",
      "         1.4013e-45, 8.0911e-02, 2.5951e-17, 4.3776e-27, 8.4078e-45, 1.2744e-28,\n",
      "         1.5572e-18, 2.2670e-24, 9.1909e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0825, 0.0479, 0.0997, 0.0501, 0.0619, 0.0692, 0.1055, 0.0876, 0.0592,\n",
      "         0.0512, 0.0658, 0.0549, 0.0500, 0.0730, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.19113\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1132.9871\n",
      "Test epoch : 39 Average loss: 1068.3696\n",
      "PP(train) = 4131.225, PP(valid) = 4243.326\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8026e-45, 2.3452e-20, 1.3292e-38, 9.1408e-23, 0.0000e+00,\n",
      "         1.2612e-44, 3.8776e-20, 6.8496e-22, 1.1180e-14, 1.4013e-45, 3.8691e-38,\n",
      "         8.4072e-11, 1.2844e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.17235\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1132.4523\n",
      "Test epoch : 40 Average loss: 1067.9608\n",
      "PP(train) = 4114.644, PP(valid) = 4229.964\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8799e-35, 8.8944e-37, 1.0824e-33, 4.1853e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.1535e-32, 3.3933e-28, 9.5288e-44, 0.0000e+00, 4.4842e-44,\n",
      "         4.0896e-27, 1.6202e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.15542\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1132.0419\n",
      "Test epoch : 41 Average loss: 1067.5514\n",
      "PP(train) = 4098.141, PP(valid) = 4216.621\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1432e-35, 4.6181e-22, 2.6138e-39, 7.4536e-17, 0.0000e+00,\n",
      "         0.0000e+00, 7.8472e-10, 4.8909e-19, 6.9199e-30, 2.1876e-41, 6.7157e-37,\n",
      "         1.8482e-12, 1.0749e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.14015\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1131.3949\n",
      "Test epoch : 42 Average loss: 1067.1460\n",
      "PP(train) = 4081.374, PP(valid) = 4203.007\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.1964e-40, 1.7849e-26, 2.9206e-28, 5.4482e-16, 0.0000e+00,\n",
      "         0.0000e+00, 4.7534e-18, 4.2771e-14, 1.9324e-41, 0.0000e+00, 1.5414e-44,\n",
      "         3.2806e-25, 5.7137e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.12638\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1130.9110\n",
      "Test epoch : 43 Average loss: 1066.7432\n",
      "PP(train) = 4064.648, PP(valid) = 4189.325\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1850e-31, 3.3217e-38, 4.1201e-23, 0.0000e+00,\n",
      "         0.0000e+00, 2.4287e-26, 1.7753e-20, 4.8169e-30, 0.0000e+00, 2.5574e-42,\n",
      "         1.0262e-26, 9.6764e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.11397\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1130.3256\n",
      "Test epoch : 44 Average loss: 1066.3423\n",
      "PP(train) = 4048.106, PP(valid) = 4175.743\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0510e-43, 1.1900e-15, 1.1486e-31, 4.3268e-25, 0.0000e+00,\n",
      "         0.0000e+00, 6.1111e-08, 2.6081e-12, 1.1650e-20, 5.1460e-39, 7.4053e-41,\n",
      "         1.4694e-17, 3.8775e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.10278\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1129.7937\n",
      "Test epoch : 45 Average loss: 1065.9394\n",
      "PP(train) = 4032.125, PP(valid) = 4162.721\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.8217e-38, 2.0941e-22, 1.3694e-28, 1.1661e-14, 1.6325e-42,\n",
      "         0.0000e+00, 7.6664e-17, 5.0394e-15, 1.9779e-33, 7.4410e-40, 2.5504e-43,\n",
      "         2.2131e-14, 3.3342e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.09269\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1129.2772\n",
      "Test epoch : 46 Average loss: 1065.5378\n",
      "PP(train) = 4016.654, PP(valid) = 4150.164\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.3788e-26, 1.5497e-38, 1.4310e-26, 0.0000e+00,\n",
      "         0.0000e+00, 3.7574e-25, 1.7530e-20, 3.2039e-31, 0.0000e+00, 0.0000e+00,\n",
      "         5.4156e-27, 2.2101e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.08359\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1128.6163\n",
      "Test epoch : 47 Average loss: 1065.1334\n",
      "PP(train) = 4001.384, PP(valid) = 4137.774\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.6634e-31, 6.9904e-17, 7.7928e-34, 1.4461e-11, 4.4182e-39,\n",
      "         0.0000e+00, 1.0787e-10, 4.6142e-13, 4.1005e-32, 4.7507e-38, 1.2647e-25,\n",
      "         1.2835e-11, 6.9543e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.07538\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1128.1322\n",
      "Test epoch : 48 Average loss: 1064.7337\n",
      "PP(train) = 3985.730, PP(valid) = 4124.965\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1210e-44, 3.2260e-25, 2.2648e-29, 2.0353e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.5208e-18, 1.1377e-21, 3.3111e-33, 0.0000e+00, 3.9937e-43,\n",
      "         7.3367e-31, 2.8828e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.06798\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1127.5945\n",
      "Test epoch : 49 Average loss: 1064.3427\n",
      "PP(train) = 3969.733, PP(valid) = 4111.851\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6949e-40, 5.2188e-31, 2.0354e-38, 2.4003e-33, 0.0000e+00,\n",
      "         0.0000e+00, 1.2656e-23, 1.3927e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.7070e-34, 1.8121e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.06130\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1127.0828\n",
      "Test epoch : 50 Average loss: 1063.9520\n",
      "PP(train) = 3954.019, PP(valid) = 4098.954\n",
      "Writing to ./topicwords/6-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 9.4073 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                 \n",
      "\n",
      "===== # 2, Topic : 14, p : 10.2378 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                  h        mm                     m Pa                                                  kw                   COLD),HOT                kw                                                 \n",
      "\n",
      "===== # 3, Topic : 14, p : 10.2662 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                \n",
      "\n",
      "===== # 4, Topic : 14, p : 10.1014 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                              \n",
      "\n",
      "===== # 5, Topic : 14, p : 10.2367 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                 \n",
      "\n",
      "===== # 6, Topic : 14, p : 11.2521 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                 \n",
      "\n",
      "===== # 7, Topic : 14, p : 9.9088 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                          \n",
      "\n",
      "===== # 8, Topic : 14, p : 10.2123 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                      \n",
      "\n",
      "===== # 9, Topic : 14, p : 10.0487 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                        \n",
      "\n",
      "===== # 10, Topic : 14, p : 10.0699 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                              \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.5706e-38, 4.7137e-39, 2.5197e-20, 0.0000e+00,\n",
      "         0.0000e+00, 4.5503e-21, 6.3532e-32, 7.5406e-38, 0.0000e+00, 2.4032e-40,\n",
      "         2.4962e-26, 5.3249e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.05529\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1141.9123\n",
      "Test epoch : 1 Average loss: 1157.1804\n",
      "PP(train) = 4089.419, PP(valid) = 4040.308\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.6805e-38, 1.7856e-32, 4.3381e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.4275e-32, 2.0550e-34, 1.6984e-42, 0.0000e+00, 0.0000e+00,\n",
      "         2.3574e-37, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.04986\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1141.5114\n",
      "Test epoch : 2 Average loss: 1156.7321\n",
      "PP(train) = 4074.642, PP(valid) = 4027.555\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.7841e-32, 2.1303e-13, 1.5480e-22, 4.1029e-13, 0.0000e+00,\n",
      "         0.0000e+00, 9.7562e-14, 6.7505e-24, 1.2910e-31, 7.7205e-39, 1.2612e-44,\n",
      "         6.7691e-17, 2.0965e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.04497\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1141.0504\n",
      "Test epoch : 3 Average loss: 1156.2866\n",
      "PP(train) = 4058.961, PP(valid) = 4014.521\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2990e-35, 6.5982e-13, 2.3243e-22, 1.0539e-11, 0.0000e+00,\n",
      "         1.4013e-44, 4.8616e-14, 4.0142e-16, 4.0854e-32, 1.3765e-40, 7.0191e-32,\n",
      "         1.1601e-12, 1.3160e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.04055\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1140.4966\n",
      "Test epoch : 4 Average loss: 1155.8412\n",
      "PP(train) = 4042.898, PP(valid) = 4001.471\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7613e-26, 5.4546e-30, 3.2197e-27, 5.7697e-17, 0.0000e+00,\n",
      "         0.0000e+00, 1.3294e-22, 6.5428e-18, 1.5246e-42, 7.0065e-45, 9.9378e-39,\n",
      "         3.1052e-19, 1.2376e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.03658\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1139.9611\n",
      "Test epoch : 5 Average loss: 1155.3912\n",
      "PP(train) = 4026.729, PP(valid) = 3988.464\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.9927e-26, 3.3796e-21, 1.1990e-30, 2.2851e-13, 0.0000e+00,\n",
      "         4.4847e-38, 5.2841e-06, 8.1451e-20, 1.5396e-33, 8.9069e-34, 1.8124e-31,\n",
      "         5.7650e-23, 6.7200e-27, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.03299\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1139.4559\n",
      "Test epoch : 6 Average loss: 1154.9415\n",
      "PP(train) = 4010.740, PP(valid) = 3975.700\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.3248e-39, 2.8639e-26, 2.5143e-31, 1.7471e-03, 2.0536e-35,\n",
      "         0.0000e+00, 3.9406e-19, 1.0520e-07, 5.7023e-25, 4.5921e-36, 7.0485e-43,\n",
      "         3.7620e-17, 1.0466e-24, 9.9825e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0601, 0.0669, 0.1076, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02975\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1138.9407\n",
      "Test epoch : 7 Average loss: 1154.4943\n",
      "PP(train) = 3994.680, PP(valid) = 3962.962\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1377e-40, 3.3653e-26, 3.1641e-29, 6.8499e-14, 5.7404e-40,\n",
      "         0.0000e+00, 1.8369e-12, 1.0332e-26, 1.2478e-30, 0.0000e+00, 7.5295e-38,\n",
      "         1.3936e-16, 2.3563e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02683\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1138.2692\n",
      "Test epoch : 8 Average loss: 1154.0471\n",
      "PP(train) = 3978.807, PP(valid) = 3950.337\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.9776e-38, 6.9244e-39, 4.4408e-31, 0.0000e+00,\n",
      "         0.0000e+00, 6.4502e-33, 6.9633e-34, 9.6742e-37, 0.0000e+00, 0.0000e+00,\n",
      "         2.7526e-32, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02420\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1137.7578\n",
      "Test epoch : 9 Average loss: 1153.6014\n",
      "PP(train) = 3963.306, PP(valid) = 3938.089\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4776e-37, 1.0893e-32, 1.2766e-38, 5.8834e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.0526e-25, 4.0406e-27, 1.2877e-41, 0.0000e+00, 1.7937e-43,\n",
      "         4.6268e-40, 5.5406e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02183\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1137.1489\n",
      "Test epoch : 10 Average loss: 1153.1551\n",
      "PP(train) = 3947.821, PP(valid) = 3925.790\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7309e-35, 2.0231e-35, 9.3157e-37, 4.8063e-23, 8.1275e-44,\n",
      "         0.0000e+00, 1.8711e-09, 1.0859e-19, 5.9579e-35, 0.0000e+00, 5.6672e-36,\n",
      "         6.7957e-25, 9.5088e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01969\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1136.6363\n",
      "Test epoch : 11 Average loss: 1152.7129\n",
      "PP(train) = 3932.070, PP(valid) = 3913.156\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.0939e-27, 6.5693e-40, 1.0954e-18, 0.0000e+00,\n",
      "         1.4013e-45, 1.0535e-12, 1.8338e-19, 0.0000e+00, 0.0000e+00, 3.5312e-37,\n",
      "         1.3294e-24, 1.5512e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01776\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1135.8949\n",
      "Test epoch : 12 Average loss: 1152.2710\n",
      "PP(train) = 3916.654, PP(valid) = 3900.923\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4379e-30, 4.5996e-38, 9.7488e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.0550e-23, 1.7293e-30, 5.6042e-36, 0.0000e+00, 0.0000e+00,\n",
      "         1.3764e-25, 4.5768e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01601\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1135.5128\n",
      "Test epoch : 13 Average loss: 1151.8357\n",
      "PP(train) = 3901.314, PP(valid) = 3888.696\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8382e-22, 1.5044e-11, 3.5778e-24, 6.9209e-12, 8.9515e-42,\n",
      "         0.0000e+00, 1.1917e-14, 2.3206e-18, 3.2796e-19, 2.0506e-39, 1.4476e-34,\n",
      "         3.6338e-15, 1.0941e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01444\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1135.0261\n",
      "Test epoch : 14 Average loss: 1151.4014\n",
      "PP(train) = 3886.156, PP(valid) = 3876.584\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.0751e-32, 0.0000e+00, 1.1952e-20, 0.0000e+00,\n",
      "         0.0000e+00, 2.4612e-26, 2.6613e-28, 1.4928e-39, 0.0000e+00, 6.8664e-44,\n",
      "         1.3692e-25, 8.0361e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01303\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1134.4250\n",
      "Test epoch : 15 Average loss: 1150.9697\n",
      "PP(train) = 3871.057, PP(valid) = 3864.510\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6816e-42, 1.0767e-31, 4.0885e-30, 2.0143e-30, 0.0000e+00,\n",
      "         0.0000e+00, 6.2008e-22, 7.7471e-25, 4.4271e-29, 0.0000e+00, 0.0000e+00,\n",
      "         2.3633e-22, 3.6320e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01175\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1133.9459\n",
      "Test epoch : 16 Average loss: 1150.5412\n",
      "PP(train) = 3855.932, PP(valid) = 3852.378\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.8473e-44, 3.9492e-28, 2.3476e-39, 2.1249e-13, 0.0000e+00,\n",
      "         0.0000e+00, 5.4955e-10, 4.5715e-20, 1.0200e-29, 0.0000e+00, 4.4977e-31,\n",
      "         1.4728e-13, 3.0676e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01060\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1133.4027\n",
      "Test epoch : 17 Average loss: 1150.1116\n",
      "PP(train) = 3841.079, PP(valid) = 3840.478\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 1.3737e-22, 2.9550e-34, 1.8347e-22, 0.0000e+00,\n",
      "         0.0000e+00, 6.4967e-24, 1.7896e-24, 1.1895e-30, 0.0000e+00, 0.0000e+00,\n",
      "         9.1038e-23, 1.0094e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00956\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1132.8636\n",
      "Test epoch : 18 Average loss: 1149.6854\n",
      "PP(train) = 3826.545, PP(valid) = 3828.831\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.4393e-41, 1.5864e-27, 9.4406e-34, 1.7019e-15, 0.0000e+00,\n",
      "         0.0000e+00, 7.4951e-18, 5.0368e-22, 8.7611e-37, 5.2784e-41, 4.2422e-40,\n",
      "         6.8904e-26, 5.8168e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00862\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1132.4179\n",
      "Test epoch : 19 Average loss: 1149.2593\n",
      "PP(train) = 3812.154, PP(valid) = 3817.291\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.3871e-39, 1.2045e-30, 2.4331e-33, 9.9767e-18, 0.0000e+00,\n",
      "         0.0000e+00, 5.3132e-15, 6.6962e-22, 6.4283e-29, 8.0081e-41, 1.8174e-30,\n",
      "         9.7418e-10, 4.5052e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00778\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1131.7056\n",
      "Test epoch : 20 Average loss: 1148.8359\n",
      "PP(train) = 3798.016, PP(valid) = 3805.990\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.7184e-31, 3.7617e-29, 1.0329e-23, 0.0000e+00,\n",
      "         0.0000e+00, 1.1446e-11, 1.1498e-19, 2.8026e-45, 8.1294e-38, 1.6395e-42,\n",
      "         3.4700e-21, 2.2727e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00701\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1131.2569\n",
      "Test epoch : 21 Average loss: 1148.4132\n",
      "PP(train) = 3783.844, PP(valid) = 3794.610\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7090e-36, 3.7858e-22, 3.2288e-37, 1.3530e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.6549e-25, 1.7169e-22, 1.7972e-37, 0.0000e+00, 4.0851e-38,\n",
      "         1.0591e-20, 2.7787e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00633\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1130.7532\n",
      "Test epoch : 22 Average loss: 1147.9915\n",
      "PP(train) = 3769.484, PP(valid) = 3782.975\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.7275e-43, 6.8328e-22, 4.2576e-30, 3.0135e-33, 0.0000e+00,\n",
      "         0.0000e+00, 7.9431e-27, 1.4919e-27, 1.5607e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.1925e-25, 5.5938e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00571\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1130.2353\n",
      "Test epoch : 23 Average loss: 1147.5742\n",
      "PP(train) = 3754.996, PP(valid) = 3771.210\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0274e-33, 2.4205e-19, 9.0802e-25, 9.2652e-12, 0.0000e+00,\n",
      "         2.4803e-43, 7.8955e-08, 4.2554e-24, 3.7013e-24, 1.4013e-45, 1.7038e-37,\n",
      "         5.6251e-24, 2.4477e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00515\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1129.8324\n",
      "Test epoch : 24 Average loss: 1147.1588\n",
      "PP(train) = 3741.228, PP(valid) = 3760.139\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4645e-31, 4.3440e-44, 6.4177e-26, 0.0000e+00,\n",
      "         0.0000e+00, 2.8815e-34, 3.1392e-27, 3.2865e-40, 0.0000e+00, 1.5414e-44,\n",
      "         1.6050e-32, 5.1848e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00464\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1129.2903\n",
      "Test epoch : 25 Average loss: 1146.7470\n",
      "PP(train) = 3727.449, PP(valid) = 3749.042\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.0694e-22, 3.5938e-39, 4.7978e-23, 2.4422e-41,\n",
      "         0.0000e+00, 6.5105e-12, 3.3065e-26, 1.0891e-35, 0.0000e+00, 0.0000e+00,\n",
      "         5.1129e-23, 2.3221e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00419\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1128.7510\n",
      "Test epoch : 26 Average loss: 1146.3333\n",
      "PP(train) = 3713.735, PP(valid) = 3737.911\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6169e-37, 4.5395e-13, 3.5846e-30, 2.9944e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.0225e-03, 1.3906e-20, 3.8163e-23, 4.2039e-45, 4.8650e-41,\n",
      "         4.0668e-22, 8.8921e-35, 9.9898e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00378\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1128.1973\n",
      "Test epoch : 27 Average loss: 1145.9216\n",
      "PP(train) = 3700.177, PP(valid) = 3726.894\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 1.8410e-32, 1.2655e-28, 1.4749e-17, 0.0000e+00,\n",
      "         0.0000e+00, 1.0850e-25, 9.9217e-26, 4.1803e-39, 1.0300e-42, 1.1817e-38,\n",
      "         7.1566e-24, 8.6375e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00341\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1127.7812\n",
      "Test epoch : 28 Average loss: 1145.5135\n",
      "PP(train) = 3686.532, PP(valid) = 3715.832\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.2550e-42, 3.7327e-24, 9.3567e-36, 2.3696e-10, 1.4013e-45,\n",
      "         0.0000e+00, 1.0655e-15, 2.6299e-13, 3.3808e-25, 0.0000e+00, 1.1977e-39,\n",
      "         1.7431e-17, 1.2829e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00307\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1127.1992\n",
      "Test epoch : 29 Average loss: 1145.1048\n",
      "PP(train) = 3673.487, PP(valid) = 3705.312\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.0522e-42, 4.9987e-23, 1.1848e-12, 4.2425e-22, 0.0000e+00,\n",
      "         2.4903e-39, 4.3760e-16, 6.4372e-12, 2.1848e-30, 0.0000e+00, 1.5377e-34,\n",
      "         2.0977e-15, 1.7916e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00277\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1126.7436\n",
      "Test epoch : 30 Average loss: 1144.6977\n",
      "PP(train) = 3660.359, PP(valid) = 3694.669\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 4.8611e-42, 3.3940e-28, 2.4509e-29, 2.5275e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.9387e-20, 9.4601e-17, 1.0021e-30, 0.0000e+00, 2.5223e-44,\n",
      "         5.9059e-32, 5.6439e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1126.2616\n",
      "Test epoch : 31 Average loss: 1144.2938\n",
      "PP(train) = 3647.188, PP(valid) = 3683.998\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.0465e-35, 7.1215e-39, 3.5326e-15, 0.0000e+00,\n",
      "         0.0000e+00, 1.9263e-21, 1.6707e-26, 6.3623e-38, 0.0000e+00, 0.0000e+00,\n",
      "         1.3389e-24, 3.0940e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00226\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1125.6290\n",
      "Test epoch : 32 Average loss: 1143.8916\n",
      "PP(train) = 3634.136, PP(valid) = 3673.385\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6536e-34, 6.3279e-26, 5.1631e-36, 1.4979e-20, 0.0000e+00,\n",
      "         0.0000e+00, 9.2359e-26, 9.6360e-19, 6.2673e-25, 0.0000e+00, 0.0000e+00,\n",
      "         8.5715e-32, 5.8014e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00203\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1125.1765\n",
      "Test epoch : 33 Average loss: 1143.4878\n",
      "PP(train) = 3621.149, PP(valid) = 3662.824\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.6052e-45, 9.8118e-21, 1.1203e-26, 1.1508e-19, 0.0000e+00,\n",
      "         4.9466e-43, 9.1712e-18, 3.1046e-21, 2.6580e-34, 6.9356e-41, 9.9588e-36,\n",
      "         4.4143e-19, 1.9310e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00183\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1124.8768\n",
      "Test epoch : 34 Average loss: 1143.0882\n",
      "PP(train) = 3608.301, PP(valid) = 3652.360\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.0406e-32, 2.1450e-20, 4.3910e-28, 1.9389e-11, 0.0000e+00,\n",
      "         2.3675e-41, 3.1538e-07, 5.1947e-12, 4.4520e-21, 9.3200e-42, 1.4379e-26,\n",
      "         1.0661e-11, 1.6019e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00166\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1124.3333\n",
      "Test epoch : 35 Average loss: 1142.6947\n",
      "PP(train) = 3595.543, PP(valid) = 3641.957\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2650e-37, 2.9820e-42, 2.0830e-04, 7.4790e-15, 3.0897e-10, 8.9683e-44,\n",
      "         9.6690e-44, 5.0199e-09, 1.8085e-14, 4.8280e-41, 4.5269e-40, 1.2811e-37,\n",
      "         1.3410e-09, 1.3634e-29, 9.9979e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00149\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1123.8200\n",
      "Test epoch : 36 Average loss: 1142.3022\n",
      "PP(train) = 3582.762, PP(valid) = 3631.545\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2017e-38, 4.2242e-16, 7.6381e-17, 2.8160e-16, 5.1428e-43,\n",
      "         2.0319e-43, 4.6429e-08, 3.0616e-10, 1.2897e-31, 1.9618e-44, 2.5793e-25,\n",
      "         1.1468e-19, 6.9739e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00135\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1123.4263\n",
      "Test epoch : 37 Average loss: 1141.9071\n",
      "PP(train) = 3569.880, PP(valid) = 3620.966\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2489e-35, 3.0184e-35, 8.4874e-29, 3.7685e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.4706e-21, 5.9627e-26, 5.5879e-32, 0.0000e+00, 9.2245e-36,\n",
      "         1.6615e-19, 4.9515e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00121\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1122.8098\n",
      "Test epoch : 38 Average loss: 1141.5170\n",
      "PP(train) = 3557.713, PP(valid) = 3611.062\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8080e-28, 1.0472e-21, 2.9175e-33, 4.4744e-11, 7.5670e-44,\n",
      "         0.0000e+00, 5.1299e-09, 2.0975e-10, 1.4764e-21, 5.1288e-43, 2.8553e-33,\n",
      "         5.3047e-10, 3.1983e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00110\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1122.3770\n",
      "Test epoch : 39 Average loss: 1141.1265\n",
      "PP(train) = 3545.621, PP(valid) = 3601.250\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1561e-38, 2.9077e-23, 3.1060e-22, 1.3480e-21, 0.0000e+00,\n",
      "         7.0065e-45, 4.1638e-08, 1.4519e-18, 6.7196e-33, 0.0000e+00, 5.3203e-36,\n",
      "         8.7020e-24, 1.1033e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00099\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1121.8891\n",
      "Test epoch : 40 Average loss: 1140.7378\n",
      "PP(train) = 3533.375, PP(valid) = 3591.219\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.6179e-31, 2.0141e-23, 6.7663e-23, 2.9562e-22, 0.0000e+00,\n",
      "         0.0000e+00, 3.3549e-08, 4.1096e-16, 9.9189e-40, 0.0000e+00, 5.9981e-41,\n",
      "         4.3853e-27, 3.0829e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00089\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1121.3937\n",
      "Test epoch : 41 Average loss: 1140.3502\n",
      "PP(train) = 3521.076, PP(valid) = 3581.140\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7763e-26, 2.9928e-26, 4.9979e-21, 2.4545e-18, 0.0000e+00,\n",
      "         0.0000e+00, 7.0409e-11, 3.1810e-15, 5.8929e-20, 0.0000e+00, 1.3368e-42,\n",
      "         9.9114e-17, 2.6350e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00080\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1120.9194\n",
      "Test epoch : 42 Average loss: 1139.9676\n",
      "PP(train) = 3508.915, PP(valid) = 3571.212\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3431e-33, 2.7371e-12, 6.6408e-23, 9.5674e-13, 2.2103e-38,\n",
      "         1.9189e-28, 9.9952e-01, 4.7904e-04, 2.0732e-35, 2.2240e-38, 1.7150e-28,\n",
      "         2.6234e-16, 1.7342e-23, 2.0019e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0443, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00073\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1120.5379\n",
      "Test epoch : 43 Average loss: 1139.5860\n",
      "PP(train) = 3496.932, PP(valid) = 3561.362\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7852e-41, 6.3813e-35, 0.0000e+00,\n",
      "         0.0000e+00, 8.2041e-32, 1.5960e-30, 8.4078e-45, 0.0000e+00, 0.0000e+00,\n",
      "         4.1638e-29, 2.0439e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00065\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1119.8580\n",
      "Test epoch : 44 Average loss: 1139.2021\n",
      "PP(train) = 3485.262, PP(valid) = 3551.824\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3561e-42, 1.4925e-32, 9.7424e-40, 5.1067e-21, 1.9158e-38,\n",
      "         0.0000e+00, 9.3267e-07, 6.0317e-23, 4.8154e-34, 0.0000e+00, 7.5810e-43,\n",
      "         6.6730e-42, 2.8830e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00059\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1119.5832\n",
      "Test epoch : 45 Average loss: 1138.8212\n",
      "PP(train) = 3473.374, PP(valid) = 3542.059\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0069e-21, 1.1430e-11, 1.3873e-14, 2.1082e-04, 4.2039e-45,\n",
      "         8.1135e-33, 8.6706e-02, 3.8911e-08, 2.1753e-13, 3.1189e-33, 1.0688e-25,\n",
      "         6.2127e-03, 6.4353e-22, 9.0687e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0823, 0.0484, 0.0992, 0.0501, 0.0619, 0.0694, 0.1049, 0.0872, 0.0593,\n",
      "         0.0514, 0.0661, 0.0549, 0.0500, 0.0732, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00053\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1119.0993\n",
      "Test epoch : 46 Average loss: 1138.4457\n",
      "PP(train) = 3461.393, PP(valid) = 3532.189\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2049e-26, 1.8895e-34, 2.0563e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.0684e-25, 3.0625e-30, 4.9860e-39, 0.0000e+00, 0.0000e+00,\n",
      "         2.3243e-25, 1.5743e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00048\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1118.6697\n",
      "Test epoch : 47 Average loss: 1138.0690\n",
      "PP(train) = 3449.698, PP(valid) = 3522.584\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.8255e-43, 2.5193e-33, 1.3710e-38, 5.2602e-26, 0.0000e+00,\n",
      "         0.0000e+00, 4.0943e-26, 3.1192e-24, 0.0000e+00, 0.0000e+00, 9.8091e-45,\n",
      "         3.5668e-27, 1.9115e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00043\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1118.0627\n",
      "Test epoch : 48 Average loss: 1137.6890\n",
      "PP(train) = 3438.399, PP(valid) = 3513.352\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9447e-32, 2.2388e-17, 1.2193e-36, 1.4288e-15, 1.4013e-45,\n",
      "         0.0000e+00, 9.9221e-23, 1.7211e-12, 4.2305e-25, 0.0000e+00, 9.8213e-31,\n",
      "         2.1215e-25, 3.6777e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00039\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1117.6770\n",
      "Test epoch : 49 Average loss: 1137.3153\n",
      "PP(train) = 3427.170, PP(valid) = 3504.149\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7656e-43, 9.6343e-27, 4.1303e-27, 1.9957e-19, 0.0000e+00,\n",
      "         0.0000e+00, 2.6579e-11, 6.9905e-16, 3.1566e-26, 5.0513e-41, 5.9292e-37,\n",
      "         8.6015e-21, 4.9633e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00035\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1117.1393\n",
      "Test epoch : 50 Average loss: 1136.9431\n",
      "PP(train) = 3415.792, PP(valid) = 3494.765\n",
      "Writing to ./topicwords/7-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 10.4299 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                  Field Programmable Gate Array   \n",
      "\n",
      "===== # 2, Topic : 14, p : 10.9315 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                 \n",
      "\n",
      "===== # 3, Topic : 14, p : 10.9263 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                            sec                                  IIIIII                                                                                                        \n",
      "\n",
      "===== # 4, Topic : 14, p : 9.7256 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                       \n",
      "\n",
      "===== # 5, Topic : 14, p : 10.9066 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                      \n",
      "\n",
      "===== # 6, Topic : 14, p : 9.7762 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                   \n",
      "\n",
      "===== # 7, Topic : 14, p : 10.2412 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                          \n",
      "\n",
      "===== # 8, Topic : 14, p : 11.9310 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                           \n",
      "\n",
      "===== # 9, Topic : 14, p : 10.3756 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                      \n",
      "\n",
      "===== # 10, Topic : 14, p : 9.2190 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                         mm                          Vol   PP    NEXCO     o           i                                    (              JSCE  mm                                G                   ](             dtex                                 F                                         JHS  vol      A B       mg     kg               N  \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9892e-41, 4.4228e-32, 4.0360e-40, 1.2117e-30, 0.0000e+00,\n",
      "         1.7264e-42, 8.0344e-24, 4.1532e-22, 6.4007e-38, 0.0000e+00, 1.1559e-41,\n",
      "         7.5967e-15, 9.8091e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00032\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1041.9314\n",
      "Test epoch : 1 Average loss: 929.7918\n",
      "PP(train) = 3935.589, PP(valid) = 3788.867\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.8024e-41, 3.7490e-30, 1.3372e-25, 3.9327e-23, 9.0396e-40,\n",
      "         4.4001e-43, 1.3721e-23, 4.8622e-14, 9.9764e-33, 0.0000e+00, 4.8181e-32,\n",
      "         3.8118e-12, 4.6954e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00029\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1041.4239\n",
      "Test epoch : 2 Average loss: 929.4841\n",
      "PP(train) = 3924.255, PP(valid) = 3779.834\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 1.7698e-21, 5.2700e-25, 1.9406e-23, 6.1657e-44,\n",
      "         2.0630e-41, 3.6380e-18, 7.8626e-16, 2.8107e-35, 1.1925e-42, 1.0537e-40,\n",
      "         2.6865e-24, 6.2912e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00026\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1041.0221\n",
      "Test epoch : 3 Average loss: 929.1868\n",
      "PP(train) = 3912.044, PP(valid) = 3770.614\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9951e-35, 7.4822e-37, 9.7493e-20, 4.9117e-12, 2.7322e-13, 1.0336e-36,\n",
      "         2.8466e-36, 2.5026e-13, 9.9823e-01, 1.1226e-28, 1.6384e-33, 3.6501e-27,\n",
      "         1.1004e-13, 5.1968e-25, 1.7653e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0443, 0.0628, 0.0304, 0.0760, 0.0630, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00023\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1040.6924\n",
      "Test epoch : 4 Average loss: 928.8858\n",
      "PP(train) = 3898.569, PP(valid) = 3760.522\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.4078e-44, 2.7750e-24, 2.4614e-34, 1.3489e-23, 1.4705e-37,\n",
      "         0.0000e+00, 1.2127e-22, 7.4519e-27, 5.9412e-38, 0.0000e+00, 2.3455e-33,\n",
      "         1.1436e-26, 1.0225e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00021\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1040.2175\n",
      "Test epoch : 5 Average loss: 928.5874\n",
      "PP(train) = 3884.433, PP(valid) = 3749.986\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.1657e-44, 2.1168e-28, 1.8608e-36, 1.0792e-15, 3.9797e-43,\n",
      "         1.6792e-39, 1.1612e-17, 1.8725e-19, 3.9626e-31, 4.1338e-42, 7.4633e-42,\n",
      "         1.5524e-20, 1.7942e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00019\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1039.7884\n",
      "Test epoch : 6 Average loss: 928.2911\n",
      "PP(train) = 3870.422, PP(valid) = 3739.658\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.4360e-38, 5.2885e-18, 6.3188e-29, 1.3752e-28, 2.5927e-34,\n",
      "         0.0000e+00, 1.1398e-15, 2.3593e-25, 2.2329e-22, 5.6552e-41, 2.4092e-39,\n",
      "         2.7185e-29, 8.3167e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00017\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1039.4277\n",
      "Test epoch : 7 Average loss: 927.9920\n",
      "PP(train) = 3856.752, PP(valid) = 3729.618\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7980e-42, 4.3197e-34, 7.3084e-29, 9.1478e-20, 4.2263e-14, 1.0213e-36,\n",
      "         5.4455e-37, 6.4231e-17, 2.9077e-14, 5.3627e-21, 8.4078e-45, 3.4301e-37,\n",
      "         1.1842e-19, 8.0234e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00015\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1039.0652\n",
      "Test epoch : 8 Average loss: 927.6932\n",
      "PP(train) = 3843.354, PP(valid) = 3719.796\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1862e-37, 4.0150e-20, 2.3125e-24, 4.5197e-06, 9.0353e-37,\n",
      "         7.7071e-44, 5.7722e-17, 4.2877e-17, 8.2691e-26, 3.3458e-27, 1.8376e-32,\n",
      "         1.7199e-20, 3.0703e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00014\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1038.5602\n",
      "Test epoch : 9 Average loss: 927.3976\n",
      "PP(train) = 3830.047, PP(valid) = 3710.078\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5929e-30, 1.1435e-29, 1.5291e-21, 6.0954e-10, 6.9193e-40,\n",
      "         0.0000e+00, 6.3288e-15, 2.2267e-21, 5.2592e-32, 8.4078e-45, 3.0972e-39,\n",
      "         2.0055e-24, 1.3284e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00013\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1038.0093\n",
      "Test epoch : 10 Average loss: 927.0997\n",
      "PP(train) = 3816.844, PP(valid) = 3700.424\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.4454e-35, 3.3579e-23, 2.1717e-35, 6.2014e-16, 1.1859e-41,\n",
      "         1.2996e-41, 6.1572e-19, 1.0344e-15, 2.2201e-32, 3.7604e-39, 1.9315e-27,\n",
      "         2.1426e-20, 8.8503e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00011\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1037.6292\n",
      "Test epoch : 11 Average loss: 926.8069\n",
      "PP(train) = 3803.562, PP(valid) = 3690.669\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0065e-45, 2.0319e-42, 2.0484e-35, 1.1385e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.5860e-40, 4.2693e-23, 2.2188e-41, 0.0000e+00, 2.6576e-41,\n",
      "         9.2282e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00010\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1037.1755\n",
      "Test epoch : 12 Average loss: 926.5115\n",
      "PP(train) = 3790.710, PP(valid) = 3681.241\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.6188e-37, 1.0167e-17, 7.7000e-24, 8.7755e-23, 0.0000e+00,\n",
      "         6.6141e-43, 1.1068e-21, 5.0815e-21, 4.2904e-18, 1.5022e-42, 1.3342e-30,\n",
      "         1.7079e-20, 1.8793e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00009\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1036.7533\n",
      "Test epoch : 13 Average loss: 926.2178\n",
      "PP(train) = 3778.061, PP(valid) = 3671.914\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7252e-40, 1.5199e-28, 5.7733e-43, 8.6748e-23, 0.0000e+00,\n",
      "         8.4078e-45, 4.0335e-14, 1.4974e-21, 2.0879e-43, 1.4013e-45, 0.0000e+00,\n",
      "         1.9906e-31, 2.9130e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00008\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1036.2821\n",
      "Test epoch : 14 Average loss: 925.9245\n",
      "PP(train) = 3765.432, PP(valid) = 3662.657\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.4663e-20, 2.4264e-31, 1.1888e-19, 7.7071e-44,\n",
      "         4.2039e-45, 4.7179e-22, 5.5090e-16, 1.0291e-22, 0.0000e+00, 7.2559e-42,\n",
      "         2.0113e-21, 3.9164e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00008\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1035.8071\n",
      "Test epoch : 15 Average loss: 925.6335\n",
      "PP(train) = 3752.777, PP(valid) = 3653.287\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.2009e-35, 1.4682e-33, 7.6339e-37, 2.8542e-25, 4.7632e-41,\n",
      "         9.8091e-45, 1.1157e-14, 1.1771e-22, 1.0681e-29, 0.0000e+00, 8.6372e-41,\n",
      "         4.5846e-26, 2.2378e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00007\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1035.4461\n",
      "Test epoch : 16 Average loss: 925.3482\n",
      "PP(train) = 3739.879, PP(valid) = 3643.701\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2521e-39, 8.3381e-18, 9.8339e-23, 7.8018e-30, 1.3333e-16, 4.6383e-43,\n",
      "         1.3499e-34, 5.7112e-23, 1.4306e-07, 2.9302e-15, 1.5013e-37, 5.8642e-32,\n",
      "         1.4103e-16, 2.3035e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00006\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1035.1437\n",
      "Test epoch : 17 Average loss: 925.0660\n",
      "PP(train) = 3727.168, PP(valid) = 3634.199\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.6291e-41, 2.9050e-32, 2.3247e-14, 3.7318e-25, 1.9150e-09, 2.0522e-28,\n",
      "         2.6927e-40, 1.4230e-05, 4.9876e-12, 3.4145e-27, 4.3100e-40, 9.6511e-28,\n",
      "         6.2119e-19, 9.9886e-21, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00006\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1034.6375\n",
      "Test epoch : 18 Average loss: 924.7853\n",
      "PP(train) = 3714.812, PP(valid) = 3625.040\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7733e-30, 4.5902e-18, 1.9734e-23, 2.1105e-15, 4.0397e-39,\n",
      "         2.7185e-43, 6.8328e-06, 4.2086e-15, 1.6341e-29, 2.8713e-42, 5.6377e-24,\n",
      "         1.3539e-18, 1.5287e-23, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00005\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1034.2071\n",
      "Test epoch : 19 Average loss: 924.4982\n",
      "PP(train) = 3702.913, PP(valid) = 3616.269\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3053e-37, 1.3047e-24, 7.5125e-24, 3.5262e-12, 1.8606e-36,\n",
      "         0.0000e+00, 2.0331e-14, 4.5648e-33, 1.4578e-28, 7.5957e-33, 1.2892e-43,\n",
      "         2.3588e-24, 1.1387e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1033.8089\n",
      "Test epoch : 20 Average loss: 924.2128\n",
      "PP(train) = 3690.986, PP(valid) = 3607.372\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.8858e-28, 1.2969e-28, 2.2186e-19, 2.7381e-28, 0.0000e+00,\n",
      "         1.2250e-28, 1.1078e-17, 1.8015e-09, 3.0441e-27, 1.2212e-35, 1.1551e-27,\n",
      "         2.6174e-25, 8.4808e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1033.4492\n",
      "Test epoch : 21 Average loss: 923.9335\n",
      "PP(train) = 3678.823, PP(valid) = 3598.284\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.6690e-43, 6.3278e-31, 1.7718e-22, 6.7068e-25, 2.9377e-20, 3.3481e-38,\n",
      "         3.2230e-44, 1.2831e-25, 1.0251e-13, 7.8380e-21, 3.5453e-42, 1.9388e-33,\n",
      "         2.2882e-13, 2.7672e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1033.0264\n",
      "Test epoch : 22 Average loss: 923.6542\n",
      "PP(train) = 3666.753, PP(valid) = 3589.178\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.4460e-44, 3.9259e-29, 2.5697e-33, 3.5369e-31, 0.0000e+00,\n",
      "         0.0000e+00, 5.3923e-29, 7.3787e-18, 1.3730e-36, 0.0000e+00, 0.0000e+00,\n",
      "         4.7954e-28, 1.5701e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00003\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1032.5360\n",
      "Test epoch : 23 Average loss: 923.3761\n",
      "PP(train) = 3654.921, PP(valid) = 3580.337\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6044e-38, 3.2006e-38, 5.5760e-23, 6.0582e-20, 3.1194e-06, 2.8306e-43,\n",
      "         2.6070e-36, 2.7871e-19, 6.2949e-11, 2.0808e-32, 1.9618e-44, 7.9889e-28,\n",
      "         3.9552e-10, 1.1569e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00003\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1032.1629\n",
      "Test epoch : 24 Average loss: 923.0962\n",
      "PP(train) = 3643.381, PP(valid) = 3571.695\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 2.2103e-34, 5.2749e-19, 9.2388e-28, 1.9231e-25, 1.9980e-35,\n",
      "         1.1092e-40, 3.1539e-08, 6.0021e-17, 5.9420e-25, 2.5010e-40, 9.7380e-34,\n",
      "         1.2399e-19, 3.1438e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00003\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1031.7523\n",
      "Test epoch : 25 Average loss: 922.8180\n",
      "PP(train) = 3631.936, PP(valid) = 3563.156\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2509e-39, 6.2259e-35, 1.0905e-18, 1.6930e-30, 1.0000e+00, 7.9061e-28,\n",
      "         1.2618e-28, 3.9244e-12, 7.7060e-19, 6.8859e-21, 1.8913e-40, 2.6097e-28,\n",
      "         4.2495e-21, 1.9486e-16, 1.5507e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1031.4475\n",
      "Test epoch : 26 Average loss: 922.5453\n",
      "PP(train) = 3619.986, PP(valid) = 3554.088\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0065e-45, 1.9315e-31, 1.9483e-24, 9.7223e-15, 0.0000e+00,\n",
      "         3.2048e-37, 7.5434e-22, 1.2069e-11, 2.5447e-31, 7.3916e-21, 5.7224e-40,\n",
      "         3.1519e-19, 3.8226e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1030.9662\n",
      "Test epoch : 27 Average loss: 922.2758\n",
      "PP(train) = 3608.284, PP(valid) = 3545.249\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1420e-38, 1.6782e-28, 2.8908e-34, 1.1889e-25, 7.9033e-43,\n",
      "         0.0000e+00, 3.9281e-11, 1.1491e-16, 1.7519e-28, 6.4460e-44, 2.6402e-22,\n",
      "         1.3944e-16, 8.3083e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1030.5502\n",
      "Test epoch : 28 Average loss: 922.0011\n",
      "PP(train) = 3597.125, PP(valid) = 3536.901\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3028e-39, 3.7145e-33, 1.5389e-39, 1.2391e-25, 0.0000e+00,\n",
      "         6.2918e-43, 5.8260e-18, 6.6495e-18, 3.1093e-32, 0.0000e+00, 9.5798e-41,\n",
      "         8.5375e-34, 3.5159e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1030.0837\n",
      "Test epoch : 29 Average loss: 921.7259\n",
      "PP(train) = 3586.188, PP(valid) = 3528.728\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4837e-41, 1.5171e-23, 3.9778e-32, 1.5291e-26, 0.0000e+00,\n",
      "         1.1210e-44, 9.0632e-16, 2.5725e-30, 1.0995e-33, 0.0000e+00, 4.2577e-39,\n",
      "         3.1969e-36, 2.8708e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1029.7048\n",
      "Test epoch : 30 Average loss: 921.4543\n",
      "PP(train) = 3575.073, PP(valid) = 3520.311\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5988e-41, 1.3823e-21, 3.6209e-19, 9.9156e-13, 2.0267e-34,\n",
      "         0.0000e+00, 7.9148e-23, 1.2833e-17, 1.7721e-19, 1.1279e-38, 9.6184e-34,\n",
      "         4.3247e-24, 4.1030e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1029.3659\n",
      "Test epoch : 31 Average loss: 921.1839\n",
      "PP(train) = 3563.975, PP(valid) = 3511.914\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2017e-27, 3.7461e-17, 1.7941e-23, 9.9959e-01, 2.6139e-33,\n",
      "         8.8133e-26, 5.1815e-15, 9.2255e-09, 2.7113e-19, 5.6906e-39, 1.0630e-25,\n",
      "         1.8463e-04, 2.3095e-25, 2.2062e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1028.9877\n",
      "Test epoch : 32 Average loss: 920.9174\n",
      "PP(train) = 3552.858, PP(valid) = 3503.491\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.9177e-42, 8.5083e-38, 2.4409e-19, 5.6910e-20, 1.5320e-12, 3.6851e-35,\n",
      "         6.7350e-40, 1.4914e-22, 2.4404e-08, 1.8282e-30, 1.6602e-37, 3.0375e-32,\n",
      "         9.9118e-10, 7.0542e-15, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1028.6048\n",
      "Test epoch : 33 Average loss: 920.6554\n",
      "PP(train) = 3541.812, PP(valid) = 3495.105\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5394e-33, 7.6681e-29, 1.3799e-14, 7.4388e-19, 1.0763e-19, 3.2381e-40,\n",
      "         2.1145e-36, 7.3598e-10, 1.6943e-17, 1.4018e-16, 1.9842e-42, 2.3649e-23,\n",
      "         2.6673e-15, 1.8243e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1028.2759\n",
      "Test epoch : 34 Average loss: 920.3937\n",
      "PP(train) = 3530.784, PP(valid) = 3486.752\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2738e-42, 6.9899e-39, 8.9185e-25, 7.3727e-22, 4.7953e-16, 6.2278e-29,\n",
      "         1.1066e-37, 1.6998e-14, 3.8269e-10, 4.4388e-22, 8.4498e-43, 9.7053e-28,\n",
      "         5.0626e-17, 3.2229e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1027.8872\n",
      "Test epoch : 35 Average loss: 920.1308\n",
      "PP(train) = 3519.871, PP(valid) = 3478.402\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9045e-44, 1.1943e-34, 3.3208e-15, 9.4981e-35, 5.6830e-21, 6.4674e-35,\n",
      "         1.2934e-40, 4.0212e-22, 3.0358e-14, 1.9541e-30, 3.7703e-40, 2.5937e-33,\n",
      "         2.9280e-14, 7.0506e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1027.4861\n",
      "Test epoch : 36 Average loss: 919.8657\n",
      "PP(train) = 3509.413, PP(valid) = 3470.443\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9071e-39, 1.8543e-35, 1.6917e-30, 4.5374e-24, 2.9021e-16, 5.4088e-29,\n",
      "         3.9584e-41, 9.9991e-01, 5.8249e-11, 1.6776e-17, 2.9938e-33, 1.0654e-34,\n",
      "         1.6601e-16, 3.8835e-13, 8.8947e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1027.1371\n",
      "Test epoch : 37 Average loss: 919.5995\n",
      "PP(train) = 3499.137, PP(valid) = 3462.698\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6621e-36, 4.8061e-25, 8.8855e-20, 5.2868e-12, 2.5171e-34,\n",
      "         0.0000e+00, 2.3175e-22, 5.6858e-10, 9.1217e-20, 8.5122e-41, 1.0662e-28,\n",
      "         3.9953e-07, 4.0747e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1026.6009\n",
      "Test epoch : 38 Average loss: 919.3345\n",
      "PP(train) = 3488.801, PP(valid) = 3454.847\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.3970e-40, 3.7031e-29, 8.6180e-23, 3.0342e-11, 1.6672e-34,\n",
      "         6.4889e-33, 1.5470e-22, 2.5924e-12, 9.8353e-22, 8.1861e-36, 4.1652e-27,\n",
      "         5.3968e-15, 2.4610e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1026.3690\n",
      "Test epoch : 39 Average loss: 919.0751\n",
      "PP(train) = 3478.293, PP(valid) = 3446.843\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2113e-34, 1.3200e-11, 1.2725e-13, 1.1206e-21, 2.0706e-38,\n",
      "         1.4013e-45, 7.9079e-26, 9.1631e-17, 6.4971e-35, 9.8091e-45, 5.5004e-39,\n",
      "         7.2803e-19, 1.7275e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1025.8950\n",
      "Test epoch : 40 Average loss: 918.8180\n",
      "PP(train) = 3467.883, PP(valid) = 3438.884\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4060e-42, 2.2647e-38, 2.1524e-17, 2.1457e-31, 2.1040e-23, 1.2587e-39,\n",
      "         2.4991e-33, 1.5628e-16, 1.6215e-09, 2.2033e-11, 1.7473e-34, 5.6803e-35,\n",
      "         4.1223e-08, 1.4562e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1025.5702\n",
      "Test epoch : 41 Average loss: 918.5581\n",
      "PP(train) = 3457.632, PP(valid) = 3431.042\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0930e-42, 7.9971e-28, 1.2576e-14, 1.0096e-21, 3.3511e-15, 5.7505e-40,\n",
      "         1.1438e-38, 1.2036e-18, 4.2224e-09, 2.3898e-29, 6.3364e-30, 2.3020e-34,\n",
      "         4.5647e-11, 1.2732e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1025.2033\n",
      "Test epoch : 42 Average loss: 918.3032\n",
      "PP(train) = 3447.484, PP(valid) = 3423.355\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0787e-38, 2.5787e-12, 1.0889e-21, 2.2718e-13, 3.3679e-33,\n",
      "         3.5873e-42, 3.2165e-10, 5.2431e-09, 9.8481e-18, 1.4013e-44, 8.1973e-36,\n",
      "         3.9969e-08, 3.9682e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1024.7328\n",
      "Test epoch : 43 Average loss: 918.0472\n",
      "PP(train) = 3437.304, PP(valid) = 3415.528\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.3387e-38, 3.0025e-26, 9.1454e-41, 2.3817e-21, 0.0000e+00,\n",
      "         2.8026e-44, 3.6047e-25, 3.4873e-18, 3.2140e-27, 7.5328e-41, 0.0000e+00,\n",
      "         7.9193e-22, 1.0576e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1024.4031\n",
      "Test epoch : 44 Average loss: 917.7923\n",
      "PP(train) = 3427.258, PP(valid) = 3407.853\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7930e-41, 4.1124e-18, 3.2221e-20, 1.1264e-17, 3.3975e-35,\n",
      "         1.3048e-40, 5.2794e-10, 2.4087e-08, 1.2418e-33, 1.7376e-42, 6.6922e-40,\n",
      "         2.1349e-16, 2.5559e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1024.0390\n",
      "Test epoch : 45 Average loss: 917.5399\n",
      "PP(train) = 3417.271, PP(valid) = 3400.191\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9703e-40, 1.7497e-39, 7.8808e-21, 3.0422e-17, 1.2711e-17, 2.9944e-30,\n",
      "         1.0426e-31, 8.9956e-08, 9.9003e-05, 3.1651e-21, 2.6208e-30, 4.8109e-22,\n",
      "         1.7144e-05, 2.2333e-17, 9.9988e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1023.7344\n",
      "Test epoch : 46 Average loss: 917.2886\n",
      "PP(train) = 3407.281, PP(valid) = 3392.512\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8026e-45, 1.7391e-34, 5.9839e-20, 1.5669e-25, 1.6452e-12, 6.6387e-37,\n",
      "         1.3722e-25, 1.0046e-07, 4.7621e-21, 1.0091e-20, 1.0981e-40, 6.1380e-28,\n",
      "         5.8327e-16, 7.9695e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1023.4287\n",
      "Test epoch : 47 Average loss: 917.0410\n",
      "PP(train) = 3397.410, PP(valid) = 3384.972\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2579e-41, 4.8004e-27, 1.9101e-24, 2.0091e-19, 1.2612e-44,\n",
      "         3.7415e-43, 6.3146e-18, 9.0619e-16, 2.0700e-35, 3.3080e-36, 2.1876e-40,\n",
      "         2.9742e-22, 5.9966e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1022.9427\n",
      "Test epoch : 48 Average loss: 916.7907\n",
      "PP(train) = 3387.688, PP(valid) = 3377.475\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 2.4442e-36, 1.8376e-20, 4.3157e-18, 4.7513e-21, 3.4919e-38,\n",
      "         5.2581e-40, 1.0015e-19, 2.3106e-02, 2.3516e-22, 5.2005e-40, 2.3277e-33,\n",
      "         4.1456e-17, 6.1730e-18, 9.7689e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0812, 0.0449, 0.1025, 0.0506, 0.0593, 0.0672, 0.1064, 0.0925, 0.0594,\n",
      "         0.0527, 0.0625, 0.0559, 0.0511, 0.0720, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1022.6478\n",
      "Test epoch : 49 Average loss: 916.5396\n",
      "PP(train) = 3378.229, PP(valid) = 3370.261\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7555e-37, 2.7725e-14, 1.0452e-24, 6.4228e-33, 0.0000e+00,\n",
      "         3.7835e-44, 3.1937e-19, 1.8160e-18, 1.1909e-30, 1.4013e-45, 6.1056e-33,\n",
      "         4.7109e-24, 3.0106e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1022.2208\n",
      "Test epoch : 50 Average loss: 916.2892\n",
      "PP(train) = 3368.847, PP(valid) = 3363.060\n",
      "Writing to ./topicwords/8-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 9.6188 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                           \n",
      "\n",
      "===== # 2, Topic : 14, p : 10.1556 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                     \n",
      "\n",
      "===== # 3, Topic : 14, p : 9.7861 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                \n",
      "\n",
      "===== # 4, Topic : 14, p : 9.9735 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                         H                                                                 Global Positioning System                 \n",
      "\n",
      "===== # 5, Topic : 14, p : 9.3816 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                        \n",
      "\n",
      "===== # 6, Topic : 14, p : 9.8287 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                   \n",
      "\n",
      "===== # 7, Topic : 14, p : 9.8121 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                          IVIV                                                                                                                                                                            \n",
      "\n",
      "===== # 8, Topic : 14, p : 9.5577 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                \n",
      "\n",
      "===== # 9, Topic : 12, p : 7.7205 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                           \n",
      "\n",
      "===== # 10, Topic : 14, p : 8.1740 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                         \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7115e-38, 7.7552e-41, 3.5936e-13, 3.8869e-19, 3.6687e-09, 5.9541e-37,\n",
      "         1.1032e-36, 4.7767e-21, 3.0659e-14, 3.7761e-06, 2.1691e-29, 4.0813e-31,\n",
      "         3.3335e-17, 3.3324e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1096.8116\n",
      "Test epoch : 1 Average loss: 1071.6260\n",
      "PP(train) = 3269.252, PP(valid) = 3349.028\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.6700e-37, 1.0615e-14, 1.0840e-38, 8.2431e-10, 1.3488e-34,\n",
      "         3.6154e-42, 1.0858e-19, 1.4251e-10, 1.9330e-37, 5.6052e-45, 4.8995e-35,\n",
      "         1.3859e-17, 8.3994e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1096.3402\n",
      "Test epoch : 2 Average loss: 1071.3373\n",
      "PP(train) = 3258.408, PP(valid) = 3340.069\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3465e-37, 4.6734e-19, 3.0527e-29, 1.1002e-09, 2.8026e-45,\n",
      "         2.2871e-40, 7.3800e-16, 2.5347e-14, 2.0298e-19, 1.3490e-40, 2.3384e-37,\n",
      "         1.8381e-17, 6.0733e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1096.0253\n",
      "Test epoch : 3 Average loss: 1071.0305\n",
      "PP(train) = 3247.343, PP(valid) = 3331.425\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0381e-14, 1.5852e-27, 1.0389e-17, 0.0000e+00,\n",
      "         0.0000e+00, 3.7859e-19, 4.8533e-22, 5.6365e-29, 1.0566e-42, 6.4649e-41,\n",
      "         4.0905e-19, 1.0308e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1095.4563\n",
      "Test epoch : 4 Average loss: 1070.7107\n",
      "PP(train) = 3236.969, PP(valid) = 3323.733\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8855e-44, 8.0381e-23, 1.0342e-15, 7.8451e-22, 8.9920e-08, 0.0000e+00,\n",
      "         1.4713e-36, 1.5614e-01, 3.9824e-20, 4.7688e-19, 0.0000e+00, 4.4269e-31,\n",
      "         3.2479e-18, 3.7482e-35, 8.4386e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0831, 0.0515, 0.0955, 0.0499, 0.0636, 0.0713, 0.1034, 0.0832, 0.0591,\n",
      "         0.0502, 0.0696, 0.0543, 0.0491, 0.0750, 0.0412]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1095.0672\n",
      "Test epoch : 5 Average loss: 1070.3919\n",
      "PP(train) = 3226.826, PP(valid) = 3316.431\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.9236e-44, 1.1134e-28, 1.0379e-32, 1.9058e-22, 0.0000e+00,\n",
      "         1.4013e-45, 8.2463e-01, 1.0985e-10, 1.6690e-23, 3.0746e-40, 1.5047e-30,\n",
      "         5.7969e-32, 2.1844e-32, 1.7537e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0844, 0.0921, 0.0618, 0.0459, 0.0765, 0.0885, 0.0828, 0.0505, 0.0549,\n",
      "         0.0399, 0.1080, 0.0464, 0.0393, 0.0907, 0.0382]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1094.5570\n",
      "Test epoch : 6 Average loss: 1070.0761\n",
      "PP(train) = 3216.241, PP(valid) = 3308.868\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1446e-28, 5.2265e-23, 2.0327e-21, 1.7605e-16, 0.0000e+00,\n",
      "         0.0000e+00, 1.0864e-13, 2.6562e-17, 7.9442e-32, 2.8026e-45, 4.7042e-42,\n",
      "         2.5144e-22, 1.1820e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1094.0456\n",
      "Test epoch : 7 Average loss: 1069.7617\n",
      "PP(train) = 3205.310, PP(valid) = 3300.948\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5414e-44, 2.0385e-33, 9.3467e-43, 1.0454e-23, 3.7500e-41,\n",
      "         1.4013e-44, 4.0864e-22, 1.7387e-23, 2.1917e-40, 1.4013e-45, 2.2150e-41,\n",
      "         2.2971e-25, 6.4385e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1093.7158\n",
      "Test epoch : 8 Average loss: 1069.4465\n",
      "PP(train) = 3194.221, PP(valid) = 3292.871\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3071e-43, 2.0261e-23, 3.7057e-33, 1.6506e-32, 0.0000e+00,\n",
      "         0.0000e+00, 8.1090e-29, 1.6556e-29, 5.3235e-34, 1.4013e-45, 0.0000e+00,\n",
      "         2.0801e-32, 9.2434e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1093.2005\n",
      "Test epoch : 9 Average loss: 1069.1328\n",
      "PP(train) = 3183.264, PP(valid) = 3284.904\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7662e-26, 2.9618e-21, 3.8476e-25, 5.3780e-18, 0.0000e+00,\n",
      "         1.0720e-42, 1.2540e-11, 2.3965e-10, 1.7602e-24, 1.6363e-35, 2.0204e-34,\n",
      "         9.9213e-01, 1.1009e-26, 7.8686e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0485, 0.0742, 0.0764, 0.0605, 0.0414, 0.0646, 0.0542, 0.0790, 0.0745,\n",
      "         0.0934, 0.0483, 0.0530, 0.0585, 0.0644, 0.1091]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1092.7891\n",
      "Test epoch : 10 Average loss: 1068.8221\n",
      "PP(train) = 3172.517, PP(valid) = 3277.131\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4618e-24, 3.0366e-20, 5.7936e-25, 4.4916e-07, 0.0000e+00,\n",
      "         2.6344e-43, 9.9997e-01, 7.2089e-15, 3.8139e-26, 3.6504e-42, 1.6764e-34,\n",
      "         1.1787e-13, 3.3894e-28, 2.4740e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1092.2576\n",
      "Test epoch : 11 Average loss: 1068.5154\n",
      "PP(train) = 3161.955, PP(valid) = 3269.564\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6511e-27, 2.1195e-22, 1.3258e-34, 7.2536e-27, 0.0000e+00,\n",
      "         0.0000e+00, 4.6344e-17, 2.8484e-17, 1.6078e-33, 9.7432e-42, 1.4812e-42,\n",
      "         6.9339e-18, 8.1679e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1091.8012\n",
      "Test epoch : 12 Average loss: 1068.2104\n",
      "PP(train) = 3151.451, PP(valid) = 3262.036\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.7581e-43, 1.6559e-31, 8.3197e-10, 1.9957e-24, 2.0011e-05, 4.6431e-32,\n",
      "         1.5668e-34, 1.1592e-04, 2.0882e-07, 5.4029e-25, 9.1164e-38, 6.4284e-34,\n",
      "         1.8425e-12, 6.3532e-30, 9.9986e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1091.4561\n",
      "Test epoch : 13 Average loss: 1067.9066\n",
      "PP(train) = 3140.926, PP(valid) = 3254.405\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4344e-38, 6.6961e-22, 2.8511e-31, 9.3064e-17, 0.0000e+00,\n",
      "         5.7453e-44, 1.4454e-06, 1.2435e-13, 4.2697e-34, 8.9627e-42, 1.5311e-37,\n",
      "         9.3128e-31, 1.6914e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1090.8880\n",
      "Test epoch : 14 Average loss: 1067.6024\n",
      "PP(train) = 3130.510, PP(valid) = 3246.813\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.4809e-40, 6.6785e-22, 1.2604e-30, 3.5485e-16, 0.0000e+00,\n",
      "         0.0000e+00, 2.3968e-17, 1.2781e-17, 5.4313e-28, 0.0000e+00, 6.4460e-44,\n",
      "         2.4544e-17, 4.7536e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1090.4278\n",
      "Test epoch : 15 Average loss: 1067.3017\n",
      "PP(train) = 3120.414, PP(valid) = 3239.554\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.6940e-38, 1.3929e-25, 2.6808e-25, 4.1040e-20, 1.4013e-45,\n",
      "         0.0000e+00, 3.3878e-26, 8.7219e-19, 9.1904e-32, 0.0000e+00, 1.4248e-35,\n",
      "         1.3301e-31, 1.8155e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1090.1499\n",
      "Test epoch : 16 Average loss: 1067.0018\n",
      "PP(train) = 3110.476, PP(valid) = 3232.373\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2757e-42, 1.4303e-16, 6.2189e-18, 2.3061e-19, 7.1631e-39,\n",
      "         0.0000e+00, 4.6200e-15, 1.1681e-11, 4.7607e-28, 8.6233e-34, 5.9220e-37,\n",
      "         9.1043e-33, 1.0814e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1089.6040\n",
      "Test epoch : 17 Average loss: 1066.7062\n",
      "PP(train) = 3100.395, PP(valid) = 3225.023\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6778e-39, 8.8260e-21, 5.0102e-26, 1.2078e-16, 0.0000e+00,\n",
      "         5.6052e-45, 3.8232e-14, 1.2969e-11, 3.8455e-22, 0.0000e+00, 3.7887e-40,\n",
      "         2.8744e-13, 2.8440e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1089.0828\n",
      "Test epoch : 18 Average loss: 1066.4116\n",
      "PP(train) = 3090.417, PP(valid) = 3217.806\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.8932e-42, 1.0790e-22, 4.9967e-33, 7.9797e-34, 0.0000e+00,\n",
      "         0.0000e+00, 6.4982e-21, 8.9186e-10, 1.1656e-37, 3.0829e-44, 2.2424e-39,\n",
      "         2.1227e-24, 3.8725e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1088.7160\n",
      "Test epoch : 19 Average loss: 1066.1175\n",
      "PP(train) = 3080.568, PP(valid) = 3210.636\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3392e-28, 1.0836e-21, 2.0994e-22, 1.2235e-14, 1.4013e-45,\n",
      "         5.0209e-42, 1.0000e+00, 2.1323e-22, 1.8983e-25, 6.7857e-39, 9.9973e-38,\n",
      "         1.8058e-19, 2.3553e-34, 2.5880e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1088.3692\n",
      "Test epoch : 20 Average loss: 1065.8281\n",
      "PP(train) = 3070.754, PP(valid) = 3203.513\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2595e-37, 1.3104e-26, 6.9855e-12, 1.5154e-19, 1.5037e-07, 3.2940e-31,\n",
      "         1.2371e-35, 3.9203e-02, 9.6079e-01, 2.3375e-17, 1.6440e-39, 3.7405e-24,\n",
      "         5.7837e-20, 6.0263e-28, 2.4250e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0572, 0.0688, 0.0448, 0.0622, 0.0317, 0.0768, 0.0636, 0.0896, 0.0570,\n",
      "         0.0678, 0.0861, 0.0643, 0.0506, 0.1301, 0.0493]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1087.9855\n",
      "Test epoch : 21 Average loss: 1065.5403\n",
      "PP(train) = 3060.974, PP(valid) = 3196.357\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1366e-22, 5.0098e-22, 2.1907e-17, 6.2753e-19, 8.1359e-39,\n",
      "         1.8511e-42, 3.5516e-01, 1.4471e-15, 1.7488e-29, 1.3312e-43, 3.0109e-34,\n",
      "         1.4287e-29, 2.1673e-21, 6.4484e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0843, 0.0618, 0.0847, 0.0491, 0.0678, 0.0768, 0.0977, 0.0724, 0.0584,\n",
      "         0.0473, 0.0801, 0.0523, 0.0464, 0.0802, 0.0407]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1087.5599\n",
      "Test epoch : 22 Average loss: 1065.2526\n",
      "PP(train) = 3051.475, PP(valid) = 3189.405\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.3029e-26, 1.6315e-30, 2.3867e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.2012e-11, 2.7078e-23, 5.6472e-43, 0.0000e+00, 1.7941e-38,\n",
      "         8.7457e-31, 1.9903e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1087.0049\n",
      "Test epoch : 23 Average loss: 1064.9612\n",
      "PP(train) = 3042.162, PP(valid) = 3182.666\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1749e-39, 1.2084e-34, 2.3066e-18, 2.0619e-23, 9.9999e-01, 2.8026e-45,\n",
      "         8.2328e-41, 1.7325e-07, 3.4774e-18, 2.4806e-23, 6.6744e-37, 4.2039e-45,\n",
      "         3.3845e-06, 1.1341e-23, 6.4232e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1086.6427\n",
      "Test epoch : 24 Average loss: 1064.6759\n",
      "PP(train) = 3032.979, PP(valid) = 3176.023\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1829e-35, 5.8875e-12, 1.9704e-25, 2.5563e-09, 0.0000e+00,\n",
      "         4.9520e-40, 1.0000e+00, 8.7780e-13, 3.1426e-20, 1.9080e-32, 7.9329e-30,\n",
      "         1.2516e-17, 1.4969e-23, 3.3664e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1086.1824\n",
      "Test epoch : 25 Average loss: 1064.3930\n",
      "PP(train) = 3023.616, PP(valid) = 3169.218\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2805e-38, 8.9844e-21, 1.1559e-23, 1.6556e-13, 5.8855e-44,\n",
      "         1.7579e-35, 1.0263e-11, 4.3560e-14, 6.5663e-23, 2.1683e-33, 8.2432e-32,\n",
      "         3.3038e-19, 2.4710e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1085.8449\n",
      "Test epoch : 26 Average loss: 1064.1114\n",
      "PP(train) = 3014.256, PP(valid) = 3162.323\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.5670e-44, 0.0000e+00, 6.0942e-25, 1.3189e-30, 7.6360e-28, 4.2039e-45,\n",
      "         0.0000e+00, 3.7589e-15, 1.9958e-13, 8.6681e-27, 0.0000e+00, 2.8026e-45,\n",
      "         5.4756e-27, 4.3449e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1085.4603\n",
      "Test epoch : 27 Average loss: 1063.8343\n",
      "PP(train) = 3004.885, PP(valid) = 3155.451\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.4921e-34, 6.4121e-17, 1.0878e-12, 1.1067e-08, 4.4055e-38,\n",
      "         0.0000e+00, 2.8637e-09, 4.1685e-14, 1.0429e-28, 0.0000e+00, 6.8221e-34,\n",
      "         3.7380e-17, 2.6762e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1084.9213\n",
      "Test epoch : 28 Average loss: 1063.5555\n",
      "PP(train) = 2995.821, PP(valid) = 3148.792\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7347e-41, 6.2358e-43, 7.4954e-33, 3.4307e-28, 1.5200e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.2942e-13, 2.3078e-09, 3.5912e-36, 1.4555e-35, 2.2976e-34,\n",
      "         1.0070e-15, 3.9246e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1084.5848\n",
      "Test epoch : 29 Average loss: 1063.2765\n",
      "PP(train) = 2986.960, PP(valid) = 3142.310\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8352e-33, 9.6315e-20, 1.8489e-34, 5.7164e-16, 0.0000e+00,\n",
      "         0.0000e+00, 2.8267e-09, 1.1488e-14, 6.6428e-38, 0.0000e+00, 1.9919e-22,\n",
      "         4.1600e-17, 5.4867e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1084.1160\n",
      "Test epoch : 30 Average loss: 1063.0014\n",
      "PP(train) = 2978.307, PP(valid) = 3136.050\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3829e-39, 1.2981e-19, 1.1390e-15, 7.9607e-21, 4.6299e-05, 6.4460e-44,\n",
      "         2.8026e-44, 4.8031e-04, 8.0786e-16, 1.3219e-32, 1.2752e-42, 6.5434e-23,\n",
      "         5.4757e-08, 1.1206e-14, 9.9947e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1083.7248\n",
      "Test epoch : 31 Average loss: 1062.7300\n",
      "PP(train) = 2969.482, PP(valid) = 3129.641\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2967e-42, 5.8214e-34, 1.7233e-26, 2.8352e-23, 5.1180e-13, 9.8000e-36,\n",
      "         2.7982e-39, 6.6746e-13, 2.1449e-12, 3.5566e-30, 7.0065e-44, 4.7947e-39,\n",
      "         7.0296e-06, 7.2516e-31, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1083.3965\n",
      "Test epoch : 32 Average loss: 1062.4578\n",
      "PP(train) = 2960.547, PP(valid) = 3123.079\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9403e-32, 6.7735e-32, 1.1601e-05, 1.8575e-17, 6.7168e-10, 1.7512e-26,\n",
      "         1.2612e-44, 1.1043e-08, 8.6580e-09, 3.7750e-24, 3.1341e-40, 1.9312e-27,\n",
      "         1.4159e-18, 1.3090e-29, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1083.0182\n",
      "Test epoch : 33 Average loss: 1062.1886\n",
      "PP(train) = 2951.733, PP(valid) = 3116.551\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.3584e-25, 1.5655e-13, 7.4494e-06, 0.0000e+00,\n",
      "         0.0000e+00, 1.7439e-16, 9.9492e-12, 4.9201e-20, 6.0583e-34, 2.6813e-31,\n",
      "         2.3581e-24, 3.1068e-20, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1082.5135\n",
      "Test epoch : 34 Average loss: 1061.9198\n",
      "PP(train) = 2942.961, PP(valid) = 3110.081\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3717e-41, 3.7825e-30, 1.0210e-41, 1.2097e-11, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-17, 1.0815e-12, 2.0281e-41, 0.0000e+00, 2.2421e-44,\n",
      "         4.5761e-17, 2.3127e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1082.0898\n",
      "Test epoch : 35 Average loss: 1061.6509\n",
      "PP(train) = 2934.387, PP(valid) = 3103.773\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1074e-41, 2.5831e-28, 1.7798e-25, 2.2872e-26, 0.0000e+00,\n",
      "         1.4013e-45, 5.9929e-13, 1.5052e-17, 8.8455e-27, 1.9618e-44, 4.8301e-34,\n",
      "         5.1202e-22, 4.3135e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1081.7618\n",
      "Test epoch : 36 Average loss: 1061.3858\n",
      "PP(train) = 2926.043, PP(valid) = 3097.682\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2820e-34, 6.4310e-29, 5.8779e-05, 1.5735e-17, 1.0049e-07, 5.6642e-31,\n",
      "         1.2746e-31, 8.2769e-01, 1.5036e-01, 8.8306e-20, 1.0065e-40, 1.3049e-34,\n",
      "         1.7601e-05, 8.8872e-15, 2.1875e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0796, 0.0982, 0.0542, 0.0474, 0.0691, 0.0902, 0.0762, 0.0503, 0.0545,\n",
      "         0.0415, 0.1133, 0.0474, 0.0392, 0.0996, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1081.4352\n",
      "Test epoch : 37 Average loss: 1061.1215\n",
      "PP(train) = 2917.632, PP(valid) = 3091.519\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7628e-42, 6.3384e-41, 1.1359e-13, 1.2715e-38, 1.4775e-15, 0.0000e+00,\n",
      "         2.3262e-43, 1.8848e-06, 2.2903e-03, 1.1929e-28, 9.6230e-34, 4.8054e-36,\n",
      "         2.0757e-17, 2.6436e-31, 9.9771e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1074, 0.0924, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0710, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1080.9606\n",
      "Test epoch : 38 Average loss: 1060.8610\n",
      "PP(train) = 2909.305, PP(valid) = 3085.446\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 4.3454e-35, 7.5305e-02, 1.0720e-11, 8.4081e-10, 3.9123e-37,\n",
      "         4.4512e-34, 1.3081e-08, 2.7533e-06, 3.4388e-16, 6.3043e-37, 2.9322e-17,\n",
      "         3.2197e-09, 3.1860e-17, 9.2469e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0470, 0.0983, 0.0518, 0.0619, 0.0656, 0.1052, 0.0914, 0.0618,\n",
      "         0.0530, 0.0623, 0.0552, 0.0513, 0.0723, 0.0411]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1080.5823\n",
      "Test epoch : 39 Average loss: 1060.6007\n",
      "PP(train) = 2900.819, PP(valid) = 3079.161\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-44, 1.1743e-32, 9.2089e-18, 1.9572e-17, 9.3409e-11, 5.5424e-38,\n",
      "         6.3092e-37, 1.0690e-07, 3.4545e-07, 1.8184e-24, 1.3211e-36, 1.6559e-27,\n",
      "         2.4663e-12, 8.6325e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1080.2665\n",
      "Test epoch : 40 Average loss: 1060.3424\n",
      "PP(train) = 2892.526, PP(valid) = 3073.053\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.0024e-31, 1.0445e-09, 3.0137e-22, 1.9882e-17, 0.0000e+00,\n",
      "         3.1950e-43, 5.7094e-17, 7.1428e-04, 2.0602e-17, 0.0000e+00, 6.4669e-32,\n",
      "         4.1460e-30, 7.5916e-34, 9.9929e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1079.7620\n",
      "Test epoch : 41 Average loss: 1060.0845\n",
      "PP(train) = 2884.500, PP(valid) = 3067.157\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 2.2501e-30, 9.2049e-20, 2.6048e-26, 2.1690e-13, 7.0065e-45,\n",
      "         6.3851e-39, 1.0023e-06, 1.0705e-10, 6.4071e-29, 6.0141e-33, 5.5811e-31,\n",
      "         6.3072e-17, 3.3110e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1079.4118\n",
      "Test epoch : 42 Average loss: 1059.8273\n",
      "PP(train) = 2876.590, PP(valid) = 3061.406\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3037e-40, 1.0396e-19, 4.6038e-30, 1.2265e-23, 1.5372e-42,\n",
      "         0.0000e+00, 6.2639e-22, 8.1685e-10, 2.9025e-20, 4.6597e-40, 2.6348e-32,\n",
      "         2.1249e-25, 7.3306e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1079.1042\n",
      "Test epoch : 43 Average loss: 1059.5714\n",
      "PP(train) = 2868.579, PP(valid) = 3055.505\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5764e-41, 3.6718e-23, 1.3838e-06, 5.4060e-14, 1.7468e-08, 0.0000e+00,\n",
      "         2.0636e-24, 4.3680e-07, 1.5456e-17, 2.0870e-30, 1.2649e-32, 4.1130e-30,\n",
      "         1.3132e-06, 1.3492e-10, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1078.7449\n",
      "Test epoch : 44 Average loss: 1059.3164\n",
      "PP(train) = 2860.527, PP(valid) = 3049.505\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6742e-37, 4.5288e-31, 4.6820e-39, 4.5088e-26, 0.0000e+00,\n",
      "         3.0254e-42, 6.9556e-19, 2.9538e-20, 6.9795e-34, 3.8554e-40, 1.5569e-38,\n",
      "         1.2010e-20, 3.2686e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1078.3659\n",
      "Test epoch : 45 Average loss: 1059.0659\n",
      "PP(train) = 2852.446, PP(valid) = 3043.494\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4086e-40, 8.7775e-28, 1.0851e-23, 2.9616e-19, 0.0000e+00,\n",
      "         0.0000e+00, 2.1175e-27, 5.6386e-13, 1.4055e-39, 1.2884e-41, 1.9946e-41,\n",
      "         7.6604e-27, 1.4386e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1077.9201\n",
      "Test epoch : 46 Average loss: 1058.8130\n",
      "PP(train) = 2844.649, PP(valid) = 3037.761\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7946e-22, 1.8683e-19, 1.5888e-26, 2.3953e-11, 0.0000e+00,\n",
      "         0.0000e+00, 1.7231e-01, 1.6182e-15, 9.2235e-21, 2.2466e-41, 2.0268e-23,\n",
      "         1.3451e-10, 1.5686e-27, 8.2769e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0832, 0.0523, 0.0946, 0.0498, 0.0639, 0.0717, 0.1030, 0.0823, 0.0591,\n",
      "         0.0500, 0.0704, 0.0541, 0.0489, 0.0755, 0.0412]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1077.4738\n",
      "Test epoch : 47 Average loss: 1058.5622\n",
      "PP(train) = 2837.020, PP(valid) = 3032.201\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0464e-24, 3.3282e-11, 3.1829e-15, 8.4006e-04, 5.2611e-28,\n",
      "         4.4299e-34, 5.5285e-19, 9.0910e-01, 1.8182e-27, 2.4424e-32, 3.8907e-26,\n",
      "         3.0669e-15, 3.4795e-28, 9.0062e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0584, 0.0653, 0.0481, 0.0620, 0.0326, 0.0756, 0.0665, 0.0926, 0.0576,\n",
      "         0.0680, 0.0828, 0.0646, 0.0514, 0.1250, 0.0493]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1077.2355\n",
      "Test epoch : 48 Average loss: 1058.3126\n",
      "PP(train) = 2829.397, PP(valid) = 3026.567\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.2843e-35, 2.2046e-32, 2.1076e-11, 2.0156e-26, 7.7615e-08, 1.1953e-40,\n",
      "         0.0000e+00, 1.1865e-18, 1.5633e-12, 6.5885e-30, 1.4244e-32, 1.1105e-30,\n",
      "         9.9239e-18, 1.5408e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1076.8586\n",
      "Test epoch : 49 Average loss: 1058.0668\n",
      "PP(train) = 2821.725, PP(valid) = 3020.916\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8026e-45, 6.9962e-35, 2.4829e-16, 5.2005e-15, 1.5695e-22, 0.0000e+00,\n",
      "         0.0000e+00, 9.9994e-01, 5.5539e-05, 8.5356e-23, 0.0000e+00, 1.6389e-35,\n",
      "         3.2177e-20, 1.5403e-23, 4.4986e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1076.3508\n",
      "Test epoch : 50 Average loss: 1057.8205\n",
      "PP(train) = 2814.074, PP(valid) = 3015.251\n",
      "Writing to ./topicwords/9-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 9.1706 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                \n",
      "\n",
      "===== # 2, Topic : 14, p : 8.2016 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                        \n",
      "\n",
      "===== # 3, Topic : 14, p : 9.5353 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                          N  \n",
      "\n",
      "===== # 4, Topic : 14, p : 10.2634 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                          Suspended Solid                                                     \n",
      "\n",
      "===== # 5, Topic : 14, p : 9.2416 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                   Suspended Solid                                                \n",
      "\n",
      "===== # 6, Topic : 14, p : 9.5702 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                         >                                                                                            <       N  \n",
      "\n",
      "===== # 7, Topic : 14, p : 9.5230 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                 \n",
      "\n",
      "===== # 8, Topic : 14, p : 8.3183 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                          \n",
      "\n",
      "===== # 9, Topic : 14, p : 9.5105 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                               \n",
      "\n",
      "===== # 10, Topic : 14, p : 10.0937 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                  -                                                                                                                                        X                     +   -                 ii                                   v v - -                               i  iii iv           +                          \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0816e-37, 3.3235e-15, 7.1617e-30, 2.2365e-06, 1.7269e-35,\n",
      "         1.2051e-43, 1.2899e-01, 1.7877e-21, 5.2367e-26, 0.0000e+00, 3.9538e-34,\n",
      "         8.1804e-22, 2.5977e-21, 8.7101e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0829, 0.0502, 0.0970, 0.0500, 0.0630, 0.0705, 0.1042, 0.0848, 0.0592,\n",
      "         0.0506, 0.0682, 0.0545, 0.0494, 0.0743, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1037.9773\n",
      "Test epoch : 1 Average loss: 1101.1107\n",
      "PP(train) = 2857.518, PP(valid) = 2911.525\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2501e-33, 8.7375e-19, 1.4222e-19, 3.6700e-13, 0.0000e+00,\n",
      "         2.8023e-35, 1.3076e-05, 3.3359e-23, 2.9233e-23, 1.8240e-29, 2.2244e-27,\n",
      "         1.7817e-14, 9.6437e-31, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1037.6835\n",
      "Test epoch : 2 Average loss: 1100.8443\n",
      "PP(train) = 2850.885, PP(valid) = 2906.452\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2039e-45, 5.8809e-24, 1.4197e-07, 4.0225e-17, 1.2128e-04, 1.4090e-41,\n",
      "         1.6073e-27, 9.9985e-01, 2.4336e-12, 5.5817e-19, 7.0183e-29, 2.4279e-31,\n",
      "         3.6498e-10, 3.3891e-29, 2.7841e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1037.3804\n",
      "Test epoch : 3 Average loss: 1100.5697\n",
      "PP(train) = 2843.156, PP(valid) = 2900.773\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.7853e-33, 4.4720e-32, 4.1965e-28, 7.6877e-22, 0.0000e+00,\n",
      "         1.8266e-40, 5.8252e-14, 4.5032e-18, 2.0319e-40, 3.7025e-41, 1.3388e-37,\n",
      "         1.5350e-25, 1.2899e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1036.9288\n",
      "Test epoch : 4 Average loss: 1100.2906\n",
      "PP(train) = 2834.740, PP(valid) = 2894.728\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9811e-35, 9.8563e-13, 2.7408e-36, 8.5698e-17, 3.6013e-40,\n",
      "         0.0000e+00, 1.0000e+00, 7.5473e-17, 1.2474e-31, 1.0631e-39, 9.0975e-33,\n",
      "         1.0983e-22, 7.1764e-27, 4.1692e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1036.5941\n",
      "Test epoch : 5 Average loss: 1100.0117\n",
      "PP(train) = 2826.113, PP(valid) = 2888.588\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8701e-35, 1.1571e-27, 1.9608e-24, 3.6877e-16, 0.0000e+00,\n",
      "         0.0000e+00, 2.0694e-14, 2.5707e-16, 8.8818e-29, 4.4392e-39, 0.0000e+00,\n",
      "         2.4893e-20, 3.6662e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1036.2183\n",
      "Test epoch : 6 Average loss: 1099.7295\n",
      "PP(train) = 2817.736, PP(valid) = 2882.721\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6804e-39, 2.3299e-20, 2.0988e-04, 3.6491e-26, 1.5277e-08, 4.0743e-36,\n",
      "         1.6816e-35, 5.9402e-13, 1.5857e-22, 2.3557e-24, 3.2838e-38, 9.2545e-34,\n",
      "         9.1325e-17, 1.9022e-28, 9.9979e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1035.9692\n",
      "Test epoch : 7 Average loss: 1099.4484\n",
      "PP(train) = 2809.493, PP(valid) = 2876.998\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6034e-24, 2.6066e-15, 1.9144e-17, 3.2901e-09, 4.0638e-44,\n",
      "         4.8695e-42, 9.9329e-11, 5.3987e-19, 1.5127e-22, 4.9481e-41, 2.9076e-30,\n",
      "         8.2681e-17, 2.5705e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1035.4819\n",
      "Test epoch : 8 Average loss: 1099.1700\n",
      "PP(train) = 2801.195, PP(valid) = 2871.242\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.7960e-28, 1.9783e-22, 6.1298e-34, 1.0431e-05, 2.6905e-43,\n",
      "         3.9236e-44, 9.8533e-01, 5.8434e-12, 7.5464e-31, 3.9362e-42, 2.4093e-30,\n",
      "         9.0402e-26, 8.1574e-30, 1.4662e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1044, 0.0549, 0.0444, 0.0789, 0.0920, 0.0774, 0.0442, 0.0532,\n",
      "         0.0372, 0.1184, 0.0440, 0.0368, 0.0937, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1034.9742\n",
      "Test epoch : 9 Average loss: 1098.8883\n",
      "PP(train) = 2792.876, PP(valid) = 2865.428\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1210e-44, 7.3674e-24, 1.7764e-31, 5.3265e-18, 0.0000e+00,\n",
      "         1.4083e-42, 2.0856e-18, 1.4076e-17, 3.0421e-35, 2.7285e-36, 7.6709e-40,\n",
      "         2.2491e-21, 1.3568e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1034.6424\n",
      "Test epoch : 10 Average loss: 1098.6118\n",
      "PP(train) = 2784.629, PP(valid) = 2859.682\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.2626e-43, 4.6590e-30, 7.9686e-18, 1.6077e-26, 2.5521e-13, 1.4321e-42,\n",
      "         4.3440e-44, 4.1984e-08, 1.0312e-21, 4.2145e-16, 2.5566e-32, 6.3197e-33,\n",
      "         1.0399e-20, 3.1867e-15, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1034.4085\n",
      "Test epoch : 11 Average loss: 1098.3352\n",
      "PP(train) = 2776.442, PP(valid) = 2853.951\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4944e-33, 5.9967e-22, 1.8102e-19, 1.2436e-10, 9.3994e-29,\n",
      "         0.0000e+00, 1.0599e-16, 1.7888e-15, 2.0911e-24, 3.1832e-41, 1.2567e-40,\n",
      "         7.8957e-11, 1.0828e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1033.9165\n",
      "Test epoch : 12 Average loss: 1098.0598\n",
      "PP(train) = 2768.305, PP(valid) = 2848.237\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1903e-29, 4.6841e-15, 2.5874e-24, 7.4756e-01, 4.7644e-44,\n",
      "         6.2196e-34, 6.8548e-06, 3.1067e-12, 1.2177e-20, 2.1762e-37, 4.8167e-36,\n",
      "         9.5741e-22, 1.2785e-27, 2.5243e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0528, 0.0476, 0.0538, 0.0414, 0.0865, 0.0669, 0.1084, 0.1056, 0.0490,\n",
      "         0.0822, 0.0668, 0.0598, 0.0476, 0.0651, 0.0666]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1033.5020\n",
      "Test epoch : 13 Average loss: 1097.7867\n",
      "PP(train) = 2760.233, PP(valid) = 2842.553\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.6849e-40, 2.7959e-19, 3.2032e-34, 2.5068e-11, 1.3495e-28,\n",
      "         0.0000e+00, 2.0806e-08, 4.3876e-26, 3.4741e-30, 2.2128e-39, 4.7843e-41,\n",
      "         5.6093e-21, 2.5991e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1033.2026\n",
      "Test epoch : 14 Average loss: 1097.5148\n",
      "PP(train) = 2752.283, PP(valid) = 2836.978\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7644e-44, 2.8635e-32, 2.0103e-06, 7.7301e-28, 1.5447e-08, 1.2892e-43,\n",
      "         8.7161e-35, 1.4830e-08, 5.9051e-14, 2.1157e-21, 3.0788e-30, 9.6536e-34,\n",
      "         6.1855e-14, 4.2536e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1032.7906\n",
      "Test epoch : 15 Average loss: 1097.2448\n",
      "PP(train) = 2744.394, PP(valid) = 2831.448\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4185e-36, 1.3133e-15, 3.4971e-15, 1.6477e-18, 4.2691e-08, 8.1836e-43,\n",
      "         2.5115e-22, 3.3424e-04, 3.8363e-02, 6.7696e-22, 6.9826e-32, 3.4666e-31,\n",
      "         1.3330e-08, 4.8473e-19, 9.6130e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0808, 0.0452, 0.1013, 0.0508, 0.0587, 0.0675, 0.1057, 0.0926, 0.0594,\n",
      "         0.0530, 0.0629, 0.0561, 0.0512, 0.0728, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1032.5151\n",
      "Test epoch : 16 Average loss: 1096.9755\n",
      "PP(train) = 2736.605, PP(valid) = 2826.006\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2981e-43, 1.2177e-34, 4.3001e-22, 8.7469e-22, 9.3293e-13, 3.6818e-41,\n",
      "         5.9212e-41, 6.1231e-15, 2.4204e-22, 1.6867e-25, 0.0000e+00, 5.6655e-36,\n",
      "         1.0799e-26, 2.9005e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1032.1240\n",
      "Test epoch : 17 Average loss: 1096.7092\n",
      "PP(train) = 2728.765, PP(valid) = 2820.483\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.0240e-39, 1.1459e-07, 4.2449e-23, 3.1769e-31, 1.7725e-41,\n",
      "         1.5724e-41, 6.3663e-08, 4.0284e-10, 1.5799e-28, 1.8661e-38, 2.7915e-34,\n",
      "         3.2849e-23, 7.0652e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1031.6907\n",
      "Test epoch : 18 Average loss: 1096.4434\n",
      "PP(train) = 2720.961, PP(valid) = 2814.968\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.0103e-43, 5.0279e-28, 1.1952e-12, 6.8739e-22, 4.6542e-10, 0.0000e+00,\n",
      "         0.0000e+00, 9.6789e-14, 2.7498e-17, 6.2689e-28, 8.5858e-37, 3.1581e-33,\n",
      "         1.5676e-13, 4.3359e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1031.3230\n",
      "Test epoch : 19 Average loss: 1096.1789\n",
      "PP(train) = 2713.380, PP(valid) = 2809.632\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1124e-24, 1.9440e-16, 4.5678e-09, 2.0809e-20, 5.4532e-01, 4.6085e-26,\n",
      "         5.2698e-34, 9.6979e-04, 1.9420e-03, 2.7388e-25, 4.5421e-28, 2.0184e-22,\n",
      "         1.7376e-16, 3.1148e-06, 4.5177e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0599, 0.0472, 0.0647, 0.0440, 0.0790, 0.0676, 0.1090, 0.1027, 0.0520,\n",
      "         0.0734, 0.0661, 0.0591, 0.0489, 0.0673, 0.0591]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1031.1159\n",
      "Test epoch : 20 Average loss: 1095.9163\n",
      "PP(train) = 2705.920, PP(valid) = 2804.378\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5470e-30, 1.5262e-26, 4.0073e-38, 4.8902e-29, 0.0000e+00,\n",
      "         0.0000e+00, 8.6879e-19, 1.1166e-22, 1.0854e-33, 0.0000e+00, 8.2677e-43,\n",
      "         2.9737e-24, 4.7396e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1030.4884\n",
      "Test epoch : 21 Average loss: 1095.6555\n",
      "PP(train) = 2698.549, PP(valid) = 2799.235\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 1.4740e-30, 2.8394e-12, 5.7310e-26, 5.9218e-01, 3.1604e-39,\n",
      "         2.1015e-35, 4.0482e-01, 2.9445e-03, 2.0165e-18, 1.5855e-32, 4.2158e-26,\n",
      "         3.0286e-06, 2.1027e-27, 4.4300e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0597, 0.0684, 0.0484, 0.0420, 0.0916, 0.0781, 0.0966, 0.0776, 0.0499,\n",
      "         0.0667, 0.0879, 0.0548, 0.0432, 0.0762, 0.0590]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1030.2101\n",
      "Test epoch : 22 Average loss: 1095.3942\n",
      "PP(train) = 2691.166, PP(valid) = 2794.041\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.5716e-39, 6.9238e-14, 2.3461e-35, 1.3534e-26, 0.0000e+00,\n",
      "         0.0000e+00, 5.5124e-17, 8.7130e-13, 9.3945e-34, 2.6421e-38, 6.6842e-43,\n",
      "         6.5634e-28, 1.1591e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1029.8089\n",
      "Test epoch : 23 Average loss: 1095.1363\n",
      "PP(train) = 2683.630, PP(valid) = 2788.670\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7739e-39, 2.0434e-10, 1.6931e-19, 4.9708e-08, 1.5316e-42,\n",
      "         8.5671e-37, 4.5859e-09, 1.7717e-06, 1.2420e-22, 6.7630e-36, 4.6309e-41,\n",
      "         6.8991e-12, 1.9012e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1029.4294\n",
      "Test epoch : 24 Average loss: 1094.8814\n",
      "PP(train) = 2676.254, PP(valid) = 2783.450\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5138e-34, 1.0330e-30, 1.4980e-26, 6.4883e-23, 2.5784e-43,\n",
      "         2.8026e-45, 1.5944e-22, 2.3450e-18, 2.6477e-38, 5.1367e-40, 7.4845e-36,\n",
      "         2.5895e-27, 8.6018e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1029.2063\n",
      "Test epoch : 25 Average loss: 1094.6268\n",
      "PP(train) = 2669.127, PP(valid) = 2778.458\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.2321e-23, 1.0959e-23, 7.5113e-24, 5.5648e-01, 5.8855e-44,\n",
      "         1.7810e-39, 3.5359e-16, 6.5553e-10, 4.1297e-27, 4.8304e-32, 1.6370e-23,\n",
      "         4.4563e-18, 9.1477e-39, 4.4352e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0596, 0.0472, 0.0642, 0.0439, 0.0795, 0.0675, 0.1091, 0.1029, 0.0518,\n",
      "         0.0739, 0.0661, 0.0591, 0.0488, 0.0671, 0.0595]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1028.7914\n",
      "Test epoch : 26 Average loss: 1094.3735\n",
      "PP(train) = 2661.935, PP(valid) = 2773.351\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1464e-42, 1.2823e-27, 1.4870e-20, 4.7950e-26, 1.1885e-05, 2.2079e-34,\n",
      "         4.4802e-31, 7.0021e-16, 2.7541e-12, 4.3963e-23, 6.3957e-20, 1.1081e-24,\n",
      "         2.7449e-09, 3.3382e-21, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1028.5747\n",
      "Test epoch : 27 Average loss: 1094.1217\n",
      "PP(train) = 2654.729, PP(valid) = 2768.196\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.0395e-27, 4.6414e-16, 2.8081e-22, 1.2978e-27, 0.0000e+00,\n",
      "         0.0000e+00, 1.9970e-15, 2.7623e-23, 7.1776e-36, 0.0000e+00, 0.0000e+00,\n",
      "         2.2856e-30, 3.6854e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1027.9754\n",
      "Test epoch : 28 Average loss: 1093.8682\n",
      "PP(train) = 2647.563, PP(valid) = 2763.093\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0884e-27, 4.3317e-33, 1.5375e-30, 4.2883e-19, 7.0065e-45,\n",
      "         2.1203e-40, 5.4907e-13, 1.3593e-28, 1.8855e-28, 3.0254e-31, 0.0000e+00,\n",
      "         3.6076e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1027.7966\n",
      "Test epoch : 29 Average loss: 1093.6167\n",
      "PP(train) = 2640.698, PP(valid) = 2758.237\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8640e-38, 6.1470e-27, 4.1173e-18, 5.4975e-13, 9.8293e-02, 6.6655e-35,\n",
      "         1.8276e-26, 1.5574e-02, 4.5382e-09, 2.7250e-08, 3.6775e-32, 7.6292e-22,\n",
      "         1.4283e-18, 2.4494e-19, 8.8613e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0777, 0.0457, 0.0953, 0.0492, 0.0637, 0.0677, 0.1078, 0.0935, 0.0581,\n",
      "         0.0556, 0.0636, 0.0563, 0.0506, 0.0708, 0.0443]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1027.5297\n",
      "Test epoch : 30 Average loss: 1093.3702\n",
      "PP(train) = 2633.918, PP(valid) = 2753.480\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2841e-28, 2.9387e-09, 5.6297e-19, 6.5353e-08, 3.3495e-41,\n",
      "         4.5992e-34, 1.3241e-08, 1.2100e-06, 3.6368e-30, 1.4557e-31, 3.7880e-35,\n",
      "         6.3631e-21, 5.2434e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1027.0177\n",
      "Test epoch : 31 Average loss: 1093.1241\n",
      "PP(train) = 2626.909, PP(valid) = 2748.460\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4074e-30, 4.8938e-17, 2.2356e-31, 9.2057e-21, 0.0000e+00,\n",
      "         2.4803e-43, 1.3407e-19, 4.5167e-11, 2.4367e-26, 6.3242e-32, 1.9899e-20,\n",
      "         5.8610e-22, 5.0070e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1026.7844\n",
      "Test epoch : 32 Average loss: 1092.8819\n",
      "PP(train) = 2619.870, PP(valid) = 2743.427\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8213e-33, 3.2564e-19, 2.2421e-23, 2.0589e-25, 1.2352e-20, 6.3457e-36,\n",
      "         2.7231e-33, 9.9933e-01, 1.2451e-13, 1.4334e-14, 2.8026e-45, 4.3536e-29,\n",
      "         1.6203e-15, 3.7793e-28, 6.6521e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1026.4883\n",
      "Test epoch : 33 Average loss: 1092.6396\n",
      "PP(train) = 2612.975, PP(valid) = 2738.509\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.4619e-28, 1.3248e-11, 1.6033e-24, 9.7286e-09, 2.2321e-37,\n",
      "         4.2039e-45, 3.6528e-09, 1.7274e-16, 7.3178e-29, 4.0020e-29, 6.5323e-25,\n",
      "         1.6418e-19, 3.8480e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1026.0647\n",
      "Test epoch : 34 Average loss: 1092.3961\n",
      "PP(train) = 2606.311, PP(valid) = 2733.777\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.0256e-44, 6.6068e-10, 2.2055e-31, 4.3022e-17, 0.0000e+00,\n",
      "         0.0000e+00, 5.4324e-10, 1.6956e-20, 4.9634e-26, 1.2450e-38, 1.7305e-30,\n",
      "         1.4152e-27, 3.4329e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1025.5991\n",
      "Test epoch : 35 Average loss: 1092.1523\n",
      "PP(train) = 2599.792, PP(valid) = 2729.158\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7492e-43, 1.0392e-26, 3.0510e-23, 1.6370e-20, 3.0757e-04, 2.3808e-40,\n",
      "         8.6745e-31, 7.7743e-05, 2.8482e-10, 1.3941e-24, 2.2491e-27, 1.9889e-31,\n",
      "         2.5052e-20, 3.2442e-17, 9.9961e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1025.4152\n",
      "Test epoch : 36 Average loss: 1091.9123\n",
      "PP(train) = 2593.229, PP(valid) = 2724.521\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8236e-40, 3.4152e-22, 1.5042e-27, 1.4205e-08, 0.0000e+00,\n",
      "         4.7891e-35, 3.7160e-12, 1.9001e-06, 6.9521e-22, 3.5629e-41, 6.8634e-35,\n",
      "         5.4719e-21, 9.5697e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1024.9661\n",
      "Test epoch : 37 Average loss: 1091.6719\n",
      "PP(train) = 2586.706, PP(valid) = 2719.874\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7381e-37, 7.2174e-30, 4.0350e-23, 1.7021e-24, 9.7191e-01, 1.1613e-37,\n",
      "         8.7727e-29, 1.1454e-02, 4.5005e-09, 4.5978e-20, 5.1128e-25, 3.6928e-22,\n",
      "         4.8405e-09, 6.5998e-06, 1.6634e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0456, 0.0484, 0.0430, 0.0384, 0.0953, 0.0661, 0.1065, 0.1073, 0.0454,\n",
      "         0.0923, 0.0677, 0.0600, 0.0457, 0.0627, 0.0755]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1024.7991\n",
      "Test epoch : 38 Average loss: 1091.4372\n",
      "PP(train) = 2580.078, PP(valid) = 2715.130\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5656e-41, 6.7279e-18, 2.8761e-22, 3.0401e-17, 8.4178e-39,\n",
      "         1.8173e-35, 1.6834e-13, 3.9195e-31, 4.8308e-32, 0.0000e+00, 0.0000e+00,\n",
      "         9.8602e-23, 8.4507e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1024.3418\n",
      "Test epoch : 39 Average loss: 1091.2017\n",
      "PP(train) = 2573.406, PP(valid) = 2710.347\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.7260e-27, 2.8903e-16, 6.2011e-18, 2.7467e-11, 5.7137e-39,\n",
      "         2.0198e-38, 8.0821e-01, 7.3750e-19, 6.0807e-15, 3.0326e-26, 9.1560e-29,\n",
      "         1.5819e-12, 6.1692e-28, 1.9179e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0844, 0.0909, 0.0626, 0.0461, 0.0763, 0.0881, 0.0833, 0.0512, 0.0551,\n",
      "         0.0402, 0.1070, 0.0466, 0.0396, 0.0904, 0.0383]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1024.0825\n",
      "Test epoch : 40 Average loss: 1090.9675\n",
      "PP(train) = 2566.850, PP(valid) = 2705.628\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4720e-33, 6.2165e-16, 3.4713e-32, 6.9712e-21, 0.0000e+00,\n",
      "         3.1669e-43, 2.5465e-19, 8.4640e-20, 3.1773e-37, 0.0000e+00, 6.9133e-34,\n",
      "         1.1812e-13, 1.9625e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1023.7148\n",
      "Test epoch : 41 Average loss: 1090.7334\n",
      "PP(train) = 2560.553, PP(valid) = 2701.149\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.6109e-30, 4.2316e-15, 2.9076e-24, 3.0320e-12, 5.2170e-42,\n",
      "         2.8026e-45, 6.9303e-19, 8.0302e-21, 2.2226e-27, 3.9447e-39, 1.0178e-36,\n",
      "         8.6358e-23, 7.0552e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1023.3754\n",
      "Test epoch : 42 Average loss: 1090.5021\n",
      "PP(train) = 2554.426, PP(valid) = 2696.851\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1512e-23, 9.8152e-22, 1.1776e-16, 1.8007e-05, 0.0000e+00,\n",
      "         4.4295e-42, 2.0631e-12, 5.9673e-08, 3.3757e-23, 1.5414e-44, 6.9122e-38,\n",
      "         4.0678e-06, 3.4707e-33, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1022.9804\n",
      "Test epoch : 43 Average loss: 1090.2714\n",
      "PP(train) = 2548.114, PP(valid) = 2692.346\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0829e-44, 1.9328e-35, 1.2705e-19, 1.1958e-32, 2.1841e-27, 0.0000e+00,\n",
      "         0.0000e+00, 1.1613e-15, 1.1632e-15, 1.7283e-30, 3.0913e-42, 1.0089e-43,\n",
      "         3.6560e-24, 3.8461e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1022.7482\n",
      "Test epoch : 44 Average loss: 1090.0431\n",
      "PP(train) = 2541.795, PP(valid) = 2687.800\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7208e-30, 1.0643e-27, 5.7252e-31, 1.0033e-16, 5.4651e-44,\n",
      "         0.0000e+00, 9.9994e-01, 4.9192e-10, 8.4078e-45, 0.0000e+00, 3.4173e-35,\n",
      "         1.6159e-16, 2.6645e-28, 6.1134e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1022.2957\n",
      "Test epoch : 45 Average loss: 1089.8162\n",
      "PP(train) = 2535.567, PP(valid) = 2683.344\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 1.3377e-30, 2.8557e-13, 1.6353e-18, 1.8341e-06, 0.0000e+00,\n",
      "         6.1889e-37, 5.7813e-10, 5.6521e-06, 3.1399e-27, 6.6706e-35, 1.4847e-31,\n",
      "         8.5502e-20, 8.0849e-27, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1022.0632\n",
      "Test epoch : 46 Average loss: 1089.5906\n",
      "PP(train) = 2529.489, PP(valid) = 2679.055\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1957e-31, 2.5581e-19, 2.3491e-30, 6.0972e-19, 0.0000e+00,\n",
      "         0.0000e+00, 4.0216e-17, 8.5762e-30, 1.8777e-43, 3.1321e-40, 0.0000e+00,\n",
      "         6.5778e-38, 4.2187e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1021.7668\n",
      "Test epoch : 47 Average loss: 1089.3646\n",
      "PP(train) = 2523.441, PP(valid) = 2674.740\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.7603e-40, 4.5285e-21, 3.3306e-28, 9.9482e-21, 0.0000e+00,\n",
      "         0.0000e+00, 3.0418e-13, 4.8480e-27, 2.3928e-31, 3.6656e-37, 6.5637e-42,\n",
      "         4.3928e-20, 6.7963e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1021.3928\n",
      "Test epoch : 48 Average loss: 1089.1411\n",
      "PP(train) = 2517.409, PP(valid) = 2670.450\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5471e-40, 3.6875e-35, 1.4115e-18, 1.3264e-21, 7.7267e-02, 2.3956e-34,\n",
      "         1.0628e-31, 4.7071e-12, 1.2296e-16, 3.1996e-31, 2.6747e-28, 5.7445e-25,\n",
      "         1.8983e-08, 2.7826e-25, 9.2273e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0785, 0.0449, 0.0978, 0.0494, 0.0627, 0.0672, 0.1081, 0.0940, 0.0584,\n",
      "         0.0550, 0.0627, 0.0562, 0.0509, 0.0705, 0.0437]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1021.2980\n",
      "Test epoch : 49 Average loss: 1088.9185\n",
      "PP(train) = 2511.328, PP(valid) = 2666.105\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.3215e-37, 1.2244e-23, 1.0669e-36, 1.6827e-21, 3.1556e-39,\n",
      "         0.0000e+00, 1.5030e-05, 3.5823e-11, 2.2794e-25, 0.0000e+00, 5.7827e-35,\n",
      "         5.4955e-26, 5.2309e-33, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1020.8060\n",
      "Test epoch : 50 Average loss: 1088.6970\n",
      "PP(train) = 2505.335, PP(valid) = 2661.822\n",
      "Writing to ./topicwords/10-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 9.5735 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                      \n",
      "\n",
      "===== # 2, Topic : 14, p : 8.3546 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                    No pp )            \n",
      "\n",
      "===== # 3, Topic : 14, p : 9.2029 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                    http://www design/#!/featured hydrogen>                       ARCHITORIUM OBAYASHI DESIGN PROJECTS      URL obayashi co jp                  \n",
      "\n",
      "===== # 4, Topic : 14, p : 9.4989 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                               \n",
      "\n",
      "===== # 5, Topic : 14, p : 9.7681 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                     \n",
      "\n",
      "===== # 6, Topic : 14, p : 9.5745 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                         IIIIII                                                                                                                             \n",
      "\n",
      "===== # 7, Topic : 14, p : 10.8929 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                        \n",
      "\n",
      "===== # 8, Topic : 14, p : 9.3954 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                   \n",
      "\n",
      "===== # 9, Topic : 14, p : 9.1349 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                            \n",
      "\n",
      "===== # 10, Topic : 14, p : 9.5777 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                      \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.5854e-38, 1.4743e-08, 3.1290e-15, 3.9255e-07, 2.6853e-38,\n",
      "         2.2281e-43, 7.9801e-03, 2.0094e-13, 1.9090e-24, 3.9526e-27, 5.7263e-26,\n",
      "         2.9583e-10, 2.3523e-14, 9.9202e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0447, 0.1039, 0.0502, 0.0603, 0.0671, 0.1073, 0.0919, 0.0593,\n",
      "         0.0522, 0.0623, 0.0555, 0.0509, 0.0711, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1043.1414\n",
      "Test epoch : 1 Average loss: 1049.8517\n",
      "PP(train) = 2616.723, PP(valid) = 2714.737\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.6972e-34, 7.0065e-45, 3.8452e-39, 0.0000e+00,\n",
      "         0.0000e+00, 8.1095e-23, 4.1301e-30, 4.5597e-32, 0.0000e+00, 0.0000e+00,\n",
      "         8.2738e-32, 4.2039e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1042.9299\n",
      "Test epoch : 2 Average loss: 1049.6457\n",
      "PP(train) = 2611.004, PP(valid) = 2710.538\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0575e-30, 4.5220e-16, 3.1928e-14, 5.8792e-12, 3.8816e-42,\n",
      "         3.4370e-39, 3.0399e-11, 2.5721e-15, 3.7878e-32, 1.9358e-38, 1.7669e-39,\n",
      "         1.0865e-08, 1.5289e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1042.5667\n",
      "Test epoch : 3 Average loss: 1049.4369\n",
      "PP(train) = 2604.876, PP(valid) = 2706.365\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.6414e-34, 2.5666e-35, 4.4672e-16, 1.4202e-11, 2.2218e-09, 7.8473e-44,\n",
      "         2.6407e-38, 9.9798e-07, 5.8390e-06, 6.6562e-16, 7.7985e-32, 2.0865e-15,\n",
      "         3.9207e-03, 3.9485e-10, 9.9607e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0816, 0.0445, 0.1043, 0.0503, 0.0600, 0.0669, 0.1073, 0.0923, 0.0594,\n",
      "         0.0524, 0.0619, 0.0556, 0.0511, 0.0708, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1042.3385\n",
      "Test epoch : 4 Average loss: 1049.2238\n",
      "PP(train) = 2598.387, PP(valid) = 2702.022\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.3329e-40, 5.4383e-22, 1.2972e-07, 2.2369e-20, 8.9505e-07, 8.8114e-34,\n",
      "         1.3627e-35, 6.5746e-06, 4.2132e-08, 1.1048e-21, 2.0220e-34, 4.7474e-29,\n",
      "         4.2604e-01, 1.1321e-24, 5.7395e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0668, 0.0566, 0.0934, 0.0556, 0.0524, 0.0674, 0.0820, 0.0884, 0.0669,\n",
      "         0.0686, 0.0570, 0.0557, 0.0553, 0.0696, 0.0642]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1041.9947\n",
      "Test epoch : 5 Average loss: 1049.0099\n",
      "PP(train) = 2591.664, PP(valid) = 2697.544\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.5244e-39, 6.8922e-26, 3.6610e-32, 1.9580e-23, 0.0000e+00,\n",
      "         8.0154e-43, 1.9702e-26, 2.3146e-21, 2.7304e-26, 0.0000e+00, 0.0000e+00,\n",
      "         7.7887e-25, 1.6347e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1041.5930\n",
      "Test epoch : 6 Average loss: 1048.7969\n",
      "PP(train) = 2584.970, PP(valid) = 2693.177\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7222e-42, 2.8065e-29, 1.4669e-07, 4.6845e-13, 1.7421e-08, 7.0127e-37,\n",
      "         1.0002e-37, 2.3733e-02, 1.0113e-05, 5.4407e-12, 1.2720e-28, 1.4841e-25,\n",
      "         6.7002e-01, 3.0158e-13, 3.0624e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0588, 0.0655, 0.0851, 0.0581, 0.0481, 0.0674, 0.0687, 0.0835, 0.0706,\n",
      "         0.0785, 0.0544, 0.0547, 0.0568, 0.0684, 0.0813]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1041.2882\n",
      "Test epoch : 7 Average loss: 1048.5837\n",
      "PP(train) = 2578.412, PP(valid) = 2688.920\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.3647e-24, 8.9526e-38, 1.1040e-17, 0.0000e+00,\n",
      "         0.0000e+00, 3.4129e-09, 2.1944e-18, 1.7098e-26, 3.0702e-42, 2.1290e-30,\n",
      "         6.9286e-23, 4.1906e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1040.8479\n",
      "Test epoch : 8 Average loss: 1048.3708\n",
      "PP(train) = 2571.926, PP(valid) = 2684.733\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4735e-32, 6.6929e-11, 1.1491e-20, 6.8642e-15, 1.5632e-35,\n",
      "         0.0000e+00, 2.1290e-08, 2.6530e-20, 1.0566e-30, 1.1190e-40, 5.8460e-39,\n",
      "         2.6151e-19, 2.2318e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1040.5642\n",
      "Test epoch : 9 Average loss: 1048.1618\n",
      "PP(train) = 2565.458, PP(valid) = 2680.612\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8297e-30, 1.8040e-22, 1.6002e-34, 1.4447e-14, 2.8867e-42,\n",
      "         7.0065e-45, 6.0495e-17, 7.4222e-13, 3.0224e-30, 9.5330e-42, 0.0000e+00,\n",
      "         1.5758e-23, 3.4481e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1040.2464\n",
      "Test epoch : 10 Average loss: 1047.9508\n",
      "PP(train) = 2558.887, PP(valid) = 2676.323\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2948e-41, 1.1899e-23, 1.5787e-23, 2.0012e-11, 6.7599e-40,\n",
      "         0.0000e+00, 2.1284e-20, 1.0547e-19, 1.5260e-32, 1.3056e-38, 5.5108e-39,\n",
      "         1.6260e-10, 1.7646e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1039.8642\n",
      "Test epoch : 11 Average loss: 1047.7436\n",
      "PP(train) = 2552.354, PP(valid) = 2672.041\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7263e-33, 1.4420e-30, 6.3849e-33, 6.4523e-17, 6.0536e-43,\n",
      "         0.0000e+00, 3.9223e-11, 7.6147e-16, 5.2276e-34, 2.7634e-40, 2.0959e-39,\n",
      "         3.3706e-21, 3.3984e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1039.5141\n",
      "Test epoch : 12 Average loss: 1047.5384\n",
      "PP(train) = 2545.916, PP(valid) = 2667.889\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.2102e-34, 8.1066e-19, 3.6564e-09, 2.7742e-23, 1.4347e-14, 5.6274e-34,\n",
      "         4.9114e-36, 9.8708e-01, 1.9753e-07, 2.2718e-19, 1.2602e-28, 1.3610e-26,\n",
      "         1.2467e-02, 1.9302e-09, 4.5792e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0830, 0.1053, 0.0547, 0.0445, 0.0786, 0.0920, 0.0767, 0.0440, 0.0534,\n",
      "         0.0375, 0.1182, 0.0440, 0.0368, 0.0937, 0.0375]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1039.3203\n",
      "Test epoch : 13 Average loss: 1047.3315\n",
      "PP(train) = 2539.635, PP(valid) = 2663.814\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.4064e-27, 6.4465e-23, 1.4251e-25, 1.4371e-17, 9.9268e-42,\n",
      "         1.1210e-44, 9.9594e-01, 6.0298e-17, 6.2019e-33, 3.2907e-37, 1.2781e-26,\n",
      "         4.9925e-23, 1.2264e-32, 4.0556e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1053, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0370, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1038.8674\n",
      "Test epoch : 14 Average loss: 1047.1270\n",
      "PP(train) = 2533.383, PP(valid) = 2659.755\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 5.3658e-40, 1.9936e-23, 9.0503e-35, 4.2222e-09, 6.6088e-38,\n",
      "         1.0229e-43, 8.8695e-18, 1.8925e-18, 1.2151e-34, 4.4149e-39, 5.4944e-37,\n",
      "         9.2126e-19, 1.3526e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1038.6012\n",
      "Test epoch : 15 Average loss: 1046.9246\n",
      "PP(train) = 2527.175, PP(valid) = 2655.736\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8284e-37, 1.5721e-23, 3.0243e-25, 3.1148e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.8558e-10, 8.3195e-28, 1.0069e-32, 0.0000e+00, 4.3098e-27,\n",
      "         2.5622e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1038.2437\n",
      "Test epoch : 16 Average loss: 1046.7206\n",
      "PP(train) = 2521.007, PP(valid) = 2651.670\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2325e-38, 5.2500e-09, 9.7412e-21, 1.1801e-15, 0.0000e+00,\n",
      "         2.3849e-38, 1.8563e-13, 3.3231e-08, 3.7606e-28, 8.4078e-45, 1.8622e-33,\n",
      "         1.3925e-15, 2.3357e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1037.8607\n",
      "Test epoch : 17 Average loss: 1046.5189\n",
      "PP(train) = 2514.853, PP(valid) = 2647.630\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2426e-18, 5.3194e-30, 1.0807e-20, 0.0000e+00,\n",
      "         0.0000e+00, 5.5634e-15, 1.6654e-14, 1.6696e-28, 1.3649e-38, 1.9927e-35,\n",
      "         6.5403e-16, 2.1586e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1037.5226\n",
      "Test epoch : 18 Average loss: 1046.3197\n",
      "PP(train) = 2508.703, PP(valid) = 2643.587\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0065e-45, 1.7132e-25, 1.2396e-19, 5.1985e-35, 6.7222e-01, 1.4731e-39,\n",
      "         1.8030e-37, 1.9169e-13, 8.2357e-09, 1.1405e-20, 7.3687e-26, 7.7533e-32,\n",
      "         6.7890e-16, 1.0513e-31, 3.2778e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0554, 0.0475, 0.0577, 0.0424, 0.0837, 0.0672, 0.1087, 0.1046, 0.0501,\n",
      "         0.0789, 0.0665, 0.0596, 0.0481, 0.0659, 0.0637]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1037.3534\n",
      "Test epoch : 19 Average loss: 1046.1208\n",
      "PP(train) = 2502.629, PP(valid) = 2639.584\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3440e-43, 3.3187e-36, 4.2963e-12, 2.3422e-18, 9.5285e-21, 4.0077e-43,\n",
      "         7.5914e-38, 7.0213e-02, 5.1996e-04, 1.9956e-21, 4.6759e-21, 6.7065e-27,\n",
      "         3.8170e-03, 1.3835e-24, 9.2545e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0823, 0.0476, 0.1002, 0.0501, 0.0616, 0.0689, 0.1055, 0.0882, 0.0593,\n",
      "         0.0515, 0.0653, 0.0550, 0.0502, 0.0728, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1036.9628\n",
      "Test epoch : 20 Average loss: 1045.9240\n",
      "PP(train) = 2496.755, PP(valid) = 2635.751\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.5156e-34, 4.4643e-14, 2.6783e-30, 4.1314e-23, 0.0000e+00,\n",
      "         3.2099e-35, 5.6778e-07, 2.5667e-16, 3.0552e-32, 2.6499e-42, 7.4576e-27,\n",
      "         6.4815e-19, 1.1246e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1036.6195\n",
      "Test epoch : 21 Average loss: 1045.7284\n",
      "PP(train) = 2490.882, PP(valid) = 2631.931\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5350e-39, 7.7453e-33, 4.7310e-16, 4.1670e-29, 1.3748e-10, 7.4042e-41,\n",
      "         4.7924e-43, 7.7472e-14, 1.3139e-21, 6.8741e-19, 1.0185e-34, 9.4647e-35,\n",
      "         1.3797e-14, 8.9885e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1036.4143\n",
      "Test epoch : 22 Average loss: 1045.5310\n",
      "PP(train) = 2484.942, PP(valid) = 2627.986\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4384e-37, 2.2635e-26, 8.6063e-29, 1.0387e-16, 0.0000e+00,\n",
      "         0.0000e+00, 7.9841e-17, 4.7389e-21, 9.2550e-35, 0.0000e+00, 6.6071e-42,\n",
      "         7.3579e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1035.9024\n",
      "Test epoch : 23 Average loss: 1045.3375\n",
      "PP(train) = 2479.160, PP(valid) = 2624.223\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3496e-39, 1.3655e-17, 2.6401e-24, 1.5320e-24, 1.1787e-38,\n",
      "         9.4365e-38, 1.0638e-11, 5.9428e-12, 6.6076e-33, 1.5964e-30, 2.8125e-36,\n",
      "         1.1299e-09, 2.8846e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1035.7767\n",
      "Test epoch : 24 Average loss: 1045.1450\n",
      "PP(train) = 2473.377, PP(valid) = 2620.420\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5009e-38, 1.0814e-09, 1.7420e-10, 6.4064e-03, 0.0000e+00,\n",
      "         2.8306e-43, 7.3672e-08, 9.7849e-23, 2.8639e-23, 5.9548e-39, 2.2333e-33,\n",
      "         2.8746e-25, 3.6765e-27, 9.9359e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0444, 0.1038, 0.0501, 0.0603, 0.0669, 0.1076, 0.0925, 0.0593,\n",
      "         0.0525, 0.0620, 0.0556, 0.0510, 0.0708, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1035.3300\n",
      "Test epoch : 25 Average loss: 1044.9542\n",
      "PP(train) = 2467.602, PP(valid) = 2616.614\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0795e-40, 4.0752e-30, 5.8183e-17, 3.2632e-25, 2.8917e-13, 3.4784e-32,\n",
      "         1.4425e-41, 1.1603e-13, 7.6618e-12, 1.6866e-24, 1.6227e-42, 1.3647e-19,\n",
      "         5.7760e-11, 1.4788e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1035.1845\n",
      "Test epoch : 26 Average loss: 1044.7646\n",
      "PP(train) = 2461.917, PP(valid) = 2612.865\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2039e-45, 2.0622e-22, 4.4539e-13, 3.2846e-17, 1.5673e-03, 7.7956e-32,\n",
      "         7.2631e-38, 4.3258e-10, 5.0276e-04, 2.1804e-20, 6.8236e-39, 5.3084e-15,\n",
      "         2.3060e-09, 7.1581e-21, 9.9793e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0601, 0.0669, 0.1075, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0709, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1034.7818\n",
      "Test epoch : 27 Average loss: 1044.5761\n",
      "PP(train) = 2456.312, PP(valid) = 2609.185\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.1620e-31, 4.4802e-18, 7.2733e-19, 5.2633e-27, 9.9072e-43,\n",
      "         0.0000e+00, 2.6712e-01, 3.9413e-04, 3.0883e-34, 3.3117e-35, 5.2041e-28,\n",
      "         1.6065e-15, 2.1691e-25, 7.3248e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0838, 0.0571, 0.0894, 0.0495, 0.0660, 0.0744, 0.1003, 0.0771, 0.0588,\n",
      "         0.0486, 0.0753, 0.0532, 0.0476, 0.0780, 0.0410]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1034.4632\n",
      "Test epoch : 28 Average loss: 1044.3867\n",
      "PP(train) = 2450.768, PP(valid) = 2605.495\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.3728e-41, 9.4981e-28, 3.9890e-27, 1.1237e-36, 4.2936e-06, 2.1869e-38,\n",
      "         0.0000e+00, 2.5616e-06, 2.8455e-11, 5.7369e-29, 1.3841e-40, 1.6425e-35,\n",
      "         1.1133e-17, 3.0827e-26, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1034.2170\n",
      "Test epoch : 29 Average loss: 1044.1990\n",
      "PP(train) = 2445.268, PP(valid) = 2601.877\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3320e-29, 3.5044e-21, 1.0548e-35, 1.0164e-08, 7.0065e-45,\n",
      "         0.0000e+00, 7.0352e-18, 3.4541e-16, 2.5232e-37, 1.1094e-37, 7.2402e-36,\n",
      "         1.8339e-17, 1.1203e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1033.9093\n",
      "Test epoch : 30 Average loss: 1044.0148\n",
      "PP(train) = 2439.805, PP(valid) = 2598.312\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5414e-44, 1.3470e-29, 1.2276e-19, 1.7915e-20, 1.6108e-16, 5.2128e-43,\n",
      "         1.0687e-34, 5.8623e-18, 2.1170e-22, 1.4675e-26, 2.1319e-29, 1.5804e-38,\n",
      "         6.2347e-20, 1.4137e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1033.6942\n",
      "Test epoch : 31 Average loss: 1043.8311\n",
      "PP(train) = 2434.351, PP(valid) = 2594.726\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8338e-35, 3.3705e-23, 1.8946e-21, 9.0564e-19, 4.8209e-18, 1.0047e-31,\n",
      "         1.2230e-32, 8.6747e-05, 1.2219e-17, 3.9855e-19, 4.6033e-28, 4.3857e-25,\n",
      "         1.1964e-20, 3.0027e-27, 9.9991e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1033.5050\n",
      "Test epoch : 32 Average loss: 1043.6488\n",
      "PP(train) = 2428.821, PP(valid) = 2591.046\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0555e-31, 2.0223e-36, 1.1930e-01, 9.2601e-18, 5.3090e-11, 1.2999e-30,\n",
      "         1.1655e-29, 3.1321e-07, 1.3633e-09, 2.2025e-26, 2.7050e-37, 1.5433e-21,\n",
      "         7.6688e-01, 9.7653e-32, 1.1382e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0551, 0.0731, 0.0753, 0.0616, 0.0478, 0.0638, 0.0618, 0.0814, 0.0762,\n",
      "         0.0845, 0.0521, 0.0535, 0.0577, 0.0686, 0.0875]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1033.1309\n",
      "Test epoch : 33 Average loss: 1043.4690\n",
      "PP(train) = 2423.244, PP(valid) = 2587.304\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6387e-18, 1.6347e-17, 4.8982e-17, 1.5436e-11, 0.0000e+00,\n",
      "         2.5736e-40, 3.2271e-02, 4.9951e-16, 5.2348e-20, 1.1998e-32, 2.4681e-28,\n",
      "         1.4413e-19, 1.3241e-25, 9.6773e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0821, 0.0458, 0.1025, 0.0502, 0.0608, 0.0678, 0.1067, 0.0904, 0.0593,\n",
      "         0.0519, 0.0635, 0.0553, 0.0506, 0.0717, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1032.7476\n",
      "Test epoch : 34 Average loss: 1043.2887\n",
      "PP(train) = 2417.916, PP(valid) = 2583.800\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5685e-39, 1.2369e-15, 1.4710e-35, 8.1065e-25, 7.8473e-44,\n",
      "         2.8950e-36, 1.3046e-03, 1.7821e-13, 8.6844e-17, 4.8440e-37, 3.2550e-27,\n",
      "         4.4189e-17, 1.2998e-29, 9.9870e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1032.4189\n",
      "Test epoch : 35 Average loss: 1043.1068\n",
      "PP(train) = 2412.785, PP(valid) = 2580.462\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.5214e-32, 1.2647e-26, 1.3100e-31, 1.5463e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.1512e-12, 1.5297e-15, 2.3594e-39, 2.0573e-38, 2.1228e-38,\n",
      "         6.0531e-22, 8.2966e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1032.0768\n",
      "Test epoch : 36 Average loss: 1042.9271\n",
      "PP(train) = 2407.737, PP(valid) = 2577.207\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4987e-41, 4.1295e-24, 1.6979e-16, 1.0519e-17, 4.3066e-18, 1.8446e-36,\n",
      "         1.5779e-38, 1.0000e+00, 2.7168e-08, 2.4778e-26, 3.3011e-34, 1.2574e-35,\n",
      "         1.5590e-16, 1.3798e-24, 2.6936e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1031.9255\n",
      "Test epoch : 37 Average loss: 1042.7481\n",
      "PP(train) = 2402.549, PP(valid) = 2573.765\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8558e-21, 1.8074e-19, 4.3938e-31, 3.4694e-15, 5.8564e-38,\n",
      "         2.6092e-42, 4.0685e-16, 2.3210e-11, 3.3337e-25, 4.5891e-25, 1.8562e-24,\n",
      "         2.6988e-07, 1.3529e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1031.5855\n",
      "Test epoch : 38 Average loss: 1042.5739\n",
      "PP(train) = 2397.252, PP(valid) = 2570.261\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0065e-45, 1.2742e-33, 2.7339e-16, 2.6172e-24, 9.6360e-17, 1.3124e-38,\n",
      "         1.3649e-42, 9.9341e-09, 7.2077e-16, 3.7146e-22, 9.5378e-39, 3.1898e-25,\n",
      "         5.8183e-06, 6.7538e-27, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1031.2521\n",
      "Test epoch : 39 Average loss: 1042.3975\n",
      "PP(train) = 2392.028, PP(valid) = 2566.753\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4627e-41, 1.5022e-31, 2.0880e-08, 2.0493e-17, 9.5115e-18, 2.3271e-39,\n",
      "         2.0045e-39, 5.0388e-07, 5.1938e-14, 1.5621e-20, 5.1984e-36, 7.5997e-30,\n",
      "         9.3108e-17, 1.1162e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1031.0591\n",
      "Test epoch : 40 Average loss: 1042.2228\n",
      "PP(train) = 2386.901, PP(valid) = 2563.349\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0497e-41, 1.0872e-30, 8.2215e-16, 1.4204e-26, 4.5945e-22, 0.0000e+00,\n",
      "         1.1210e-44, 5.3621e-17, 5.2789e-10, 1.9451e-34, 3.1861e-39, 2.3910e-30,\n",
      "         6.1212e-26, 1.2174e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1030.7508\n",
      "Test epoch : 41 Average loss: 1042.0479\n",
      "PP(train) = 2381.926, PP(valid) = 2560.076\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6052e-45, 1.2307e-36, 3.4646e-12, 1.9093e-25, 5.5592e-08, 6.3733e-34,\n",
      "         1.7583e-36, 8.8988e-10, 8.3019e-15, 1.0148e-32, 1.4044e-37, 1.1868e-26,\n",
      "         7.8934e-19, 1.6930e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1030.4665\n",
      "Test epoch : 42 Average loss: 1041.8754\n",
      "PP(train) = 2376.968, PP(valid) = 2556.825\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.8136e-36, 2.4551e-16, 1.1258e-10, 1.6401e-22, 6.6541e-05, 8.4416e-37,\n",
      "         2.2953e-39, 6.3446e-10, 9.9811e-13, 7.8744e-20, 1.3095e-26, 8.4221e-18,\n",
      "         9.9987e-01, 1.3818e-13, 6.6159e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1030.2410\n",
      "Test epoch : 43 Average loss: 1041.7034\n",
      "PP(train) = 2371.952, PP(valid) = 2553.477\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8026e-45, 5.4610e-24, 9.0448e-17, 3.3142e-23, 0.0000e+00,\n",
      "         8.1275e-44, 1.0144e-11, 5.5453e-13, 1.8972e-27, 4.5462e-38, 3.3442e-31,\n",
      "         1.7936e-25, 1.1603e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1029.8219\n",
      "Test epoch : 44 Average loss: 1041.5343\n",
      "PP(train) = 2366.975, PP(valid) = 2550.218\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.7038e-40, 4.9136e-18, 2.3150e-38, 5.8459e-24, 5.6052e-44,\n",
      "         1.2191e-43, 1.2949e-14, 2.7217e-24, 1.2581e-30, 1.0187e-42, 7.9912e-37,\n",
      "         8.5169e-12, 5.5179e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1029.6123\n",
      "Test epoch : 45 Average loss: 1041.3643\n",
      "PP(train) = 2362.121, PP(valid) = 2547.012\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0065e-45, 1.3132e-17, 1.1694e-15, 1.6523e-19, 4.1673e-05, 3.0540e-36,\n",
      "         9.9902e-35, 2.7560e-13, 9.4696e-06, 1.1096e-30, 3.4179e-35, 1.6005e-22,\n",
      "         3.5184e-12, 6.6562e-18, 9.9995e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1029.3822\n",
      "Test epoch : 46 Average loss: 1041.1969\n",
      "PP(train) = 2357.279, PP(valid) = 2543.835\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7404e-38, 1.9280e-20, 1.4765e-05, 4.2894e-20, 7.5003e-11, 1.1566e-40,\n",
      "         9.6371e-41, 4.1376e-12, 3.1308e-11, 1.1295e-24, 2.0160e-22, 5.2080e-32,\n",
      "         2.2813e-19, 3.8006e-30, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1029.0790\n",
      "Test epoch : 47 Average loss: 1041.0290\n",
      "PP(train) = 2352.432, PP(valid) = 2540.607\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.6880e-34, 1.2796e-23, 2.7210e-28, 3.1530e-08, 0.0000e+00,\n",
      "         1.1064e-32, 4.9613e-20, 6.6179e-13, 3.4851e-18, 6.6184e-38, 1.6181e-24,\n",
      "         1.3185e-31, 2.4847e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1028.8581\n",
      "Test epoch : 48 Average loss: 1040.8639\n",
      "PP(train) = 2347.620, PP(valid) = 2537.438\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7800e-24, 7.5727e-18, 1.5126e-16, 1.1672e-08, 1.7136e-32,\n",
      "         5.1253e-31, 5.3687e-12, 6.6084e-12, 1.6586e-11, 9.4120e-31, 2.6249e-36,\n",
      "         2.6224e-09, 2.8346e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1028.5428\n",
      "Test epoch : 49 Average loss: 1040.6980\n",
      "PP(train) = 2342.856, PP(valid) = 2534.275\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9292e-35, 6.4890e-20, 1.0270e-31, 8.6857e-23, 2.4999e-42,\n",
      "         1.4588e-42, 1.8876e-16, 4.0558e-12, 2.6830e-32, 0.0000e+00, 1.2823e-40,\n",
      "         9.0587e-18, 9.2692e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1028.2609\n",
      "Test epoch : 50 Average loss: 1040.5322\n",
      "PP(train) = 2338.174, PP(valid) = 2531.161\n",
      "Writing to ./topicwords/11-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 8.1047 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                             \n",
      "\n",
      "===== # 2, Topic : 14, p : 9.8134 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                        \n",
      "\n",
      "===== # 3, Topic : 14, p : 9.6590 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                               \n",
      "\n",
      "===== # 4, Topic : 14, p : 8.3979 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                  \n",
      "\n",
      "===== # 5, Topic : 14, p : 9.0204 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                         \n",
      "\n",
      "===== # 6, Topic : 14, p : 9.2071 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                \n",
      "\n",
      "===== # 7, Topic : 14, p : 10.1764 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                    ton                                                                                                                                                         \n",
      "\n",
      "===== # 8, Topic : 14, p : 9.7675 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                          \n",
      "\n",
      "===== # 9, Topic : 7, p : 8.5759 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                  b       a)(b)    c) d) e) f)               a c d e f                                 \n",
      "\n",
      "===== # 10, Topic : 14, p : 10.0438 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                              ton                                                                                                                                                                                 \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4725e-35, 1.2471e-16, 3.6266e-16, 1.7910e-17, 8.6381e-04, 4.6622e-34,\n",
      "         2.6440e-31, 9.9907e-01, 4.3254e-05, 3.0820e-25, 4.2140e-30, 2.2312e-34,\n",
      "         2.8567e-09, 2.5707e-27, 2.3141e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0543, 0.0442, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1038.7056\n",
      "Test epoch : 1 Average loss: 903.3229\n",
      "PP(train) = 2609.597, PP(valid) = 2538.833\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2228e-35, 8.2873e-28, 6.5857e-21, 2.0964e-19, 2.1427e-13, 5.8574e-43,\n",
      "         1.5254e-32, 1.0702e-02, 9.8930e-01, 1.3944e-23, 2.7009e-31, 3.1315e-24,\n",
      "         5.4300e-07, 2.5659e-22, 3.1696e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0564, 0.0678, 0.0444, 0.0627, 0.0307, 0.0762, 0.0631, 0.0913, 0.0570,\n",
      "         0.0689, 0.0850, 0.0649, 0.0509, 0.1310, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1038.5038\n",
      "Test epoch : 2 Average loss: 903.1589\n",
      "PP(train) = 2604.090, PP(valid) = 2534.966\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.1042e-34, 5.5642e-18, 1.2035e-22, 1.6568e-26, 4.3936e-09, 1.2478e-27,\n",
      "         5.6052e-45, 3.2616e-13, 7.4483e-14, 1.0292e-14, 3.8411e-34, 5.9800e-29,\n",
      "         3.0199e-12, 6.4951e-10, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1038.3116\n",
      "Test epoch : 3 Average loss: 902.9852\n",
      "PP(train) = 2598.222, PP(valid) = 2531.159\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1449e-41, 6.2119e-16, 3.8902e-13, 4.0994e-20, 1.0369e-14, 9.5378e-33,\n",
      "         2.0919e-31, 9.3259e-13, 1.4461e-05, 1.3202e-29, 8.5239e-33, 4.9788e-20,\n",
      "         7.5545e-18, 4.4454e-22, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1037.9997\n",
      "Test epoch : 4 Average loss: 902.8059\n",
      "PP(train) = 2592.008, PP(valid) = 2527.239\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0833e-41, 1.0859e-24, 2.7586e-07, 2.3590e-31, 6.3265e-12, 0.0000e+00,\n",
      "         1.8623e-42, 8.3610e-14, 1.2282e-19, 1.1368e-26, 8.0929e-37, 9.8372e-36,\n",
      "         1.7443e-19, 3.3879e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1037.5486\n",
      "Test epoch : 5 Average loss: 902.6240\n",
      "PP(train) = 2585.775, PP(valid) = 2523.423\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0741e-38, 2.2319e-23, 8.8055e-11, 3.2787e-25, 1.5513e-15, 2.9407e-30,\n",
      "         1.5001e-28, 1.3359e-08, 1.2581e-01, 8.9258e-24, 1.4303e-24, 2.7974e-30,\n",
      "         1.0489e-20, 1.8798e-22, 8.7419e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0787, 0.0472, 0.0946, 0.0521, 0.0557, 0.0686, 0.1015, 0.0932, 0.0596,\n",
      "         0.0547, 0.0651, 0.0572, 0.0515, 0.0773, 0.0428]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1037.3043\n",
      "Test epoch : 6 Average loss: 902.4427\n",
      "PP(train) = 2579.377, PP(valid) = 2519.479\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1701e-28, 2.8110e-18, 5.9119e-35, 1.5239e-17, 1.1901e-37,\n",
      "         2.8026e-45, 1.0000e+00, 1.8239e-18, 3.8830e-29, 6.7262e-44, 5.2607e-30,\n",
      "         3.4084e-24, 8.5128e-37, 1.8810e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1036.9062\n",
      "Test epoch : 7 Average loss: 902.2612\n",
      "PP(train) = 2572.807, PP(valid) = 2515.422\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7094e-35, 6.5980e-39, 3.3639e-09, 1.0501e-32, 1.4118e-12, 3.5053e-33,\n",
      "         3.3564e-34, 4.9766e-13, 8.1686e-05, 3.0734e-33, 9.4218e-25, 7.2569e-26,\n",
      "         1.7058e-10, 8.5544e-26, 9.9992e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1036.6428\n",
      "Test epoch : 8 Average loss: 902.0808\n",
      "PP(train) = 2566.352, PP(valid) = 2511.434\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2371e-35, 3.5242e-19, 8.9741e-39, 1.2056e-26, 1.1423e-40,\n",
      "         2.7760e-42, 6.6900e-17, 2.1892e-26, 1.7212e-41, 0.0000e+00, 1.2864e-33,\n",
      "         8.6025e-19, 1.9692e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1036.2088\n",
      "Test epoch : 9 Average loss: 901.8994\n",
      "PP(train) = 2560.054, PP(valid) = 2507.555\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7648e-42, 9.3930e-31, 1.3826e-29, 8.5877e-31, 2.7991e-15, 2.3292e-37,\n",
      "         1.0723e-36, 1.0000e+00, 2.5968e-12, 2.1451e-27, 1.9242e-32, 6.6541e-34,\n",
      "         8.8355e-12, 9.5151e-23, 1.7533e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1035.9348\n",
      "Test epoch : 10 Average loss: 901.7199\n",
      "PP(train) = 2553.845, PP(valid) = 2503.770\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4950e-39, 4.2559e-28, 1.2695e-26, 1.9986e-30, 1.4024e-12, 2.7185e-43,\n",
      "         1.1554e-37, 6.5459e-13, 1.9943e-12, 1.2304e-24, 7.1925e-38, 4.9476e-40,\n",
      "         2.8267e-08, 5.4453e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1035.5862\n",
      "Test epoch : 11 Average loss: 901.5427\n",
      "PP(train) = 2547.595, PP(valid) = 2499.911\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0623e-40, 3.2608e-35, 2.8922e-23, 3.2405e-22, 2.5382e-14, 5.6914e-35,\n",
      "         0.0000e+00, 3.0492e-11, 3.9134e-15, 5.6358e-33, 8.9683e-44, 2.5924e-37,\n",
      "         1.5566e-21, 3.6336e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1035.2380\n",
      "Test epoch : 12 Average loss: 901.3676\n",
      "PP(train) = 2541.316, PP(valid) = 2496.029\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0816e-35, 2.3620e-29, 4.6435e-07, 5.2395e-19, 2.3721e-13, 3.6747e-31,\n",
      "         0.0000e+00, 6.2653e-07, 1.6357e-11, 8.0215e-19, 9.6594e-25, 3.8083e-25,\n",
      "         2.8246e-21, 8.4303e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1034.9929\n",
      "Test epoch : 13 Average loss: 901.1936\n",
      "PP(train) = 2535.183, PP(valid) = 2492.238\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7143e-35, 2.5238e-29, 2.0122e-21, 1.5019e-19, 1.2721e-09, 1.6349e-33,\n",
      "         0.0000e+00, 3.9892e-01, 3.9467e-10, 3.5955e-20, 9.5758e-37, 1.0452e-35,\n",
      "         1.8400e-01, 6.4912e-23, 4.1708e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0775, 0.0715, 0.0787, 0.0512, 0.0649, 0.0784, 0.0859, 0.0690, 0.0614,\n",
      "         0.0526, 0.0797, 0.0519, 0.0475, 0.0808, 0.0491]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1034.6282\n",
      "Test epoch : 14 Average loss: 901.0198\n",
      "PP(train) = 2529.161, PP(valid) = 2488.537\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.4254e-29, 9.6176e-20, 9.5595e-22, 2.5783e-18, 2.1840e-32,\n",
      "         3.8975e-31, 5.2396e-10, 7.9969e-17, 9.6943e-22, 1.8676e-35, 2.3471e-38,\n",
      "         9.4382e-25, 3.8744e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1034.3672\n",
      "Test epoch : 15 Average loss: 900.8447\n",
      "PP(train) = 2523.172, PP(valid) = 2484.805\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5763e-38, 1.4957e-21, 9.6374e-11, 7.6381e-11, 1.4916e-11, 9.8505e-31,\n",
      "         2.0767e-37, 7.1615e-05, 1.4274e-09, 2.3883e-09, 5.2390e-22, 3.8141e-18,\n",
      "         1.7355e-13, 2.9126e-22, 9.9993e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1034.0765\n",
      "Test epoch : 16 Average loss: 900.6735\n",
      "PP(train) = 2517.150, PP(valid) = 2481.049\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3883e-32, 7.4493e-18, 1.2284e-17, 6.5802e-05, 3.5125e-34,\n",
      "         2.7015e-36, 2.6028e-05, 8.8245e-07, 6.6989e-17, 5.6164e-37, 1.5149e-30,\n",
      "         2.0361e-09, 4.1379e-21, 9.9991e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1033.6361\n",
      "Test epoch : 17 Average loss: 900.5038\n",
      "PP(train) = 2511.272, PP(valid) = 2477.398\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6781e-36, 1.8116e-20, 1.0977e-15, 2.1720e-30, 8.7167e-01, 1.3932e-39,\n",
      "         3.6803e-29, 8.6248e-02, 6.8613e-10, 1.7278e-15, 8.6739e-35, 1.0800e-24,\n",
      "         3.2917e-18, 1.6273e-23, 4.2082e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0491, 0.0518, 0.0454, 0.0396, 0.0938, 0.0687, 0.1052, 0.1010, 0.0468,\n",
      "         0.0858, 0.0713, 0.0591, 0.0456, 0.0656, 0.0712]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1033.4225\n",
      "Test epoch : 18 Average loss: 900.3372\n",
      "PP(train) = 2505.507, PP(valid) = 2473.875\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0433e-38, 4.7190e-23, 3.7255e-09, 7.3630e-23, 3.8291e-11, 8.0038e-32,\n",
      "         2.3032e-36, 1.8555e-03, 3.0756e-10, 1.5033e-29, 1.8082e-23, 2.6576e-24,\n",
      "         4.4332e-24, 6.7676e-20, 9.9814e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1042, 0.0502, 0.0601, 0.0670, 0.1075, 0.0922, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1033.1000\n",
      "Test epoch : 19 Average loss: 900.1700\n",
      "PP(train) = 2499.791, PP(valid) = 2470.367\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7714e-30, 3.0925e-26, 1.3127e-09, 9.3455e-21, 4.5863e-12, 3.0370e-23,\n",
      "         2.0213e-36, 9.9570e-01, 7.9216e-09, 9.6148e-18, 8.4347e-20, 6.7515e-34,\n",
      "         1.8873e-17, 1.2043e-26, 4.3023e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1052, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0371, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1032.8811\n",
      "Test epoch : 20 Average loss: 900.0038\n",
      "PP(train) = 2493.932, PP(valid) = 2466.629\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.7511e-33, 6.4285e-08, 1.0296e-05, 2.7658e-14, 3.5055e-16, 9.4129e-38,\n",
      "         1.5938e-37, 1.1522e-03, 7.6526e-09, 2.1959e-08, 1.6008e-29, 1.9522e-24,\n",
      "         4.0430e-16, 1.6634e-21, 9.9884e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1032.5996\n",
      "Test epoch : 21 Average loss: 899.8391\n",
      "PP(train) = 2488.215, PP(valid) = 2463.025\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8124e-37, 6.6454e-17, 2.6364e-12, 3.2350e-14, 5.6614e-02, 1.1857e-34,\n",
      "         1.1750e-34, 1.4295e-08, 2.7276e-12, 1.0184e-12, 1.6748e-30, 2.1898e-19,\n",
      "         9.2073e-01, 2.3542e-07, 2.2656e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0489, 0.0722, 0.0746, 0.0591, 0.0439, 0.0651, 0.0573, 0.0811, 0.0726,\n",
      "         0.0932, 0.0497, 0.0537, 0.0579, 0.0647, 0.1060]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1032.2180\n",
      "Test epoch : 22 Average loss: 899.6738\n",
      "PP(train) = 2482.745, PP(valid) = 2459.670\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6379e-38, 3.9002e-06, 6.4388e-28, 2.1471e-08, 1.9415e-33,\n",
      "         0.0000e+00, 4.4186e-12, 1.4557e-18, 3.4147e-18, 7.1166e-39, 5.5087e-23,\n",
      "         7.1005e-27, 4.5331e-10, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1031.8335\n",
      "Test epoch : 23 Average loss: 899.5099\n",
      "PP(train) = 2477.306, PP(valid) = 2456.282\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5848e-32, 7.5510e-34, 2.0974e-05, 5.0821e-26, 4.5069e-09, 8.3996e-37,\n",
      "         2.7674e-26, 3.4637e-11, 5.1245e-17, 6.7470e-25, 1.0942e-23, 7.6435e-21,\n",
      "         4.7705e-14, 1.5699e-22, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1031.6785\n",
      "Test epoch : 24 Average loss: 899.3482\n",
      "PP(train) = 2471.781, PP(valid) = 2452.839\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.3998e-34, 5.4908e-27, 5.2243e-08, 4.7539e-10, 1.1941e-08, 1.3834e-20,\n",
      "         1.6756e-24, 2.6945e-06, 9.9949e-01, 7.0426e-21, 3.8241e-39, 5.0253e-26,\n",
      "         2.4707e-07, 2.6144e-16, 5.0392e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1031.3434\n",
      "Test epoch : 25 Average loss: 899.1877\n",
      "PP(train) = 2466.197, PP(valid) = 2449.325\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2654e-42, 8.7825e-36, 1.9296e-22, 4.2126e-26, 9.5973e-08, 0.0000e+00,\n",
      "         1.7256e-33, 1.5799e-09, 1.3097e-18, 3.8437e-32, 3.1904e-40, 2.7376e-30,\n",
      "         3.5068e-18, 2.7005e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1031.0061\n",
      "Test epoch : 26 Average loss: 899.0291\n",
      "PP(train) = 2460.716, PP(valid) = 2445.899\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1616e-26, 1.1930e-13, 1.4776e-14, 3.7836e-15, 3.5032e-43,\n",
      "         1.1322e-42, 1.6477e-11, 5.1877e-13, 1.1884e-29, 3.4090e-30, 7.7636e-19,\n",
      "         2.1083e-13, 1.1425e-13, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1030.7098\n",
      "Test epoch : 27 Average loss: 898.8688\n",
      "PP(train) = 2455.480, PP(valid) = 2442.617\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3902e-37, 5.9069e-15, 3.1338e-04, 2.0611e-11, 6.4513e-13, 6.3492e-37,\n",
      "         7.0065e-45, 1.2240e-19, 1.1125e-06, 6.8474e-18, 2.2197e-29, 1.2060e-20,\n",
      "         1.8338e-05, 3.0113e-23, 9.9967e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1030.4743\n",
      "Test epoch : 28 Average loss: 898.7107\n",
      "PP(train) = 2450.227, PP(valid) = 2439.356\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0233e-37, 6.6083e-25, 8.0896e-12, 4.7792e-18, 9.9791e-01, 1.6857e-32,\n",
      "         3.0056e-32, 7.7781e-07, 2.0896e-03, 1.1374e-21, 1.0574e-32, 2.8488e-32,\n",
      "         1.9851e-11, 7.4761e-19, 9.1528e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0957, 0.0657, 0.1065, 0.1084, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0622, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1030.1855\n",
      "Test epoch : 29 Average loss: 898.5552\n",
      "PP(train) = 2444.885, PP(valid) = 2436.012\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.1380e-30, 6.9298e-14, 3.2947e-21, 1.6319e-10, 1.1216e-41,\n",
      "         5.7235e-41, 8.8336e-01, 1.1421e-19, 1.6330e-26, 1.1702e-41, 8.5836e-37,\n",
      "         2.5325e-19, 2.8669e-20, 1.1664e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0841, 0.0965, 0.0593, 0.0454, 0.0774, 0.0898, 0.0808, 0.0481, 0.0543,\n",
      "         0.0389, 0.1118, 0.0455, 0.0384, 0.0919, 0.0378]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1029.8110\n",
      "Test epoch : 30 Average loss: 898.4008\n",
      "PP(train) = 2439.626, PP(valid) = 2432.717\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1533e-36, 1.6233e-26, 3.6832e-02, 2.7019e-08, 1.5773e-14, 1.0318e-38,\n",
      "         1.6137e-35, 2.4811e-02, 4.0819e-14, 1.1352e-24, 2.0715e-29, 3.6080e-20,\n",
      "         4.1956e-05, 2.0190e-21, 9.3832e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0820, 0.0467, 0.1000, 0.0510, 0.0616, 0.0670, 0.1058, 0.0904, 0.0605,\n",
      "         0.0523, 0.0633, 0.0552, 0.0509, 0.0722, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1029.5635\n",
      "Test epoch : 31 Average loss: 898.2460\n",
      "PP(train) = 2434.512, PP(valid) = 2429.506\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.1243e-34, 6.7377e-28, 4.2449e-18, 5.5434e-22, 3.6966e-42,\n",
      "         4.6262e-33, 2.5652e-08, 1.6528e-15, 5.3658e-28, 3.3564e-28, 2.2209e-33,\n",
      "         1.1907e-09, 2.4532e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1029.3288\n",
      "Test epoch : 32 Average loss: 898.0917\n",
      "PP(train) = 2429.421, PP(valid) = 2426.312\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2779e-29, 1.0554e-32, 3.3828e-06, 3.0998e-20, 1.5999e-20, 5.0457e-27,\n",
      "         6.1651e-36, 1.5016e-07, 8.0505e-01, 1.0132e-16, 6.6727e-37, 7.8418e-20,\n",
      "         4.0660e-14, 4.6369e-18, 1.9495e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0612, 0.0629, 0.0530, 0.0610, 0.0352, 0.0751, 0.0708, 0.0932, 0.0582,\n",
      "         0.0665, 0.0807, 0.0640, 0.0517, 0.1180, 0.0486]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1029.0607\n",
      "Test epoch : 33 Average loss: 897.9397\n",
      "PP(train) = 2424.282, PP(valid) = 2423.066\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.9645e-43, 1.6360e-38, 5.4186e-14, 2.3018e-20, 1.7797e-13, 4.2940e-33,\n",
      "         5.3855e-36, 4.4676e-17, 2.6117e-09, 1.4408e-16, 7.6902e-33, 9.5187e-31,\n",
      "         3.9892e-17, 6.0837e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1028.7636\n",
      "Test epoch : 34 Average loss: 897.7882\n",
      "PP(train) = 2419.137, PP(valid) = 2419.780\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1013e-25, 8.0006e-23, 4.0678e-20, 2.5551e-19, 0.0000e+00,\n",
      "         2.8026e-45, 4.6467e-23, 1.1752e-23, 5.0796e-34, 1.9558e-40, 4.0982e-37,\n",
      "         9.8348e-21, 1.5645e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1028.4381\n",
      "Test epoch : 35 Average loss: 897.6386\n",
      "PP(train) = 2414.167, PP(valid) = 2416.680\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4951e-34, 4.2604e-22, 5.7094e-20, 1.0454e-14, 4.9046e-10, 1.1617e-35,\n",
      "         7.2615e-30, 1.3902e-06, 8.2700e-12, 6.0214e-20, 6.7605e-39, 1.6446e-38,\n",
      "         6.0571e-12, 5.2814e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1028.2295\n",
      "Test epoch : 36 Average loss: 897.4894\n",
      "PP(train) = 2409.307, PP(valid) = 2413.675\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2755e-38, 3.0682e-29, 3.3095e-08, 1.5167e-22, 3.9412e-12, 1.1210e-44,\n",
      "         1.5272e-36, 2.3846e-18, 8.6488e-21, 4.5870e-20, 0.0000e+00, 3.3189e-29,\n",
      "         4.9360e-15, 6.6922e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1027.9316\n",
      "Test epoch : 37 Average loss: 897.3430\n",
      "PP(train) = 2404.416, PP(valid) = 2410.632\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7823e-29, 2.4248e-15, 2.4972e-09, 3.0777e-16, 3.3442e-09, 9.8709e-34,\n",
      "         8.9532e-34, 9.8004e-01, 1.1634e-15, 9.2959e-26, 6.5494e-35, 1.2246e-22,\n",
      "         8.1909e-05, 2.7243e-27, 1.9880e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1040, 0.0552, 0.0444, 0.0788, 0.0919, 0.0775, 0.0444, 0.0533,\n",
      "         0.0373, 0.1180, 0.0441, 0.0369, 0.0936, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1027.6731\n",
      "Test epoch : 38 Average loss: 897.1957\n",
      "PP(train) = 2399.417, PP(valid) = 2407.450\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5590e-32, 3.1759e-25, 4.5227e-08, 2.2079e-24, 1.9113e-04, 5.8629e-33,\n",
      "         1.4831e-33, 1.1358e-10, 4.9459e-09, 2.6229e-22, 1.1242e-21, 5.4873e-23,\n",
      "         1.2768e-07, 1.8990e-21, 9.9981e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1027.4766\n",
      "Test epoch : 39 Average loss: 897.0484\n",
      "PP(train) = 2394.514, PP(valid) = 2404.307\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1765e-35, 2.0007e-25, 2.3267e-25, 2.5038e-17, 5.2722e-17, 8.2160e-38,\n",
      "         1.7304e-38, 5.9326e-14, 5.9828e-05, 2.6922e-28, 2.8551e-41, 1.1722e-21,\n",
      "         6.7130e-10, 4.7323e-29, 9.9994e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1027.1867\n",
      "Test epoch : 40 Average loss: 896.9036\n",
      "PP(train) = 2389.733, PP(valid) = 2401.310\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4969e-36, 1.4784e-26, 3.3742e-18, 3.0901e-10, 3.9030e-16, 1.3792e-32,\n",
      "         3.6003e-38, 4.3452e-09, 2.4231e-15, 7.9499e-29, 9.8824e-29, 3.7370e-27,\n",
      "         6.1990e-08, 4.6787e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1026.9003\n",
      "Test epoch : 41 Average loss: 896.7601\n",
      "PP(train) = 2385.054, PP(valid) = 2398.428\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.5955e-41, 3.5010e-23, 1.5228e-10, 1.0392e-26, 4.9330e-18, 4.0012e-36,\n",
      "         1.0425e-33, 2.8713e-14, 1.6675e-13, 2.5284e-09, 3.7959e-36, 1.7395e-24,\n",
      "         3.1892e-22, 3.6074e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1026.6396\n",
      "Test epoch : 42 Average loss: 896.6149\n",
      "PP(train) = 2380.361, PP(valid) = 2395.445\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9214e-40, 9.8719e-31, 2.0984e-25, 2.0909e-19, 1.6338e-13, 2.3632e-40,\n",
      "         0.0000e+00, 8.5307e-08, 3.9163e-05, 3.9299e-30, 6.5989e-35, 1.7975e-15,\n",
      "         1.2226e-11, 5.2689e-21, 9.9996e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1026.3011\n",
      "Test epoch : 43 Average loss: 896.4724\n",
      "PP(train) = 2375.612, PP(valid) = 2392.457\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9236e-44, 1.7974e-26, 1.5363e-04, 1.4486e-14, 2.8523e-07, 1.3468e-32,\n",
      "         4.1502e-36, 3.7631e-11, 1.8812e-03, 6.9877e-26, 1.0824e-27, 3.4383e-27,\n",
      "         4.5839e-19, 1.3366e-22, 9.9796e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1074, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1026.0886\n",
      "Test epoch : 44 Average loss: 896.3297\n",
      "PP(train) = 2370.987, PP(valid) = 2389.520\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.8000e-41, 3.6042e-36, 2.0373e-20, 3.4421e-29, 1.4235e-09, 0.0000e+00,\n",
      "         1.9870e-42, 2.2505e-18, 2.7886e-18, 1.2991e-23, 5.8154e-43, 5.6860e-33,\n",
      "         8.9547e-25, 3.3695e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1025.7628\n",
      "Test epoch : 45 Average loss: 896.1895\n",
      "PP(train) = 2366.343, PP(valid) = 2386.592\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5298e-38, 1.7398e-26, 1.4423e-18, 1.0799e-23, 5.2445e-29, 0.0000e+00,\n",
      "         1.4013e-45, 3.7924e-12, 1.1075e-31, 2.2142e-31, 5.6052e-45, 4.7924e-43,\n",
      "         3.9872e-32, 5.5370e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1025.6324\n",
      "Test epoch : 46 Average loss: 896.0488\n",
      "PP(train) = 2361.813, PP(valid) = 2383.740\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5086e-34, 5.1209e-19, 5.7879e-14, 9.2934e-04, 3.0858e-13, 2.1139e-33,\n",
      "         3.8522e-32, 2.3607e-11, 2.6471e-02, 7.0823e-27, 1.7292e-42, 3.0517e-23,\n",
      "         7.8406e-01, 4.9171e-17, 1.8854e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0544, 0.0684, 0.0809, 0.0595, 0.0447, 0.0664, 0.0627, 0.0829, 0.0721,\n",
      "         0.0848, 0.0522, 0.0546, 0.0577, 0.0678, 0.0910]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1025.3532\n",
      "Test epoch : 47 Average loss: 895.9106\n",
      "PP(train) = 2357.262, PP(valid) = 2380.879\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8169e-41, 5.9484e-21, 6.8676e-06, 8.5480e-16, 5.0554e-16, 1.6202e-29,\n",
      "         2.2143e-25, 2.4357e-03, 4.3184e-16, 5.6560e-21, 4.3374e-35, 9.1155e-25,\n",
      "         9.9755e-01, 1.0555e-14, 6.1147e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0483, 0.0745, 0.0761, 0.0605, 0.0414, 0.0647, 0.0539, 0.0788, 0.0746,\n",
      "         0.0936, 0.0483, 0.0529, 0.0584, 0.0644, 0.1096]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1025.0909\n",
      "Test epoch : 48 Average loss: 895.7729\n",
      "PP(train) = 2352.613, PP(valid) = 2377.928\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0236e-33, 1.0338e-27, 1.2181e-25, 8.3740e-22, 0.0000e+00,\n",
      "         2.4548e-41, 3.2064e-15, 8.8027e-19, 6.3577e-37, 1.8217e-44, 6.3458e-41,\n",
      "         1.0554e-09, 5.1183e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1024.7393\n",
      "Test epoch : 49 Average loss: 895.6357\n",
      "PP(train) = 2348.175, PP(valid) = 2375.141\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5015e-32, 3.7694e-31, 1.5457e-21, 1.7732e-11, 9.9996e-01, 6.8539e-38,\n",
      "         1.4013e-45, 6.9012e-12, 3.4046e-11, 1.2948e-26, 2.6352e-32, 3.0388e-33,\n",
      "         1.9642e-19, 1.5715e-17, 4.2526e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1024.5811\n",
      "Test epoch : 50 Average loss: 895.4983\n",
      "PP(train) = 2343.862, PP(valid) = 2372.450\n",
      "Writing to ./topicwords/12-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 7, p : 8.0478 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                          \n",
      "\n",
      "===== # 2, Topic : 7, p : 8.2668 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                  \n",
      "\n",
      "===== # 3, Topic : 12, p : 8.0521 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                 \n",
      "\n",
      "===== # 4, Topic : 14, p : 8.6335 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                         mm                                                         mm  ii         \n",
      "\n",
      "===== # 5, Topic : 14, p : 8.8688 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                            \n",
      "\n",
      "===== # 6, Topic : 14, p : 8.4429 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                 )                                               \n",
      "\n",
      "===== # 7, Topic : 14, p : 8.7834 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                            \n",
      "\n",
      "===== # 8, Topic : 7, p : 8.7611 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                \n",
      "\n",
      "===== # 9, Topic : 7, p : 8.5758 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                  \n",
      "\n",
      "===== # 10, Topic : 14, p : 9.7561 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                Triangulated Irregular Network                                        \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 5.4474e-37, 2.0416e-23, 2.7469e-24, 8.6961e-31, 6.1951e-42,\n",
      "         0.0000e+00, 9.5982e-27, 1.2757e-14, 3.6960e-28, 6.0355e-37, 1.3096e-33,\n",
      "         1.2919e-19, 3.7516e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 921.7447\n",
      "Test epoch : 1 Average loss: 999.8067\n",
      "PP(train) = 2680.779, PP(valid) = 2564.299\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1735e-27, 1.1627e-24, 1.5367e-14, 9.1538e-10, 8.6324e-09, 3.5258e-39,\n",
      "         4.6200e-34, 1.9053e-30, 4.4622e-07, 4.9199e-21, 3.7635e-26, 4.5112e-22,\n",
      "         1.1037e-03, 1.6403e-19, 9.9890e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0594,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 921.5637\n",
      "Test epoch : 2 Average loss: 999.6828\n",
      "PP(train) = 2676.392, PP(valid) = 2562.483\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0676e-31, 2.3777e-22, 6.1781e-07, 8.2294e-18, 1.9399e-15, 3.9191e-31,\n",
      "         2.7923e-31, 7.7361e-12, 7.0908e-01, 6.5046e-23, 3.2197e-21, 1.0787e-27,\n",
      "         6.8532e-13, 1.5979e-20, 2.9092e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0637, 0.0607, 0.0578, 0.0599, 0.0377, 0.0745, 0.0748, 0.0937, 0.0587,\n",
      "         0.0650, 0.0786, 0.0633, 0.0519, 0.1117, 0.0480]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 921.3455\n",
      "Test epoch : 3 Average loss: 999.5544\n",
      "PP(train) = 2670.413, PP(valid) = 2559.805\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9618e-39, 1.6183e-33, 8.8999e-14, 1.7177e-27, 7.7365e-22, 1.0676e-28,\n",
      "         4.3565e-31, 4.4995e-15, 2.1447e-18, 8.1803e-26, 1.4013e-45, 5.3186e-23,\n",
      "         2.4339e-24, 4.3974e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 921.0401\n",
      "Test epoch : 4 Average loss: 999.4252\n",
      "PP(train) = 2663.757, PP(valid) = 2556.825\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1017e-36, 1.4548e-32, 6.7283e-13, 8.2974e-15, 2.8530e-10, 5.7299e-30,\n",
      "         5.3429e-30, 6.4531e-10, 8.5896e-23, 1.3232e-25, 5.1558e-34, 3.0766e-26,\n",
      "         6.5888e-09, 6.2585e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 920.8081\n",
      "Test epoch : 5 Average loss: 999.2956\n",
      "PP(train) = 2657.255, PP(valid) = 2554.066\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.2304e-30, 9.1676e-32, 4.6270e-21, 7.1702e-19, 6.7388e-01, 1.2115e-39,\n",
      "         4.6762e-23, 9.0545e-18, 1.0941e-04, 1.6082e-18, 6.4671e-28, 5.4136e-23,\n",
      "         2.6805e-15, 1.2261e-18, 3.2601e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0554, 0.0475, 0.0576, 0.0424, 0.0838, 0.0672, 0.1087, 0.1046, 0.0501,\n",
      "         0.0789, 0.0665, 0.0596, 0.0481, 0.0659, 0.0638]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 920.4898\n",
      "Test epoch : 6 Average loss: 999.1630\n",
      "PP(train) = 2651.057, PP(valid) = 2551.642\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2609e-27, 2.5056e-31, 5.7891e-20, 8.4311e-22, 4.7870e-29, 1.5250e-24,\n",
      "         1.0632e-40, 3.3183e-22, 6.4752e-17, 2.0869e-27, 4.7647e-30, 7.3770e-24,\n",
      "         3.5404e-19, 3.3859e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 920.3209\n",
      "Test epoch : 7 Average loss: 999.0315\n",
      "PP(train) = 2644.806, PP(valid) = 2549.187\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1084e-44, 0.0000e+00, 5.9672e-20, 5.6175e-21, 1.6992e-15, 1.7705e-30,\n",
      "         2.9680e-42, 2.1152e-22, 1.1042e-03, 5.6381e-16, 2.7097e-28, 1.5387e-28,\n",
      "         5.6035e-19, 2.5586e-22, 9.9890e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 919.8758\n",
      "Test epoch : 8 Average loss: 998.9014\n",
      "PP(train) = 2638.396, PP(valid) = 2546.570\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7327e-37, 8.9627e-24, 1.1488e-12, 8.4249e-25, 2.2705e-09, 3.4245e-19,\n",
      "         1.8672e-26, 2.0135e-10, 1.0000e+00, 2.0832e-29, 1.0746e-28, 3.7396e-27,\n",
      "         6.5195e-14, 3.0199e-19, 1.8973e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 919.6526\n",
      "Test epoch : 9 Average loss: 998.7709\n",
      "PP(train) = 2631.895, PP(valid) = 2543.792\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.9492e-44, 1.8676e-35, 1.0331e-19, 7.2939e-30, 2.7093e-18, 1.5817e-38,\n",
      "         1.1403e-33, 1.0142e-13, 1.6188e-10, 4.4035e-26, 1.7759e-35, 2.3022e-26,\n",
      "         8.3193e-17, 2.2399e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 919.3549\n",
      "Test epoch : 10 Average loss: 998.6435\n",
      "PP(train) = 2625.593, PP(valid) = 2541.228\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5951e-22, 1.0639e-20, 1.1006e-05, 5.0953e-15, 3.1969e-02, 1.8244e-14,\n",
      "         2.4950e-30, 7.6923e-09, 4.6750e-02, 2.5128e-09, 2.7259e-29, 1.1574e-15,\n",
      "         9.1956e-08, 1.1973e-10, 9.2127e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0793, 0.0456, 0.0980, 0.0506, 0.0595, 0.0677, 0.1055, 0.0934, 0.0591,\n",
      "         0.0543, 0.0634, 0.0565, 0.0512, 0.0731, 0.0429]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 919.1260\n",
      "Test epoch : 11 Average loss: 998.5184\n",
      "PP(train) = 2619.514, PP(valid) = 2538.819\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5414e-44, 3.6981e-38, 1.9407e-12, 2.5726e-29, 3.6220e-16, 8.0407e-19,\n",
      "         4.9029e-34, 2.6294e-19, 1.9553e-21, 4.2690e-21, 4.0597e-30, 3.1960e-30,\n",
      "         8.7671e-07, 4.1229e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 918.7921\n",
      "Test epoch : 12 Average loss: 998.3913\n",
      "PP(train) = 2613.446, PP(valid) = 2536.355\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2348e-33, 8.7745e-39, 7.6091e-12, 1.3255e-03, 9.8774e-01, 1.3573e-36,\n",
      "         1.7139e-30, 1.0934e-02, 6.0322e-14, 1.2383e-15, 3.4037e-27, 8.0809e-19,\n",
      "         5.9106e-12, 4.3056e-33, 8.4497e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0484, 0.0424, 0.0383, 0.0958, 0.0660, 0.1063, 0.1074, 0.0452,\n",
      "         0.0932, 0.0677, 0.0600, 0.0455, 0.0624, 0.0762]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 918.5235\n",
      "Test epoch : 13 Average loss: 998.2652\n",
      "PP(train) = 2607.376, PP(valid) = 2533.842\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2948e-42, 5.1930e-31, 2.5491e-26, 1.1151e-15, 1.7606e-12, 9.8679e-25,\n",
      "         2.2079e-29, 2.6498e-22, 2.3362e-15, 5.1220e-35, 3.2331e-36, 5.2305e-34,\n",
      "         4.5421e-23, 2.0238e-13, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 918.2437\n",
      "Test epoch : 14 Average loss: 998.1430\n",
      "PP(train) = 2601.347, PP(valid) = 2531.377\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.9753e-39, 4.9331e-27, 1.7807e-15, 1.5584e-10, 4.0811e-09, 1.6168e-37,\n",
      "         5.4364e-31, 2.6755e-16, 2.4092e-14, 1.2585e-09, 1.0457e-30, 4.9830e-26,\n",
      "         1.4924e-10, 2.0895e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 917.9396\n",
      "Test epoch : 15 Average loss: 998.0189\n",
      "PP(train) = 2595.395, PP(valid) = 2528.921\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 1.6494e-35, 4.3269e-10, 1.2791e-19, 5.2162e-14, 5.8709e-34,\n",
      "         4.7720e-27, 2.2832e-11, 5.4819e-14, 1.7954e-12, 3.8425e-32, 2.0454e-25,\n",
      "         1.8528e-12, 1.8853e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 917.6872\n",
      "Test epoch : 16 Average loss: 997.8963\n",
      "PP(train) = 2589.615, PP(valid) = 2526.594\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.9880e-38, 4.2413e-33, 1.8858e-17, 2.8375e-24, 1.0000e+00, 1.7253e-19,\n",
      "         3.7876e-36, 5.0705e-13, 1.5217e-12, 3.2970e-26, 1.4947e-32, 5.1296e-23,\n",
      "         1.6842e-11, 3.1788e-36, 1.0753e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 917.4606\n",
      "Test epoch : 17 Average loss: 997.7763\n",
      "PP(train) = 2583.789, PP(valid) = 2524.209\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2927e-27, 2.9611e-18, 6.0343e-16, 9.9042e-13, 6.3925e-01, 5.0486e-35,\n",
      "         3.4655e-32, 4.4734e-12, 1.3972e-07, 8.9266e-25, 4.0234e-28, 2.9841e-16,\n",
      "         2.0031e-01, 3.4902e-08, 1.6044e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0511, 0.0528, 0.0561, 0.0447, 0.0769, 0.0671, 0.0952, 0.1013, 0.0532,\n",
      "         0.0874, 0.0634, 0.0591, 0.0499, 0.0653, 0.0764]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 917.2100\n",
      "Test epoch : 18 Average loss: 997.6568\n",
      "PP(train) = 2577.951, PP(valid) = 2521.743\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7687e-37, 5.8172e-28, 1.1682e-32, 5.3414e-16, 1.8301e-36,\n",
      "         8.6217e-25, 1.8290e-21, 1.5898e-19, 9.9260e-32, 1.7876e-32, 2.9154e-26,\n",
      "         2.0484e-21, 2.5240e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 916.9728\n",
      "Test epoch : 19 Average loss: 997.5358\n",
      "PP(train) = 2572.229, PP(valid) = 2519.322\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7526e-35, 3.2022e-28, 2.6345e-10, 7.6984e-14, 1.1234e-11, 2.0696e-19,\n",
      "         3.0078e-20, 8.8263e-08, 1.6916e-01, 3.4724e-13, 1.1968e-22, 6.6337e-18,\n",
      "         4.3229e-14, 2.2559e-20, 8.3084e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0777, 0.0482, 0.0914, 0.0528, 0.0542, 0.0692, 0.0994, 0.0934, 0.0597,\n",
      "         0.0555, 0.0661, 0.0578, 0.0517, 0.0796, 0.0433]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 916.7271\n",
      "Test epoch : 20 Average loss: 997.4176\n",
      "PP(train) = 2566.685, PP(valid) = 2517.055\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.4661e-32, 8.4143e-25, 1.5149e-16, 3.0121e-18, 1.2947e-11, 5.3611e-16,\n",
      "         8.8776e-25, 9.5345e-11, 9.9705e-01, 3.2297e-22, 3.1778e-18, 1.1823e-24,\n",
      "         2.6237e-03, 9.5537e-10, 3.2429e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0444, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0845, 0.0651, 0.0511, 0.1311, 0.0499]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 916.5314\n",
      "Test epoch : 21 Average loss: 997.3025\n",
      "PP(train) = 2561.077, PP(valid) = 2514.752\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2815e-36, 5.6256e-28, 1.0766e-04, 4.0696e-13, 7.1717e-07, 3.0316e-21,\n",
      "         1.2335e-38, 7.4887e-10, 5.3836e-07, 7.3479e-19, 1.0275e-20, 1.0252e-21,\n",
      "         2.1274e-12, 4.7737e-23, 9.9989e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 916.1503\n",
      "Test epoch : 22 Average loss: 997.1856\n",
      "PP(train) = 2555.531, PP(valid) = 2512.442\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9741e-39, 2.2872e-24, 5.2429e-24, 4.6815e-19, 7.7285e-13, 3.9674e-32,\n",
      "         7.7859e-40, 4.1446e-13, 2.2217e-13, 1.5227e-20, 2.0079e-26, 7.4988e-18,\n",
      "         3.9387e-12, 2.7740e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 915.9659\n",
      "Test epoch : 23 Average loss: 997.0702\n",
      "PP(train) = 2550.077, PP(valid) = 2510.199\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5461e-38, 1.9332e-38, 1.3105e-05, 6.9301e-13, 5.1991e-09, 1.2421e-19,\n",
      "         4.2722e-30, 1.3197e-17, 5.9220e-06, 7.2787e-21, 1.3125e-28, 1.5822e-17,\n",
      "         1.6015e-18, 1.7726e-30, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 915.6688\n",
      "Test epoch : 24 Average loss: 996.9558\n",
      "PP(train) = 2544.681, PP(valid) = 2507.994\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1160e-43, 1.6960e-32, 2.0625e-07, 5.7054e-26, 2.3261e-14, 2.1313e-23,\n",
      "         5.9179e-24, 1.6923e-22, 1.0264e-15, 2.0135e-27, 3.9247e-37, 6.8308e-31,\n",
      "         8.3388e-17, 1.6791e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 915.4494\n",
      "Test epoch : 25 Average loss: 996.8438\n",
      "PP(train) = 2539.241, PP(valid) = 2505.716\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3770e-41, 1.2202e-26, 1.6148e-12, 5.3866e-20, 2.1766e-07, 4.5174e-29,\n",
      "         4.2039e-45, 2.0922e-11, 8.8486e-15, 1.6131e-25, 7.1361e-28, 1.3496e-28,\n",
      "         2.0708e-18, 3.0443e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 915.1532\n",
      "Test epoch : 26 Average loss: 996.7330\n",
      "PP(train) = 2533.997, PP(valid) = 2503.594\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 3.5473e-23, 2.6932e-26, 2.3423e-29, 2.9698e-20, 5.5489e-30,\n",
      "         1.3203e-38, 1.8151e-29, 4.2522e-30, 5.3420e-26, 9.2065e-43, 4.2534e-34,\n",
      "         3.2650e-30, 1.4911e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 915.0032\n",
      "Test epoch : 27 Average loss: 996.6221\n",
      "PP(train) = 2528.768, PP(valid) = 2501.447\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3159e-40, 1.2883e-25, 2.5608e-10, 5.7518e-18, 3.7366e-07, 1.1193e-18,\n",
      "         1.2602e-31, 8.1264e-07, 7.2979e-01, 8.0509e-17, 5.6079e-21, 6.6858e-13,\n",
      "         1.1095e-10, 2.6859e-01, 1.6154e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0521, 0.0571, 0.0517, 0.0714, 0.0396, 0.0808, 0.0595, 0.0839, 0.0731,\n",
      "         0.0655, 0.0806, 0.0615, 0.0546, 0.1215, 0.0470]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 914.7371\n",
      "Test epoch : 28 Average loss: 996.5122\n",
      "PP(train) = 2523.513, PP(valid) = 2499.259\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3987e-32, 9.3257e-29, 1.0038e-09, 2.1749e-16, 3.1705e-12, 3.2589e-21,\n",
      "         3.6760e-23, 2.9306e-15, 7.2136e-10, 6.7607e-24, 9.1101e-25, 5.8571e-15,\n",
      "         4.0265e-18, 8.2671e-17, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 914.5840\n",
      "Test epoch : 29 Average loss: 996.4035\n",
      "PP(train) = 2518.306, PP(valid) = 2497.079\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.4339e-23, 1.9374e-22, 1.5090e-02, 3.9567e-18, 2.8483e-16, 1.1533e-25,\n",
      "         2.8987e-26, 8.3201e-03, 9.7630e-01, 2.0823e-11, 1.5494e-20, 9.4381e-10,\n",
      "         4.5841e-09, 2.2828e-30, 2.8598e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0567, 0.0680, 0.0444, 0.0629, 0.0312, 0.0757, 0.0633, 0.0912, 0.0575,\n",
      "         0.0689, 0.0846, 0.0647, 0.0510, 0.1304, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 914.2914\n",
      "Test epoch : 30 Average loss: 996.2967\n",
      "PP(train) = 2513.160, PP(valid) = 2494.970\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.9126e-34, 1.5027e-27, 4.6764e-09, 2.1030e-15, 4.5346e-13, 5.7448e-31,\n",
      "         1.8032e-22, 1.1692e-11, 3.5634e-07, 1.9181e-34, 2.3490e-36, 6.0423e-27,\n",
      "         1.9450e-07, 2.0138e-17, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 913.9911\n",
      "Test epoch : 31 Average loss: 996.1895\n",
      "PP(train) = 2508.135, PP(valid) = 2492.952\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.6553e-29, 1.9656e-35, 3.3789e-33, 1.0000e+00, 3.7028e-32,\n",
      "         1.1325e-31, 3.1365e-26, 3.5388e-22, 2.4617e-26, 1.6831e-35, 7.8173e-32,\n",
      "         4.0008e-38, 6.4460e-39, 2.2392e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 913.7871\n",
      "Test epoch : 32 Average loss: 996.0804\n",
      "PP(train) = 2503.086, PP(valid) = 2490.829\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8211e-30, 7.7503e-28, 5.0731e-12, 2.5868e-20, 2.4090e-06, 9.4285e-30,\n",
      "         2.3058e-25, 1.1979e-06, 4.2062e-02, 2.0068e-19, 1.9317e-17, 3.4013e-14,\n",
      "         5.2722e-12, 2.5265e-13, 9.5793e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0808, 0.0453, 0.1010, 0.0509, 0.0586, 0.0675, 0.1055, 0.0926, 0.0594,\n",
      "         0.0531, 0.0630, 0.0561, 0.0512, 0.0730, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 913.5213\n",
      "Test epoch : 33 Average loss: 995.9743\n",
      "PP(train) = 2497.998, PP(valid) = 2488.662\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5489e-33, 4.2593e-16, 1.6745e-12, 3.9462e-24, 1.6713e-11, 4.9267e-15,\n",
      "         4.0240e-30, 6.6771e-09, 7.6070e-05, 4.3405e-17, 1.4003e-21, 3.8197e-24,\n",
      "         1.3465e-06, 5.3251e-24, 9.9992e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 913.3603\n",
      "Test epoch : 34 Average loss: 995.8697\n",
      "PP(train) = 2493.098, PP(valid) = 2486.668\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8664e-44, 6.4787e-21, 1.1772e-15, 5.0297e-17, 3.4719e-04, 2.5037e-36,\n",
      "         6.9295e-32, 2.1493e-16, 6.0749e-14, 2.2255e-14, 4.1400e-29, 1.5629e-20,\n",
      "         1.2789e-05, 1.2841e-18, 9.9964e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 913.0075\n",
      "Test epoch : 35 Average loss: 995.7646\n",
      "PP(train) = 2488.257, PP(valid) = 2484.726\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.6004e-35, 2.9425e-20, 6.6238e-09, 6.3381e-13, 3.6479e-02, 1.5029e-32,\n",
      "         1.0464e-28, 4.2328e-15, 1.6616e-02, 4.6881e-15, 1.7525e-27, 1.9403e-27,\n",
      "         1.6583e-06, 4.6472e-23, 9.4690e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0798, 0.0450, 0.0999, 0.0501, 0.0607, 0.0673, 0.1070, 0.0933, 0.0589,\n",
      "         0.0539, 0.0627, 0.0561, 0.0510, 0.0715, 0.0427]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 912.7734\n",
      "Test epoch : 36 Average loss: 995.6610\n",
      "PP(train) = 2483.417, PP(valid) = 2482.760\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.4031e-26, 1.1795e-21, 2.4728e-06, 5.3350e-17, 9.9288e-01, 1.7036e-19,\n",
      "         1.6459e-29, 4.7058e-13, 1.4382e-08, 6.4747e-15, 1.4653e-23, 2.3962e-16,\n",
      "         9.7647e-09, 3.4844e-27, 7.1147e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0449, 0.0479, 0.0424, 0.0382, 0.0957, 0.0657, 0.1067, 0.1083, 0.0451,\n",
      "         0.0936, 0.0672, 0.0601, 0.0456, 0.0622, 0.0764]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 912.6792\n",
      "Test epoch : 37 Average loss: 995.5597\n",
      "PP(train) = 2478.523, PP(valid) = 2480.738\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8281e-30, 8.7803e-41, 8.8092e-19, 5.9845e-22, 7.4250e-14, 1.6018e-27,\n",
      "         2.8138e-28, 5.3076e-10, 1.4468e-02, 3.1989e-21, 7.9966e-27, 1.8808e-29,\n",
      "         4.6061e-26, 1.5040e-31, 9.8553e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.0447, 0.1032, 0.0504, 0.0596, 0.0671, 0.1068, 0.0924, 0.0594,\n",
      "         0.0526, 0.0623, 0.0558, 0.0511, 0.0716, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 912.3644\n",
      "Test epoch : 38 Average loss: 995.4593\n",
      "PP(train) = 2473.656, PP(valid) = 2478.660\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7312e-39, 9.8837e-34, 7.1826e-16, 8.8764e-27, 1.0747e-13, 1.1713e-35,\n",
      "         1.1935e-22, 3.2962e-25, 9.5350e-27, 4.5083e-27, 2.4959e-31, 3.0321e-35,\n",
      "         1.3865e-19, 6.7338e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 912.1472\n",
      "Test epoch : 39 Average loss: 995.3606\n",
      "PP(train) = 2468.948, PP(valid) = 2476.757\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7275e-38, 3.2485e-25, 6.9353e-22, 1.7435e-23, 5.6589e-21, 7.9006e-32,\n",
      "         3.8615e-29, 7.6883e-14, 2.3417e-08, 6.3373e-25, 3.9876e-23, 5.4319e-15,\n",
      "         1.3604e-08, 9.4704e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 911.9523\n",
      "Test epoch : 40 Average loss: 995.2588\n",
      "PP(train) = 2464.277, PP(valid) = 2474.853\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.3207e-23, 2.3875e-20, 5.7649e-08, 3.2276e-08, 2.5276e-04, 8.3863e-17,\n",
      "         1.4059e-18, 5.6264e-07, 2.1806e-07, 3.1076e-12, 2.1250e-18, 2.4201e-16,\n",
      "         9.9417e-01, 8.2020e-16, 5.5811e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0484, 0.0742, 0.0763, 0.0605, 0.0414, 0.0646, 0.0541, 0.0790, 0.0745,\n",
      "         0.0935, 0.0483, 0.0529, 0.0585, 0.0644, 0.1094]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 911.7971\n",
      "Test epoch : 41 Average loss: 995.1608\n",
      "PP(train) = 2459.568, PP(valid) = 2472.912\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3737e-22, 8.0789e-21, 3.7852e-12, 1.7348e-20, 8.1052e-07, 1.5693e-18,\n",
      "         6.3482e-31, 1.7773e-05, 2.2638e-05, 1.1734e-20, 6.5823e-25, 3.5497e-13,\n",
      "         3.5650e-10, 1.3760e-24, 9.9996e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 911.5804\n",
      "Test epoch : 42 Average loss: 995.0639\n",
      "PP(train) = 2454.908, PP(valid) = 2470.980\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6707e-42, 1.8357e-38, 2.6819e-14, 6.8738e-22, 1.8772e-20, 7.2552e-30,\n",
      "         8.8282e-44, 7.0302e-13, 2.4958e-22, 2.8411e-24, 9.9424e-35, 1.7820e-31,\n",
      "         8.4245e-15, 5.1690e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 911.2285\n",
      "Test epoch : 43 Average loss: 994.9643\n",
      "PP(train) = 2450.341, PP(valid) = 2469.128\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0577e-40, 4.6413e-28, 2.2775e-14, 8.8821e-27, 7.9277e-20, 1.8343e-39,\n",
      "         5.1107e-26, 7.0558e-13, 5.6316e-11, 1.2381e-24, 4.2136e-29, 3.9789e-34,\n",
      "         4.4847e-23, 8.9587e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 911.0081\n",
      "Test epoch : 44 Average loss: 994.8674\n",
      "PP(train) = 2445.861, PP(valid) = 2467.387\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5164e-35, 3.6429e-28, 9.0598e-24, 1.2149e-21, 5.5321e-15, 3.5251e-36,\n",
      "         1.2871e-32, 1.2499e-16, 5.8972e-10, 1.1371e-23, 4.0672e-36, 6.4011e-26,\n",
      "         2.3595e-06, 1.5954e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 910.8092\n",
      "Test epoch : 45 Average loss: 994.7724\n",
      "PP(train) = 2441.375, PP(valid) = 2465.569\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6279e-35, 3.0285e-23, 9.7883e-09, 2.5148e-11, 5.0901e-04, 1.0875e-24,\n",
      "         2.2975e-30, 2.5356e-03, 9.7104e-04, 3.4524e-24, 4.7114e-16, 2.5174e-23,\n",
      "         6.7836e-04, 2.1196e-09, 9.9531e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0445, 0.1041, 0.0502, 0.0601, 0.0670, 0.1074, 0.0922, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0710, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 910.5598\n",
      "Test epoch : 46 Average loss: 994.6796\n",
      "PP(train) = 2436.788, PP(valid) = 2463.679\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6052e-45, 4.3505e-38, 3.3141e-17, 1.9565e-18, 1.9760e-19, 1.3347e-35,\n",
      "         4.1653e-39, 4.7885e-27, 1.0245e-15, 9.4085e-37, 1.1930e-38, 3.4014e-37,\n",
      "         6.2588e-24, 1.9079e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 910.3384\n",
      "Test epoch : 47 Average loss: 994.5839\n",
      "PP(train) = 2432.311, PP(valid) = 2461.846\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1054e-36, 3.1875e-24, 5.8775e-24, 2.4877e-19, 2.8843e-35,\n",
      "         2.8026e-44, 1.6724e-21, 3.2392e-13, 3.4910e-28, 7.8639e-26, 7.6273e-26,\n",
      "         3.8384e-21, 1.3226e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 910.1002\n",
      "Test epoch : 48 Average loss: 994.4887\n",
      "PP(train) = 2427.924, PP(valid) = 2460.041\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.0326e-40, 4.5275e-34, 1.1760e-11, 1.2518e-14, 3.7454e-18, 3.7940e-25,\n",
      "         1.2559e-27, 1.2584e-13, 1.2097e-02, 7.3320e-21, 8.2988e-30, 8.5097e-23,\n",
      "         1.0499e-15, 4.2832e-20, 9.8790e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0446, 0.1034, 0.0504, 0.0597, 0.0671, 0.1070, 0.0924, 0.0594,\n",
      "         0.0525, 0.0623, 0.0557, 0.0511, 0.0715, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 909.9516\n",
      "Test epoch : 49 Average loss: 994.3960\n",
      "PP(train) = 2423.609, PP(valid) = 2458.297\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.0110e-32, 1.8172e-28, 7.3960e-10, 1.6558e-05, 4.6540e-14, 8.9020e-26,\n",
      "         1.6881e-24, 1.3449e-15, 7.6316e-01, 5.7563e-15, 2.4960e-16, 1.5138e-20,\n",
      "         6.2718e-08, 9.5922e-19, 2.3682e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0623, 0.0620, 0.0551, 0.0605, 0.0363, 0.0749, 0.0725, 0.0934, 0.0584,\n",
      "         0.0658, 0.0798, 0.0637, 0.0518, 0.1152, 0.0484]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 909.7773\n",
      "Test epoch : 50 Average loss: 994.3073\n",
      "PP(train) = 2419.198, PP(valid) = 2456.520\n",
      "Writing to ./topicwords/13-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 8.2751 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                           II                                                   II                                    \n",
      "\n",
      "===== # 2, Topic : 8, p : 8.0191 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                     \n",
      "\n",
      "===== # 3, Topic : 14, p : 9.4462 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                         \n",
      "\n",
      "===== # 4, Topic : 8, p : 7.7452 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                           \n",
      "\n",
      "===== # 5, Topic : 8, p : 7.9543 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                 a                                                                                                                                                                                         a        a,                                 b                              \n",
      "\n",
      "===== # 6, Topic : 14, p : 9.2086 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                 I     II                                                                                           \n",
      "\n",
      "===== # 7, Topic : 14, p : 8.5323 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                           (                                                                                                                                                                                              \n",
      "\n",
      "===== # 8, Topic : 8, p : 7.8684 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                     (                                                       \n",
      "\n",
      "===== # 9, Topic : 14, p : 10.0623 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                          p                                                                                 \n",
      "\n",
      "===== # 10, Topic : 14, p : 8.5207 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                             \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.6839e-37, 8.4047e-34, 3.6231e-32, 4.0688e-17, 4.2832e-17, 4.3490e-39,\n",
      "         2.0319e-43, 6.1292e-08, 2.0949e-19, 5.2821e-24, 6.0934e-33, 4.3292e-28,\n",
      "         1.4118e-22, 1.7413e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 967.6998\n",
      "Test epoch : 1 Average loss: 927.1873\n",
      "PP(train) = 2316.365, PP(valid) = 2376.933\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3312e-43, 6.7580e-27, 4.7306e-09, 2.0785e-20, 4.0600e-06, 9.5257e-41,\n",
      "         7.2633e-27, 1.9279e-16, 4.7467e-17, 1.7910e-23, 8.3816e-31, 1.8390e-21,\n",
      "         3.8479e-07, 4.7042e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 967.5477\n",
      "Test epoch : 2 Average loss: 927.0730\n",
      "PP(train) = 2312.470, PP(valid) = 2374.347\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5057e-25, 1.4767e-26, 1.1754e-07, 4.5866e-15, 5.5142e-11, 3.8262e-37,\n",
      "         1.3230e-27, 1.0000e+00, 3.8476e-12, 7.9555e-23, 2.4235e-21, 1.3341e-23,\n",
      "         3.1677e-18, 1.6013e-21, 1.2614e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 967.4116\n",
      "Test epoch : 3 Average loss: 926.9482\n",
      "PP(train) = 2308.389, PP(valid) = 2371.916\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5309e-31, 3.4292e-32, 1.5754e-10, 4.3119e-08, 2.9239e-03, 5.1116e-37,\n",
      "         2.2988e-33, 5.0603e-03, 7.8974e-11, 1.9377e-14, 5.6189e-25, 6.8596e-26,\n",
      "         2.0451e-07, 1.3178e-14, 9.9202e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0446, 0.1038, 0.0502, 0.0603, 0.0671, 0.1074, 0.0921, 0.0593,\n",
      "         0.0523, 0.0622, 0.0556, 0.0510, 0.0710, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 967.0351\n",
      "Test epoch : 4 Average loss: 926.8167\n",
      "PP(train) = 2304.144, PP(valid) = 2369.489\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9157e-31, 2.4919e-22, 3.7895e-33, 2.0452e-18, 1.1071e-34,\n",
      "         5.1091e-41, 6.9240e-09, 3.2614e-15, 3.7428e-26, 3.0275e-29, 2.2121e-30,\n",
      "         8.4649e-18, 1.1377e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 966.8001\n",
      "Test epoch : 5 Average loss: 926.6797\n",
      "PP(train) = 2299.676, PP(valid) = 2366.904\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1719e-27, 1.6069e-24, 8.9956e-15, 5.2667e-15, 1.0747e-05, 1.1129e-25,\n",
      "         1.7747e-27, 3.4779e-01, 4.1729e-08, 3.2934e-08, 2.7994e-26, 1.0513e-25,\n",
      "         3.2924e-02, 1.3871e-17, 6.1928e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0830, 0.0626, 0.0844, 0.0496, 0.0670, 0.0767, 0.0960, 0.0726, 0.0590,\n",
      "         0.0485, 0.0792, 0.0524, 0.0468, 0.0799, 0.0422]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 966.6058\n",
      "Test epoch : 6 Average loss: 926.5424\n",
      "PP(train) = 2294.902, PP(valid) = 2364.039\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3106e-32, 5.7600e-35, 1.4259e-09, 2.8032e-28, 4.4011e-10, 4.2550e-23,\n",
      "         1.2439e-25, 2.2171e-08, 1.8565e-11, 1.9668e-21, 5.4771e-27, 3.8123e-28,\n",
      "         1.3684e-01, 5.0969e-26, 8.6316e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0769, 0.0481, 0.1010, 0.0521, 0.0577, 0.0673, 0.0989, 0.0914, 0.0619,\n",
      "         0.0573, 0.0605, 0.0558, 0.0526, 0.0707, 0.0479]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 966.3112\n",
      "Test epoch : 7 Average loss: 926.4068\n",
      "PP(train) = 2290.144, PP(valid) = 2361.232\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7960e-29, 1.3114e-23, 1.2918e-02, 8.3560e-18, 4.6028e-09, 2.4037e-33,\n",
      "         9.4258e-35, 1.1570e-12, 1.7525e-15, 1.8325e-21, 1.1256e-25, 3.6453e-26,\n",
      "         1.1066e-09, 1.1643e-20, 9.8708e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0448, 0.1033, 0.0505, 0.0604, 0.0667, 0.1071, 0.0922, 0.0597,\n",
      "         0.0524, 0.0620, 0.0555, 0.0511, 0.0711, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 966.1535\n",
      "Test epoch : 8 Average loss: 926.2709\n",
      "PP(train) = 2285.533, PP(valid) = 2358.539\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5251e-36, 3.8636e-17, 1.4902e-02, 7.9940e-08, 1.0021e-06, 9.7530e-26,\n",
      "         2.7936e-31, 5.9115e-03, 1.0160e-04, 4.9634e-14, 1.1724e-23, 2.0955e-22,\n",
      "         1.2738e-05, 3.3170e-21, 9.7907e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0451, 0.1028, 0.0505, 0.0606, 0.0668, 0.1069, 0.0918, 0.0598,\n",
      "         0.0524, 0.0623, 0.0555, 0.0510, 0.0713, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 965.8096\n",
      "Test epoch : 9 Average loss: 926.1347\n",
      "PP(train) = 2281.040, PP(valid) = 2355.936\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7830e-41, 2.7819e-41, 3.2685e-17, 4.5417e-25, 4.4540e-14, 6.0862e-40,\n",
      "         0.0000e+00, 4.3422e-23, 4.2974e-17, 3.2912e-31, 3.6714e-43, 3.5258e-31,\n",
      "         5.1269e-18, 1.1382e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 965.5454\n",
      "Test epoch : 10 Average loss: 925.9978\n",
      "PP(train) = 2276.502, PP(valid) = 2353.286\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.7062e-31, 3.7686e-26, 9.9890e-01, 4.8474e-27, 2.9578e-17, 1.0344e-32,\n",
      "         3.4790e-31, 3.5633e-12, 9.2196e-14, 1.9415e-14, 1.7292e-38, 7.9174e-33,\n",
      "         8.3215e-13, 8.0625e-19, 1.1021e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0898, 0.0443, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0948,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 965.3586\n",
      "Test epoch : 11 Average loss: 925.8642\n",
      "PP(train) = 2271.971, PP(valid) = 2350.661\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2549e-30, 4.1162e-22, 2.7888e-19, 3.7522e-06, 9.0785e-10, 1.5602e-28,\n",
      "         1.1376e-29, 2.4959e-05, 1.8365e-04, 2.0512e-24, 2.2946e-14, 6.3374e-11,\n",
      "         3.2044e-04, 4.1353e-05, 9.9943e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 965.1806\n",
      "Test epoch : 12 Average loss: 925.7312\n",
      "PP(train) = 2267.452, PP(valid) = 2347.971\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6957e-36, 1.6489e-33, 4.2663e-16, 8.6321e-26, 1.9501e-08, 6.7816e-40,\n",
      "         1.8913e-41, 6.8250e-12, 2.1281e-12, 3.4546e-28, 1.5361e-36, 6.2395e-22,\n",
      "         4.9041e-18, 2.8022e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 964.8054\n",
      "Test epoch : 13 Average loss: 925.6004\n",
      "PP(train) = 2263.047, PP(valid) = 2345.407\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5142e-31, 3.4788e-25, 3.9300e-19, 1.6061e-20, 6.2173e-05, 7.3218e-29,\n",
      "         1.0662e-24, 5.2936e-03, 9.7015e-01, 1.3046e-19, 2.3276e-19, 6.0946e-32,\n",
      "         2.2444e-04, 8.4594e-16, 2.4273e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0569, 0.0670, 0.0454, 0.0626, 0.0311, 0.0760, 0.0640, 0.0918, 0.0572,\n",
      "         0.0688, 0.0843, 0.0649, 0.0511, 0.1295, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 964.6290\n",
      "Test epoch : 14 Average loss: 925.4704\n",
      "PP(train) = 2258.688, PP(valid) = 2342.881\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0594e-42, 1.5196e-19, 2.6232e-18, 2.0049e-12, 6.7415e-09, 3.6328e-40,\n",
      "         1.8865e-25, 5.2135e-10, 4.8179e-11, 5.1402e-14, 1.5192e-26, 2.4766e-15,\n",
      "         8.0729e-01, 1.1852e-10, 1.9271e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0542, 0.0684, 0.0821, 0.0593, 0.0450, 0.0660, 0.0625, 0.0825, 0.0725,\n",
      "         0.0851, 0.0513, 0.0542, 0.0578, 0.0665, 0.0925]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 964.3697\n",
      "Test epoch : 15 Average loss: 925.3418\n",
      "PP(train) = 2254.432, PP(valid) = 2340.425\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1408e-29, 1.5386e-20, 3.1235e-07, 6.4798e-17, 9.6130e-08, 4.7425e-23,\n",
      "         4.2481e-29, 2.7420e-06, 2.6254e-03, 2.6467e-22, 1.2235e-20, 8.6498e-13,\n",
      "         9.7866e-01, 5.3928e-17, 1.8715e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0738, 0.0766, 0.0604, 0.0416, 0.0648, 0.0547, 0.0793, 0.0744,\n",
      "         0.0929, 0.0486, 0.0531, 0.0584, 0.0647, 0.1079]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 964.2127\n",
      "Test epoch : 16 Average loss: 925.2143\n",
      "PP(train) = 2250.115, PP(valid) = 2337.902\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0991e-40, 5.9450e-37, 8.5075e-16, 2.9755e-28, 2.9400e-16, 5.4580e-40,\n",
      "         4.2185e-38, 2.7837e-04, 2.1026e-10, 5.4551e-25, 5.9876e-35, 2.3067e-30,\n",
      "         1.7979e-15, 7.8888e-27, 9.9972e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 963.7974\n",
      "Test epoch : 17 Average loss: 925.0852\n",
      "PP(train) = 2245.862, PP(valid) = 2335.384\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0574e-19, 6.1785e-33, 3.1179e-13, 5.1399e-12, 2.5875e-15, 7.2045e-26,\n",
      "         2.1688e-21, 2.1354e-09, 9.9998e-01, 1.5625e-14, 7.8493e-23, 3.3878e-31,\n",
      "         1.0900e-12, 5.9056e-20, 1.7690e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 963.7770\n",
      "Test epoch : 18 Average loss: 924.9582\n",
      "PP(train) = 2241.640, PP(valid) = 2332.874\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0343e-40, 1.0831e-37, 1.5207e-20, 1.2270e-17, 7.6919e-08, 8.4139e-28,\n",
      "         3.1073e-25, 6.3850e-14, 1.0000e+00, 1.1683e-28, 1.1471e-38, 4.1976e-28,\n",
      "         3.4394e-15, 4.6425e-23, 1.3223e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 963.4841\n",
      "Test epoch : 19 Average loss: 924.8352\n",
      "PP(train) = 2237.443, PP(valid) = 2330.419\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6778e-37, 1.7607e-36, 2.7130e-03, 7.8835e-30, 3.1726e-12, 3.2242e-35,\n",
      "         4.9045e-44, 6.1271e-07, 5.4284e-03, 1.0846e-27, 3.8625e-28, 3.0770e-29,\n",
      "         1.7247e-29, 8.9933e-27, 9.9186e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0816, 0.0446, 0.1037, 0.0503, 0.0600, 0.0669, 0.1072, 0.0923, 0.0594,\n",
      "         0.0524, 0.0621, 0.0556, 0.0511, 0.0712, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 963.1297\n",
      "Test epoch : 20 Average loss: 924.7090\n",
      "PP(train) = 2233.383, PP(valid) = 2328.030\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2368e-41, 1.0693e-25, 2.7627e-12, 3.8555e-12, 1.3518e-15, 3.2910e-23,\n",
      "         2.1889e-33, 9.9551e-01, 1.8514e-10, 8.4005e-21, 1.3267e-19, 5.3567e-31,\n",
      "         7.7644e-18, 1.4019e-32, 4.4911e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1052, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0371, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 962.9691\n",
      "Test epoch : 21 Average loss: 924.5854\n",
      "PP(train) = 2229.330, PP(valid) = 2325.671\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5801e-33, 1.1408e-24, 1.4913e-21, 2.9105e-16, 8.5164e-03, 3.5610e-33,\n",
      "         1.0256e-24, 4.9663e-21, 1.5105e-17, 2.2967e-23, 1.6089e-27, 5.5969e-28,\n",
      "         2.7125e-14, 4.0639e-25, 9.9148e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.0444, 0.1036, 0.0501, 0.0604, 0.0669, 0.1076, 0.0925, 0.0592,\n",
      "         0.0526, 0.0621, 0.0557, 0.0510, 0.0708, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 962.7661\n",
      "Test epoch : 22 Average loss: 924.4630\n",
      "PP(train) = 2225.249, PP(valid) = 2323.269\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0928e-25, 1.6909e-36, 2.2229e-09, 4.5965e-10, 6.8757e-14, 6.6815e-36,\n",
      "         1.9588e-40, 2.8912e-07, 1.6441e-06, 1.4494e-17, 9.3778e-23, 3.0450e-28,\n",
      "         2.0983e-15, 1.2897e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 962.4947\n",
      "Test epoch : 23 Average loss: 924.3419\n",
      "PP(train) = 2221.214, PP(valid) = 2320.916\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8006e-34, 1.4216e-21, 7.8348e-20, 1.4712e-21, 1.5380e-03, 7.4659e-24,\n",
      "         6.2267e-27, 3.7824e-02, 3.5348e-05, 6.4305e-15, 1.2943e-21, 1.4904e-21,\n",
      "         1.4497e-03, 2.7321e-13, 9.5915e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0820, 0.0461, 0.1020, 0.0501, 0.0610, 0.0680, 0.1065, 0.0901, 0.0593,\n",
      "         0.0519, 0.0638, 0.0553, 0.0506, 0.0719, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 962.3221\n",
      "Test epoch : 24 Average loss: 924.2220\n",
      "PP(train) = 2217.220, PP(valid) = 2318.564\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5835e-42, 5.5118e-31, 1.5144e-24, 1.5155e-25, 6.3637e-12, 2.0620e-38,\n",
      "         2.9465e-37, 1.0000e+00, 6.1889e-10, 3.1210e-30, 3.6647e-25, 7.6739e-16,\n",
      "         9.1235e-15, 3.7189e-32, 1.9313e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 962.0366\n",
      "Test epoch : 25 Average loss: 924.1015\n",
      "PP(train) = 2213.336, PP(valid) = 2316.278\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0999e-37, 6.5200e-35, 7.0750e-13, 1.3349e-17, 8.2526e-05, 1.4121e-33,\n",
      "         3.7391e-28, 9.2797e-09, 3.2833e-06, 1.1211e-18, 3.7012e-24, 7.9147e-26,\n",
      "         1.3355e-14, 9.3809e-29, 9.9991e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 961.8270\n",
      "Test epoch : 26 Average loss: 923.9831\n",
      "PP(train) = 2209.461, PP(valid) = 2314.029\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.9492e-44, 5.0166e-43, 3.5491e-08, 1.7267e-16, 7.0105e-14, 6.7119e-32,\n",
      "         2.8026e-45, 5.1168e-07, 4.0033e-11, 2.1252e-24, 1.2217e-18, 2.0825e-28,\n",
      "         2.3719e-18, 5.0765e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 961.5634\n",
      "Test epoch : 27 Average loss: 923.8646\n",
      "PP(train) = 2205.609, PP(valid) = 2311.764\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0486e-29, 6.2541e-19, 2.3521e-05, 5.2929e-13, 2.1225e-04, 2.3423e-27,\n",
      "         3.1245e-34, 6.0316e-03, 1.3286e-05, 2.1602e-22, 1.2662e-19, 3.2001e-25,\n",
      "         3.0895e-09, 1.3975e-08, 9.9372e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0446, 0.1040, 0.0502, 0.0602, 0.0671, 0.1074, 0.0920, 0.0593,\n",
      "         0.0522, 0.0623, 0.0555, 0.0510, 0.0710, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 961.3949\n",
      "Test epoch : 28 Average loss: 923.7478\n",
      "PP(train) = 2201.709, PP(valid) = 2309.451\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1805e-32, 2.3553e-31, 8.1679e-09, 4.2438e-20, 1.7265e-01, 8.7687e-27,\n",
      "         7.7185e-40, 7.1376e-09, 2.9404e-11, 1.5505e-19, 2.4583e-26, 3.7263e-18,\n",
      "         9.0119e-17, 2.5471e-19, 8.2735e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0745, 0.0455, 0.0902, 0.0484, 0.0659, 0.0674, 0.1086, 0.0960, 0.0572,\n",
      "         0.0585, 0.0635, 0.0570, 0.0506, 0.0700, 0.0466]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 961.2065\n",
      "Test epoch : 29 Average loss: 923.6325\n",
      "PP(train) = 2197.844, PP(valid) = 2307.168\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7096e-43, 7.6145e-29, 1.4260e-12, 2.5779e-21, 2.2561e-26, 1.6421e-29,\n",
      "         7.5591e-36, 2.5735e-15, 6.6218e-14, 2.4255e-28, 6.8271e-31, 2.3456e-32,\n",
      "         5.7506e-21, 3.0166e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 960.9422\n",
      "Test epoch : 30 Average loss: 923.5182\n",
      "PP(train) = 2194.083, PP(valid) = 2304.981\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3250e-28, 6.6857e-38, 9.9357e-16, 2.5308e-32, 2.2469e-19, 0.0000e+00,\n",
      "         1.3747e-42, 4.3866e-11, 1.0000e+00, 3.6536e-24, 7.3724e-41, 8.8663e-32,\n",
      "         1.9183e-19, 1.2860e-26, 2.7739e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 960.6806\n",
      "Test epoch : 31 Average loss: 923.4047\n",
      "PP(train) = 2190.396, PP(valid) = 2302.857\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3091e-37, 4.0341e-41, 5.8833e-21, 1.0911e-19, 2.5545e-26, 1.4095e-22,\n",
      "         8.8695e-32, 3.5641e-18, 2.1416e-16, 1.0898e-18, 2.6148e-27, 1.1317e-26,\n",
      "         2.5758e-21, 1.3611e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 960.6341\n",
      "Test epoch : 32 Average loss: 923.2934\n",
      "PP(train) = 2186.701, PP(valid) = 2300.720\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1604e-32, 1.8409e-35, 2.8351e-18, 6.9598e-24, 1.5704e-18, 1.6146e-30,\n",
      "         1.4649e-28, 7.3370e-13, 1.0000e+00, 4.6108e-27, 2.0493e-28, 9.0387e-29,\n",
      "         2.2008e-09, 4.5618e-31, 4.3004e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 960.3787\n",
      "Test epoch : 33 Average loss: 923.1823\n",
      "PP(train) = 2182.997, PP(valid) = 2298.523\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7588e-31, 2.5015e-35, 8.8708e-31, 5.5946e-26, 1.6366e-06, 7.9576e-31,\n",
      "         4.9037e-33, 3.3267e-06, 3.1318e-08, 4.7024e-17, 3.6998e-34, 1.6100e-29,\n",
      "         2.9480e-09, 3.1662e-18, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 960.1398\n",
      "Test epoch : 34 Average loss: 923.0701\n",
      "PP(train) = 2179.330, PP(valid) = 2296.341\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9502e-32, 1.8303e-23, 1.4624e-04, 7.5247e-23, 1.4072e-05, 4.6098e-38,\n",
      "         6.9628e-23, 5.2978e-03, 9.9454e-01, 6.3261e-08, 8.9247e-24, 1.3968e-33,\n",
      "         5.9861e-13, 8.7614e-17, 1.2751e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0563, 0.0676, 0.0443, 0.0628, 0.0306, 0.0761, 0.0630, 0.0916, 0.0570,\n",
      "         0.0691, 0.0848, 0.0650, 0.0510, 0.1312, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 959.9277\n",
      "Test epoch : 35 Average loss: 922.9593\n",
      "PP(train) = 2175.756, PP(valid) = 2294.264\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0152e-35, 1.6448e-34, 2.8945e-09, 5.7592e-23, 3.1487e-12, 5.6866e-35,\n",
      "         4.1493e-28, 9.9988e-01, 1.2056e-04, 1.4600e-24, 1.8561e-28, 1.0718e-24,\n",
      "         1.4972e-13, 6.4433e-26, 7.4092e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 959.7081\n",
      "Test epoch : 36 Average loss: 922.8488\n",
      "PP(train) = 2172.250, PP(valid) = 2292.227\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5638e-32, 3.6834e-32, 2.7289e-08, 1.9330e-14, 2.4361e-03, 2.5464e-24,\n",
      "         7.4532e-28, 1.0909e-04, 3.4120e-09, 4.7007e-20, 1.7947e-22, 4.6617e-20,\n",
      "         2.1237e-09, 1.4998e-21, 9.9745e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0816, 0.0444, 0.1041, 0.0502, 0.0602, 0.0669, 0.1076, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 959.5109\n",
      "Test epoch : 37 Average loss: 922.7402\n",
      "PP(train) = 2168.666, PP(valid) = 2290.121\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7217e-35, 5.8035e-25, 8.2469e-14, 1.2567e-19, 7.1247e-04, 6.6501e-29,\n",
      "         2.5399e-38, 2.2028e-04, 8.0947e-02, 3.7663e-19, 7.5307e-30, 4.6525e-32,\n",
      "         9.1812e-01, 8.4541e-29, 2.0062e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0491, 0.0742, 0.0733, 0.0611, 0.0405, 0.0658, 0.0549, 0.0803, 0.0734,\n",
      "         0.0921, 0.0508, 0.0541, 0.0582, 0.0685, 0.1036]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 959.2756\n",
      "Test epoch : 38 Average loss: 922.6326\n",
      "PP(train) = 2165.073, PP(valid) = 2287.990\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2888e-42, 1.0740e-28, 1.2196e-08, 5.3363e-30, 1.1782e-04, 4.1436e-42,\n",
      "         1.2442e-35, 3.4252e-05, 6.2766e-10, 5.0582e-23, 2.2298e-32, 1.1493e-34,\n",
      "         5.4513e-19, 3.7125e-21, 9.9985e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 958.9966\n",
      "Test epoch : 39 Average loss: 922.5250\n",
      "PP(train) = 2161.606, PP(valid) = 2285.995\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.5034e-38, 1.4400e-23, 3.4285e-08, 1.3398e-21, 1.5479e-08, 1.9415e-35,\n",
      "         8.5090e-36, 2.9980e-04, 9.9969e-01, 1.3164e-14, 9.6742e-31, 6.2938e-24,\n",
      "         6.3438e-06, 1.5990e-09, 1.7285e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 958.8242\n",
      "Test epoch : 40 Average loss: 922.4190\n",
      "PP(train) = 2158.182, PP(valid) = 2284.036\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8643e-29, 2.3443e-21, 5.8580e-11, 8.3771e-23, 4.7136e-18, 3.0436e-25,\n",
      "         8.3358e-27, 8.4737e-04, 8.1297e-06, 1.8446e-25, 1.0971e-18, 1.2019e-18,\n",
      "         2.3646e-02, 2.1304e-25, 9.7550e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0450, 0.1037, 0.0505, 0.0597, 0.0670, 0.1060, 0.0921, 0.0598,\n",
      "         0.0531, 0.0618, 0.0556, 0.0513, 0.0709, 0.0425]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 958.7265\n",
      "Test epoch : 41 Average loss: 922.3153\n",
      "PP(train) = 2154.713, PP(valid) = 2282.023\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2563e-34, 1.3264e-38, 3.4856e-23, 4.9124e-26, 3.7817e-12, 7.6102e-40,\n",
      "         9.3576e-37, 2.0902e-11, 1.7310e-23, 7.4654e-32, 4.7909e-31, 2.8072e-31,\n",
      "         2.0449e-16, 1.3847e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 958.4988\n",
      "Test epoch : 42 Average loss: 922.2138\n",
      "PP(train) = 2151.270, PP(valid) = 2280.026\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0766e-42, 1.3570e-27, 1.0628e-18, 8.8858e-20, 2.5930e-14, 5.9125e-32,\n",
      "         2.1121e-31, 9.4961e-07, 9.9999e-01, 1.5264e-26, 8.8319e-37, 1.8827e-33,\n",
      "         3.2583e-10, 2.4983e-13, 4.9947e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 958.2397\n",
      "Test epoch : 43 Average loss: 922.1095\n",
      "PP(train) = 2147.881, PP(valid) = 2278.039\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5109e-18, 2.7903e-28, 1.8843e-07, 1.8231e-20, 5.9068e-06, 1.2166e-17,\n",
      "         6.1176e-26, 2.9520e-01, 1.5308e-10, 1.7750e-14, 8.7825e-18, 2.4858e-23,\n",
      "         1.9283e-03, 2.8717e-09, 7.0287e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0839, 0.0586, 0.0879, 0.0494, 0.0665, 0.0751, 0.0994, 0.0756, 0.0587,\n",
      "         0.0483, 0.0768, 0.0529, 0.0472, 0.0786, 0.0410]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 958.1699\n",
      "Test epoch : 44 Average loss: 922.0072\n",
      "PP(train) = 2144.540, PP(valid) = 2276.115\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4928e-35, 3.6514e-37, 1.7476e-16, 5.5388e-37, 2.3872e-20, 0.0000e+00,\n",
      "         5.3249e-44, 9.5748e-29, 1.7455e-18, 1.4074e-36, 2.3297e-33, 7.0065e-44,\n",
      "         5.5198e-34, 3.8683e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 957.9006\n",
      "Test epoch : 45 Average loss: 921.9058\n",
      "PP(train) = 2141.250, PP(valid) = 2274.240\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3465e-32, 1.6615e-32, 1.3318e-20, 1.9718e-26, 1.2559e-12, 5.3746e-22,\n",
      "         7.1887e-42, 1.7327e-12, 2.0583e-09, 1.3515e-21, 3.5069e-26, 3.3944e-25,\n",
      "         2.2864e-22, 4.4046e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 957.7191\n",
      "Test epoch : 46 Average loss: 921.8064\n",
      "PP(train) = 2137.877, PP(valid) = 2272.277\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8962e-40, 4.8693e-19, 4.6148e-18, 9.6399e-26, 3.5227e-13, 9.3750e-28,\n",
      "         1.2580e-21, 8.3011e-18, 1.4150e-10, 2.5002e-22, 1.2902e-34, 4.9070e-33,\n",
      "         1.0981e-12, 2.2910e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 957.5745\n",
      "Test epoch : 47 Average loss: 921.7036\n",
      "PP(train) = 2134.540, PP(valid) = 2270.267\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2782e-26, 4.0683e-22, 2.9004e-17, 9.0292e-17, 4.2193e-04, 7.7350e-33,\n",
      "         1.7688e-28, 1.1837e-08, 1.4608e-18, 3.9722e-26, 5.8692e-37, 8.6123e-32,\n",
      "         1.8955e-18, 3.6228e-24, 9.9958e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 957.3696\n",
      "Test epoch : 48 Average loss: 921.6052\n",
      "PP(train) = 2131.325, PP(valid) = 2268.436\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3039e-25, 3.1691e-28, 6.2453e-17, 2.8186e-22, 1.9227e-10, 4.2267e-33,\n",
      "         8.9372e-24, 2.9450e-12, 4.8873e-18, 4.6533e-20, 2.4232e-24, 9.7995e-19,\n",
      "         1.7265e-23, 2.6338e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 957.2313\n",
      "Test epoch : 49 Average loss: 921.5069\n",
      "PP(train) = 2128.135, PP(valid) = 2266.618\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9765e-34, 1.0042e-28, 1.4982e-15, 3.3064e-18, 3.0029e-06, 5.5577e-26,\n",
      "         7.0077e-22, 5.3496e-10, 6.5252e-09, 3.9846e-09, 1.0841e-20, 4.0437e-12,\n",
      "         1.7620e-05, 6.1754e-11, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 956.9661\n",
      "Test epoch : 50 Average loss: 921.4085\n",
      "PP(train) = 2124.892, PP(valid) = 2264.730\n",
      "Writing to ./topicwords/14-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 14, p : 9.1723 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                            \n",
      "\n",
      "===== # 2, Topic : 14, p : 10.8808 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                              \n",
      "\n",
      "===== # 3, Topic : 8, p : 7.6532 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "\n",
      "===== # 4, Topic : 14, p : 8.6066 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                             \n",
      "\n",
      "===== # 5, Topic : 7, p : 8.5597 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                         \n",
      "\n",
      "===== # 6, Topic : 14, p : 8.5406 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                    \n",
      "\n",
      "===== # 7, Topic : 14, p : 8.3768 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                 \n",
      "\n",
      "===== # 8, Topic : 14, p : 9.2564 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                    \n",
      "\n",
      "===== # 9, Topic : 14, p : 8.6519 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                         \n",
      "\n",
      "===== # 10, Topic : 14, p : 10.9871 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                           \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.1428e-43, 1.2232e-25, 1.7574e-13, 4.9742e-21, 1.4348e-15, 2.0493e-25,\n",
      "         2.5634e-31, 7.9302e-01, 1.0941e-12, 6.4969e-19, 3.8446e-32, 3.4077e-22,\n",
      "         4.6158e-13, 2.4690e-21, 2.0698e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0845, 0.0898, 0.0633, 0.0462, 0.0760, 0.0878, 0.0838, 0.0518, 0.0552,\n",
      "         0.0404, 0.1060, 0.0468, 0.0398, 0.0901, 0.0384]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 998.0508\n",
      "Test epoch : 1 Average loss: 1136.8121\n",
      "PP(train) = 2273.350, PP(valid) = 2317.046\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2932e-30, 6.3658e-20, 1.6302e-10, 1.1607e-14, 9.9966e-01, 1.8756e-21,\n",
      "         4.4991e-24, 2.6261e-08, 3.1109e-04, 1.8365e-14, 7.8105e-22, 1.9516e-19,\n",
      "         1.7067e-05, 7.8847e-19, 1.3601e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 997.9292\n",
      "Test epoch : 2 Average loss: 1136.7140\n",
      "PP(train) = 2270.214, PP(valid) = 2315.482\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3779e-35, 1.0490e-24, 2.6144e-04, 6.1327e-19, 1.2284e-08, 2.4887e-27,\n",
      "         1.4813e-24, 3.9758e-06, 2.3176e-14, 2.7880e-17, 2.1999e-25, 6.1465e-14,\n",
      "         3.1328e-13, 8.1685e-18, 9.9973e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 997.7144\n",
      "Test epoch : 3 Average loss: 1136.6070\n",
      "PP(train) = 2266.364, PP(valid) = 2313.707\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7133e-25, 2.1391e-18, 5.0675e-13, 2.2072e-07, 5.5769e-04, 5.4094e-27,\n",
      "         1.1608e-24, 2.4438e-08, 1.7418e-03, 6.7384e-18, 1.8877e-27, 2.2065e-20,\n",
      "         9.6495e-23, 8.4060e-21, 9.9770e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1075, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0709, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 997.5023\n",
      "Test epoch : 4 Average loss: 1136.5001\n",
      "PP(train) = 2262.339, PP(valid) = 2312.094\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2795e-28, 1.3830e-15, 1.4235e-11, 3.8441e-10, 3.5612e-09, 1.0554e-29,\n",
      "         1.3072e-34, 9.9784e-01, 7.3181e-05, 2.3930e-25, 2.7684e-22, 3.8612e-30,\n",
      "         1.1518e-03, 4.0931e-27, 9.3007e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1192, 0.0438, 0.0366, 0.0939, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 997.2404\n",
      "Test epoch : 5 Average loss: 1136.3930\n",
      "PP(train) = 2258.205, PP(valid) = 2310.469\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1056e-27, 1.1690e-14, 8.9541e-05, 1.7114e-08, 4.4193e-02, 4.8887e-18,\n",
      "         5.2830e-27, 8.9316e-01, 4.9216e-02, 3.5512e-15, 1.8295e-20, 5.1482e-09,\n",
      "         7.0013e-12, 8.5800e-21, 1.3340e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0805, 0.0997, 0.0543, 0.0453, 0.0767, 0.0906, 0.0784, 0.0481, 0.0536,\n",
      "         0.0404, 0.1147, 0.0460, 0.0381, 0.0945, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 997.0402\n",
      "Test epoch : 6 Average loss: 1136.2861\n",
      "PP(train) = 2254.012, PP(valid) = 2308.813\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7951e-31, 8.4284e-26, 6.8599e-06, 2.8922e-18, 5.2298e-12, 7.5170e-26,\n",
      "         4.2409e-25, 1.3577e-09, 3.9169e-08, 1.5431e-21, 7.0223e-26, 7.9222e-20,\n",
      "         9.9999e-01, 5.7542e-21, 1.4061e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 996.7643\n",
      "Test epoch : 7 Average loss: 1136.1803\n",
      "PP(train) = 2249.649, PP(valid) = 2307.044\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7995e-29, 6.4787e-11, 4.6974e-06, 4.4561e-15, 6.2164e-02, 1.5163e-31,\n",
      "         2.6066e-13, 4.5479e-04, 1.0997e-07, 5.8871e-18, 4.0831e-21, 1.2340e-22,\n",
      "         6.2960e-14, 3.6614e-11, 9.3738e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0791, 0.0448, 0.0991, 0.0496, 0.0622, 0.0671, 0.1080, 0.0937, 0.0586,\n",
      "         0.0545, 0.0626, 0.0561, 0.0509, 0.0706, 0.0432]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 996.6057\n",
      "Test epoch : 8 Average loss: 1136.0704\n",
      "PP(train) = 2245.414, PP(valid) = 2305.322\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5383e-24, 7.3963e-27, 2.0580e-10, 2.0374e-15, 9.0368e-03, 6.6302e-37,\n",
      "         2.0183e-28, 9.6757e-01, 1.0082e-08, 2.5192e-17, 3.9766e-17, 2.3378e-18,\n",
      "         9.5715e-09, 5.2453e-14, 2.3397e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0832, 0.1031, 0.0552, 0.0445, 0.0790, 0.0916, 0.0780, 0.0449, 0.0533,\n",
      "         0.0377, 0.1173, 0.0443, 0.0370, 0.0933, 0.0374]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 996.2220\n",
      "Test epoch : 9 Average loss: 1135.9672\n",
      "PP(train) = 2241.236, PP(valid) = 2303.750\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2120e-30, 4.8275e-22, 3.0362e-13, 1.4385e-11, 4.9327e-13, 3.3743e-34,\n",
      "         1.6946e-31, 4.2236e-06, 1.1178e-11, 4.9534e-26, 1.3873e-18, 1.1803e-31,\n",
      "         4.8895e-10, 5.0649e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 996.0066\n",
      "Test epoch : 10 Average loss: 1135.8639\n",
      "PP(train) = 2237.143, PP(valid) = 2302.205\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0342e-27, 6.1598e-22, 3.0469e-05, 1.8756e-15, 3.2968e-05, 2.4352e-32,\n",
      "         9.4774e-25, 2.9517e-11, 7.2699e-01, 3.5270e-15, 4.9885e-24, 1.8712e-23,\n",
      "         5.9024e-24, 5.0995e-21, 2.7294e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0632, 0.0611, 0.0569, 0.0601, 0.0372, 0.0746, 0.0741, 0.0936, 0.0586,\n",
      "         0.0652, 0.0790, 0.0634, 0.0519, 0.1129, 0.0481]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 995.7587\n",
      "Test epoch : 11 Average loss: 1135.7577\n",
      "PP(train) = 2232.994, PP(valid) = 2300.519\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0486e-27, 9.2580e-11, 4.3641e-10, 2.2344e-19, 1.8890e-11, 3.8755e-34,\n",
      "         5.2146e-22, 1.9080e-10, 1.0000e+00, 1.2474e-30, 8.3431e-13, 2.2528e-20,\n",
      "         3.5162e-08, 8.1012e-18, 6.2673e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 995.6059\n",
      "Test epoch : 12 Average loss: 1135.6562\n",
      "PP(train) = 2228.866, PP(valid) = 2298.858\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8585e-32, 1.0387e-34, 7.9791e-20, 6.7872e-18, 2.5722e-06, 3.4739e-39,\n",
      "         1.5119e-37, 3.7665e-14, 6.0797e-15, 1.7777e-30, 4.0297e-32, 1.2938e-24,\n",
      "         6.4774e-24, 2.4186e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 995.2789\n",
      "Test epoch : 13 Average loss: 1135.5554\n",
      "PP(train) = 2224.813, PP(valid) = 2297.262\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4053e-26, 5.0043e-13, 9.9989e-01, 8.0173e-17, 8.5209e-07, 9.5257e-33,\n",
      "         2.8586e-31, 1.9290e-13, 2.7625e-06, 1.1040e-09, 1.7733e-29, 1.2855e-21,\n",
      "         3.9572e-07, 2.3595e-19, 1.0183e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 995.1043\n",
      "Test epoch : 14 Average loss: 1135.4559\n",
      "PP(train) = 2220.879, PP(valid) = 2295.782\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3720e-25, 1.0955e-09, 1.1557e-06, 2.8490e-16, 5.5718e-11, 2.4142e-25,\n",
      "         2.3092e-24, 4.4419e-08, 2.0862e-12, 1.7216e-23, 2.9207e-22, 1.7212e-23,\n",
      "         2.2263e-06, 2.0757e-15, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 994.8934\n",
      "Test epoch : 15 Average loss: 1135.3581\n",
      "PP(train) = 2216.949, PP(valid) = 2294.293\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5353e-32, 4.2491e-23, 4.1220e-12, 4.4211e-18, 5.9089e-14, 2.5313e-39,\n",
      "         2.2022e-33, 4.3832e-10, 1.8443e-13, 4.3568e-20, 1.7867e-21, 2.0490e-28,\n",
      "         2.2301e-18, 1.5573e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 994.6282\n",
      "Test epoch : 16 Average loss: 1135.2615\n",
      "PP(train) = 2212.989, PP(valid) = 2292.733\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6413e-26, 2.3639e-18, 1.2667e-01, 1.0358e-09, 1.6052e-02, 2.3568e-17,\n",
      "         1.5000e-19, 8.0437e-01, 5.8411e-08, 2.7124e-12, 2.9628e-20, 2.3511e-12,\n",
      "         1.5809e-09, 7.6319e-10, 5.2908e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0827, 0.0990, 0.0554, 0.0479, 0.0799, 0.0843, 0.0795, 0.0501, 0.0581,\n",
      "         0.0411, 0.1067, 0.0457, 0.0395, 0.0923, 0.0379]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 994.4247\n",
      "Test epoch : 17 Average loss: 1135.1622\n",
      "PP(train) = 2209.065, PP(valid) = 2291.122\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3514e-23, 7.4214e-17, 9.8911e-07, 1.4296e-20, 5.2993e-02, 1.2140e-27,\n",
      "         1.0988e-21, 9.2880e-01, 1.3713e-06, 4.1232e-14, 2.1004e-22, 3.0491e-17,\n",
      "         2.0285e-12, 2.7608e-13, 1.8208e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.1005, 0.0547, 0.0444, 0.0802, 0.0909, 0.0794, 0.0468, 0.0532,\n",
      "         0.0394, 0.1154, 0.0451, 0.0375, 0.0922, 0.0388]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 994.2135\n",
      "Test epoch : 18 Average loss: 1135.0680\n",
      "PP(train) = 2205.291, PP(valid) = 2289.724\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1894e-32, 8.3409e-12, 9.8494e-01, 3.6794e-08, 2.4180e-12, 2.3355e-31,\n",
      "         9.9924e-25, 4.7146e-05, 3.1361e-03, 1.9300e-11, 5.3508e-33, 2.9959e-09,\n",
      "         5.4128e-09, 6.9307e-23, 1.1872e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0891, 0.0447, 0.0713, 0.0835, 0.0487, 0.0752, 0.0756, 0.0943,\n",
      "         0.0582, 0.0624, 0.0479, 0.0512, 0.0864, 0.0354]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 993.9140\n",
      "Test epoch : 19 Average loss: 1134.9730\n",
      "PP(train) = 2201.557, PP(valid) = 2288.314\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.6668e-37, 4.7360e-26, 7.9099e-08, 9.3153e-14, 9.1774e-13, 4.5223e-09,\n",
      "         3.3593e-27, 1.0000e+00, 1.0901e-07, 9.7258e-18, 5.2255e-25, 9.7992e-21,\n",
      "         1.0621e-10, 7.0073e-17, 4.7842e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 993.7398\n",
      "Test epoch : 20 Average loss: 1134.8802\n",
      "PP(train) = 2197.697, PP(valid) = 2286.773\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5900e-26, 1.4893e-17, 7.8879e-14, 4.0909e-19, 9.9985e-01, 2.7823e-35,\n",
      "         4.1967e-23, 1.2336e-04, 9.5645e-10, 2.4631e-18, 9.4112e-16, 5.5780e-20,\n",
      "         1.3782e-11, 2.8465e-22, 2.6334e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 993.5150\n",
      "Test epoch : 21 Average loss: 1134.7865\n",
      "PP(train) = 2194.002, PP(valid) = 2285.324\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7481e-27, 8.4397e-22, 1.2546e-15, 7.6935e-15, 8.8361e-01, 3.0535e-36,\n",
      "         4.4674e-17, 1.4561e-02, 5.4601e-02, 3.3924e-27, 2.2237e-26, 6.0107e-20,\n",
      "         2.1662e-09, 5.3072e-08, 4.7228e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0475, 0.0497, 0.0447, 0.0402, 0.0888, 0.0673, 0.1042, 0.1063, 0.0468,\n",
      "         0.0896, 0.0690, 0.0605, 0.0464, 0.0661, 0.0728]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 993.2719\n",
      "Test epoch : 22 Average loss: 1134.6950\n",
      "PP(train) = 2190.387, PP(valid) = 2283.979\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5294e-38, 3.0265e-20, 4.7642e-15, 7.6728e-25, 2.7666e-11, 5.3685e-32,\n",
      "         3.1930e-24, 2.9672e-07, 1.5427e-12, 1.3308e-22, 7.2617e-36, 1.1702e-21,\n",
      "         2.4811e-21, 2.8906e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 993.0488\n",
      "Test epoch : 23 Average loss: 1134.6029\n",
      "PP(train) = 2186.822, PP(valid) = 2282.648\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4106e-35, 9.1496e-24, 2.7500e-11, 7.5662e-23, 8.0171e-13, 3.0937e-31,\n",
      "         1.4190e-41, 1.4814e-11, 2.1342e-09, 7.2116e-20, 1.0062e-36, 3.8767e-36,\n",
      "         1.2235e-14, 2.3478e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 992.8146\n",
      "Test epoch : 24 Average loss: 1134.5129\n",
      "PP(train) = 2183.172, PP(valid) = 2281.259\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6326e-20, 5.0096e-26, 8.6932e-01, 4.9459e-11, 1.7804e-02, 2.6817e-22,\n",
      "         5.5244e-25, 1.1022e-01, 1.6012e-04, 2.6368e-16, 9.5637e-18, 1.3465e-22,\n",
      "         1.0255e-09, 2.5441e-13, 2.4943e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0767, 0.0911, 0.0457, 0.0677, 0.0843, 0.0527, 0.0762, 0.0720, 0.0885,\n",
      "         0.0563, 0.0676, 0.0479, 0.0496, 0.0874, 0.0363]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 992.6335\n",
      "Test epoch : 25 Average loss: 1134.4254\n",
      "PP(train) = 2179.500, PP(valid) = 2279.802\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.6610e-25, 2.7793e-25, 2.1985e-17, 5.7202e-21, 1.9884e-11, 6.9779e-21,\n",
      "         3.1135e-31, 9.9592e-01, 2.3609e-11, 3.5864e-15, 7.8073e-19, 1.7068e-22,\n",
      "         4.2223e-11, 1.2746e-26, 4.0841e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1053, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0370, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 992.4811\n",
      "Test epoch : 26 Average loss: 1134.3379\n",
      "PP(train) = 2175.881, PP(valid) = 2278.383\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1046e-31, 1.5792e-27, 1.2139e-05, 2.2122e-17, 1.3566e-13, 6.3420e-26,\n",
      "         1.1028e-30, 2.0704e-11, 1.0555e-05, 5.6255e-29, 1.8041e-25, 2.5285e-21,\n",
      "         3.2091e-07, 4.5211e-18, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 992.2176\n",
      "Test epoch : 27 Average loss: 1134.2507\n",
      "PP(train) = 2172.445, PP(valid) = 2277.122\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6020e-33, 1.0830e-23, 9.1505e-10, 4.9960e-09, 5.4128e-01, 2.0448e-24,\n",
      "         5.6232e-29, 1.0310e-11, 6.1836e-07, 1.8503e-20, 4.7635e-22, 9.3115e-20,\n",
      "         7.8840e-02, 1.1635e-14, 3.7988e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0579, 0.0492, 0.0637, 0.0449, 0.0769, 0.0676, 0.1037, 0.1017, 0.0532,\n",
      "         0.0769, 0.0649, 0.0591, 0.0496, 0.0669, 0.0639]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 991.9546\n",
      "Test epoch : 28 Average loss: 1134.1684\n",
      "PP(train) = 2169.015, PP(valid) = 2275.913\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0096e-35, 2.4421e-13, 1.8991e-10, 1.0306e-15, 2.2353e-14, 3.1812e-27,\n",
      "         5.3748e-23, 1.1590e-05, 4.8802e-10, 1.0128e-23, 7.1278e-32, 4.3721e-15,\n",
      "         1.2426e-17, 1.7327e-25, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 991.8602\n",
      "Test epoch : 29 Average loss: 1134.0821\n",
      "PP(train) = 2165.552, PP(valid) = 2274.563\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2084e-26, 1.7371e-20, 2.6189e-10, 1.2191e-13, 9.3770e-15, 1.4758e-30,\n",
      "         9.8777e-22, 1.3967e-10, 5.0767e-08, 9.1430e-23, 2.4430e-34, 3.8657e-25,\n",
      "         4.9916e-12, 3.7903e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 991.5947\n",
      "Test epoch : 30 Average loss: 1133.9992\n",
      "PP(train) = 2162.060, PP(valid) = 2273.213\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0678e-30, 2.2534e-26, 3.4267e-20, 2.8202e-24, 6.9593e-30, 1.0986e-29,\n",
      "         3.0837e-28, 6.2517e-16, 9.9999e-01, 6.4446e-25, 1.7059e-28, 4.0562e-26,\n",
      "         9.6750e-11, 1.0571e-19, 9.8365e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 991.4348\n",
      "Test epoch : 31 Average loss: 1133.9174\n",
      "PP(train) = 2158.642, PP(valid) = 2271.938\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9200e-27, 3.3091e-22, 1.3536e-08, 1.7408e-22, 7.7174e-07, 5.7705e-35,\n",
      "         3.1231e-25, 9.9881e-01, 3.4988e-21, 7.2175e-18, 1.4412e-25, 1.7684e-26,\n",
      "         2.1287e-14, 3.2665e-12, 1.1929e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 991.2221\n",
      "Test epoch : 32 Average loss: 1133.8348\n",
      "PP(train) = 2155.347, PP(valid) = 2270.746\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0535e-27, 1.7066e-11, 3.6288e-09, 2.2131e-11, 3.4867e-10, 5.0569e-20,\n",
      "         4.9682e-23, 5.6100e-01, 1.4916e-03, 3.0437e-20, 1.0284e-23, 2.5410e-17,\n",
      "         1.7599e-10, 2.6472e-20, 4.3751e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0848, 0.0741, 0.0741, 0.0480, 0.0718, 0.0822, 0.0913, 0.0622, 0.0572,\n",
      "         0.0442, 0.0919, 0.0499, 0.0434, 0.0852, 0.0398]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 990.9915\n",
      "Test epoch : 33 Average loss: 1133.7538\n",
      "PP(train) = 2151.986, PP(valid) = 2269.483\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1718e-24, 1.9352e-19, 1.9182e-07, 1.1961e-14, 7.8853e-12, 1.2020e-27,\n",
      "         4.4278e-19, 2.1804e-05, 2.2953e-07, 3.0431e-15, 2.0149e-30, 6.5878e-21,\n",
      "         9.4541e-10, 2.2563e-18, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 990.7995\n",
      "Test epoch : 34 Average loss: 1133.6729\n",
      "PP(train) = 2148.683, PP(valid) = 2268.239\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9025e-33, 1.2678e-18, 7.4709e-07, 1.1684e-15, 5.8986e-11, 9.7397e-30,\n",
      "         1.9616e-25, 7.0313e-05, 1.4528e-07, 7.4666e-24, 5.7332e-14, 3.4784e-23,\n",
      "         5.3577e-04, 5.9600e-14, 9.9939e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 990.5526\n",
      "Test epoch : 35 Average loss: 1133.5931\n",
      "PP(train) = 2145.448, PP(valid) = 2267.051\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1733e-28, 2.7563e-14, 4.7039e-08, 1.8397e-14, 6.3731e-05, 8.7537e-28,\n",
      "         4.9385e-23, 9.9483e-01, 7.4152e-07, 5.5231e-15, 6.6751e-13, 3.0418e-15,\n",
      "         3.9600e-03, 1.3105e-23, 1.1406e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1054, 0.0545, 0.0443, 0.0789, 0.0922, 0.0768, 0.0438, 0.0532,\n",
      "         0.0372, 0.1189, 0.0439, 0.0366, 0.0938, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 990.4213\n",
      "Test epoch : 36 Average loss: 1133.5171\n",
      "PP(train) = 2142.205, PP(valid) = 2265.877\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7200e-32, 2.7123e-19, 9.6144e-20, 4.3665e-10, 1.4808e-09, 3.0436e-28,\n",
      "         1.5328e-25, 1.6593e-10, 1.0000e+00, 6.1944e-18, 1.3268e-20, 2.4197e-28,\n",
      "         9.7580e-14, 3.9829e-20, 1.1166e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 990.2613\n",
      "Test epoch : 37 Average loss: 1133.4378\n",
      "PP(train) = 2138.933, PP(valid) = 2264.612\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8652e-18, 7.2786e-05, 3.5001e-02, 4.4218e-04, 1.6550e-04, 9.6228e-30,\n",
      "         5.9719e-14, 3.8166e-07, 8.9960e-01, 6.4715e-02, 4.3006e-08, 4.8537e-10,\n",
      "         7.5805e-08, 9.1234e-15, 5.7760e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0568, 0.0697, 0.0434, 0.0656, 0.0336, 0.0751, 0.0647, 0.0894, 0.0583,\n",
      "         0.0694, 0.0801, 0.0647, 0.0505, 0.1288, 0.0498]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 990.0468\n",
      "Test epoch : 38 Average loss: 1133.3628\n",
      "PP(train) = 2135.696, PP(valid) = 2263.409\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1666e-34, 5.5026e-27, 4.3029e-18, 3.2715e-12, 4.2387e-10, 7.6023e-28,\n",
      "         1.6229e-37, 9.9028e-01, 1.3776e-04, 1.3697e-19, 1.3733e-26, 3.1494e-27,\n",
      "         1.6296e-08, 5.0529e-22, 9.5785e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1048, 0.0547, 0.0443, 0.0790, 0.0921, 0.0772, 0.0440, 0.0532,\n",
      "         0.0371, 0.1187, 0.0440, 0.0367, 0.0938, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 989.7567\n",
      "Test epoch : 39 Average loss: 1133.2844\n",
      "PP(train) = 2132.583, PP(valid) = 2262.284\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9692e-20, 5.5032e-18, 2.6792e-06, 2.4823e-05, 1.5929e-01, 1.9206e-26,\n",
      "         2.4622e-21, 6.5861e-07, 1.0745e-07, 1.6310e-21, 4.2474e-19, 3.2811e-17,\n",
      "         1.3754e-16, 4.8592e-16, 8.4068e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0751, 0.0454, 0.0913, 0.0485, 0.0654, 0.0674, 0.1085, 0.0957, 0.0574,\n",
      "         0.0580, 0.0634, 0.0569, 0.0506, 0.0701, 0.0462]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 989.7041\n",
      "Test epoch : 40 Average loss: 1133.2071\n",
      "PP(train) = 2129.513, PP(valid) = 2261.185\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6353e-34, 4.9021e-32, 2.4927e-06, 1.1726e-09, 8.6531e-03, 6.7859e-27,\n",
      "         9.9349e-34, 8.8111e-13, 4.0244e-04, 3.6370e-16, 2.2391e-13, 2.0128e-28,\n",
      "         1.1984e-08, 1.2813e-18, 9.9094e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.0444, 0.1036, 0.0501, 0.0604, 0.0669, 0.1076, 0.0925, 0.0592,\n",
      "         0.0526, 0.0621, 0.0557, 0.0510, 0.0708, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 989.3875\n",
      "Test epoch : 41 Average loss: 1133.1329\n",
      "PP(train) = 2126.390, PP(valid) = 2260.024\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0620e-29, 2.8483e-13, 1.1423e-02, 7.9146e-13, 3.8223e-12, 7.2979e-26,\n",
      "         1.3176e-17, 9.1880e-06, 9.5744e-01, 1.7352e-15, 1.0338e-25, 1.8075e-17,\n",
      "         1.5079e-14, 4.7070e-09, 3.1123e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0572, 0.0669, 0.0456, 0.0627, 0.0315, 0.0756, 0.0643, 0.0920, 0.0576,\n",
      "         0.0688, 0.0838, 0.0648, 0.0512, 0.1287, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 989.2561\n",
      "Test epoch : 42 Average loss: 1133.0620\n",
      "PP(train) = 2123.178, PP(valid) = 2258.816\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0539e-20, 1.4755e-27, 1.6109e-06, 1.8232e-15, 1.3521e-05, 6.7466e-25,\n",
      "         1.3201e-18, 5.8211e-10, 9.9925e-01, 6.1207e-17, 2.5000e-28, 4.4220e-27,\n",
      "         1.1176e-15, 1.4937e-09, 7.3030e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 989.0853\n",
      "Test epoch : 43 Average loss: 1132.9879\n",
      "PP(train) = 2120.099, PP(valid) = 2257.668\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3647e-20, 8.0250e-15, 2.8307e-03, 5.0904e-12, 6.0630e-11, 3.9502e-24,\n",
      "         1.3689e-18, 3.3539e-10, 3.0142e-02, 1.2447e-12, 1.6324e-20, 8.1005e-19,\n",
      "         2.6492e-03, 1.7350e-20, 9.6438e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0452, 0.1017, 0.0508, 0.0590, 0.0673, 0.1058, 0.0925, 0.0596,\n",
      "         0.0530, 0.0627, 0.0560, 0.0512, 0.0724, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 988.9249\n",
      "Test epoch : 44 Average loss: 1132.9180\n",
      "PP(train) = 2117.170, PP(valid) = 2256.698\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7166e-33, 2.6819e-13, 7.1983e-10, 1.9881e-11, 7.2990e-03, 7.2701e-29,\n",
      "         6.7774e-36, 9.1305e-03, 2.6189e-02, 2.2637e-32, 3.3433e-28, 2.3268e-19,\n",
      "         3.3584e-12, 1.2222e-13, 9.5738e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0454, 0.1011, 0.0505, 0.0596, 0.0676, 0.1061, 0.0921, 0.0593,\n",
      "         0.0529, 0.0631, 0.0559, 0.0510, 0.0724, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 988.6319\n",
      "Test epoch : 45 Average loss: 1132.8452\n",
      "PP(train) = 2114.192, PP(valid) = 2255.617\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3156e-20, 7.6586e-13, 1.0926e-17, 3.3242e-10, 8.9955e-06, 1.0546e-25,\n",
      "         1.4077e-25, 1.8818e-08, 1.6211e-09, 2.9101e-19, 8.0135e-28, 2.9943e-17,\n",
      "         3.3528e-07, 2.1004e-16, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 988.5611\n",
      "Test epoch : 46 Average loss: 1132.7767\n",
      "PP(train) = 2111.170, PP(valid) = 2254.544\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7573e-27, 2.0538e-25, 9.7920e-01, 4.8463e-19, 5.3341e-10, 6.6666e-39,\n",
      "         9.2228e-22, 1.7746e-02, 8.0908e-07, 2.2306e-11, 2.5566e-29, 8.6926e-29,\n",
      "         3.0573e-03, 7.1545e-23, 2.9454e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0761, 0.0902, 0.0445, 0.0710, 0.0838, 0.0491, 0.0749, 0.0747, 0.0940,\n",
      "         0.0579, 0.0630, 0.0477, 0.0509, 0.0866, 0.0355]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 988.3000\n",
      "Test epoch : 47 Average loss: 1132.7062\n",
      "PP(train) = 2108.153, PP(valid) = 2253.411\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5577e-30, 4.5983e-23, 5.8372e-20, 1.4268e-18, 8.9899e-03, 2.7515e-24,\n",
      "         4.3608e-30, 1.1680e-06, 1.4344e-07, 2.5585e-14, 4.7248e-28, 1.1022e-33,\n",
      "         9.9098e-01, 1.2607e-14, 2.6599e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0742, 0.0758, 0.0603, 0.0416, 0.0646, 0.0543, 0.0792, 0.0743,\n",
      "         0.0939, 0.0484, 0.0530, 0.0584, 0.0643, 0.1096]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 988.1459\n",
      "Test epoch : 48 Average loss: 1132.6363\n",
      "PP(train) = 2105.278, PP(valid) = 2252.420\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3295e-30, 9.8523e-18, 8.8690e-04, 3.3636e-09, 9.9664e-01, 3.3189e-18,\n",
      "         5.8196e-15, 5.0840e-04, 5.8453e-06, 2.0156e-13, 1.2633e-22, 9.9560e-20,\n",
      "         1.9627e-03, 9.7869e-21, 5.8208e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0422, 0.0382, 0.0958, 0.0657, 0.1065, 0.1083, 0.0451,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 988.0146\n",
      "Test epoch : 49 Average loss: 1132.5676\n",
      "PP(train) = 2102.324, PP(valid) = 2251.332\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2298e-30, 1.4341e-22, 8.9634e-23, 4.5155e-22, 4.0341e-13, 6.6782e-32,\n",
      "         1.1770e-31, 5.2071e-12, 6.9789e-04, 7.3368e-21, 5.9052e-35, 1.8135e-30,\n",
      "         5.7119e-14, 6.7297e-19, 9.9930e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 987.7626\n",
      "Test epoch : 50 Average loss: 1132.5039\n",
      "PP(train) = 2099.439, PP(valid) = 2250.376\n",
      "Writing to ./topicwords/15-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 7, p : 8.4959 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                           a     \n",
      "\n",
      "===== # 2, Topic : 12, p : 7.9822 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                          \n",
      "\n",
      "===== # 3, Topic : 14, p : 8.8348 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "===== # 4, Topic : 14, p : 8.9720 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                   \n",
      "\n",
      "===== # 5, Topic : 7, p : 8.0516 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                           New Austrian Tunnelling Method                                        \n",
      "\n",
      "===== # 6, Topic : 14, p : 7.7862 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                 \n",
      "\n",
      "===== # 7, Topic : 14, p : 7.6117 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                          DC                                                    a                                                b                                                                                                             c d \n",
      "\n",
      "===== # 8, Topic : 14, p : 7.9809 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                             \n",
      "\n",
      "===== # 9, Topic : 14, p : 8.4835 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                   \n",
      "\n",
      "===== # 10, Topic : 14, p : 9.0156 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                m  m                         T               \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2748e-37, 2.1089e-22, 9.9999e-01, 3.0120e-12, 1.0192e-19, 1.0356e-42,\n",
      "         5.1333e-21, 5.3689e-06, 7.7930e-25, 1.2523e-33, 3.1183e-30, 3.1648e-21,\n",
      "         1.3607e-10, 8.0780e-27, 3.5265e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1062.8923\n",
      "Test epoch : 1 Average loss: 1010.7732\n",
      "PP(train) = 2214.410, PP(valid) = 2343.348\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8245e-27, 9.6971e-19, 3.1216e-13, 2.4236e-15, 3.2214e-10, 1.8698e-39,\n",
      "         7.5639e-27, 1.4094e-12, 5.6623e-05, 2.3076e-13, 4.9271e-27, 1.7600e-27,\n",
      "         5.5680e-17, 4.1079e-06, 9.9994e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1062.7227\n",
      "Test epoch : 2 Average loss: 1010.6861\n",
      "PP(train) = 2211.948, PP(valid) = 2342.059\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9463e-19, 4.6839e-08, 2.0694e-06, 1.2906e-10, 1.0415e-16, 3.7333e-12,\n",
      "         6.4474e-15, 2.9678e-03, 2.3223e-10, 6.0483e-19, 1.2609e-19, 7.0968e-15,\n",
      "         1.5879e-05, 1.5913e-10, 9.9701e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0445, 0.1042, 0.0502, 0.0602, 0.0670, 0.1075, 0.0922, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1062.6584\n",
      "Test epoch : 3 Average loss: 1010.5752\n",
      "PP(train) = 2208.284, PP(valid) = 2339.799\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4421e-20, 9.5070e-16, 7.6561e-01, 2.0172e-13, 4.8563e-04, 1.7888e-19,\n",
      "         2.7367e-18, 2.3291e-01, 8.0905e-08, 2.0715e-20, 9.0835e-24, 1.1561e-29,\n",
      "         3.2525e-13, 3.7460e-22, 1.0011e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0786, 0.0944, 0.0470, 0.0648, 0.0839, 0.0570, 0.0763, 0.0672, 0.0839,\n",
      "         0.0530, 0.0734, 0.0474, 0.0479, 0.0892, 0.0361]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1062.3806\n",
      "Test epoch : 4 Average loss: 1010.4614\n",
      "PP(train) = 2204.413, PP(valid) = 2337.529\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6022e-23, 6.3530e-24, 1.6105e-20, 1.6513e-11, 2.5027e-14, 1.0741e-39,\n",
      "         3.7477e-24, 4.2354e-06, 9.9999e-01, 1.1045e-14, 1.3792e-27, 7.2813e-24,\n",
      "         3.8914e-11, 7.9467e-08, 5.3137e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1062.1248\n",
      "Test epoch : 5 Average loss: 1010.3408\n",
      "PP(train) = 2200.902, PP(valid) = 2335.631\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1595e-38, 9.8217e-30, 9.7056e-01, 1.0541e-07, 5.4954e-15, 7.1406e-27,\n",
      "         6.2920e-15, 2.9436e-02, 4.3145e-09, 8.2145e-15, 1.9539e-35, 4.9046e-18,\n",
      "         4.3819e-13, 1.1720e-15, 1.7840e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0763, 0.0905, 0.0446, 0.0707, 0.0840, 0.0494, 0.0750, 0.0743, 0.0935,\n",
      "         0.0575, 0.0636, 0.0477, 0.0507, 0.0868, 0.0354]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1061.8601\n",
      "Test epoch : 6 Average loss: 1010.2240\n",
      "PP(train) = 2197.180, PP(valid) = 2333.641\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.1564e-32, 9.7228e-15, 4.6955e-04, 1.1884e-13, 9.4321e-01, 4.0964e-11,\n",
      "         1.8642e-16, 1.0180e-09, 5.6055e-02, 4.5126e-10, 2.4413e-26, 2.1430e-06,\n",
      "         1.9946e-19, 4.8435e-15, 2.6092e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0456, 0.0491, 0.0425, 0.0394, 0.0905, 0.0666, 0.1041, 0.1080, 0.0459,\n",
      "         0.0929, 0.0684, 0.0607, 0.0461, 0.0651, 0.0753]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1061.7117\n",
      "Test epoch : 7 Average loss: 1010.1020\n",
      "PP(train) = 2193.125, PP(valid) = 2331.278\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1441e-23, 4.9726e-15, 3.4844e-05, 4.0568e-13, 2.5843e-04, 1.4623e-09,\n",
      "         1.1435e-09, 1.5587e-05, 9.9902e-01, 1.2256e-07, 1.8870e-12, 9.6793e-24,\n",
      "         5.9328e-04, 3.0840e-08, 7.8529e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0498]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1061.4506\n",
      "Test epoch : 8 Average loss: 1009.9860\n",
      "PP(train) = 2189.173, PP(valid) = 2329.053\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0720e-29, 1.0457e-17, 2.4766e-19, 4.6345e-30, 9.9837e-01, 2.5995e-35,\n",
      "         3.5072e-33, 3.4716e-04, 6.8046e-19, 1.7322e-26, 3.0285e-28, 2.2586e-26,\n",
      "         2.5944e-13, 4.8733e-21, 1.2873e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1061.1124\n",
      "Test epoch : 9 Average loss: 1009.8656\n",
      "PP(train) = 2185.586, PP(valid) = 2327.089\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.4039e-23, 3.0087e-13, 3.5530e-16, 9.0191e-21, 1.2822e-07, 3.2989e-14,\n",
      "         2.2430e-26, 4.9519e-07, 7.9240e-19, 1.9857e-19, 5.6237e-20, 2.0113e-19,\n",
      "         2.7895e-14, 9.6017e-14, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1060.9455\n",
      "Test epoch : 10 Average loss: 1009.7474\n",
      "PP(train) = 2181.865, PP(valid) = 2325.030\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.3069e-27, 4.8154e-18, 4.0706e-06, 8.3248e-19, 6.4824e-16, 1.0459e-30,\n",
      "         2.9733e-21, 6.9310e-16, 7.3233e-09, 4.3748e-22, 2.1971e-27, 1.4951e-24,\n",
      "         1.2042e-18, 5.0361e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1060.6572\n",
      "Test epoch : 11 Average loss: 1009.6344\n",
      "PP(train) = 2178.112, PP(valid) = 2322.979\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.4604e-40, 1.9735e-16, 1.1837e-11, 6.1382e-11, 1.1887e-28, 3.5418e-33,\n",
      "         4.5407e-21, 1.0000e+00, 8.5924e-17, 1.5717e-29, 9.7259e-27, 1.0348e-34,\n",
      "         2.2029e-18, 5.5192e-28, 3.9857e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1060.4392\n",
      "Test epoch : 12 Average loss: 1009.5187\n",
      "PP(train) = 2174.404, PP(valid) = 2320.883\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0247e-31, 7.3389e-28, 1.1247e-16, 2.2029e-12, 4.6136e-08, 2.3786e-26,\n",
      "         3.3836e-16, 9.9981e-01, 1.9105e-18, 3.0499e-16, 1.3939e-27, 8.8650e-23,\n",
      "         6.1650e-08, 4.0167e-20, 1.8817e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1060.2036\n",
      "Test epoch : 13 Average loss: 1009.4089\n",
      "PP(train) = 2170.905, PP(valid) = 2319.041\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6686e-25, 4.4840e-28, 2.0516e-18, 5.5934e-12, 9.9999e-01, 1.2894e-21,\n",
      "         6.6043e-42, 5.9713e-06, 4.4154e-12, 2.0197e-14, 1.4849e-24, 3.5198e-23,\n",
      "         2.2813e-28, 1.1063e-16, 5.0192e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1059.9741\n",
      "Test epoch : 14 Average loss: 1009.2978\n",
      "PP(train) = 2167.279, PP(valid) = 2317.011\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8490e-21, 2.2323e-26, 8.6950e-01, 4.0715e-13, 2.3063e-12, 6.6279e-25,\n",
      "         1.3897e-25, 8.5993e-12, 4.5856e-18, 9.0034e-15, 4.7707e-25, 8.0776e-22,\n",
      "         4.1409e-16, 1.5165e-34, 1.3050e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0773, 0.0826, 0.0498, 0.0689, 0.0810, 0.0509, 0.0791, 0.0780, 0.0900,\n",
      "         0.0578, 0.0627, 0.0491, 0.0515, 0.0849, 0.0363]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1059.7791\n",
      "Test epoch : 15 Average loss: 1009.1855\n",
      "PP(train) = 2163.726, PP(valid) = 2315.006\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9497e-12, 2.4036e-19, 3.6921e-06, 1.5061e-06, 4.5345e-04, 1.9910e-24,\n",
      "         4.4503e-19, 6.0705e-01, 5.6274e-02, 2.2595e-15, 5.8547e-20, 4.3173e-21,\n",
      "         1.5170e-09, 2.8672e-10, 3.3621e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0831, 0.0789, 0.0686, 0.0483, 0.0702, 0.0841, 0.0873, 0.0601, 0.0568,\n",
      "         0.0442, 0.0964, 0.0498, 0.0428, 0.0893, 0.0401]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1059.5148\n",
      "Test epoch : 16 Average loss: 1009.0765\n",
      "PP(train) = 2160.212, PP(valid) = 2313.052\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4919e-25, 4.9178e-15, 1.2303e-02, 4.6341e-21, 4.5273e-04, 1.0527e-06,\n",
      "         1.2219e-14, 9.0359e-01, 5.9614e-09, 2.5656e-19, 5.7353e-11, 4.2024e-14,\n",
      "         1.3785e-06, 2.3985e-12, 8.3657e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0839, 0.0988, 0.0577, 0.0454, 0.0781, 0.0899, 0.0797, 0.0472, 0.0544,\n",
      "         0.0386, 0.1130, 0.0451, 0.0381, 0.0924, 0.0376]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1059.3451\n",
      "Test epoch : 17 Average loss: 1008.9704\n",
      "PP(train) = 2156.744, PP(valid) = 2311.153\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9661e-34, 3.0390e-16, 2.3335e-15, 2.2449e-18, 2.7065e-13, 4.1163e-24,\n",
      "         1.1981e-31, 5.1592e-08, 1.6450e-07, 1.0520e-22, 5.6916e-29, 3.7033e-33,\n",
      "         2.1091e-10, 3.4978e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1059.0565\n",
      "Test epoch : 18 Average loss: 1008.8641\n",
      "PP(train) = 2153.369, PP(valid) = 2309.337\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1937e-37, 1.3294e-18, 1.6896e-05, 3.5231e-10, 2.8882e-06, 5.2253e-27,\n",
      "         8.3430e-28, 7.4384e-01, 1.6749e-12, 3.8869e-29, 1.9505e-04, 5.4429e-17,\n",
      "         1.9771e-01, 4.3717e-17, 5.8238e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0770, 0.0965, 0.0622, 0.0489, 0.0705, 0.0869, 0.0753, 0.0528, 0.0589,\n",
      "         0.0467, 0.0989, 0.0475, 0.0421, 0.0883, 0.0475]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1058.8413\n",
      "Test epoch : 19 Average loss: 1008.7601\n",
      "PP(train) = 2150.118, PP(valid) = 2307.627\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.6529e-29, 9.7917e-12, 2.2478e-02, 9.9440e-18, 2.3160e-17, 8.3162e-26,\n",
      "         3.0393e-30, 8.0516e-06, 3.5505e-16, 4.3937e-23, 5.0553e-16, 1.0655e-18,\n",
      "         2.1490e-12, 1.8386e-23, 9.7751e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0451, 0.1025, 0.0507, 0.0606, 0.0665, 0.1068, 0.0921, 0.0601,\n",
      "         0.0525, 0.0621, 0.0555, 0.0511, 0.0713, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1058.6566\n",
      "Test epoch : 20 Average loss: 1008.6604\n",
      "PP(train) = 2146.736, PP(valid) = 2305.782\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8222e-22, 4.9410e-23, 1.8140e-03, 1.0167e-10, 9.9807e-01, 2.9697e-16,\n",
      "         1.1230e-17, 4.0398e-05, 1.6211e-08, 1.4199e-24, 6.7827e-22, 2.0488e-21,\n",
      "         2.8825e-08, 3.7364e-13, 7.8854e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0451,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1058.5194\n",
      "Test epoch : 21 Average loss: 1008.5589\n",
      "PP(train) = 2143.321, PP(valid) = 2303.863\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3057e-39, 1.6007e-20, 4.3682e-18, 1.6107e-21, 8.1326e-09, 2.7013e-33,\n",
      "         6.4741e-39, 2.6544e-17, 1.6996e-07, 1.5987e-18, 3.3940e-16, 1.3069e-18,\n",
      "         4.8302e-12, 5.2198e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1058.1956\n",
      "Test epoch : 22 Average loss: 1008.4588\n",
      "PP(train) = 2140.127, PP(valid) = 2302.179\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0018e-32, 2.2693e-23, 1.8473e-19, 8.7860e-21, 6.2072e-18, 4.9047e-36,\n",
      "         1.5887e-21, 1.0389e-07, 5.7148e-21, 2.5477e-18, 5.9591e-18, 5.4391e-28,\n",
      "         4.1110e-15, 2.2484e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1058.0013\n",
      "Test epoch : 23 Average loss: 1008.3563\n",
      "PP(train) = 2136.950, PP(valid) = 2300.479\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9483e-27, 1.4245e-21, 3.1602e-05, 6.3414e-15, 7.8350e-15, 3.9596e-22,\n",
      "         2.9279e-30, 4.0269e-09, 3.9267e-13, 3.9130e-14, 2.4638e-25, 6.4244e-25,\n",
      "         9.3744e-09, 5.7305e-15, 9.9997e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1057.8005\n",
      "Test epoch : 24 Average loss: 1008.2592\n",
      "PP(train) = 2133.683, PP(valid) = 2298.686\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3886e-29, 4.7989e-20, 8.8030e-03, 1.8834e-19, 9.9054e-07, 7.6216e-21,\n",
      "         7.6597e-20, 3.2300e-12, 2.7134e-07, 2.8729e-16, 2.3116e-24, 1.3887e-22,\n",
      "         8.2437e-01, 4.5668e-12, 1.6683e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0536, 0.0693, 0.0810, 0.0596, 0.0449, 0.0657, 0.0615, 0.0821, 0.0730,\n",
      "         0.0859, 0.0511, 0.0540, 0.0579, 0.0665, 0.0938]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1057.6187\n",
      "Test epoch : 25 Average loss: 1008.1632\n",
      "PP(train) = 2130.515, PP(valid) = 2296.985\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2951e-17, 3.8987e-16, 5.6457e-10, 6.2652e-24, 1.6938e-16, 2.3403e-19,\n",
      "         1.3928e-17, 2.0238e-02, 1.1961e-15, 7.4570e-30, 2.4824e-25, 1.6479e-17,\n",
      "         1.1040e-06, 1.5354e-25, 9.7976e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0819, 0.0452, 0.1032, 0.0502, 0.0605, 0.0675, 0.1070, 0.0911, 0.0593,\n",
      "         0.0520, 0.0629, 0.0554, 0.0508, 0.0714, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1057.4514\n",
      "Test epoch : 26 Average loss: 1008.0653\n",
      "PP(train) = 2127.372, PP(valid) = 2295.259\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2796e-30, 1.2790e-13, 8.9384e-21, 8.3341e-23, 2.0304e-17, 8.2968e-24,\n",
      "         9.4738e-32, 1.1325e-12, 2.0047e-07, 1.1517e-23, 6.5367e-34, 1.3834e-19,\n",
      "         1.1304e-10, 5.1150e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1057.1732\n",
      "Test epoch : 27 Average loss: 1007.9664\n",
      "PP(train) = 2124.205, PP(valid) = 2293.491\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6159e-26, 1.0902e-32, 3.9829e-23, 6.7264e-27, 1.8691e-22, 4.1576e-29,\n",
      "         6.7167e-28, 7.6590e-11, 1.4451e-09, 8.0846e-23, 1.3045e-36, 4.0752e-20,\n",
      "         1.0000e+00, 7.5484e-15, 4.2391e-17]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1056.9955\n",
      "Test epoch : 28 Average loss: 1007.8739\n",
      "PP(train) = 2121.156, PP(valid) = 2291.894\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4681e-24, 7.0351e-03, 1.0248e-05, 4.7844e-08, 9.6542e-01, 2.3414e-24,\n",
      "         2.8123e-17, 2.4926e-05, 5.2822e-07, 6.4453e-27, 2.8384e-17, 7.2918e-19,\n",
      "         5.8721e-03, 2.5039e-13, 2.1640e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0458, 0.0481, 0.0433, 0.0388, 0.0943, 0.0657, 0.1063, 0.1074, 0.0457,\n",
      "         0.0930, 0.0669, 0.0600, 0.0460, 0.0627, 0.0761]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1056.7953\n",
      "Test epoch : 29 Average loss: 1007.7820\n",
      "PP(train) = 2118.159, PP(valid) = 2290.309\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9995e-33, 2.6343e-17, 1.7020e-23, 4.7667e-20, 1.0000e+00, 2.7975e-31,\n",
      "         1.1814e-33, 5.6442e-09, 4.8378e-16, 1.5628e-17, 6.4717e-19, 1.7053e-20,\n",
      "         1.6363e-16, 2.5777e-12, 8.6435e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1056.6175\n",
      "Test epoch : 30 Average loss: 1007.6945\n",
      "PP(train) = 2115.194, PP(valid) = 2288.795\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0179e-22, 1.6891e-19, 2.4831e-05, 7.7576e-10, 4.5755e-04, 6.9108e-26,\n",
      "         4.0909e-19, 1.6066e-03, 4.5225e-08, 1.5325e-11, 2.9971e-09, 7.1801e-14,\n",
      "         1.3845e-01, 1.1672e-14, 8.5946e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0768, 0.0483, 0.1009, 0.0521, 0.0577, 0.0674, 0.0988, 0.0913, 0.0619,\n",
      "         0.0573, 0.0606, 0.0558, 0.0526, 0.0707, 0.0480]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1056.3725\n",
      "Test epoch : 31 Average loss: 1007.6022\n",
      "PP(train) = 2112.152, PP(valid) = 2287.133\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.8698e-22, 4.5752e-05, 2.0076e-01, 1.4809e-10, 5.3185e-02, 5.5350e-19,\n",
      "         7.1484e-08, 1.2838e-09, 1.7650e-06, 2.8586e-05, 6.3902e-15, 3.9228e-12,\n",
      "         2.1628e-08, 6.5445e-17, 7.4598e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0792, 0.0521, 0.0849, 0.0539, 0.0669, 0.0636, 0.1014, 0.0907, 0.0652,\n",
      "         0.0560, 0.0632, 0.0549, 0.0515, 0.0743, 0.0421]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1056.2672\n",
      "Test epoch : 32 Average loss: 1007.5122\n",
      "PP(train) = 2109.182, PP(valid) = 2285.532\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7455e-18, 2.3795e-22, 1.5343e-08, 4.8152e-11, 7.2497e-10, 1.3600e-21,\n",
      "         1.0942e-23, 1.8251e-09, 1.5533e-14, 2.3081e-21, 2.1133e-26, 5.3537e-18,\n",
      "         7.9013e-08, 2.7805e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1056.0359\n",
      "Test epoch : 33 Average loss: 1007.4240\n",
      "PP(train) = 2106.251, PP(valid) = 2283.972\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3570e-20, 4.8510e-16, 4.8583e-14, 7.7480e-10, 1.2568e-07, 1.4848e-24,\n",
      "         6.2164e-28, 9.9786e-01, 1.0736e-06, 3.8145e-12, 4.4949e-13, 1.4601e-22,\n",
      "         2.1349e-03, 8.6448e-10, 1.7528e-19]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1055, 0.0544, 0.0443, 0.0790, 0.0922, 0.0768, 0.0437, 0.0531,\n",
      "         0.0371, 0.1192, 0.0438, 0.0366, 0.0939, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1055.8892\n",
      "Test epoch : 34 Average loss: 1007.3346\n",
      "PP(train) = 2103.364, PP(valid) = 2282.450\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4483e-34, 2.2875e-20, 2.3496e-09, 7.3323e-04, 4.2338e-07, 6.1495e-09,\n",
      "         7.1113e-29, 5.5097e-01, 1.3469e-03, 4.7415e-14, 3.8931e-16, 1.0375e-14,\n",
      "         2.1779e-05, 1.9694e-14, 4.4693e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0847, 0.0734, 0.0746, 0.0481, 0.0717, 0.0819, 0.0916, 0.0626, 0.0572,\n",
      "         0.0444, 0.0913, 0.0500, 0.0436, 0.0850, 0.0399]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1055.5918\n",
      "Test epoch : 35 Average loss: 1007.2458\n",
      "PP(train) = 2100.511, PP(valid) = 2280.956\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.2291e-25, 1.4955e-12, 6.8807e-18, 1.9032e-15, 8.0305e-11, 4.8993e-27,\n",
      "         4.0836e-21, 9.5138e-01, 2.6429e-09, 1.2796e-18, 6.3905e-17, 1.8301e-15,\n",
      "         4.8365e-02, 9.5023e-26, 2.5204e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.1046, 0.0557, 0.0453, 0.0772, 0.0914, 0.0761, 0.0452, 0.0544,\n",
      "         0.0390, 0.1151, 0.0445, 0.0377, 0.0929, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1055.4694\n",
      "Test epoch : 36 Average loss: 1007.1596\n",
      "PP(train) = 2097.716, PP(valid) = 2279.504\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9646e-13, 1.3738e-14, 9.3096e-05, 1.2381e-13, 1.0317e-09, 1.7109e-23,\n",
      "         3.6931e-27, 2.1290e-14, 3.3508e-07, 2.1658e-05, 1.5362e-18, 4.5494e-18,\n",
      "         5.1277e-13, 2.5606e-04, 9.9963e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1055.3110\n",
      "Test epoch : 37 Average loss: 1007.0749\n",
      "PP(train) = 2094.845, PP(valid) = 2277.938\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2234e-21, 2.0789e-24, 2.4410e-24, 1.5064e-10, 3.0243e-20, 3.0037e-29,\n",
      "         1.0372e-26, 3.1875e-08, 3.3939e-08, 8.1519e-22, 8.5282e-25, 2.4949e-25,\n",
      "         1.6812e-18, 7.0505e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1055.1097\n",
      "Test epoch : 38 Average loss: 1006.9951\n",
      "PP(train) = 2091.992, PP(valid) = 2276.467\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.1438e-25, 1.9186e-24, 1.7882e-11, 3.2861e-10, 2.9073e-11, 2.6896e-16,\n",
      "         5.0333e-24, 2.3065e-01, 1.9534e-13, 9.9680e-16, 2.4862e-21, 1.7671e-26,\n",
      "         4.3391e-10, 8.8339e-17, 7.6935e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0836, 0.0552, 0.0914, 0.0496, 0.0652, 0.0734, 0.1013, 0.0791, 0.0589,\n",
      "         0.0492, 0.0734, 0.0536, 0.0481, 0.0770, 0.0411]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1054.9293\n",
      "Test epoch : 39 Average loss: 1006.9125\n",
      "PP(train) = 2089.278, PP(valid) = 2275.110\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0804e-25, 2.1593e-14, 8.4114e-08, 3.7019e-03, 5.6650e-04, 1.9636e-19,\n",
      "         1.6522e-22, 4.5144e-01, 1.3726e-02, 9.5659e-20, 1.1038e-12, 2.0358e-20,\n",
      "         5.2198e-01, 1.2702e-10, 8.5880e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0648, 0.0902, 0.0678, 0.0548, 0.0578, 0.0793, 0.0665, 0.0631, 0.0663,\n",
      "         0.0638, 0.0767, 0.0508, 0.0492, 0.0804, 0.0686]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1054.7254\n",
      "Test epoch : 40 Average loss: 1006.8315\n",
      "PP(train) = 2086.596, PP(valid) = 2273.745\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.4481e-36, 7.1152e-26, 3.3770e-16, 3.1408e-24, 2.5375e-17, 1.1882e-26,\n",
      "         0.0000e+00, 3.2925e-20, 3.6350e-14, 1.7753e-26, 4.0203e-28, 4.8633e-28,\n",
      "         1.2612e-17, 2.1507e-14, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1054.4959\n",
      "Test epoch : 41 Average loss: 1006.7502\n",
      "PP(train) = 2083.877, PP(valid) = 2272.320\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8426e-29, 7.2937e-22, 4.4994e-10, 7.5978e-20, 1.6641e-09, 4.3517e-22,\n",
      "         5.8097e-28, 3.5321e-16, 1.3005e-12, 5.0464e-16, 1.0033e-20, 2.0812e-15,\n",
      "         3.2908e-09, 3.1558e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1054.3817\n",
      "Test epoch : 42 Average loss: 1006.6720\n",
      "PP(train) = 2081.200, PP(valid) = 2270.948\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0098e-23, 6.3215e-31, 7.7254e-14, 1.8203e-06, 4.5238e-16, 7.8959e-34,\n",
      "         4.4424e-28, 9.9953e-01, 4.6934e-04, 3.6358e-22, 4.2952e-32, 3.2204e-32,\n",
      "         1.0628e-06, 3.5108e-17, 8.5403e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0443, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1054.1438\n",
      "Test epoch : 43 Average loss: 1006.5937\n",
      "PP(train) = 2078.552, PP(valid) = 2269.613\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5320e-18, 1.4714e-07, 3.7654e-11, 1.8380e-09, 2.5740e-16, 1.2013e-26,\n",
      "         5.9486e-15, 9.9998e-01, 7.1404e-08, 8.0889e-16, 4.3924e-25, 1.2406e-22,\n",
      "         9.2472e-17, 2.1769e-13, 2.3978e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1054.0390\n",
      "Test epoch : 44 Average loss: 1006.5147\n",
      "PP(train) = 2075.909, PP(valid) = 2268.220\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8820e-24, 2.7757e-25, 2.9930e-17, 1.0783e-09, 1.7008e-08, 1.9940e-33,\n",
      "         8.1380e-16, 1.1229e-02, 9.8877e-01, 2.3306e-12, 3.2574e-21, 1.4195e-19,\n",
      "         4.5139e-12, 3.6890e-08, 2.0202e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0564, 0.0678, 0.0444, 0.0627, 0.0307, 0.0762, 0.0631, 0.0912, 0.0570,\n",
      "         0.0689, 0.0850, 0.0649, 0.0509, 0.1310, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1053.8596\n",
      "Test epoch : 45 Average loss: 1006.4371\n",
      "PP(train) = 2073.291, PP(valid) = 2266.879\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7948e-30, 6.9555e-33, 3.8539e-16, 4.6757e-25, 4.7298e-25, 1.4255e-34,\n",
      "         1.4465e-39, 4.7189e-12, 1.0322e-12, 1.2240e-19, 2.0493e-26, 1.1334e-21,\n",
      "         3.9053e-26, 5.7597e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1053.6323\n",
      "Test epoch : 46 Average loss: 1006.3579\n",
      "PP(train) = 2070.697, PP(valid) = 2265.552\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.9413e-32, 5.1807e-26, 2.8022e-10, 1.9119e-18, 9.1368e-12, 2.5120e-23,\n",
      "         5.4709e-24, 1.6765e-10, 1.0000e+00, 6.7305e-17, 8.2469e-33, 8.1216e-24,\n",
      "         2.8252e-22, 1.6263e-20, 1.6680e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1053.4667\n",
      "Test epoch : 47 Average loss: 1006.2850\n",
      "PP(train) = 2068.122, PP(valid) = 2264.271\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2097e-22, 8.3807e-29, 1.0058e-13, 1.2898e-20, 9.7457e-12, 1.6521e-21,\n",
      "         8.6401e-28, 8.9342e-01, 4.6875e-16, 6.0268e-17, 4.4913e-26, 1.3095e-29,\n",
      "         5.9869e-22, 3.0229e-22, 1.0658e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0841, 0.0972, 0.0588, 0.0453, 0.0776, 0.0900, 0.0805, 0.0477, 0.0542,\n",
      "         0.0388, 0.1124, 0.0454, 0.0382, 0.0921, 0.0377]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1053.3200\n",
      "Test epoch : 48 Average loss: 1006.2095\n",
      "PP(train) = 2065.564, PP(valid) = 2262.936\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8895e-30, 8.4495e-24, 9.9547e-01, 2.5852e-12, 1.2535e-12, 1.1349e-34,\n",
      "         6.4076e-08, 4.5286e-03, 1.2154e-11, 3.1657e-19, 8.9288e-18, 5.0060e-12,\n",
      "         1.3693e-17, 1.1260e-16, 6.1390e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0900, 0.0443, 0.0714, 0.0840, 0.0485, 0.0748, 0.0752, 0.0947,\n",
      "         0.0581, 0.0625, 0.0477, 0.0511, 0.0865, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1053.1765\n",
      "Test epoch : 49 Average loss: 1006.1383\n",
      "PP(train) = 2063.098, PP(valid) = 2261.740\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.9795e-24, 1.2007e-13, 8.1134e-10, 5.7046e-18, 2.6594e-10, 4.2785e-19,\n",
      "         2.2196e-25, 7.0971e-01, 2.6852e-18, 2.1213e-17, 3.8565e-23, 1.0464e-21,\n",
      "         2.9029e-01, 5.2560e-09, 2.1471e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0736, 0.0987, 0.0620, 0.0501, 0.0678, 0.0861, 0.0717, 0.0536, 0.0606,\n",
      "         0.0501, 0.0949, 0.0479, 0.0433, 0.0871, 0.0524]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1052.9377\n",
      "Test epoch : 50 Average loss: 1006.0635\n",
      "PP(train) = 2060.524, PP(valid) = 2260.385\n",
      "Writing to ./topicwords/16-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 8, p : 8.0267 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                    \n",
      "\n",
      "===== # 2, Topic : 14, p : 8.4000 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                               \n",
      "\n",
      "===== # 3, Topic : 14, p : 8.2335 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                               \n",
      "\n",
      "===== # 4, Topic : 10, p : 7.7831 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                                                                             \n",
      "\n",
      "===== # 5, Topic : 14, p : 9.0134 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                               \n",
      "\n",
      "===== # 6, Topic : 2, p : 8.2298 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                     \n",
      "\n",
      "===== # 7, Topic : 14, p : 7.5649 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                        \n",
      "\n",
      "===== # 8, Topic : 14, p : 7.9460 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                              \n",
      "\n",
      "===== # 9, Topic : 4, p : 7.6753 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                          \n",
      "\n",
      "===== # 10, Topic : 14, p : 7.8357 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                         \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9978e-26, 3.0712e-19, 1.0002e-10, 1.6033e-17, 2.0603e-05, 3.4737e-28,\n",
      "         5.6019e-25, 3.6543e-02, 7.4287e-05, 4.9601e-14, 7.6494e-34, 2.1212e-29,\n",
      "         5.6071e-14, 1.6466e-19, 9.6336e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0821, 0.0459, 0.1022, 0.0501, 0.0609, 0.0679, 0.1066, 0.0902, 0.0593,\n",
      "         0.0518, 0.0637, 0.0553, 0.0506, 0.0718, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1011.3688\n",
      "Test epoch : 1 Average loss: 1069.4710\n",
      "PP(train) = 2060.185, PP(valid) = 2133.895\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.9077e-20, 9.9682e-01, 2.0372e-08, 1.6442e-09, 4.1423e-07, 5.1056e-26,\n",
      "         2.1525e-19, 5.9980e-09, 5.0572e-12, 2.6330e-11, 1.2227e-17, 7.9064e-08,\n",
      "         3.0949e-03, 6.5529e-19, 8.0014e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1093, 0.0480, 0.0399, 0.1057, 0.0449, 0.0465, 0.0730, 0.0406, 0.0766,\n",
      "         0.0767, 0.0375, 0.0459, 0.0599, 0.1148, 0.0807]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1011.3510\n",
      "Test epoch : 2 Average loss: 1069.3895\n",
      "PP(train) = 2057.487, PP(valid) = 2132.643\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3351e-24, 1.3604e-21, 2.5493e-06, 3.1523e-19, 8.4495e-09, 6.2305e-31,\n",
      "         3.5397e-17, 3.9123e-10, 7.7600e-01, 1.0569e-24, 3.5018e-19, 9.6145e-16,\n",
      "         2.2399e-01, 2.4594e-10, 2.5917e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0550, 0.0699, 0.0507, 0.0632, 0.0330, 0.0743, 0.0616, 0.0901, 0.0614,\n",
      "         0.0752, 0.0756, 0.0630, 0.0534, 0.1135, 0.0602]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1011.0657\n",
      "Test epoch : 3 Average loss: 1069.3063\n",
      "PP(train) = 2054.087, PP(valid) = 2131.224\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.9625e-28, 1.4801e-20, 6.0950e-08, 1.7402e-11, 7.2601e-11, 9.7477e-20,\n",
      "         1.6310e-23, 4.1881e-02, 1.2689e-10, 8.0578e-12, 2.5935e-25, 1.1072e-12,\n",
      "         2.9080e-05, 1.4344e-16, 9.5809e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0822, 0.0462, 0.1019, 0.0501, 0.0610, 0.0681, 0.1065, 0.0898, 0.0593,\n",
      "         0.0518, 0.0640, 0.0552, 0.0505, 0.0720, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1010.8975\n",
      "Test epoch : 4 Average loss: 1069.2223\n",
      "PP(train) = 2050.672, PP(valid) = 2130.037\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9401e-21, 1.5970e-17, 4.0503e-17, 2.7108e-12, 1.7801e-08, 5.6111e-19,\n",
      "         4.6324e-15, 2.0547e-14, 3.9068e-04, 7.6407e-21, 9.9383e-17, 2.2143e-11,\n",
      "         1.1128e-07, 8.4715e-13, 9.9961e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1010.7420\n",
      "Test epoch : 5 Average loss: 1069.1370\n",
      "PP(train) = 2047.032, PP(valid) = 2128.728\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9623e-22, 9.6511e-22, 1.3408e-11, 2.2486e-08, 1.6918e-02, 2.0706e-12,\n",
      "         2.9722e-23, 9.8051e-01, 3.7905e-05, 4.3209e-19, 5.4654e-17, 5.0866e-17,\n",
      "         2.5299e-03, 1.3683e-31, 1.3030e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0826, 0.1044, 0.0543, 0.0443, 0.0795, 0.0919, 0.0774, 0.0445, 0.0531,\n",
      "         0.0378, 0.1182, 0.0442, 0.0368, 0.0934, 0.0376]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1010.4236\n",
      "Test epoch : 6 Average loss: 1069.0539\n",
      "PP(train) = 2043.272, PP(valid) = 2127.366\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0971e-23, 3.0365e-30, 9.9909e-01, 1.4185e-11, 2.8845e-08, 2.7599e-30,\n",
      "         6.9335e-29, 9.1759e-15, 2.5918e-04, 2.9183e-19, 8.5188e-35, 9.6283e-14,\n",
      "         1.6336e-07, 3.8662e-12, 6.5399e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0898, 0.0442, 0.0716, 0.0839, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1010.2016\n",
      "Test epoch : 7 Average loss: 1068.9723\n",
      "PP(train) = 2039.700, PP(valid) = 2126.217\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3621e-23, 6.5731e-24, 3.6167e-14, 7.5150e-23, 3.6480e-02, 1.3623e-27,\n",
      "         1.2481e-20, 1.0565e-15, 9.6352e-01, 4.9020e-25, 8.9390e-17, 2.8358e-25,\n",
      "         2.1526e-10, 1.0761e-07, 1.1067e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0558, 0.0668, 0.0443, 0.0619, 0.0318, 0.0759, 0.0644, 0.0928, 0.0567,\n",
      "         0.0703, 0.0842, 0.0651, 0.0510, 0.1283, 0.0507]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1009.9975\n",
      "Test epoch : 8 Average loss: 1068.8896\n",
      "PP(train) = 2036.057, PP(valid) = 2124.931\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0146e-25, 2.0404e-24, 7.1210e-07, 6.7683e-24, 6.9937e-02, 3.3337e-20,\n",
      "         1.5277e-21, 9.3006e-01, 3.0506e-14, 2.2114e-26, 1.1784e-14, 6.4130e-28,\n",
      "         2.3602e-09, 6.5974e-22, 7.9405e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0806, 0.1008, 0.0539, 0.0442, 0.0809, 0.0909, 0.0794, 0.0469, 0.0529,\n",
      "         0.0398, 0.1157, 0.0452, 0.0374, 0.0921, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1009.7201\n",
      "Test epoch : 9 Average loss: 1068.8095\n",
      "PP(train) = 2032.446, PP(valid) = 2123.676\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9750e-33, 1.3900e-13, 4.6965e-05, 3.1691e-05, 1.4864e-07, 3.2076e-17,\n",
      "         3.4557e-24, 2.1424e-02, 9.5512e-01, 2.6785e-22, 1.7526e-19, 1.0955e-10,\n",
      "         3.4124e-06, 1.8196e-14, 2.3374e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0573, 0.0676, 0.0455, 0.0623, 0.0316, 0.0764, 0.0642, 0.0908, 0.0572,\n",
      "         0.0682, 0.0850, 0.0646, 0.0509, 0.1291, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1009.4685\n",
      "Test epoch : 10 Average loss: 1068.7304\n",
      "PP(train) = 2028.926, PP(valid) = 2122.529\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7000e-21, 1.0714e-18, 2.7121e-14, 1.9004e-14, 3.7778e-08, 4.0800e-22,\n",
      "         6.2129e-16, 1.4759e-12, 3.3446e-04, 1.9899e-12, 1.1654e-23, 1.2643e-17,\n",
      "         6.4918e-02, 1.8362e-10, 9.3475e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0794, 0.0461, 0.1028, 0.0511, 0.0590, 0.0671, 0.1034, 0.0919, 0.0606,\n",
      "         0.0546, 0.0613, 0.0557, 0.0518, 0.0708, 0.0444]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1009.2765\n",
      "Test epoch : 11 Average loss: 1068.6523\n",
      "PP(train) = 2025.438, PP(valid) = 2121.378\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3436e-24, 2.7560e-17, 8.9613e-17, 5.0024e-14, 4.8154e-11, 2.6036e-37,\n",
      "         1.1274e-21, 2.5789e-07, 9.9670e-01, 1.3353e-24, 3.9630e-11, 2.8406e-22,\n",
      "         3.7964e-06, 1.7965e-22, 3.2996e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0444, 0.0628, 0.0305, 0.0760, 0.0630, 0.0919, 0.0570,\n",
      "         0.0692, 0.0846, 0.0651, 0.0511, 0.1311, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1009.0159\n",
      "Test epoch : 12 Average loss: 1068.5762\n",
      "PP(train) = 2021.996, PP(valid) = 2120.243\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4918e-16, 8.2835e-22, 9.1600e-01, 5.4651e-04, 3.2151e-10, 4.1910e-18,\n",
      "         1.3213e-25, 2.1856e-12, 7.8306e-02, 2.5278e-16, 2.4784e-11, 9.3543e-17,\n",
      "         5.1516e-03, 1.0864e-23, 4.6875e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0744, 0.0883, 0.0446, 0.0712, 0.0777, 0.0505, 0.0741, 0.0770, 0.0916,\n",
      "         0.0595, 0.0641, 0.0492, 0.0515, 0.0897, 0.0367]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1008.8784\n",
      "Test epoch : 13 Average loss: 1068.5011\n",
      "PP(train) = 2018.546, PP(valid) = 2119.090\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7031e-28, 3.4777e-16, 8.7574e-15, 3.5656e-20, 2.9844e-06, 6.0944e-20,\n",
      "         3.2005e-17, 4.3246e-12, 9.9079e-01, 6.0583e-17, 8.5367e-11, 8.9203e-24,\n",
      "         2.7562e-14, 3.1244e-18, 9.2037e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0564, 0.0672, 0.0447, 0.0628, 0.0306, 0.0760, 0.0633, 0.0920, 0.0570,\n",
      "         0.0692, 0.0844, 0.0651, 0.0511, 0.1307, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1008.6135\n",
      "Test epoch : 14 Average loss: 1068.4282\n",
      "PP(train) = 2015.116, PP(valid) = 2117.897\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5877e-22, 2.1423e-24, 2.6897e-23, 4.9158e-19, 1.0000e+00, 5.3973e-29,\n",
      "         1.9670e-12, 1.4035e-13, 3.6576e-15, 1.7931e-12, 6.9718e-24, 5.9279e-13,\n",
      "         7.7727e-16, 3.0640e-22, 1.8734e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1008.4512\n",
      "Test epoch : 15 Average loss: 1068.3543\n",
      "PP(train) = 2011.811, PP(valid) = 2116.795\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0934e-29, 3.2257e-19, 1.0205e-04, 4.0005e-10, 5.7700e-04, 7.4751e-26,\n",
      "         2.2273e-21, 9.9931e-01, 2.6223e-06, 2.0173e-08, 5.2272e-20, 2.1925e-12,\n",
      "         3.2842e-06, 2.6366e-12, 9.6415e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0543, 0.0442, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1008.1533\n",
      "Test epoch : 16 Average loss: 1068.2798\n",
      "PP(train) = 2008.620, PP(valid) = 2115.786\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1817e-21, 3.9795e-16, 5.1085e-19, 6.5332e-18, 3.3148e-06, 5.3718e-24,\n",
      "         1.8194e-27, 1.0000e+00, 1.1573e-15, 3.6566e-18, 4.7516e-20, 4.0794e-30,\n",
      "         2.9284e-14, 1.1399e-24, 7.0530e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1007.9996\n",
      "Test epoch : 17 Average loss: 1068.2088\n",
      "PP(train) = 2005.324, PP(valid) = 2114.691\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4933e-29, 1.7250e-22, 4.8861e-03, 1.0057e-33, 4.3945e-08, 1.0387e-25,\n",
      "         3.5713e-25, 9.5765e-01, 1.2856e-16, 1.1320e-15, 3.7483e-30, 1.6960e-24,\n",
      "         9.9463e-11, 8.4158e-21, 3.7465e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0836, 0.1025, 0.0559, 0.0447, 0.0786, 0.0912, 0.0781, 0.0452, 0.0537,\n",
      "         0.0377, 0.1166, 0.0444, 0.0372, 0.0933, 0.0372]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1007.6805\n",
      "Test epoch : 18 Average loss: 1068.1400\n",
      "PP(train) = 2002.097, PP(valid) = 2113.674\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6992e-25, 1.6598e-15, 2.1313e-09, 1.6062e-09, 3.7027e-01, 5.0962e-19,\n",
      "         3.0134e-31, 1.8426e-05, 3.9161e-06, 1.0302e-14, 6.0869e-21, 2.6320e-24,\n",
      "         4.6509e-12, 4.7461e-15, 6.2971e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0666, 0.0465, 0.0760, 0.0461, 0.0728, 0.0677, 0.1092, 0.0998, 0.0545,\n",
      "         0.0662, 0.0650, 0.0582, 0.0498, 0.0687, 0.0530]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1007.4465\n",
      "Test epoch : 19 Average loss: 1068.0719\n",
      "PP(train) = 1998.964, PP(valid) = 2112.663\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9294e-16, 2.9724e-19, 2.1944e-11, 1.1951e-20, 5.8869e-11, 1.8314e-10,\n",
      "         2.8576e-22, 3.7125e-03, 6.7312e-10, 8.6612e-20, 2.8977e-22, 1.8463e-15,\n",
      "         2.1150e-15, 2.2090e-17, 9.9629e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0445, 0.1041, 0.0502, 0.0602, 0.0670, 0.1075, 0.0921, 0.0593,\n",
      "         0.0523, 0.0621, 0.0555, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1007.3200\n",
      "Test epoch : 20 Average loss: 1068.0035\n",
      "PP(train) = 1995.726, PP(valid) = 2111.540\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1271e-29, 2.0436e-12, 1.1040e-13, 5.7123e-17, 4.2761e-12, 1.7644e-35,\n",
      "         4.2679e-29, 1.4593e-13, 5.0276e-06, 3.3468e-27, 6.3929e-32, 4.0285e-19,\n",
      "         3.8282e-08, 5.7774e-32, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1007.0486\n",
      "Test epoch : 21 Average loss: 1067.9357\n",
      "PP(train) = 1992.677, PP(valid) = 2110.562\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0310e-17, 3.2445e-18, 1.0672e-05, 9.9763e-01, 7.6727e-09, 5.6805e-07,\n",
      "         4.0503e-25, 3.3811e-05, 4.7671e-17, 9.0074e-20, 1.0695e-24, 4.7563e-25,\n",
      "         7.6870e-07, 7.7503e-17, 2.3246e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0417, 0.0551, 0.0918, 0.0503, 0.0527, 0.0525, 0.0533, 0.0686,\n",
      "         0.1349, 0.1029, 0.0485, 0.0563, 0.0522, 0.0905]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1006.9146\n",
      "Test epoch : 22 Average loss: 1067.8691\n",
      "PP(train) = 1989.661, PP(valid) = 2109.622\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.7737e-24, 4.5993e-03, 6.6233e-03, 8.9161e-12, 5.0538e-07, 3.1658e-10,\n",
      "         5.2835e-15, 9.8751e-01, 3.7917e-06, 1.2429e-13, 3.1176e-10, 6.3835e-11,\n",
      "         1.9488e-04, 2.2802e-13, 1.0707e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0836, 0.1051, 0.0543, 0.0446, 0.0790, 0.0917, 0.0770, 0.0439, 0.0534,\n",
      "         0.0373, 0.1183, 0.0439, 0.0368, 0.0941, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1006.6970\n",
      "Test epoch : 23 Average loss: 1067.8045\n",
      "PP(train) = 1986.513, PP(valid) = 2108.546\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.0878e-19, 2.6782e-22, 9.3992e-04, 6.4895e-15, 2.7448e-14, 5.3577e-17,\n",
      "         8.9945e-28, 1.2691e-08, 1.8437e-02, 2.6812e-09, 7.9679e-14, 7.4678e-11,\n",
      "         9.8058e-01, 1.8975e-13, 4.1738e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0485, 0.0744, 0.0754, 0.0607, 0.0411, 0.0648, 0.0541, 0.0792, 0.0744,\n",
      "         0.0934, 0.0488, 0.0532, 0.0584, 0.0653, 0.1084]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1006.4976\n",
      "Test epoch : 24 Average loss: 1067.7403\n",
      "PP(train) = 1983.535, PP(valid) = 2107.581\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7199e-27, 3.3337e-21, 2.1898e-17, 9.9619e-27, 3.7533e-14, 1.0719e-18,\n",
      "         1.6658e-18, 1.0000e+00, 6.2681e-08, 8.3323e-22, 1.5626e-20, 2.3944e-16,\n",
      "         3.1202e-09, 9.8828e-21, 4.0509e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1006.2986\n",
      "Test epoch : 25 Average loss: 1067.6755\n",
      "PP(train) = 1980.619, PP(valid) = 2106.658\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4213e-13, 9.6666e-19, 4.6031e-15, 2.2346e-08, 7.0620e-08, 3.5897e-20,\n",
      "         6.4124e-23, 4.1479e-05, 4.3464e-05, 1.7503e-09, 1.9074e-21, 9.8505e-23,\n",
      "         9.9991e-01, 1.0429e-08, 1.1664e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1006.1007\n",
      "Test epoch : 26 Average loss: 1067.6136\n",
      "PP(train) = 1977.680, PP(valid) = 2105.741\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4491e-19, 2.8835e-13, 7.8083e-01, 5.8386e-10, 1.7683e-01, 1.6558e-06,\n",
      "         7.2701e-14, 5.1745e-08, 3.0657e-12, 3.3278e-09, 1.1073e-09, 4.0478e-10,\n",
      "         4.2336e-02, 1.2760e-13, 2.2276e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0690, 0.0811, 0.0456, 0.0647, 0.0849, 0.0526, 0.0799, 0.0819, 0.0838,\n",
      "         0.0657, 0.0635, 0.0508, 0.0513, 0.0819, 0.0432]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1005.9413\n",
      "Test epoch : 27 Average loss: 1067.5540\n",
      "PP(train) = 1974.737, PP(valid) = 2104.794\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0513e-20, 7.1692e-28, 2.0496e-02, 1.2072e-15, 3.4218e-04, 4.1166e-28,\n",
      "         5.7147e-20, 9.7916e-01, 4.1466e-11, 9.1713e-19, 3.1440e-25, 5.9978e-14,\n",
      "         2.1726e-08, 7.8470e-13, 5.9019e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1054, 0.0542, 0.0447, 0.0793, 0.0912, 0.0769, 0.0442, 0.0538,\n",
      "         0.0374, 0.1179, 0.0440, 0.0368, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1005.6822\n",
      "Test epoch : 28 Average loss: 1067.4942\n",
      "PP(train) = 1971.834, PP(valid) = 2103.903\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.0868e-18, 3.3717e-23, 3.0646e-17, 1.8082e-16, 3.7791e-09, 4.4601e-30,\n",
      "         3.0948e-32, 1.0000e+00, 2.2731e-13, 6.4660e-13, 2.1672e-28, 4.2233e-25,\n",
      "         1.6512e-21, 9.5888e-18, 5.2400e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1005.4982\n",
      "Test epoch : 29 Average loss: 1067.4305\n",
      "PP(train) = 1969.095, PP(valid) = 2103.045\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4293e-35, 6.3793e-23, 1.2170e-06, 2.7218e-28, 3.3267e-22, 1.1623e-24,\n",
      "         1.1136e-19, 1.3372e-09, 8.4520e-15, 2.2332e-24, 3.4199e-23, 3.6027e-27,\n",
      "         3.3838e-18, 7.7423e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1005.3032\n",
      "Test epoch : 30 Average loss: 1067.3717\n",
      "PP(train) = 1966.267, PP(valid) = 2102.157\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.7237e-20, 3.7899e-22, 1.1464e-13, 7.4725e-08, 9.9417e-15, 1.5196e-22,\n",
      "         7.5466e-24, 1.5891e-03, 9.9841e-01, 2.2093e-29, 2.8820e-30, 1.5628e-24,\n",
      "         3.6569e-10, 2.0685e-20, 9.8995e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0674, 0.0443, 0.0628, 0.0304, 0.0760, 0.0629, 0.0918, 0.0570,\n",
      "         0.0692, 0.0847, 0.0651, 0.0510, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1005.1307\n",
      "Test epoch : 31 Average loss: 1067.3168\n",
      "PP(train) = 1963.405, PP(valid) = 2101.260\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3292e-18, 1.4639e-22, 2.9454e-17, 7.6506e-16, 1.0286e-12, 3.1317e-25,\n",
      "         4.2242e-19, 1.5909e-17, 9.9937e-01, 1.0779e-18, 2.7618e-20, 1.4991e-16,\n",
      "         2.9311e-12, 9.3288e-15, 6.2757e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1004.9583\n",
      "Test epoch : 32 Average loss: 1067.2575\n",
      "PP(train) = 1960.631, PP(valid) = 2100.339\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3248e-21, 3.1026e-25, 1.8933e-17, 8.9484e-19, 1.3295e-07, 5.6926e-24,\n",
      "         1.0705e-20, 3.2505e-02, 9.6231e-01, 3.6583e-20, 1.3383e-22, 2.0504e-21,\n",
      "         7.8447e-12, 3.1626e-20, 5.1812e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0571, 0.0684, 0.0449, 0.0623, 0.0316, 0.0767, 0.0637, 0.0900, 0.0570,\n",
      "         0.0680, 0.0857, 0.0644, 0.0507, 0.1300, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1004.6921\n",
      "Test epoch : 33 Average loss: 1067.2007\n",
      "PP(train) = 1957.979, PP(valid) = 2099.568\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3250e-27, 1.8534e-25, 1.6415e-15, 1.1871e-21, 3.8168e-10, 1.8243e-34,\n",
      "         6.6428e-37, 1.4675e-07, 1.2854e-03, 2.8138e-09, 1.9714e-28, 8.0366e-19,\n",
      "         4.2238e-15, 4.6063e-31, 9.9871e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1004.4466\n",
      "Test epoch : 34 Average loss: 1067.1462\n",
      "PP(train) = 1955.326, PP(valid) = 2098.815\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.7232e-24, 3.0652e-11, 1.6263e-16, 3.7025e-24, 1.6567e-08, 2.7714e-24,\n",
      "         3.2445e-29, 9.3101e-12, 2.9373e-13, 6.3947e-12, 1.5309e-21, 1.8514e-26,\n",
      "         2.6943e-12, 9.6840e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1004.3724\n",
      "Test epoch : 35 Average loss: 1067.0908\n",
      "PP(train) = 1952.569, PP(valid) = 2097.927\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.1173e-15, 2.1679e-05, 3.4194e-18, 6.4062e-12, 8.3411e-05, 1.5139e-23,\n",
      "         9.4507e-26, 6.3307e-02, 8.4091e-01, 5.4222e-15, 1.1750e-20, 1.0595e-04,\n",
      "         1.9033e-05, 2.1394e-09, 9.5549e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0604, 0.0674, 0.0493, 0.0609, 0.0349, 0.0769, 0.0679, 0.0888, 0.0576,\n",
      "         0.0656, 0.0850, 0.0633, 0.0506, 0.1227, 0.0486]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1004.2248\n",
      "Test epoch : 36 Average loss: 1067.0388\n",
      "PP(train) = 1949.893, PP(valid) = 2097.075\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.3324e-32, 8.9506e-29, 5.5504e-18, 1.7682e-14, 1.5322e-20, 4.7800e-30,\n",
      "         1.1836e-31, 2.1299e-25, 6.0949e-18, 5.5628e-23, 3.4762e-21, 3.8614e-27,\n",
      "         8.0568e-28, 2.1323e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1004.0392\n",
      "Test epoch : 37 Average loss: 1066.9874\n",
      "PP(train) = 1947.273, PP(valid) = 2096.338\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8200e-35, 2.1218e-34, 7.1986e-04, 7.5210e-19, 4.3353e-22, 9.7830e-39,\n",
      "         7.2774e-31, 1.8884e-02, 2.0783e-16, 6.8335e-13, 9.0725e-21, 4.7081e-27,\n",
      "         1.1987e-02, 4.0343e-23, 9.6841e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0455, 0.1029, 0.0504, 0.0603, 0.0675, 0.1063, 0.0911, 0.0596,\n",
      "         0.0525, 0.0627, 0.0555, 0.0509, 0.0714, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1003.7362\n",
      "Test epoch : 38 Average loss: 1066.9343\n",
      "PP(train) = 1944.789, PP(valid) = 2095.660\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.2330e-30, 1.4163e-22, 2.1096e-32, 3.1678e-17, 1.0000e+00, 9.0642e-26,\n",
      "         4.6596e-14, 3.6139e-08, 3.2226e-09, 1.2178e-29, 5.7945e-26, 2.6190e-22,\n",
      "         4.7187e-17, 3.1979e-20, 5.5718e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1003.6348\n",
      "Test epoch : 39 Average loss: 1066.8844\n",
      "PP(train) = 1942.141, PP(valid) = 2094.867\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2875e-28, 5.1640e-14, 1.2182e-08, 7.2159e-10, 4.3040e-06, 1.0007e-15,\n",
      "         5.7902e-20, 1.7488e-07, 1.2144e-10, 4.9901e-16, 1.7122e-10, 4.1731e-11,\n",
      "         3.4384e-20, 4.3870e-14, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1003.4877\n",
      "Test epoch : 40 Average loss: 1066.8346\n",
      "PP(train) = 1939.481, PP(valid) = 2093.988\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1755e-22, 1.1135e-24, 2.7495e-09, 1.2899e-23, 3.6905e-20, 2.9131e-30,\n",
      "         4.8251e-20, 1.0000e+00, 8.3163e-16, 2.4898e-28, 3.9949e-06, 2.7761e-19,\n",
      "         1.5574e-08, 8.0394e-30, 1.3109e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1003.3434\n",
      "Test epoch : 41 Average loss: 1066.7848\n",
      "PP(train) = 1937.010, PP(valid) = 2093.296\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1488e-21, 2.6432e-20, 1.8565e-18, 3.5853e-07, 1.2614e-10, 6.2296e-21,\n",
      "         7.9031e-26, 3.8178e-08, 1.0537e-04, 6.3339e-11, 2.0204e-21, 2.7649e-17,\n",
      "         1.2613e-12, 5.5165e-18, 9.9989e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1003.1295\n",
      "Test epoch : 42 Average loss: 1066.7347\n",
      "PP(train) = 1934.593, PP(valid) = 2092.638\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3308e-25, 1.1184e-24, 1.7536e-12, 1.6498e-12, 2.0046e-19, 4.4039e-27,\n",
      "         5.2635e-22, 5.7356e-11, 1.0949e-10, 1.5218e-23, 6.0361e-27, 2.8877e-16,\n",
      "         3.2939e-09, 3.4324e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1002.9382\n",
      "Test epoch : 43 Average loss: 1066.6828\n",
      "PP(train) = 1932.086, PP(valid) = 2091.863\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.6550e-24, 4.1863e-21, 1.8619e-04, 3.5037e-16, 4.0495e-10, 4.5509e-25,\n",
      "         1.9470e-16, 8.6833e-01, 1.0315e-01, 4.1106e-12, 3.0375e-20, 2.8437e-18,\n",
      "         3.3993e-06, 1.5580e-22, 2.8330e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0995, 0.0548, 0.0466, 0.0719, 0.0906, 0.0769, 0.0487, 0.0542,\n",
      "         0.0403, 0.1143, 0.0465, 0.0386, 0.0976, 0.0386]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1002.7336\n",
      "Test epoch : 44 Average loss: 1066.6359\n",
      "PP(train) = 1929.639, PP(valid) = 2091.160\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9730e-17, 4.4668e-18, 1.5209e-04, 9.3505e-05, 1.0570e-05, 4.0118e-22,\n",
      "         2.6968e-16, 2.4109e-07, 3.7685e-01, 6.4862e-23, 6.3453e-27, 2.3338e-22,\n",
      "         6.2289e-01, 8.6340e-18, 1.8213e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0520, 0.0730, 0.0632, 0.0626, 0.0375, 0.0700, 0.0582, 0.0851, 0.0687,\n",
      "         0.0853, 0.0607, 0.0583, 0.0566, 0.0858, 0.0830]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1002.5828\n",
      "Test epoch : 45 Average loss: 1066.5896\n",
      "PP(train) = 1927.203, PP(valid) = 2090.461\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8020e-31, 5.7043e-21, 4.0706e-09, 6.2571e-07, 9.9535e-01, 1.3812e-24,\n",
      "         3.6292e-25, 6.5873e-07, 4.6486e-03, 6.2087e-30, 1.7060e-21, 4.5095e-21,\n",
      "         1.5589e-10, 1.7758e-19, 3.7007e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0422, 0.0382, 0.0955, 0.0658, 0.1064, 0.1084, 0.0451,\n",
      "         0.0939, 0.0673, 0.0601, 0.0456, 0.0623, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1002.4203\n",
      "Test epoch : 46 Average loss: 1066.5435\n",
      "PP(train) = 1924.812, PP(valid) = 2089.807\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5635e-36, 9.5174e-19, 2.1432e-10, 1.3099e-23, 3.9928e-16, 1.0049e-19,\n",
      "         4.7059e-25, 9.9966e-01, 2.8477e-12, 1.4029e-17, 9.3379e-24, 1.0904e-19,\n",
      "         8.4677e-23, 4.0280e-20, 3.3864e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0544, 0.0442, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1002.2332\n",
      "Test epoch : 47 Average loss: 1066.4984\n",
      "PP(train) = 1922.410, PP(valid) = 2089.120\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4148e-23, 9.4187e-14, 1.9384e-07, 3.6002e-03, 6.1401e-02, 1.1161e-17,\n",
      "         3.0180e-24, 4.5273e-09, 1.3190e-07, 2.3370e-18, 1.7362e-17, 2.4703e-17,\n",
      "         5.9245e-11, 1.8448e-16, 9.3500e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0790, 0.0448, 0.0990, 0.0497, 0.0621, 0.0671, 0.1078, 0.0935, 0.0587,\n",
      "         0.0547, 0.0627, 0.0561, 0.0509, 0.0705, 0.0434]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1002.0714\n",
      "Test epoch : 48 Average loss: 1066.4531\n",
      "PP(train) = 1919.993, PP(valid) = 2088.401\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2380e-23, 5.3031e-23, 2.6513e-04, 7.7285e-12, 9.9968e-01, 5.1488e-28,\n",
      "         3.0752e-19, 5.9227e-05, 1.1943e-21, 1.6150e-13, 1.7985e-21, 5.2663e-17,\n",
      "         4.3595e-12, 8.3406e-15, 3.7263e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1002.0056\n",
      "Test epoch : 49 Average loss: 1066.4095\n",
      "PP(train) = 1917.638, PP(valid) = 2087.717\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1472e-27, 4.9535e-23, 3.6496e-14, 2.5925e-15, 2.4342e-17, 3.2578e-30,\n",
      "         9.9976e-39, 8.2561e-01, 1.5754e-01, 1.6820e-16, 1.4965e-31, 7.6242e-27,\n",
      "         1.6713e-02, 3.0090e-25, 1.2997e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0788, 0.0992, 0.0537, 0.0477, 0.0683, 0.0903, 0.0751, 0.0503, 0.0548,\n",
      "         0.0421, 0.1130, 0.0475, 0.0394, 0.0999, 0.0400]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1001.7179\n",
      "Test epoch : 50 Average loss: 1066.3645\n",
      "PP(train) = 1915.512, PP(valid) = 2087.222\n",
      "Writing to ./topicwords/17-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 7, p : 7.6296 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                         B                                                                   A                      \n",
      "\n",
      "===== # 2, Topic : 4, p : 7.7482 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                       \n",
      "\n",
      "===== # 3, Topic : 8, p : 7.9131 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                 \n",
      "\n",
      "===== # 4, Topic : 8, p : 8.2634 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                    \n",
      "\n",
      "===== # 5, Topic : 14, p : 8.2007 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                              \n",
      "\n",
      "===== # 6, Topic : 14, p : 8.3155 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                         \n",
      "\n",
      "===== # 7, Topic : 7, p : 7.3986 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                     ]                                ^              ^                           t)]                n m            ^        ^      t                                   \n",
      "\n",
      "===== # 8, Topic : 14, p : 7.4700 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                 U                                                                                                   LNG LPG                           m         m                                                    \n",
      "\n",
      "===== # 9, Topic : 8, p : 7.8664 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                       \n",
      "\n",
      "===== # 10, Topic : 14, p : 8.7436 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :              [      ]                  []      [          [                                                                                                                                               \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.7194e-18, 1.6532e-14, 2.3813e-02, 5.2701e-07, 4.6729e-02, 2.7615e-11,\n",
      "         1.7323e-18, 1.7184e-05, 9.2733e-01, 2.0002e-03, 7.2094e-21, 6.0824e-19,\n",
      "         4.8655e-05, 6.2580e-15, 6.5567e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0563, 0.0672, 0.0444, 0.0620, 0.0331, 0.0751, 0.0652, 0.0927, 0.0574,\n",
      "         0.0704, 0.0835, 0.0648, 0.0511, 0.1263, 0.0507]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 983.0473\n",
      "Test epoch : 1 Average loss: 1029.9408\n",
      "PP(train) = 1996.306, PP(valid) = 2115.631\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8495e-27, 9.8535e-23, 1.4303e-10, 2.2716e-21, 7.4775e-13, 4.2126e-25,\n",
      "         1.5786e-26, 1.1001e-15, 5.7383e-10, 4.3923e-26, 9.8364e-27, 9.3901e-13,\n",
      "         7.3474e-30, 2.0435e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 982.8907\n",
      "Test epoch : 2 Average loss: 1029.8897\n",
      "PP(train) = 1993.966, PP(valid) = 2114.767\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8856e-17, 5.8597e-12, 3.7620e-05, 1.5439e-13, 2.6604e-04, 1.0429e-13,\n",
      "         8.2080e-12, 9.9630e-01, 3.6632e-05, 2.6436e-15, 1.7444e-11, 9.1376e-08,\n",
      "         3.2689e-03, 3.6975e-16, 8.6367e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1055, 0.0544, 0.0443, 0.0790, 0.0922, 0.0768, 0.0437, 0.0532,\n",
      "         0.0371, 0.1190, 0.0439, 0.0366, 0.0939, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 982.8164\n",
      "Test epoch : 3 Average loss: 1029.8369\n",
      "PP(train) = 1991.258, PP(valid) = 2113.960\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9619e-21, 5.0494e-24, 2.4979e-07, 1.0392e-06, 3.3814e-04, 3.4153e-18,\n",
      "         4.1609e-18, 9.9932e-01, 3.3598e-04, 5.3418e-23, 3.8369e-20, 7.4522e-14,\n",
      "         1.0333e-07, 7.0764e-18, 3.8468e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0443, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 982.5684\n",
      "Test epoch : 4 Average loss: 1029.7832\n",
      "PP(train) = 1988.237, PP(valid) = 2113.090\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0772e-23, 4.1663e-18, 6.3502e-02, 1.2441e-06, 4.2855e-08, 1.3914e-12,\n",
      "         2.8148e-12, 1.6938e-09, 9.3000e-01, 1.8509e-06, 5.1450e-15, 4.5336e-06,\n",
      "         2.5361e-06, 2.0123e-10, 6.4830e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0576, 0.0687, 0.0447, 0.0636, 0.0327, 0.0741, 0.0641, 0.0912, 0.0591,\n",
      "         0.0687, 0.0832, 0.0641, 0.0513, 0.1280, 0.0488]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 982.3953\n",
      "Test epoch : 5 Average loss: 1029.7309\n",
      "PP(train) = 1985.111, PP(valid) = 2112.237\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0303e-26, 1.4922e-20, 1.2997e-05, 5.0566e-07, 6.5244e-04, 5.3032e-22,\n",
      "         2.7278e-15, 4.8943e-06, 9.9170e-01, 1.4387e-25, 5.5809e-18, 1.9829e-08,\n",
      "         9.5323e-07, 2.9982e-04, 7.3283e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0563, 0.0672, 0.0446, 0.0628, 0.0306, 0.0760, 0.0632, 0.0920, 0.0570,\n",
      "         0.0692, 0.0845, 0.0651, 0.0511, 0.1308, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 982.1634\n",
      "Test epoch : 6 Average loss: 1029.6765\n",
      "PP(train) = 1981.992, PP(valid) = 2111.398\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0395e-13, 2.1435e-16, 6.2677e-06, 5.2524e-19, 9.7517e-01, 2.1299e-11,\n",
      "         2.3728e-15, 6.0833e-11, 1.1482e-02, 2.9388e-14, 1.3331e-02, 2.6741e-10,\n",
      "         4.6032e-08, 1.3881e-21, 6.5336e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0454, 0.0489, 0.0423, 0.0384, 0.0945, 0.0659, 0.1048, 0.1080, 0.0454,\n",
      "         0.0928, 0.0680, 0.0601, 0.0455, 0.0632, 0.0768]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 982.0489\n",
      "Test epoch : 7 Average loss: 1029.6229\n",
      "PP(train) = 1978.872, PP(valid) = 2110.562\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1055e-12, 1.6448e-19, 3.4330e-15, 1.1813e-06, 2.0978e-05, 6.6130e-41,\n",
      "         2.2897e-27, 9.9998e-01, 4.7375e-10, 8.1158e-20, 1.8756e-10, 3.8684e-26,\n",
      "         1.5244e-08, 4.3194e-08, 3.4020e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 981.8207\n",
      "Test epoch : 8 Average loss: 1029.5728\n",
      "PP(train) = 1975.750, PP(valid) = 2109.759\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7396e-23, 5.5069e-10, 5.5998e-16, 5.6770e-11, 9.9915e-01, 1.1731e-14,\n",
      "         4.8373e-19, 5.7391e-04, 3.9997e-06, 8.6987e-11, 1.1380e-17, 4.4453e-11,\n",
      "         3.6960e-11, 2.6895e-04, 2.5268e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 981.6373\n",
      "Test epoch : 9 Average loss: 1029.5193\n",
      "PP(train) = 1972.620, PP(valid) = 2108.872\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6052e-10, 1.0332e-17, 1.4841e-10, 4.6250e-12, 2.8306e-09, 5.1446e-22,\n",
      "         2.5305e-15, 1.0000e+00, 4.9447e-12, 5.0769e-07, 1.1835e-17, 5.9171e-21,\n",
      "         1.9055e-08, 1.1607e-11, 1.2207e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 981.4216\n",
      "Test epoch : 10 Average loss: 1029.4696\n",
      "PP(train) = 1969.602, PP(valid) = 2108.129\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9802e-20, 1.0806e-06, 1.0000e+00, 2.9178e-10, 6.5007e-21, 1.0418e-16,\n",
      "         1.0815e-13, 5.1718e-12, 9.3212e-10, 2.3421e-12, 5.8596e-21, 8.1681e-20,\n",
      "         3.3651e-18, 4.3947e-25, 1.9028e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 981.2453\n",
      "Test epoch : 11 Average loss: 1029.4208\n",
      "PP(train) = 1966.640, PP(valid) = 2107.395\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2536e-12, 5.7173e-10, 1.5024e-06, 2.0781e-14, 9.9424e-01, 2.8392e-12,\n",
      "         3.0382e-16, 9.6681e-07, 6.3208e-07, 1.2236e-13, 3.1162e-13, 7.7951e-14,\n",
      "         1.6338e-03, 1.0614e-15, 4.1255e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0449, 0.0479, 0.0424, 0.0382, 0.0957, 0.0657, 0.1065, 0.1083, 0.0451,\n",
      "         0.0938, 0.0671, 0.0601, 0.0456, 0.0621, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 980.9753\n",
      "Test epoch : 12 Average loss: 1029.3743\n",
      "PP(train) = 1963.628, PP(valid) = 2106.630\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.1098e-40, 2.0568e-20, 4.8385e-07, 1.6522e-18, 9.0315e-11, 6.0183e-26,\n",
      "         4.1267e-25, 1.0000e+00, 8.6866e-09, 1.0046e-12, 6.0845e-23, 4.8014e-16,\n",
      "         3.5923e-10, 1.7483e-24, 2.3068e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 980.7618\n",
      "Test epoch : 13 Average loss: 1029.3246\n",
      "PP(train) = 1960.780, PP(valid) = 2105.934\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8351e-28, 1.3267e-14, 1.4582e-07, 1.0159e-16, 2.3117e-11, 3.6452e-17,\n",
      "         1.0317e-12, 1.5334e-03, 2.2402e-04, 1.3734e-20, 7.8916e-19, 2.5848e-20,\n",
      "         2.0543e-11, 1.1398e-11, 9.9824e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1042, 0.0502, 0.0601, 0.0670, 0.1075, 0.0922, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 980.5680\n",
      "Test epoch : 14 Average loss: 1029.2766\n",
      "PP(train) = 1957.884, PP(valid) = 2105.195\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0663e-13, 2.0891e-09, 3.1547e-15, 2.5367e-12, 9.7593e-01, 8.4825e-14,\n",
      "         5.1136e-12, 1.6596e-04, 4.5348e-13, 6.4436e-10, 1.5815e-16, 3.1233e-10,\n",
      "         4.9385e-14, 2.9819e-04, 2.3608e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0455, 0.0479, 0.0431, 0.0384, 0.0951, 0.0658, 0.1068, 0.1081, 0.0454,\n",
      "         0.0928, 0.0672, 0.0601, 0.0458, 0.0624, 0.0757]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 980.4220\n",
      "Test epoch : 15 Average loss: 1029.2315\n",
      "PP(train) = 1954.917, PP(valid) = 2104.387\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1701e-23, 3.0820e-22, 4.8355e-08, 1.2238e-24, 4.4782e-11, 1.5830e-18,\n",
      "         1.2515e-27, 1.1487e-04, 8.5544e-19, 7.6228e-19, 3.1194e-21, 2.7604e-16,\n",
      "         5.5281e-12, 1.0536e-27, 9.9989e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 980.2160\n",
      "Test epoch : 16 Average loss: 1029.1850\n",
      "PP(train) = 1952.195, PP(valid) = 2103.729\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8581e-18, 1.7365e-22, 3.4581e-03, 2.8714e-18, 4.5247e-16, 5.6168e-25,\n",
      "         2.5192e-22, 1.1142e-12, 1.8991e-19, 8.4044e-24, 2.5474e-21, 1.1530e-17,\n",
      "         2.4418e-12, 1.2388e-24, 9.9654e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0445, 0.1041, 0.0503, 0.0602, 0.0668, 0.1074, 0.0923, 0.0594,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 980.0379\n",
      "Test epoch : 17 Average loss: 1029.1413\n",
      "PP(train) = 1949.412, PP(valid) = 2103.070\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.4156e-18, 6.7516e-22, 2.5641e-08, 1.6716e-03, 7.5849e-16, 2.4596e-12,\n",
      "         8.3665e-16, 2.3338e-02, 1.0388e-08, 4.7300e-11, 2.1400e-21, 9.7499e-01,\n",
      "         2.5655e-11, 6.4132e-15, 1.2699e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0621, 0.0630, 0.0811, 0.0550, 0.0553, 0.1195, 0.0465, 0.0776, 0.0972,\n",
      "         0.0678, 0.0453, 0.0659, 0.0589, 0.0671, 0.0378]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 979.8650\n",
      "Test epoch : 18 Average loss: 1029.0990\n",
      "PP(train) = 1946.521, PP(valid) = 2102.315\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8216e-26, 1.0819e-21, 3.2248e-15, 9.9071e-01, 8.0544e-17, 1.5815e-33,\n",
      "         6.4432e-25, 5.7927e-11, 1.3062e-11, 3.7965e-19, 1.8648e-19, 2.6908e-25,\n",
      "         9.2945e-03, 4.4168e-22, 7.8010e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0420, 0.0552, 0.0916, 0.0502, 0.0528, 0.0524, 0.0534, 0.0687,\n",
      "         0.1348, 0.1023, 0.0485, 0.0563, 0.0522, 0.0909]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 979.6654\n",
      "Test epoch : 19 Average loss: 1029.0565\n",
      "PP(train) = 1943.871, PP(valid) = 2101.721\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4503e-29, 7.0274e-11, 9.9974e-01, 1.4084e-15, 8.0625e-20, 1.5376e-21,\n",
      "         5.1437e-11, 5.0487e-10, 8.8063e-11, 9.9834e-15, 1.6320e-17, 2.4156e-20,\n",
      "         2.6232e-04, 9.5609e-22, 1.2002e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 979.5213\n",
      "Test epoch : 20 Average loss: 1029.0149\n",
      "PP(train) = 1941.193, PP(valid) = 2101.090\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7180e-31, 1.1415e-22, 9.5656e-18, 1.0188e-24, 9.9975e-01, 1.0521e-19,\n",
      "         1.7890e-22, 2.3559e-15, 2.3217e-16, 1.2221e-23, 1.3428e-33, 1.2275e-21,\n",
      "         2.3739e-08, 1.5276e-19, 2.4623e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 979.2903\n",
      "Test epoch : 21 Average loss: 1028.9744\n",
      "PP(train) = 1938.451, PP(valid) = 2100.419\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5220e-23, 1.5586e-16, 9.9972e-01, 4.3319e-10, 2.4048e-04, 4.5866e-30,\n",
      "         3.9715e-24, 5.2062e-07, 6.8110e-08, 2.2180e-23, 1.6933e-17, 1.1988e-20,\n",
      "         3.5897e-05, 1.4047e-11, 7.8537e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0898, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 979.1129\n",
      "Test epoch : 22 Average loss: 1028.9335\n",
      "PP(train) = 1935.880, PP(valid) = 2099.838\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5437e-18, 5.1977e-13, 3.3973e-08, 3.1351e-18, 3.1922e-07, 1.5357e-15,\n",
      "         3.7496e-19, 5.0801e-15, 1.2017e-20, 9.5212e-12, 9.8451e-22, 1.2911e-17,\n",
      "         1.4114e-12, 4.5660e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 978.9666\n",
      "Test epoch : 23 Average loss: 1028.8944\n",
      "PP(train) = 1933.210, PP(valid) = 2099.197\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.6317e-24, 1.0845e-20, 1.1939e-21, 4.2937e-16, 3.0650e-17, 8.7050e-24,\n",
      "         7.8713e-28, 4.8727e-29, 1.8314e-16, 6.2759e-21, 3.4036e-24, 1.6843e-15,\n",
      "         7.6138e-12, 9.4998e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 978.8054\n",
      "Test epoch : 24 Average loss: 1028.8551\n",
      "PP(train) = 1930.577, PP(valid) = 2098.557\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7724e-38, 3.2948e-20, 4.8173e-11, 1.1367e-30, 6.6807e-22, 3.9295e-35,\n",
      "         1.1132e-38, 9.0454e-24, 1.7882e-17, 6.7588e-28, 7.1839e-28, 2.3043e-26,\n",
      "         6.5552e-15, 7.3309e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 978.5804\n",
      "Test epoch : 25 Average loss: 1028.8185\n",
      "PP(train) = 1928.100, PP(valid) = 2098.063\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0820e-14, 8.2791e-16, 9.9803e-01, 3.3879e-21, 2.2536e-07, 5.0363e-17,\n",
      "         9.6248e-22, 5.3010e-08, 4.9979e-09, 1.9720e-03, 1.3235e-13, 2.9439e-16,\n",
      "         1.1037e-15, 9.9060e-11, 7.4550e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0759, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0948,\n",
      "         0.0582, 0.0622, 0.0478, 0.0511, 0.0865, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 978.4557\n",
      "Test epoch : 26 Average loss: 1028.7826\n",
      "PP(train) = 1925.567, PP(valid) = 2097.517\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.2402e-22, 3.8770e-23, 7.1770e-01, 1.6359e-07, 2.8207e-01, 9.1678e-35,\n",
      "         1.3955e-21, 7.5848e-05, 4.5778e-13, 5.9750e-09, 2.1493e-17, 2.4466e-20,\n",
      "         1.0160e-05, 8.6279e-16, 1.4389e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0667, 0.0767, 0.0445, 0.0611, 0.0889, 0.0538, 0.0843, 0.0851, 0.0784,\n",
      "         0.0679, 0.0649, 0.0519, 0.0505, 0.0803, 0.0448]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 978.2020\n",
      "Test epoch : 27 Average loss: 1028.7486\n",
      "PP(train) = 1922.946, PP(valid) = 2096.912\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.8524e-31, 1.3192e-15, 3.9918e-08, 5.7046e-16, 9.9227e-01, 3.8892e-21,\n",
      "         1.9043e-29, 9.8639e-05, 7.6312e-03, 5.6086e-18, 9.5875e-29, 1.3587e-20,\n",
      "         3.0214e-14, 1.9218e-12, 1.1193e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0422, 0.0383, 0.0952, 0.0658, 0.1063, 0.1083, 0.0451,\n",
      "         0.0938, 0.0673, 0.0602, 0.0456, 0.0625, 0.0765]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 978.1097\n",
      "Test epoch : 28 Average loss: 1028.7155\n",
      "PP(train) = 1920.598, PP(valid) = 2096.498\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0152e-20, 4.6189e-15, 2.0830e-11, 5.7799e-13, 2.0228e-15, 2.8922e-22,\n",
      "         2.4054e-16, 2.9126e-02, 1.5640e-23, 8.2145e-20, 2.9008e-30, 7.7321e-05,\n",
      "         1.3506e-09, 5.0882e-19, 9.7080e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0820, 0.0456, 0.1027, 0.0502, 0.0607, 0.0677, 0.1068, 0.0906, 0.0593,\n",
      "         0.0519, 0.0633, 0.0554, 0.0507, 0.0716, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 977.9385\n",
      "Test epoch : 29 Average loss: 1028.6785\n",
      "PP(train) = 1918.052, PP(valid) = 2095.857\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.9342e-15, 2.1203e-23, 2.2001e-13, 1.5509e-12, 4.2759e-13, 2.8639e-27,\n",
      "         1.0579e-12, 9.9999e-01, 7.3137e-13, 1.1899e-05, 1.1233e-17, 1.5304e-18,\n",
      "         4.7588e-09, 1.6836e-11, 4.3291e-20]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 977.7711\n",
      "Test epoch : 30 Average loss: 1028.6444\n",
      "PP(train) = 1915.525, PP(valid) = 2095.249\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8140e-24, 2.4239e-08, 2.2678e-08, 1.1237e-09, 9.9977e-01, 4.5960e-13,\n",
      "         3.8027e-23, 1.6342e-05, 4.5307e-08, 9.4523e-07, 2.5659e-11, 1.2618e-04,\n",
      "         7.7982e-19, 1.2935e-15, 8.2464e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 977.6078\n",
      "Test epoch : 31 Average loss: 1028.6116\n",
      "PP(train) = 1913.234, PP(valid) = 2094.842\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8136e-23, 1.1050e-17, 2.4092e-05, 3.2175e-20, 4.6197e-10, 6.3883e-27,\n",
      "         1.4627e-23, 3.4502e-12, 1.8533e-08, 1.6862e-16, 2.0909e-15, 8.4662e-21,\n",
      "         3.0263e-09, 8.7790e-22, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 977.3965\n",
      "Test epoch : 32 Average loss: 1028.5775\n",
      "PP(train) = 1910.922, PP(valid) = 2094.365\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2785e-21, 2.9544e-10, 2.5559e-10, 2.2821e-24, 9.9999e-01, 2.1847e-22,\n",
      "         3.5136e-26, 2.6807e-08, 3.6105e-14, 3.4116e-19, 7.2646e-24, 6.2414e-21,\n",
      "         1.2642e-05, 4.4868e-10, 2.1542e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 977.2827\n",
      "Test epoch : 33 Average loss: 1028.5450\n",
      "PP(train) = 1908.427, PP(valid) = 2093.777\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4791e-18, 3.5361e-24, 8.2620e-08, 1.0311e-25, 1.6691e-09, 2.3682e-20,\n",
      "         1.1016e-23, 9.8182e-01, 1.8545e-06, 1.2727e-17, 8.4471e-16, 1.8267e-19,\n",
      "         3.5696e-13, 2.6181e-13, 1.8179e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1041, 0.0551, 0.0444, 0.0789, 0.0919, 0.0775, 0.0443, 0.0533,\n",
      "         0.0373, 0.1182, 0.0441, 0.0368, 0.0936, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 977.0862\n",
      "Test epoch : 34 Average loss: 1028.5152\n",
      "PP(train) = 1906.112, PP(valid) = 2093.343\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.4749e-11, 1.0984e-12, 3.2755e-07, 1.1566e-11, 1.4132e-14, 1.0796e-21,\n",
      "         2.0459e-20, 3.2738e-09, 1.1067e-07, 1.3746e-15, 2.9541e-22, 1.7185e-14,\n",
      "         6.1004e-09, 9.4797e-01, 5.2025e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0410, 0.0345, 0.0743, 0.0912, 0.0749, 0.0877, 0.0495, 0.0622, 0.1284,\n",
      "         0.0524, 0.0656, 0.0495, 0.0606, 0.0905, 0.0377]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 976.9230\n",
      "Test epoch : 35 Average loss: 1028.4840\n",
      "PP(train) = 1903.805, PP(valid) = 2092.851\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2356e-17, 4.9039e-19, 4.6991e-12, 1.5172e-13, 1.4123e-03, 4.6834e-18,\n",
      "         1.5410e-16, 3.9128e-13, 1.7207e-07, 8.3753e-15, 1.1455e-26, 9.1741e-18,\n",
      "         2.9319e-07, 9.9859e-01, 2.5528e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0393, 0.0339, 0.0725, 0.0936, 0.0754, 0.0885, 0.0472, 0.0606, 0.1330,\n",
      "         0.0522, 0.0655, 0.0489, 0.0609, 0.0912, 0.0374]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 976.8188\n",
      "Test epoch : 36 Average loss: 1028.4522\n",
      "PP(train) = 1901.463, PP(valid) = 2092.328\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6454e-08, 3.6829e-10, 5.4163e-03, 2.5913e-14, 2.6913e-04, 6.9589e-17,\n",
      "         1.8542e-19, 9.6316e-01, 2.5452e-02, 1.5066e-14, 1.1472e-04, 2.8379e-20,\n",
      "         9.4057e-07, 1.9014e-07, 5.5864e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0828, 0.1041, 0.0544, 0.0449, 0.0774, 0.0916, 0.0769, 0.0449, 0.0535,\n",
      "         0.0379, 0.1178, 0.0445, 0.0371, 0.0948, 0.0373]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 976.6705\n",
      "Test epoch : 37 Average loss: 1028.4248\n",
      "PP(train) = 1899.231, PP(valid) = 2091.936\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1004e-15, 4.6540e-14, 8.5137e-09, 1.1685e-12, 4.3550e-13, 1.3566e-20,\n",
      "         4.4468e-15, 1.0000e+00, 8.2859e-14, 2.3821e-11, 1.3371e-20, 6.5620e-20,\n",
      "         3.1649e-11, 2.7630e-28, 2.9506e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 976.5078\n",
      "Test epoch : 38 Average loss: 1028.3954\n",
      "PP(train) = 1896.959, PP(valid) = 2091.483\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.6616e-22, 2.7588e-31, 1.4861e-07, 1.2348e-03, 4.1062e-06, 6.2301e-16,\n",
      "         8.5450e-16, 2.6889e-03, 8.4360e-06, 2.7575e-15, 2.3309e-10, 5.3421e-17,\n",
      "         2.1074e-08, 1.4225e-13, 9.9606e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0445, 0.1041, 0.0502, 0.0601, 0.0670, 0.1074, 0.0921, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0709, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 976.3214\n",
      "Test epoch : 39 Average loss: 1028.3678\n",
      "PP(train) = 1894.706, PP(valid) = 2091.047\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5742e-28, 9.1749e-14, 2.9657e-04, 1.2807e-03, 6.7331e-01, 1.8041e-17,\n",
      "         3.0941e-13, 1.4059e-06, 4.0226e-09, 9.7024e-12, 4.5040e-05, 4.3078e-16,\n",
      "         2.6266e-05, 3.2504e-01, 2.4662e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0445, 0.0443, 0.0521, 0.0530, 0.0918, 0.0750, 0.0847, 0.0928, 0.0664,\n",
      "         0.0804, 0.0691, 0.0582, 0.0519, 0.0729, 0.0629]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 976.1795\n",
      "Test epoch : 40 Average loss: 1028.3402\n",
      "PP(train) = 1892.540, PP(valid) = 2090.637\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6005e-14, 4.3547e-05, 3.7142e-01, 2.7104e-10, 5.0823e-01, 2.9624e-09,\n",
      "         2.2220e-20, 6.1539e-05, 8.8930e-02, 4.1197e-13, 2.7249e-11, 3.3427e-07,\n",
      "         1.4730e-07, 1.9096e-15, 3.1314e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0582, 0.0640, 0.0456, 0.0522, 0.0835, 0.0611, 0.0917, 0.0954, 0.0629,\n",
      "         0.0773, 0.0684, 0.0570, 0.0496, 0.0775, 0.0558]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 976.0152\n",
      "Test epoch : 41 Average loss: 1028.3168\n",
      "PP(train) = 1890.341, PP(valid) = 2090.271\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4328e-26, 3.5541e-18, 9.9997e-01, 1.6056e-16, 1.5037e-21, 1.1254e-22,\n",
      "         2.3200e-16, 1.4461e-16, 3.8413e-13, 1.3735e-25, 9.0805e-18, 2.0072e-24,\n",
      "         3.1683e-05, 1.9023e-12, 4.1117e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 975.8746\n",
      "Test epoch : 42 Average loss: 1028.2889\n",
      "PP(train) = 1888.079, PP(valid) = 2089.778\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6093e-18, 2.5763e-16, 1.1964e-16, 6.0573e-12, 9.0694e-01, 9.8763e-24,\n",
      "         5.5981e-22, 2.7463e-07, 2.7203e-12, 2.4931e-11, 3.8974e-12, 4.7028e-09,\n",
      "         1.4857e-14, 3.2663e-07, 9.3058e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0476, 0.0478, 0.0461, 0.0393, 0.0924, 0.0662, 0.1074, 0.1075, 0.0465,\n",
      "         0.0896, 0.0671, 0.0600, 0.0463, 0.0632, 0.0729]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 975.7587\n",
      "Test epoch : 43 Average loss: 1028.2595\n",
      "PP(train) = 1885.995, PP(valid) = 2089.368\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3495e-13, 8.1351e-19, 1.0000e+00, 5.7017e-22, 7.5243e-11, 1.3041e-23,\n",
      "         4.7788e-20, 9.3308e-14, 3.2982e-16, 5.7710e-23, 2.6491e-18, 1.4869e-15,\n",
      "         6.4808e-14, 4.6814e-18, 1.0114e-19]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 975.6557\n",
      "Test epoch : 44 Average loss: 1028.2361\n",
      "PP(train) = 1883.911, PP(valid) = 2089.029\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.8064e-18, 1.1648e-14, 3.0254e-17, 6.1031e-13, 2.4704e-02, 6.4546e-17,\n",
      "         3.0622e-14, 4.2523e-11, 5.8603e-11, 1.4491e-24, 6.0953e-16, 4.1255e-14,\n",
      "         1.3210e-18, 1.3712e-16, 9.7530e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0807, 0.0445, 0.1022, 0.0500, 0.0609, 0.0670, 0.1077, 0.0929, 0.0590,\n",
      "         0.0532, 0.0622, 0.0558, 0.0510, 0.0707, 0.0421]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 975.4673\n",
      "Test epoch : 45 Average loss: 1028.2122\n",
      "PP(train) = 1881.696, PP(valid) = 2088.588\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.9407e-15, 2.8666e-05, 2.4161e-07, 5.7255e-09, 2.7509e-03, 1.6798e-13,\n",
      "         3.6549e-13, 2.4290e-05, 9.8836e-01, 5.0262e-07, 8.1025e-11, 1.9196e-20,\n",
      "         5.2701e-03, 3.7259e-10, 3.5685e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0446, 0.0628, 0.0306, 0.0759, 0.0631, 0.0920, 0.0571,\n",
      "         0.0694, 0.0843, 0.0650, 0.0511, 0.1305, 0.0500]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 975.3050\n",
      "Test epoch : 46 Average loss: 1028.1862\n",
      "PP(train) = 1879.623, PP(valid) = 2088.222\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5665e-21, 1.8680e-02, 3.7616e-08, 4.9015e-11, 9.8093e-01, 9.4376e-25,\n",
      "         6.2084e-11, 6.2197e-09, 1.6210e-04, 1.7934e-14, 1.0384e-16, 3.6834e-11,\n",
      "         2.1983e-04, 2.8871e-06, 9.3169e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0456, 0.0480, 0.0422, 0.0389, 0.0948, 0.0655, 0.1062, 0.1067, 0.0456,\n",
      "         0.0939, 0.0666, 0.0600, 0.0459, 0.0630, 0.0770]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 975.1030\n",
      "Test epoch : 47 Average loss: 1028.1629\n",
      "PP(train) = 1877.612, PP(valid) = 2087.923\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5625e-16, 2.9851e-08, 1.2355e-02, 4.5765e-02, 2.1578e-10, 8.0665e-11,\n",
      "         9.2600e-07, 3.1836e-01, 6.2351e-01, 4.4712e-12, 9.7641e-13, 8.2499e-10,\n",
      "         4.0361e-07, 1.8394e-22, 5.1560e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0651, 0.0783, 0.0490, 0.0588, 0.0438, 0.0811, 0.0684, 0.0723, 0.0580,\n",
      "         0.0599, 0.0973, 0.0579, 0.0473, 0.1155, 0.0475]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 975.0404\n",
      "Test epoch : 48 Average loss: 1028.1424\n",
      "PP(train) = 1875.509, PP(valid) = 2087.583\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9453e-01, 3.3654e-06, 5.3896e-06, 1.6821e-01, 1.4855e-03, 1.0140e-07,\n",
      "         1.8306e-16, 6.6984e-05, 2.5927e-10, 1.0552e-08, 4.8353e-16, 2.1431e-15,\n",
      "         1.2986e-05, 2.1360e-07, 5.3568e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0806, 0.0513, 0.0674, 0.0716, 0.0554, 0.0597, 0.0729, 0.0906, 0.0660,\n",
      "         0.0719, 0.0751, 0.0694, 0.0590, 0.0625, 0.0465]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 974.8729\n",
      "Test epoch : 49 Average loss: 1028.1202\n",
      "PP(train) = 1873.385, PP(valid) = 2087.188\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3642e-20, 4.7051e-10, 4.2316e-26, 3.7470e-15, 9.7358e-01, 3.9791e-08,\n",
      "         1.1962e-19, 1.0066e-09, 2.6418e-02, 3.3202e-08, 3.2260e-19, 4.3761e-15,\n",
      "         6.7124e-22, 2.5879e-21, 2.3883e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0484, 0.0423, 0.0387, 0.0933, 0.0661, 0.1054, 0.1082, 0.0454,\n",
      "         0.0935, 0.0678, 0.0604, 0.0458, 0.0635, 0.0760]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 974.7796\n",
      "Test epoch : 50 Average loss: 1028.0988\n",
      "PP(train) = 1871.419, PP(valid) = 2086.878\n",
      "Writing to ./topicwords/18-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 4, p : 7.8326 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                    \n",
      "\n",
      "===== # 2, Topic : 4, p : 7.3828 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                          \n",
      "\n",
      "===== # 3, Topic : 14, p : 7.8174 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                            -                                 \n",
      "\n",
      "===== # 4, Topic : 4, p : 7.6290 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                        \n",
      "\n",
      "===== # 5, Topic : 2, p : 7.3761 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                \n",
      "\n",
      "===== # 6, Topic : 14, p : 8.3711 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                b                   a                 b)(c                 (c                           a)(b    c                                                                \n",
      "\n",
      "===== # 7, Topic : 14, p : 7.5928 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                 \n",
      "\n",
      "===== # 8, Topic : 14, p : 7.7380 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                      \n",
      "\n",
      "===== # 9, Topic : 0, p : 7.6508 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                  II                                              III                                     I                                                                                                                                                                                                                        \n",
      "\n",
      "===== # 10, Topic : 5, p : 7.2264 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                  III IV                                      II                                                                                                                               III           II                                                                                                   \n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3823e-25, 4.0998e-24, 5.3646e-16, 1.7430e-15, 3.1048e-06, 5.6338e-19,\n",
      "         8.5602e-23, 9.7677e-01, 2.3221e-02, 2.6836e-19, 3.5886e-21, 2.0545e-19,\n",
      "         1.8556e-10, 7.0327e-26, 3.6096e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0828, 0.1047, 0.0542, 0.0447, 0.0775, 0.0920, 0.0767, 0.0445, 0.0533,\n",
      "         0.0376, 0.1187, 0.0443, 0.0369, 0.0949, 0.0372]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1005.3677\n",
      "Test epoch : 1 Average loss: 964.8451\n",
      "PP(train) = 2020.220, PP(valid) = 2089.818\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7995e-21, 6.4295e-25, 3.9868e-11, 3.3745e-22, 1.0000e+00, 6.0259e-22,\n",
      "         5.3533e-30, 2.2707e-08, 2.4385e-07, 4.8414e-12, 5.7644e-19, 4.7828e-18,\n",
      "         3.5878e-18, 2.4892e-12, 1.7179e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1005.2768\n",
      "Test epoch : 2 Average loss: 964.8022\n",
      "PP(train) = 2017.742, PP(valid) = 2088.989\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5624e-26, 3.1535e-17, 6.3089e-09, 2.1597e-06, 1.6295e-12, 1.8013e-25,\n",
      "         6.3381e-18, 3.0178e-09, 9.9999e-01, 5.3071e-06, 1.1334e-16, 2.3773e-12,\n",
      "         5.4995e-17, 7.1222e-20, 4.4268e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1005.1174\n",
      "Test epoch : 3 Average loss: 964.7541\n",
      "PP(train) = 2014.889, PP(valid) = 2088.175\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0746e-16, 3.7893e-09, 6.6022e-10, 9.9989e-01, 2.6911e-05, 1.3417e-15,\n",
      "         5.3898e-15, 6.9169e-06, 1.6481e-06, 1.9064e-10, 4.1214e-12, 4.1435e-08,\n",
      "         1.9330e-14, 5.4265e-07, 7.8108e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0417, 0.0550, 0.0919, 0.0502, 0.0527, 0.0524, 0.0532, 0.0686,\n",
      "         0.1351, 0.1029, 0.0484, 0.0562, 0.0521, 0.0906]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1004.9651\n",
      "Test epoch : 4 Average loss: 964.7049\n",
      "PP(train) = 2011.921, PP(valid) = 2087.440\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2418e-16, 7.1549e-16, 8.5652e-01, 2.0202e-10, 1.4344e-01, 1.8660e-10,\n",
      "         6.0442e-12, 3.9268e-05, 9.7062e-09, 2.3618e-10, 3.5119e-11, 1.6038e-14,\n",
      "         1.0921e-08, 2.6743e-08, 1.1662e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0713, 0.0831, 0.0444, 0.0662, 0.0866, 0.0512, 0.0797, 0.0803, 0.0863,\n",
      "         0.0631, 0.0637, 0.0499, 0.0509, 0.0834, 0.0399]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1004.7364\n",
      "Test epoch : 5 Average loss: 964.6503\n",
      "PP(train) = 2008.606, PP(valid) = 2086.424\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3787e-26, 1.2200e-15, 6.2936e-10, 2.9449e-27, 3.8610e-16, 7.3609e-37,\n",
      "         1.3513e-13, 1.0000e+00, 2.8613e-17, 1.1656e-12, 3.3760e-21, 7.2492e-20,\n",
      "         2.3016e-27, 3.3333e-21, 2.0054e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1004.5088\n",
      "Test epoch : 6 Average loss: 964.5979\n",
      "PP(train) = 2005.519, PP(valid) = 2085.670\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1843e-18, 4.4976e-16, 6.7376e-17, 9.9997e-01, 6.5153e-18, 3.2438e-25,\n",
      "         6.4229e-21, 3.3859e-05, 3.1314e-08, 8.1057e-25, 4.7336e-20, 3.8694e-15,\n",
      "         1.8045e-14, 6.8013e-16, 1.7258e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0417, 0.0550, 0.0919, 0.0502, 0.0527, 0.0524, 0.0532, 0.0686,\n",
      "         0.1351, 0.1029, 0.0484, 0.0562, 0.0521, 0.0906]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1004.2671\n",
      "Test epoch : 7 Average loss: 964.5439\n",
      "PP(train) = 2002.335, PP(valid) = 2084.781\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5594e-17, 1.1168e-11, 9.7124e-05, 4.3099e-14, 2.5367e-19, 3.8273e-11,\n",
      "         3.8334e-18, 9.9990e-01, 2.1201e-14, 7.2397e-19, 7.5963e-17, 2.4096e-18,\n",
      "         1.8896e-08, 7.3060e-10, 1.6156e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1004.0797\n",
      "Test epoch : 8 Average loss: 964.4885\n",
      "PP(train) = 1999.047, PP(valid) = 2083.740\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0932e-32, 2.3637e-24, 1.0000e+00, 5.9891e-21, 2.7869e-21, 3.1300e-33,\n",
      "         7.6492e-23, 7.7090e-11, 5.1046e-18, 7.1893e-29, 2.6969e-14, 2.3898e-20,\n",
      "         5.6460e-15, 9.4429e-20, 2.2764e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1003.9141\n",
      "Test epoch : 9 Average loss: 964.4396\n",
      "PP(train) = 1996.075, PP(valid) = 2083.103\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0222e-18, 1.4140e-20, 3.1225e-06, 2.2295e-14, 2.1463e-01, 4.5683e-14,\n",
      "         3.4674e-03, 1.3666e-03, 4.5134e-13, 5.3296e-14, 6.1248e-14, 9.7335e-07,\n",
      "         1.5638e-03, 9.3404e-14, 7.7897e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0728, 0.0459, 0.0867, 0.0480, 0.0673, 0.0676, 0.1084, 0.0966, 0.0568,\n",
      "         0.0602, 0.0640, 0.0574, 0.0504, 0.0698, 0.0481]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1003.6984\n",
      "Test epoch : 10 Average loss: 964.3869\n",
      "PP(train) = 1992.905, PP(valid) = 2082.177\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3362e-21, 4.5705e-18, 1.7995e-06, 1.0027e-16, 4.1154e-07, 2.5933e-17,\n",
      "         2.9810e-12, 5.9561e-10, 1.0300e-05, 3.4577e-08, 5.9928e-27, 9.6002e-07,\n",
      "         3.0120e-09, 1.1124e-04, 9.9988e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1003.4772\n",
      "Test epoch : 11 Average loss: 964.3329\n",
      "PP(train) = 1989.720, PP(valid) = 2081.215\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0223e-13, 2.5162e-12, 3.5650e-04, 1.1126e-08, 9.5152e-04, 7.1433e-15,\n",
      "         7.8664e-16, 2.5147e-15, 7.6830e-01, 9.8509e-05, 9.7428e-13, 8.8733e-16,\n",
      "         2.3030e-01, 3.4163e-11, 3.0568e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0550, 0.0699, 0.0509, 0.0632, 0.0331, 0.0742, 0.0616, 0.0900, 0.0615,\n",
      "         0.0754, 0.0754, 0.0629, 0.0534, 0.1129, 0.0606]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1003.3337\n",
      "Test epoch : 12 Average loss: 964.2848\n",
      "PP(train) = 1986.764, PP(valid) = 2080.517\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1104e-12, 4.1916e-10, 7.6706e-10, 6.7617e-20, 4.7301e-10, 6.6017e-19,\n",
      "         7.0207e-17, 5.2209e-13, 2.1030e-09, 3.9803e-16, 3.2152e-12, 9.1571e-14,\n",
      "         1.2188e-09, 3.5730e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1003.1166\n",
      "Test epoch : 13 Average loss: 964.2356\n",
      "PP(train) = 1983.721, PP(valid) = 2079.666\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8036e-18, 2.7109e-22, 7.5018e-07, 7.2478e-10, 1.1383e-07, 1.3462e-25,\n",
      "         6.2684e-15, 8.3316e-06, 9.6281e-01, 2.3765e-08, 5.2037e-25, 1.5612e-16,\n",
      "         3.7168e-02, 4.3602e-13, 1.0554e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0560, 0.0678, 0.0453, 0.0630, 0.0308, 0.0758, 0.0627, 0.0917, 0.0577,\n",
      "         0.0703, 0.0831, 0.0648, 0.0515, 0.1283, 0.0514]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1002.8224\n",
      "Test epoch : 14 Average loss: 964.1851\n",
      "PP(train) = 1980.744, PP(valid) = 2078.835\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7283e-05, 1.6566e-16, 1.3314e-02, 9.5281e-14, 3.8758e-01, 5.9418e-01,\n",
      "         1.1728e-17, 2.5240e-15, 7.8127e-08, 3.3302e-03, 4.4528e-06, 1.5631e-03,\n",
      "         4.3415e-12, 1.9611e-10, 4.9035e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0577, 0.0575, 0.0400, 0.0526, 0.0559, 0.0583, 0.1147, 0.0763,\n",
      "         0.0988, 0.0554, 0.0804, 0.0478, 0.0855, 0.0742]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1002.7239\n",
      "Test epoch : 15 Average loss: 964.1383\n",
      "PP(train) = 1977.840, PP(valid) = 2078.094\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2439e-26, 3.0175e-17, 3.3952e-04, 5.0202e-06, 1.3874e-06, 1.6254e-16,\n",
      "         7.4129e-17, 1.5583e-15, 3.2119e-01, 4.7826e-20, 1.0394e-25, 2.2945e-06,\n",
      "         1.7112e-18, 7.9380e-18, 6.7846e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0738, 0.0517, 0.0807, 0.0550, 0.0492, 0.0710, 0.0923, 0.0940, 0.0597,\n",
      "         0.0583, 0.0698, 0.0596, 0.0520, 0.0881, 0.0448]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1002.4851\n",
      "Test epoch : 16 Average loss: 964.0907\n",
      "PP(train) = 1974.924, PP(valid) = 2077.268\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1020e-06, 3.9674e-10, 1.0457e-11, 8.2608e-06, 5.9940e-10, 4.3670e-13,\n",
      "         1.5725e-15, 9.9864e-01, 5.4360e-09, 5.4872e-17, 3.9613e-15, 1.2770e-05,\n",
      "         5.4272e-10, 3.6990e-07, 1.3341e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1002.2998\n",
      "Test epoch : 17 Average loss: 964.0440\n",
      "PP(train) = 1972.060, PP(valid) = 2076.519\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8164e-11, 3.3428e-10, 1.2559e-04, 6.3191e-12, 1.2207e-08, 3.9502e-25,\n",
      "         7.5835e-12, 1.8039e-07, 5.0978e-05, 1.3816e-15, 4.3494e-15, 3.1300e-22,\n",
      "         9.7506e-01, 5.3661e-12, 2.4766e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0490, 0.0736, 0.0769, 0.0604, 0.0418, 0.0648, 0.0549, 0.0794, 0.0744,\n",
      "         0.0927, 0.0486, 0.0531, 0.0584, 0.0646, 0.1075]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1002.1052\n",
      "Test epoch : 18 Average loss: 964.0000\n",
      "PP(train) = 1969.344, PP(valid) = 2075.866\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.4542e-11, 5.4940e-21, 9.9091e-01, 1.4449e-15, 1.6521e-08, 8.1537e-15,\n",
      "         1.8853e-19, 9.0897e-03, 4.9603e-10, 1.4792e-10, 4.1501e-14, 1.8518e-08,\n",
      "         2.2142e-07, 2.9053e-16, 3.3567e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0761, 0.0900, 0.0443, 0.0713, 0.0840, 0.0487, 0.0749, 0.0750, 0.0945,\n",
      "         0.0580, 0.0627, 0.0477, 0.0510, 0.0865, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1001.9484\n",
      "Test epoch : 19 Average loss: 963.9561\n",
      "PP(train) = 1966.486, PP(valid) = 2075.070\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2996e-06, 8.2578e-20, 2.2073e-08, 2.7866e-01, 9.6400e-14, 5.4629e-03,\n",
      "         1.4103e-12, 1.7131e-05, 9.3329e-03, 1.1667e-05, 3.1771e-20, 1.2937e-07,\n",
      "         6.8330e-08, 1.4180e-16, 7.0651e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0724, 0.0452, 0.0890, 0.0612, 0.0583, 0.0644, 0.0897, 0.0816, 0.0638,\n",
      "         0.0706, 0.0736, 0.0553, 0.0540, 0.0675, 0.0533]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1001.7591\n",
      "Test epoch : 20 Average loss: 963.9114\n",
      "PP(train) = 1963.621, PP(valid) = 2074.255\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1568e-18, 7.0996e-22, 3.0939e-10, 9.5707e-11, 7.7178e-01, 3.6674e-08,\n",
      "         5.3832e-18, 8.6386e-15, 2.2011e-01, 4.4562e-15, 7.4984e-06, 4.1673e-03,\n",
      "         1.4409e-07, 1.0623e-12, 3.9371e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0481, 0.0526, 0.0437, 0.0434, 0.0756, 0.0693, 0.0964, 0.1063, 0.0485,\n",
      "         0.0892, 0.0718, 0.0623, 0.0477, 0.0746, 0.0706]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1001.5888\n",
      "Test epoch : 21 Average loss: 963.8704\n",
      "PP(train) = 1961.002, PP(valid) = 2073.672\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1241e-10, 9.2787e-01, 6.5090e-06, 5.2801e-02, 8.8818e-07, 1.5157e-10,\n",
      "         1.5902e-11, 1.2743e-06, 6.5058e-12, 7.1036e-16, 8.9673e-12, 7.7151e-11,\n",
      "         1.8301e-03, 5.8088e-14, 1.7488e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1050, 0.0479, 0.0415, 0.1044, 0.0457, 0.0475, 0.0728, 0.0421, 0.0764,\n",
      "         0.0790, 0.0402, 0.0466, 0.0599, 0.1101, 0.0808]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1001.4167\n",
      "Test epoch : 22 Average loss: 963.8285\n",
      "PP(train) = 1958.294, PP(valid) = 2072.987\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7858e-24, 4.0982e-17, 2.0552e-12, 9.7524e-02, 9.0248e-01, 1.4493e-26,\n",
      "         1.2192e-19, 4.3645e-08, 9.4438e-11, 2.9695e-15, 3.5033e-13, 1.9896e-17,\n",
      "         8.0427e-19, 3.7387e-16, 1.0121e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0455, 0.0477, 0.0437, 0.0419, 0.0909, 0.0649, 0.1004, 0.1021, 0.0473,\n",
      "         0.0983, 0.0707, 0.0594, 0.0469, 0.0616, 0.0787]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1001.1608\n",
      "Test epoch : 23 Average loss: 963.7846\n",
      "PP(train) = 1955.595, PP(valid) = 2072.270\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8280e-16, 1.7073e-06, 2.8679e-08, 2.7274e-19, 1.5946e-20, 6.8143e-18,\n",
      "         1.3640e-15, 3.8616e-08, 1.0761e-10, 2.9048e-22, 1.4849e-05, 3.4151e-20,\n",
      "         1.3485e-04, 2.9613e-16, 9.9985e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1001.0207\n",
      "Test epoch : 24 Average loss: 963.7467\n",
      "PP(train) = 1952.989, PP(valid) = 2071.680\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0243e-20, 6.1054e-17, 1.0000e+00, 7.6799e-17, 2.3632e-07, 3.7801e-09,\n",
      "         4.2471e-13, 4.5445e-09, 2.4118e-14, 4.3171e-13, 7.8371e-17, 2.5332e-22,\n",
      "         7.9501e-09, 1.3275e-06, 5.6897e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1000.8591\n",
      "Test epoch : 25 Average loss: 963.7069\n",
      "PP(train) = 1950.328, PP(valid) = 2070.970\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5175e-25, 3.9719e-12, 1.7435e-12, 1.1498e-08, 2.6868e-04, 8.1550e-16,\n",
      "         5.5560e-16, 6.9187e-16, 9.9552e-01, 7.3474e-22, 7.0987e-15, 2.4688e-14,\n",
      "         5.5194e-06, 1.1814e-19, 4.2087e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0444, 0.0628, 0.0305, 0.0760, 0.0631, 0.0920, 0.0570,\n",
      "         0.0692, 0.0845, 0.0651, 0.0511, 0.1311, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1000.5971\n",
      "Test epoch : 26 Average loss: 963.6695\n",
      "PP(train) = 1947.789, PP(valid) = 2070.403\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1934e-13, 1.4826e-13, 4.4226e-05, 3.7996e-18, 9.4109e-04, 1.3561e-19,\n",
      "         9.4389e-11, 1.9896e-07, 9.9901e-01, 2.2942e-10, 1.1142e-11, 2.1517e-10,\n",
      "         2.1521e-08, 1.1244e-20, 6.8162e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0628, 0.0304, 0.0760, 0.0630, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0498]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1000.4660\n",
      "Test epoch : 27 Average loss: 963.6313\n",
      "PP(train) = 1945.225, PP(valid) = 2069.783\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8133e-09, 1.6466e-11, 4.5707e-12, 7.3527e-13, 1.6638e-08, 1.3381e-20,\n",
      "         3.3938e-04, 6.4321e-06, 1.2927e-07, 2.7062e-15, 2.2493e-14, 5.3814e-14,\n",
      "         2.9112e-07, 5.9902e-13, 9.9965e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1000.3361\n",
      "Test epoch : 28 Average loss: 963.5920\n",
      "PP(train) = 1942.694, PP(valid) = 2069.120\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1778e-05, 1.6583e-09, 3.0518e-11, 1.0547e-10, 1.0182e-10, 3.9990e-15,\n",
      "         4.2660e-17, 2.8503e-06, 2.0214e-03, 9.4482e-21, 6.8309e-14, 2.3529e-15,\n",
      "         9.9788e-01, 8.2011e-07, 3.6846e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0483, 0.0744, 0.0760, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0937, 0.0482, 0.0529, 0.0585, 0.0644, 0.1097]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1000.1570\n",
      "Test epoch : 29 Average loss: 963.5545\n",
      "PP(train) = 1940.188, PP(valid) = 2068.490\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1581e-12, 4.7681e-05, 5.9042e-14, 2.8846e-16, 3.3777e-03, 9.4336e-29,\n",
      "         4.0675e-19, 9.9657e-01, 9.4020e-18, 5.9320e-10, 2.4807e-13, 3.6329e-17,\n",
      "         3.8724e-06, 6.9706e-20, 4.3459e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1054, 0.0543, 0.0442, 0.0792, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0371, 0.1192, 0.0439, 0.0366, 0.0938, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 999.9303\n",
      "Test epoch : 30 Average loss: 963.5190\n",
      "PP(train) = 1937.722, PP(valid) = 2067.951\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.6688e-13, 5.5894e-23, 3.1104e-12, 1.3355e-26, 1.9633e-13, 9.8168e-17,\n",
      "         1.9210e-12, 4.6208e-18, 7.8378e-12, 4.5808e-15, 2.1438e-15, 1.0032e-16,\n",
      "         9.9544e-01, 1.2626e-04, 4.4386e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0484, 0.0743, 0.0763, 0.0605, 0.0414, 0.0646, 0.0541, 0.0790, 0.0746,\n",
      "         0.0936, 0.0483, 0.0529, 0.0585, 0.0644, 0.1095]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 999.7391\n",
      "Test epoch : 31 Average loss: 963.4850\n",
      "PP(train) = 1935.244, PP(valid) = 2067.361\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3892e-10, 1.4138e-04, 2.2495e-08, 1.1398e-17, 9.9986e-01, 7.8345e-34,\n",
      "         7.4730e-19, 1.6463e-11, 2.0377e-09, 1.8937e-16, 7.2388e-28, 3.8449e-09,\n",
      "         6.3179e-14, 5.8803e-19, 1.6000e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 999.6148\n",
      "Test epoch : 32 Average loss: 963.4507\n",
      "PP(train) = 1932.884, PP(valid) = 2066.867\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6155e-11, 2.2801e-18, 1.0423e-08, 9.8060e-15, 4.6126e-09, 6.2171e-20,\n",
      "         5.2909e-28, 1.2555e-03, 2.7880e-10, 2.3974e-08, 1.2793e-17, 9.9874e-01,\n",
      "         7.4062e-23, 1.4826e-22, 1.0552e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0615, 0.0622, 0.0816, 0.0550, 0.0547, 0.1200, 0.0458, 0.0785, 0.0983,\n",
      "         0.0684, 0.0441, 0.0663, 0.0594, 0.0664, 0.0377]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 999.5125\n",
      "Test epoch : 33 Average loss: 963.4166\n",
      "PP(train) = 1930.428, PP(valid) = 2066.250\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9514e-04, 9.9960e-01, 1.1976e-13, 4.8603e-07, 1.2798e-10, 9.8500e-15,\n",
      "         1.3028e-08, 6.8856e-06, 2.5139e-09, 2.0955e-17, 4.1489e-10, 2.4378e-15,\n",
      "         1.0604e-06, 1.9430e-13, 1.0049e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1095, 0.0479, 0.0398, 0.1059, 0.0449, 0.0465, 0.0731, 0.0405, 0.0766,\n",
      "         0.0766, 0.0374, 0.0459, 0.0599, 0.1150, 0.0806]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 999.3394\n",
      "Test epoch : 34 Average loss: 963.3820\n",
      "PP(train) = 1928.004, PP(valid) = 2065.634\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0652e-08, 4.5448e-07, 1.0531e-16, 5.0919e-17, 6.2314e-01, 1.0662e-24,\n",
      "         3.6537e-17, 3.4864e-01, 4.3803e-11, 2.8212e-02, 1.8132e-19, 2.9908e-06,\n",
      "         1.0499e-13, 2.5739e-17, 2.5160e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0578, 0.0665, 0.0472, 0.0427, 0.0922, 0.0768, 0.0977, 0.0804, 0.0496,\n",
      "         0.0697, 0.0837, 0.0558, 0.0435, 0.0755, 0.0610]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 999.1439\n",
      "Test epoch : 35 Average loss: 963.3477\n",
      "PP(train) = 1925.721, PP(valid) = 2065.164\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7254e-23, 3.4076e-20, 1.0000e+00, 4.2014e-16, 2.6375e-12, 1.3627e-22,\n",
      "         5.1426e-20, 6.3965e-15, 2.8700e-16, 5.2911e-16, 1.7377e-10, 1.6780e-23,\n",
      "         1.9402e-14, 1.1848e-15, 5.2174e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 998.9340\n",
      "Test epoch : 36 Average loss: 963.3147\n",
      "PP(train) = 1923.365, PP(valid) = 2064.610\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7524e-25, 7.2072e-22, 6.0969e-01, 6.5599e-07, 4.9355e-05, 1.7859e-20,\n",
      "         3.3269e-20, 9.5411e-12, 2.6762e-09, 2.6773e-08, 8.4894e-16, 6.5199e-16,\n",
      "         2.4594e-13, 2.2481e-16, 3.9026e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0795, 0.0694, 0.0629, 0.0634, 0.0750, 0.0558, 0.0877, 0.0830, 0.0804,\n",
      "         0.0568, 0.0632, 0.0515, 0.0520, 0.0813, 0.0382]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 998.8239\n",
      "Test epoch : 37 Average loss: 963.2835\n",
      "PP(train) = 1921.062, PP(valid) = 2064.095\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8632e-24, 2.7684e-23, 1.5181e-21, 8.5439e-20, 9.5689e-18, 9.2660e-22,\n",
      "         1.2257e-16, 1.0000e+00, 6.8086e-24, 1.3294e-16, 2.3722e-21, 1.2806e-13,\n",
      "         1.5150e-19, 3.9481e-28, 3.6983e-17]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 998.6393\n",
      "Test epoch : 38 Average loss: 963.2522\n",
      "PP(train) = 1918.757, PP(valid) = 2063.581\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4201e-18, 1.2768e-09, 2.3646e-09, 2.4838e-12, 1.0896e-06, 7.0495e-18,\n",
      "         1.3594e-14, 9.4952e-10, 8.7213e-13, 2.2992e-13, 1.0000e+00, 2.8993e-13,\n",
      "         6.5420e-09, 1.2330e-35, 5.6598e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0920, 0.1292, 0.0394, 0.0377, 0.0644, 0.0584, 0.0361, 0.0756, 0.0541,\n",
      "         0.0360, 0.1119, 0.0427, 0.0304, 0.1005, 0.0917]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 998.5016\n",
      "Test epoch : 39 Average loss: 963.2212\n",
      "PP(train) = 1916.412, PP(valid) = 2063.027\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4100e-08, 1.3496e-19, 9.3498e-13, 3.7057e-04, 3.4433e-13, 2.5454e-18,\n",
      "         9.9382e-19, 1.1840e-08, 9.2402e-01, 2.1429e-02, 7.1144e-16, 1.6691e-11,\n",
      "         6.7682e-20, 9.0727e-19, 5.4182e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0575, 0.0666, 0.0463, 0.0631, 0.0323, 0.0758, 0.0655, 0.0916, 0.0574,\n",
      "         0.0687, 0.0823, 0.0649, 0.0510, 0.1273, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 998.4006\n",
      "Test epoch : 40 Average loss: 963.1948\n",
      "PP(train) = 1914.237, PP(valid) = 2062.644\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2971e-14, 4.8514e-12, 1.5489e-10, 4.9892e-13, 9.9793e-01, 1.9953e-19,\n",
      "         2.0642e-22, 2.6745e-16, 1.3702e-11, 2.0649e-03, 8.5345e-10, 1.3654e-15,\n",
      "         1.4296e-10, 8.8090e-17, 3.7010e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0421, 0.0382, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0671, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 998.2177\n",
      "Test epoch : 41 Average loss: 963.1655\n",
      "PP(train) = 1911.984, PP(valid) = 2062.172\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.6073e-26, 1.2677e-20, 1.7502e-28, 9.5502e-24, 5.9052e-11, 2.8186e-23,\n",
      "         5.9980e-25, 3.4596e-06, 1.0000e+00, 4.9076e-24, 3.2442e-14, 1.0762e-21,\n",
      "         4.9114e-17, 2.6782e-19, 1.3049e-16]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 998.0027\n",
      "Test epoch : 42 Average loss: 963.1337\n",
      "PP(train) = 1909.769, PP(valid) = 2061.658\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9883e-16, 1.5168e-11, 1.7557e-07, 1.3755e-09, 8.2868e-08, 1.0339e-03,\n",
      "         5.3744e-01, 1.5340e-07, 1.1922e-05, 2.5807e-17, 1.3297e-11, 3.0638e-07,\n",
      "         2.6752e-09, 4.6151e-01, 3.8345e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0626, 0.0639, 0.0538, 0.0794, 0.0669, 0.0750, 0.0484, 0.0549, 0.1103,\n",
      "         0.0512, 0.0731, 0.0845, 0.0509, 0.0781, 0.0469]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 997.8619\n",
      "Test epoch : 43 Average loss: 963.1054\n",
      "PP(train) = 1907.602, PP(valid) = 2061.205\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6738e-22, 4.9209e-25, 2.0138e-18, 1.1135e-22, 5.2393e-17, 7.1549e-17,\n",
      "         1.9643e-18, 9.5933e-01, 1.7350e-17, 5.3575e-24, 5.4461e-09, 6.3097e-19,\n",
      "         9.7669e-05, 1.0904e-21, 4.0574e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0837, 0.1023, 0.0560, 0.0447, 0.0785, 0.0914, 0.0782, 0.0452, 0.0535,\n",
      "         0.0377, 0.1167, 0.0444, 0.0372, 0.0932, 0.0372]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 997.7220\n",
      "Test epoch : 44 Average loss: 963.0771\n",
      "PP(train) = 1905.451, PP(valid) = 2060.744\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6092e-12, 2.9100e-19, 4.1207e-01, 1.6984e-20, 1.0314e-05, 1.2009e-11,\n",
      "         4.0664e-30, 2.0555e-08, 1.9206e-06, 4.4079e-21, 3.2369e-19, 5.8792e-01,\n",
      "         1.1850e-07, 5.7197e-09, 2.2130e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0684, 0.0737, 0.0646, 0.0625, 0.0665, 0.0841, 0.0571, 0.0787, 0.0988,\n",
      "         0.0652, 0.0518, 0.0590, 0.0569, 0.0754, 0.0374]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 997.5739\n",
      "Test epoch : 45 Average loss: 963.0516\n",
      "PP(train) = 1903.334, PP(valid) = 2060.342\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3515e-16, 3.3997e-06, 5.3421e-03, 5.8830e-04, 9.9588e-05, 3.5911e-12,\n",
      "         1.0826e-09, 1.4137e-06, 1.0472e-02, 9.5263e-01, 1.3981e-05, 4.4781e-06,\n",
      "         4.9288e-04, 5.3062e-17, 3.0353e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0538, 0.0879, 0.0316, 0.1000, 0.0729, 0.0739, 0.0816, 0.0625, 0.0561,\n",
      "         0.0708, 0.0400, 0.0643, 0.0397, 0.1101, 0.0549]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 997.4470\n",
      "Test epoch : 46 Average loss: 963.0228\n",
      "PP(train) = 1901.163, PP(valid) = 2059.838\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9654e-16, 5.7886e-14, 4.0671e-10, 1.2105e-20, 9.9906e-01, 1.8894e-18,\n",
      "         4.8405e-15, 9.0998e-04, 8.8048e-08, 8.4390e-20, 4.0234e-13, 5.0773e-20,\n",
      "         6.8484e-07, 4.4580e-19, 2.9142e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 997.2801\n",
      "Test epoch : 47 Average loss: 962.9951\n",
      "PP(train) = 1899.075, PP(valid) = 2059.413\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0400e-09, 8.8109e-01, 4.4831e-07, 7.6568e-08, 3.7992e-02, 1.3552e-21,\n",
      "         1.2863e-17, 1.6658e-05, 2.4232e-14, 4.2960e-03, 6.5839e-13, 1.3154e-10,\n",
      "         6.2853e-02, 9.9257e-07, 1.3753e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1011, 0.0500, 0.0426, 0.0986, 0.0469, 0.0490, 0.0741, 0.0450, 0.0756,\n",
      "         0.0788, 0.0397, 0.0476, 0.0597, 0.1090, 0.0822]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 997.0872\n",
      "Test epoch : 48 Average loss: 962.9696\n",
      "PP(train) = 1896.973, PP(valid) = 2059.013\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8823e-13, 6.3277e-26, 1.5281e-17, 2.5323e-18, 6.5709e-18, 3.3754e-24,\n",
      "         2.2922e-22, 1.0000e+00, 7.1432e-11, 6.8567e-27, 1.3948e-20, 1.9947e-24,\n",
      "         3.7686e-15, 2.3403e-18, 1.8638e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 996.9455\n",
      "Test epoch : 49 Average loss: 962.9441\n",
      "PP(train) = 1894.889, PP(valid) = 2058.594\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4649e-22, 1.6587e-14, 2.1577e-06, 4.5814e-09, 9.9998e-01, 5.6730e-13,\n",
      "         1.6481e-20, 1.0496e-07, 5.0721e-16, 2.3209e-16, 1.7346e-20, 7.9254e-08,\n",
      "         3.2743e-19, 9.4335e-19, 1.2609e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 996.8254\n",
      "Test epoch : 50 Average loss: 962.9197\n",
      "PP(train) = 1892.844, PP(valid) = 2058.173\n",
      "Writing to ./topicwords/19-topwords_e50.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "\n",
      "===== # 1, Topic : 12, p : 7.8058 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                  \n",
      "\n",
      "===== # 2, Topic : 1, p : 7.4451 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                \n",
      "\n",
      "===== # 3, Topic : 14, p : 8.2194 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                               \n",
      "\n",
      "===== # 4, Topic : 14, p : 8.7895 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                           \n",
      "\n",
      "===== # 5, Topic : 5, p : 7.2219 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "===== # 6, Topic : 4, p : 7.2058 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                              m s     \n",
      "\n",
      "===== # 7, Topic : 12, p : 7.5498 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                           \n",
      "\n",
      "===== # 8, Topic : 1, p : 7.4121 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                    \n",
      "\n",
      "===== # 9, Topic : 14, p : 7.6387 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                         (            JIS          JIS           JIS                                      a b               RH       \n",
      "\n",
      "===== # 10, Topic : 14, p : 8.1607 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                          (                                                      \n"
     ]
    }
   ],
   "source": [
    "def splitdata(dicts):\n",
    "    test_valid_size = int(len(dicts) * 0.1)\n",
    "    test_data  = dicts[:test_valid_size]\n",
    "    valid_data = dicts[test_valid_size : test_valid_size*2]\n",
    "    train_data = dicts[test_valid_size*2 :]\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "sitas = []\n",
    "sita_hats = []\n",
    "files = glob.glob('./Dataset/*')\n",
    "for period, file in enumerate(files):\n",
    "    contents = open(file, 'rb')\n",
    "    dicts = pickle.load(contents)\n",
    "    dicts = dicts[:200] # \n",
    "    if period==0:\n",
    "        bow_vocab = gensim.corpora.Dictionary(dicts)\n",
    "        bow_vocab_size = len(bow_vocab)\n",
    "        hidden_dim = 500\n",
    "        topic_num = 15\n",
    "        batch_size = 32\n",
    "        sita_hat = None\n",
    "        model = trainer.Estimator(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num)\n",
    "    train_data, valid_data, test_data = splitdata(dicts)\n",
    "    test_valid_size = int(len(dicts) * 0.1)\n",
    "    last_batch_idx = len(dicts[test_valid_size*2 :])//batch_size-1\n",
    "    # ntm_model, pred_sita, train, z_valid = model.fit(train_data, valid_data, bow_vocab, batch_size, period, n_epoch=1)\n",
    "    # sita_hat = None\n",
    "    # model, z_train, z_valid = model.fit(train_data, valid_data, bow_vocab, batch_size, last_batch_idx, sita_hat, period, n_epoch=1)\n",
    "    sita, next_sita_hat = model.fit(train_data, valid_data, bow_vocab, batch_size, last_batch_idx, sita_hat, period, n_epoch=50)\n",
    "    sitas.append(sita)\n",
    "    sita_hats.append(next_sita_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.1900e-36, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          6.0856e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "        grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 2.5036e-31, 0.0000e+00, 1.8754e-19, 0.0000e+00,\n",
       "          0.0000e+00, 9.2792e-38, 3.1118e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.4930e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 2.8026e-44, 0.0000e+00, 6.5096e-32, 0.0000e+00,\n",
       "          0.0000e+00, 5.6052e-44, 1.2170e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.9062e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 8.6841e-34, 0.0000e+00, 2.9853e-35, 0.0000e+00,\n",
       "          0.0000e+00, 2.2055e-27, 1.4237e-23, 8.5186e-41, 0.0000e+00, 0.0000e+00,\n",
       "          7.1978e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 4.7386e-38, 1.7065e-18, 2.5047e-26, 1.9055e-27, 0.0000e+00,\n",
       "          2.0739e-43, 4.4991e-08, 3.6034e-36, 1.3263e-40, 0.0000e+00, 9.8846e-39,\n",
       "          1.3649e-24, 1.7849e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 1.6949e-40, 5.2188e-31, 2.0354e-38, 2.4003e-33, 0.0000e+00,\n",
       "          0.0000e+00, 1.2656e-23, 1.3927e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          4.7070e-34, 1.8121e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 1.7656e-43, 9.6343e-27, 4.1303e-27, 1.9957e-19, 0.0000e+00,\n",
       "          0.0000e+00, 2.6579e-11, 6.9905e-16, 3.1566e-26, 5.0513e-41, 5.9292e-37,\n",
       "          8.6015e-21, 4.9633e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 1.7555e-37, 2.7725e-14, 1.0452e-24, 6.4228e-33, 0.0000e+00,\n",
       "          3.7835e-44, 3.1937e-19, 1.8160e-18, 1.1909e-30, 1.4013e-45, 6.1056e-33,\n",
       "          4.7109e-24, 3.0106e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[2.8026e-45, 6.9962e-35, 2.4829e-16, 5.2005e-15, 1.5695e-22, 0.0000e+00,\n",
       "          0.0000e+00, 9.9994e-01, 5.5539e-05, 8.5356e-23, 0.0000e+00, 1.6389e-35,\n",
       "          3.2177e-20, 1.5403e-23, 4.4986e-06]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 4.3215e-37, 1.2244e-23, 1.0669e-36, 1.6827e-21, 3.1556e-39,\n",
       "          0.0000e+00, 1.5030e-05, 3.5823e-11, 2.2794e-25, 0.0000e+00, 5.7827e-35,\n",
       "          5.4955e-26, 5.2309e-33, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 2.9292e-35, 6.4890e-20, 1.0270e-31, 8.6857e-23, 2.4999e-42,\n",
       "          1.4588e-42, 1.8876e-16, 4.0558e-12, 2.6830e-32, 0.0000e+00, 1.2823e-40,\n",
       "          9.0587e-18, 9.2692e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[4.5015e-32, 3.7694e-31, 1.5457e-21, 1.7732e-11, 9.9996e-01, 6.8539e-38,\n",
       "          1.4013e-45, 6.9012e-12, 3.4046e-11, 1.2948e-26, 2.6352e-32, 3.0388e-33,\n",
       "          1.9642e-19, 1.5715e-17, 4.2526e-05]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[8.0110e-32, 1.8172e-28, 7.3960e-10, 1.6558e-05, 4.6540e-14, 8.9020e-26,\n",
       "          1.6881e-24, 1.3449e-15, 7.6316e-01, 5.7563e-15, 2.4960e-16, 1.5138e-20,\n",
       "          6.2718e-08, 9.5922e-19, 2.3682e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[3.9765e-34, 1.0042e-28, 1.4982e-15, 3.3064e-18, 3.0029e-06, 5.5577e-26,\n",
       "          7.0077e-22, 5.3496e-10, 6.5252e-09, 3.9846e-09, 1.0841e-20, 4.0437e-12,\n",
       "          1.7620e-05, 6.1754e-11, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[4.2298e-30, 1.4341e-22, 8.9634e-23, 4.5155e-22, 4.0341e-13, 6.6782e-32,\n",
       "          1.1770e-31, 5.2071e-12, 6.9789e-04, 7.3368e-21, 5.9052e-35, 1.8135e-30,\n",
       "          5.7119e-14, 6.7297e-19, 9.9930e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[8.9795e-24, 1.2007e-13, 8.1134e-10, 5.7046e-18, 2.6594e-10, 4.2785e-19,\n",
       "          2.2196e-25, 7.0971e-01, 2.6852e-18, 2.1213e-17, 3.8565e-23, 1.0464e-21,\n",
       "          2.9029e-01, 5.2560e-09, 2.1471e-08]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[1.1472e-27, 4.9535e-23, 3.6496e-14, 2.5925e-15, 2.4342e-17, 3.2578e-30,\n",
       "          9.9976e-39, 8.2561e-01, 1.5754e-01, 1.6820e-16, 1.4965e-31, 7.6242e-27,\n",
       "          1.6713e-02, 3.0090e-25, 1.2997e-04]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[3.3642e-20, 4.7051e-10, 4.2316e-26, 3.7470e-15, 9.7358e-01, 3.9791e-08,\n",
       "          1.1962e-19, 1.0066e-09, 2.6418e-02, 3.3202e-08, 3.2260e-19, 4.3761e-15,\n",
       "          6.7124e-22, 2.5879e-21, 2.3883e-08]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[1.4649e-22, 1.6587e-14, 2.1577e-06, 4.5814e-09, 9.9998e-01, 5.6730e-13,\n",
       "          1.6481e-20, 1.0496e-07, 5.0721e-16, 2.3209e-16, 1.7346e-20, 7.9254e-08,\n",
       "          3.2743e-19, 9.4335e-19, 1.2609e-05]], grad_fn=<ReshapeAliasBackward0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
       "          0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
       "          0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0623, 0.0620, 0.0551, 0.0605, 0.0363, 0.0749, 0.0725, 0.0934, 0.0584,\n",
       "          0.0658, 0.0798, 0.0637, 0.0518, 0.1152, 0.0484]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0736, 0.0987, 0.0620, 0.0501, 0.0678, 0.0861, 0.0717, 0.0536, 0.0606,\n",
       "          0.0501, 0.0949, 0.0479, 0.0433, 0.0871, 0.0524]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0788, 0.0992, 0.0537, 0.0477, 0.0683, 0.0903, 0.0751, 0.0503, 0.0548,\n",
       "          0.0421, 0.1130, 0.0475, 0.0394, 0.0999, 0.0400]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0451, 0.0484, 0.0423, 0.0387, 0.0933, 0.0661, 0.1054, 0.1082, 0.0454,\n",
       "          0.0935, 0.0678, 0.0604, 0.0458, 0.0635, 0.0760]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
       "          0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
       "        grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sita_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9ElEQVR4nO3df6zd9V3H8efLdsSxHzLtZW79YaupbM0CG14Zuqg4RFtcqCb+UTYH4pamCUw0GmFZMv9YYmbwx1xgqw3WbpFADENXlw5GNpU/NkwL8qsg7KYovRSliE4df9SOt3+cU3M4Pfeeb8tpz+3H5yO5uff7/X44553b3me/93vv95CqQpJ05vuuaQ8gSZoMgy5JjTDoktQIgy5JjTDoktSI5dN64hUrVtTatWun9fSSdEZ64IEHXqiqmVHHphb0tWvXsm/fvmk9vSSdkZL880LHvOQiSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiLFBT7IzyfNJHlvgeJJ8OslckkeSXDj5MSVJ43Q5Q98FbFzk+CZgff9tK/DZVz+WJOlEjQ16Vd0HvLjIks3A56vnfuCcJG+Z1ICSpG4mcafoSuDgwPZ8f99zwwuTbKV3Fs+aNWsm8NSSdHrdsu1rr/oxrt3+3glMcrxJ/FA0I/aN/N8gVdWOqpqtqtmZmZEvRSBJOkmTCPo8sHpgexVwaAKPK0k6AZMI+m7gqv5vu1wMfKuqjrvcIkk6tcZeQ09yO3AJsCLJPPA7wGsAqmo7sAe4HJgDXgKuOVXDSpIWNjboVXXlmOMFXDuxiSRJJ8U7RSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHP+eJH+d5OEk+5NcM/lRJUmLGRv0JMuAW4BNwAbgyiQbhpZdCzxeVRcAlwB/kOSsCc8qSVpElzP0i4C5qjpQVUeAO4DNQ2sKeEOSAK8HXgSOTnRSSdKiugR9JXBwYHu+v2/QzcDbgUPAo8D1VfXy8AMl2ZpkX5J9hw8fPsmRJUmjdAl6Ruyroe2fAx4C3gq8E7g5yRuP+4+qdlTVbFXNzszMnOCokqTFdAn6PLB6YHsVvTPxQdcAd1XPHPA08LbJjChJ6qJL0PcC65Os6/+gcwuwe2jNM8ClAEneDJwHHJjkoJKkxS0ft6Cqjia5DrgHWAbsrKr9Sbb1j28HPgHsSvIovUs0N1TVC6dwbknSkLFBB6iqPcCeoX3bBz4+BPzsZEeTJJ0I7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mY5Mkkc0luXGDNJUkeSrI/yd9NdkxJ0jjLxy1Isgy4BbgMmAf2JtldVY8PrDkH+AywsaqeSXLuKZpXkrSALmfoFwFzVXWgqo4AdwCbh9a8H7irqp4BqKrnJzumJGmcLkFfCRwc2J7v7xv0w8CbkvxtkgeSXDWpASVJ3Yy95AJkxL4a8Tg/AlwKvBb4RpL7q+qpVzxQshXYCrBmzZoTn1aStKAuZ+jzwOqB7VXAoRFr7q6qb1fVC8B9wAXDD1RVO6pqtqpmZ2ZmTnZmSdIIXYK+F1ifZF2Ss4AtwO6hNV8EfiLJ8iRnA+8GnpjsqJKkxYy95FJVR5NcB9wDLAN2VtX+JNv6x7dX1RNJ7gYeAV4Gbq2qx07l4JKkV+pyDZ2q2gPsGdq3fWj7JuCmyY0mSToR3ikqSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY3oFPQkG5M8mWQuyY2LrPvRJN9J8kuTG1GS1MXYoCdZBtwCbAI2AFcm2bDAut8D7pn0kJKk8bqcoV8EzFXVgao6AtwBbB6x7iPAF4DnJzifJKmjLkFfCRwc2J7v7/s/SVYCvwhsX+yBkmxNsi/JvsOHD5/orJKkRXQJekbsq6HtTwE3VNV3FnugqtpRVbNVNTszM9NxRElSF8s7rJkHVg9srwIODa2ZBe5IArACuDzJ0ar6q0kMKUkar0vQ9wLrk6wDngW2AO8fXFBV6459nGQX8CVjLkmn19igV9XRJNfR++2VZcDOqtqfZFv/+KLXzSVJp0eXM3Sqag+wZ2jfyJBX1a+8+rEkSSfKO0UlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa0SnoSTYmeTLJXJIbRxz/QJJH+m9fT3LB5EeVJC1mbNCTLANuATYBG4Ark2wYWvY08FNVdT7wCWDHpAeVJC2uyxn6RcBcVR2oqiPAHcDmwQVV9fWq+vf+5v3AqsmOKUkap0vQVwIHB7bn+/sW8iHgy6MOJNmaZF+SfYcPH+4+pSRprC5Bz4h9NXJh8tP0gn7DqONVtaOqZqtqdmZmpvuUkqSxlndYMw+sHtheBRwaXpTkfOBWYFNV/dtkxpMkddXlDH0vsD7JuiRnAVuA3YMLkqwB7gI+WFVPTX5MSdI4Y8/Qq+pokuuAe4BlwM6q2p9kW//4duDjwPcBn0kCcLSqZk/d2JKkYV0uuVBVe4A9Q/u2D3z8YeDDkx1NknQivFNUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHE+ST/ePP5LkwsmPKklazNigJ1kG3AJsAjYAVybZMLRsE7C+/7YV+OyE55QkjdHlDP0iYK6qDlTVEeAOYPPQms3A56vnfuCcJG+Z8KySpEUs77BmJXBwYHseeHeHNSuB5wYXJdlK7wwe4L+TPHlC056YFcALp/DxJ8EZJ8MZJ8MZJ2fROa/7k1f12D+w0IEuQc+IfXUSa6iqHcCODs/5qiXZV1Wzp+O5TpYzToYzToYzTs605uxyyWUeWD2wvQo4dBJrJEmnUJeg7wXWJ1mX5CxgC7B7aM1u4Kr+b7tcDHyrqp4bfiBJ0qkz9pJLVR1Nch1wD7AM2FlV+5Ns6x/fDuwBLgfmgJeAa07dyJ2dlks7r5IzToYzToYzTs5U5kzVcZe6JUlnIO8UlaRGGHRJakRzQR/3MgXTlmR1kr9J8kSS/Umun/ZMC0myLMk/JPnStGdZSJJzktyZ5B/7n9Mfm/ZMw5L8Rv/P+rEktyf57iUw084kzyd5bGDf9ya5N8k3++/ftARnvKn/Z/1Ikr9Mcs4URxw548Cx30pSSVacrnmaCnrHlymYtqPAb1bV24GLgWuX4IzHXA88Me0hxvhj4O6qehtwAUts3iQrgV8DZqvqHfR+sWDLdKcCYBewcWjfjcBXq2o98NX+9jTt4vgZ7wXeUVXnA08BHz3dQw3ZxfEzkmQ1cBnwzOkcpqmg0+1lCqaqqp6rqgf7H/8XvQCtnO5Ux0uyCvh54NZpz7KQJG8EfhL4U4CqOlJV/zHVoUZbDrw2yXLgbJbAPRpVdR/w4tDuzcDn+h9/DviF0znTsFEzVtVXqupof/N+eve8TM0Cn0eAPwJ+mxE3WJ5KrQV9oZcgWJKSrAXeBfz9lEcZ5VP0/kK+POU5FvODwGHgz/qXhm5N8rppDzWoqp4Ffp/emdpz9O7R+Mp0p1rQm4/dP9J/f+6U5xnnV4EvT3uIYUmuAJ6tqodP93O3FvROL0GwFCR5PfAF4Ner6j+nPc+gJO8Dnq+qB6Y9yxjLgQuBz1bVu4BvM/3LBK/Qvw69GVgHvBV4XZJfnu5UZ74kH6N3+fK2ac8yKMnZwMeAj0/j+VsL+hnxEgRJXkMv5rdV1V3TnmeE9wBXJPknepet3pvkz6c70kjzwHxVHfsO5056gV9KfgZ4uqoOV9X/AHcBPz7lmRbyr8deJbX//vkpzzNSkquB9wEfqKV3I80P0fvH++H+188q4MEk3386nry1oHd5mYKpShJ613yfqKo/nPY8o1TVR6tqVVWtpfc5/FpVLbmzyqr6F+BgkvP6uy4FHp/iSKM8A1yc5Oz+n/2lLLEf3A7YDVzd//hq4ItTnGWkJBuBG4Arquqlac8zrKoerapzq2pt/+tnHriw/3f1lGsq6P0flhx7mYIngL+oqv3Tneo47wE+SO+s96H+2+XTHuoM9hHgtiSPAO8Efne647xS/7uHO4EHgUfpfc1N/fb1JLcD3wDOSzKf5EPAJ4HLknyT3m9ofHIJzngz8Abg3v7XzvYlOOP05ll637FIkk5GU2fokvT/mUGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxP8CTqqCUBqHI38AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9ElEQVR4nO3df6zd9V3H8efLdsSxHzLtZW79YaupbM0CG14Zuqg4RFtcqCb+UTYH4pamCUw0GmFZMv9YYmbwx1xgqw3WbpFADENXlw5GNpU/NkwL8qsg7KYovRSliE4df9SOt3+cU3M4Pfeeb8tpz+3H5yO5uff7/X44553b3me/93vv95CqQpJ05vuuaQ8gSZoMgy5JjTDoktQIgy5JjTDoktSI5dN64hUrVtTatWun9fSSdEZ64IEHXqiqmVHHphb0tWvXsm/fvmk9vSSdkZL880LHvOQiSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiLFBT7IzyfNJHlvgeJJ8OslckkeSXDj5MSVJ43Q5Q98FbFzk+CZgff9tK/DZVz+WJOlEjQ16Vd0HvLjIks3A56vnfuCcJG+Z1ICSpG4mcafoSuDgwPZ8f99zwwuTbKV3Fs+aNWsm8NSSdHrdsu1rr/oxrt3+3glMcrxJ/FA0I/aN/N8gVdWOqpqtqtmZmZEvRSBJOkmTCPo8sHpgexVwaAKPK0k6AZMI+m7gqv5vu1wMfKuqjrvcIkk6tcZeQ09yO3AJsCLJPPA7wGsAqmo7sAe4HJgDXgKuOVXDSpIWNjboVXXlmOMFXDuxiSRJJ8U7RSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHP+eJH+d5OEk+5NcM/lRJUmLGRv0JMuAW4BNwAbgyiQbhpZdCzxeVRcAlwB/kOSsCc8qSVpElzP0i4C5qjpQVUeAO4DNQ2sKeEOSAK8HXgSOTnRSSdKiugR9JXBwYHu+v2/QzcDbgUPAo8D1VfXy8AMl2ZpkX5J9hw8fPsmRJUmjdAl6Ruyroe2fAx4C3gq8E7g5yRuP+4+qdlTVbFXNzszMnOCokqTFdAn6PLB6YHsVvTPxQdcAd1XPHPA08LbJjChJ6qJL0PcC65Os6/+gcwuwe2jNM8ClAEneDJwHHJjkoJKkxS0ft6Cqjia5DrgHWAbsrKr9Sbb1j28HPgHsSvIovUs0N1TVC6dwbknSkLFBB6iqPcCeoX3bBz4+BPzsZEeTJJ0I7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mY5Mkkc0luXGDNJUkeSrI/yd9NdkxJ0jjLxy1Isgy4BbgMmAf2JtldVY8PrDkH+AywsaqeSXLuKZpXkrSALmfoFwFzVXWgqo4AdwCbh9a8H7irqp4BqKrnJzumJGmcLkFfCRwc2J7v7xv0w8CbkvxtkgeSXDWpASVJ3Yy95AJkxL4a8Tg/AlwKvBb4RpL7q+qpVzxQshXYCrBmzZoTn1aStKAuZ+jzwOqB7VXAoRFr7q6qb1fVC8B9wAXDD1RVO6pqtqpmZ2ZmTnZmSdIIXYK+F1ifZF2Ss4AtwO6hNV8EfiLJ8iRnA+8GnpjsqJKkxYy95FJVR5NcB9wDLAN2VtX+JNv6x7dX1RNJ7gYeAV4Gbq2qx07l4JKkV+pyDZ2q2gPsGdq3fWj7JuCmyY0mSToR3ikqSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY3oFPQkG5M8mWQuyY2LrPvRJN9J8kuTG1GS1MXYoCdZBtwCbAI2AFcm2bDAut8D7pn0kJKk8bqcoV8EzFXVgao6AtwBbB6x7iPAF4DnJzifJKmjLkFfCRwc2J7v7/s/SVYCvwhsX+yBkmxNsi/JvsOHD5/orJKkRXQJekbsq6HtTwE3VNV3FnugqtpRVbNVNTszM9NxRElSF8s7rJkHVg9srwIODa2ZBe5IArACuDzJ0ar6q0kMKUkar0vQ9wLrk6wDngW2AO8fXFBV6459nGQX8CVjLkmn19igV9XRJNfR++2VZcDOqtqfZFv/+KLXzSVJp0eXM3Sqag+wZ2jfyJBX1a+8+rEkSSfKO0UlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa0SnoSTYmeTLJXJIbRxz/QJJH+m9fT3LB5EeVJC1mbNCTLANuATYBG4Ark2wYWvY08FNVdT7wCWDHpAeVJC2uyxn6RcBcVR2oqiPAHcDmwQVV9fWq+vf+5v3AqsmOKUkap0vQVwIHB7bn+/sW8iHgy6MOJNmaZF+SfYcPH+4+pSRprC5Bz4h9NXJh8tP0gn7DqONVtaOqZqtqdmZmpvuUkqSxlndYMw+sHtheBRwaXpTkfOBWYFNV/dtkxpMkddXlDH0vsD7JuiRnAVuA3YMLkqwB7gI+WFVPTX5MSdI4Y8/Qq+pokuuAe4BlwM6q2p9kW//4duDjwPcBn0kCcLSqZk/d2JKkYV0uuVBVe4A9Q/u2D3z8YeDDkx1NknQivFNUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHE+ST/ePP5LkwsmPKklazNigJ1kG3AJsAjYAVybZMLRsE7C+/7YV+OyE55QkjdHlDP0iYK6qDlTVEeAOYPPQms3A56vnfuCcJG+Z8KySpEUs77BmJXBwYHseeHeHNSuB5wYXJdlK7wwe4L+TPHlC056YFcALp/DxJ8EZJ8MZJ8MZJ2fROa/7k1f12D+w0IEuQc+IfXUSa6iqHcCODs/5qiXZV1Wzp+O5TpYzToYzToYzTs605uxyyWUeWD2wvQo4dBJrJEmnUJeg7wXWJ1mX5CxgC7B7aM1u4Kr+b7tcDHyrqp4bfiBJ0qkz9pJLVR1Nch1wD7AM2FlV+5Ns6x/fDuwBLgfmgJeAa07dyJ2dlks7r5IzToYzToYzTs5U5kzVcZe6JUlnIO8UlaRGGHRJakRzQR/3MgXTlmR1kr9J8kSS/Umun/ZMC0myLMk/JPnStGdZSJJzktyZ5B/7n9Mfm/ZMw5L8Rv/P+rEktyf57iUw084kzyd5bGDf9ya5N8k3++/ftARnvKn/Z/1Ikr9Mcs4URxw548Cx30pSSVacrnmaCnrHlymYtqPAb1bV24GLgWuX4IzHXA88Me0hxvhj4O6qehtwAUts3iQrgV8DZqvqHfR+sWDLdKcCYBewcWjfjcBXq2o98NX+9jTt4vgZ7wXeUVXnA08BHz3dQw3ZxfEzkmQ1cBnwzOkcpqmg0+1lCqaqqp6rqgf7H/8XvQCtnO5Ux0uyCvh54NZpz7KQJG8EfhL4U4CqOlJV/zHVoUZbDrw2yXLgbJbAPRpVdR/w4tDuzcDn+h9/DviF0znTsFEzVtVXqupof/N+eve8TM0Cn0eAPwJ+mxE3WJ5KrQV9oZcgWJKSrAXeBfz9lEcZ5VP0/kK+POU5FvODwGHgz/qXhm5N8rppDzWoqp4Ffp/emdpz9O7R+Mp0p1rQm4/dP9J/f+6U5xnnV4EvT3uIYUmuAJ6tqodP93O3FvROL0GwFCR5PfAF4Ner6j+nPc+gJO8Dnq+qB6Y9yxjLgQuBz1bVu4BvM/3LBK/Qvw69GVgHvBV4XZJfnu5UZ74kH6N3+fK2ac8yKMnZwMeAj0/j+VsL+hnxEgRJXkMv5rdV1V3TnmeE9wBXJPknepet3pvkz6c70kjzwHxVHfsO5056gV9KfgZ4uqoOV9X/AHcBPz7lmRbyr8deJbX//vkpzzNSkquB9wEfqKV3I80P0fvH++H+188q4MEk3386nry1oHd5mYKpShJ613yfqKo/nPY8o1TVR6tqVVWtpfc5/FpVLbmzyqr6F+BgkvP6uy4FHp/iSKM8A1yc5Oz+n/2lLLEf3A7YDVzd//hq4ItTnGWkJBuBG4Arquqlac8zrKoerapzq2pt/+tnHriw/3f1lGsq6P0flhx7mYIngL+oqv3Tneo47wE+SO+s96H+2+XTHuoM9hHgtiSPAO8Efne647xS/7uHO4EHgUfpfc1N/fb1JLcD3wDOSzKf5EPAJ4HLknyT3m9ofHIJzngz8Abg3v7XzvYlOOP05ll637FIkk5GU2fokvT/mUGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxP8CTqqCUBqHI38AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN80lEQVR4nO3df6zd9V3H8efLdkTZD5n2Mre22Go6tmaBDa8MXVQc4lq2UE38AzYH4pamCUw0GumyOP9YYmbwx1xg1AZrt0gghqGrSzdGNpU/JoaC/CpYdgNKL0W5iE6FP2rH2z/OwRxOz73n23Lac/vx+Uhu7v1+v5+e885t75Pv/d77PaSqkCSd+r5r2gNIkibDoEtSIwy6JDXCoEtSIwy6JDVi5bSeeNWqVbVu3bppPb0knZLuu+++56pqZtSxqQV93bp17Nu3b1pPL0mnpCT/vNgxL7lIUiMMuiQ1wqBLUiMMuiQ1wqBLUiPGBj3JriTPJnlkkeNJ8tkkc0keSnLe5MeUJI3T5Qx9N7BpieObgQ39t63ATa9+LEnSsRob9Kq6G3h+iSVbgC9Uzz3AGUnePKkBJUndTOIa+mrg4MD2fH+fJOkkmsSdohmxb+T/NSPJVnqXZTjrrLMm8NSSdHLduO0br/oxrt7x3glMcrRJnKHPA2sHttcAh0YtrKqdVTVbVbMzMyNfikCSdJwmEfQ9wBX933a5APh2VT0zgceVJB2DsZdcktwKXAisSjIP/DbwGoCq2gHsBS4B5oAXgatO1LCSpMWNDXpVXT7meAFXT2wiSdJx8U5RSWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWpEp6An2ZTkQJK5JNtHHP/eJH+V5MEk+5NcNflRJUlLGRv0JCuAG4HNwEbg8iQbh5ZdDTxaVecCFwK/n+S0Cc8qSVpClzP084G5qnqiqg4DtwFbhtYU8PokAV4HPA8cmeikkqQldQn6auDgwPZ8f9+gG4C3A4eAh4Frq+ql4QdKsjXJviT7FhYWjnNkSdIoXYKeEftqaPt9wAPAW4B3AjckecNRf6hqZ1XNVtXszMzMMY4qSVpKl6DPA2sHttfQOxMfdBVwR/XMAU8Cb5vMiJKkLroE/V5gQ5L1/R90XgbsGVrzFHARQJI3AWcDT0xyUEnS0laOW1BVR5JcA9wJrAB2VdX+JNv6x3cAnwJ2J3mY3iWa66rquRM4tyRpyNigA1TVXmDv0L4dAx8fAn52sqNJko6Fd4pKUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1olPQk2xKciDJXJLti6y5MMkDSfYn+dvJjilJGmfluAVJVgA3AhcD88C9SfZU1aMDa84APgdsqqqnkpx5guaVJC2iyxn6+cBcVT1RVYeB24AtQ2s+CNxRVU8BVNWzkx1TkjROl6CvBg4ObM/39w16K/DGJH+T5L4kV4x6oCRbk+xLsm9hYeH4JpYkjdQl6Bmxr4a2VwI/ArwfeB/wW0neetQfqtpZVbNVNTszM3PMw0qSFjf2Gjq9M/K1A9trgEMj1jxXVS8ALyS5GzgXeHwiU0qSxupyhn4vsCHJ+iSnAZcBe4bWfAn4iSQrk5wOvBt4bLKjSpKWMvYMvaqOJLkGuBNYAeyqqv1JtvWP76iqx5J8FXgIeAm4uaoeOZGDS5JeqcslF6pqL7B3aN+Ooe3rgesnN5ok6Vh4p6gkNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNaJT0JNsSnIgyVyS7Uus+9Ek30nyC5MbUZLUxdigJ1kB3AhsBjYClyfZuMi63wXunPSQkqTxupyhnw/MVdUTVXUYuA3YMmLdx4AvAs9OcD5JUkddgr4aODiwPd/f93+SrAZ+Htix1AMl2ZpkX5J9CwsLxzqrJGkJXYKeEftqaPszwHVV9Z2lHqiqdlbVbFXNzszMdBxRktTFyg5r5oG1A9trgENDa2aB25IArAIuSXKkqv5yEkNKksbrEvR7gQ1J1gNPA5cBHxxcUFXrX/44yW7gy8Zckk6usUGvqiNJrqH32ysrgF1VtT/Jtv7xJa+bS5JOji5n6FTVXmDv0L6RIa+qX3r1Y0mSjpV3ikpSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDWiU9CTbEpyIMlcku0jjn8oyUP9t28mOXfyo0qSljI26ElWADcCm4GNwOVJNg4texL4qao6B/gUsHPSg0qSltblDP18YK6qnqiqw8BtwJbBBVX1zar69/7mPcCayY4pSRqnS9BXAwcHtuf7+xbzEeArow4k2ZpkX5J9CwsL3aeUJI3VJegZsa9GLkx+ml7Qrxt1vKp2VtVsVc3OzMx0n1KSNNbKDmvmgbUD22uAQ8OLkpwD3Axsrqp/m8x4kqSuupyh3wtsSLI+yWnAZcCewQVJzgLuAD5cVY9PfkxJ0jhjz9Cr6kiSa4A7gRXArqran2Rb//gO4JPA9wOfSwJwpKpmT9zYkqRhXS65UFV7gb1D+3YMfPxR4KOTHU2SdCy8U1SSGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGtEp6Ek2JTmQZC7J9hHHk+Sz/eMPJTlv8qNKkpYyNuhJVgA3ApuBjcDlSTYOLdsMbOi/bQVumvCckqQxupyhnw/MVdUTVXUYuA3YMrRmC/CF6rkHOCPJmyc8qyRpCSs7rFkNHBzYngfe3WHNauCZwUVJttI7gwf47yQHjmnaY7MKeO4EPv4kOONkOONkOOPkLDnnNX/8qh77Bxc70CXoGbGvjmMNVbUT2NnhOV+1JPuqavZkPNfxcsbJcMbJcMbJmdacXS65zANrB7bXAIeOY40k6QTqEvR7gQ1J1ic5DbgM2DO0Zg9wRf+3XS4Avl1Vzww/kCTpxBl7yaWqjiS5BrgTWAHsqqr9Sbb1j+8A9gKXAHPAi8BVJ27kzk7KpZ1XyRknwxknwxknZypzpuqoS92SpFOQd4pKUiMMuiQ1ormgj3uZgmlLsjbJXyd5LMn+JNdOe6bFJFmR5B+SfHnasywmyRlJbk/yj/3P6Y9Ne6ZhSX6t/3f9SJJbk3z3MphpV5JnkzwysO/7ktyV5Fv9929chjNe3/+7fijJXyQ5Y4ojjpxx4NhvJKkkq07WPE0FvePLFEzbEeDXq+rtwAXA1ctwxpddCzw27SHG+CPgq1X1NuBcltm8SVYDvwLMVtU76P1iwWXTnQqA3cCmoX3bga9X1Qbg6/3tadrN0TPeBbyjqs4BHgc+frKHGrKbo2ckyVrgYuCpkzlMU0Gn28sUTFVVPVNV9/c//i96AVo93amOlmQN8H7g5mnPspgkbwB+EvgTgKo6XFX/MdWhRlsJfE+SlcDpLIN7NKrqbuD5od1bgM/3P/488HMnc6Zho2asqq9V1ZH+5j307nmZmkU+jwB/CPwmI26wPJFaC/piL0GwLCVZB7wL+PspjzLKZ+j9g3xpynMs5YeABeBP+5eGbk7y2mkPNaiqngZ+j96Z2jP07tH42nSnWtSbXr5/pP/+zCnPM84vA1+Z9hDDklwKPF1VD57s524t6J1egmA5SPI64IvAr1bVf057nkFJPgA8W1X3TXuWMVYC5wE3VdW7gBeY/mWCV+hfh94CrAfeArw2yS9Od6pTX5JP0Lt8ecu0ZxmU5HTgE8Anp/H8rQX9lHgJgiSvoRfzW6rqjmnPM8J7gEuT/BO9y1bvTfJn0x1ppHlgvqpe/g7ndnqBX05+Bniyqhaq6n+AO4Afn/JMi/nXl18ltf/+2SnPM1KSK4EPAB+q5XcjzQ/T+4/3g/2vnzXA/Ul+4GQ8eWtB7/IyBVOVJPSu+T5WVX8w7XlGqaqPV9WaqlpH73P4japadmeVVfUvwMEkZ/d3XQQ8OsWRRnkKuCDJ6f2/+4tYZj+4HbAHuLL/8ZXAl6Y4y0hJNgHXAZdW1YvTnmdYVT1cVWdW1br+1888cF7/3+oJ11TQ+z8sefllCh4D/ryq9k93qqO8B/gwvbPeB/pvl0x7qFPYx4BbkjwEvBP4nemO80r97x5uB+4HHqb3NTf129eT3Ar8HXB2kvkkHwE+DVyc5Fv0fkPj08twxhuA1wN39b92dizDGac3z/L7jkWSdDyaOkOXpP/PDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1Ij/heQfn6onhsTJAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# fig,axes = plt.subplots(nrows=5,ncols=4,figsize=(20,10))\n",
    "for j in range(20):\n",
    "    for i, item in enumerate(sitas[j][0]):\n",
    "        # print(i, item.item())\n",
    "        plt.bar(i, item.item())\n",
    "\n",
    "    # axes[i//4, i//5].show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARU0lEQVR4nO3df6zdd13H8efLlgUH4jC7ymwbW0zD1oC45WZWiSZukLSDUP8wcYuwZWqaJhtsBqJFEt1fZomIutisaWDAwsI0Y8YGKoMwjJKwpXdjbpTL5DqQXta5SwiDuITS8PaP89Ucz87t/bb33J7bT5+P5KTn+/nx/b7Pvb2v+7nfe77fm6pCktSun5h2AZKktWXQS1LjDHpJapxBL0mNM+glqXEbp13AOJdeemlt3bp12mVI0nnjscce+05VzYzrW5dBv3XrVubm5qZdhiSdN5L853J9nrqRpMYZ9JLUOINekhpn0EtS43oFfZJdSZ5OspBk/5j+y5N8KckPk7x3qH1Lki8kmU9yLMltkyxekrSyFd91k2QDcAB4C7AIHE1yuKq+OjTsu8C7gd8amX4KeE9VPZ7kp4DHknxuZK4kaQ31WdFfDSxU1TNVdRK4H9gzPKCqnq+qo8CPRtpPVNXj3fMfAPPApolULknqpU/QbwKOD20vchZhnWQrcCXw6JnOlSSdvT5BnzFtZ3QT+ySvBD4J3F5V319mzN4kc0nmlpaWzmT3kqTT6HNl7CKwZWh7M/Bs3wMkeRmDkL+vqh5cblxVHQIOAczOzvrXUKRzZOv+T696H9+8860TqERrpc+K/iiwPcm2JBcB1wOH++w8SYAPA/NV9cGzL1OSdLZWXNFX1akktwIPARuAe6rqWJJ9Xf/BJK8B5oBXAT9OcjuwA/gl4J3AU0me6Hb5J1V1ZOKvRJI0Vq+bmnXBfGSk7eDQ8+cYnNIZ9UXGn+OXJJ0jXhkrSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG9frj4Be0O356Avt4YfX7kKSz5Ipekhpn0EtS4wx6SWqcQS9JjTPoJalxvYI+ya4kTydZSLJ/TP/lSb6U5IdJ3nsmcyVJa2vFoE+yATgA7AZ2ADck2TEy7LvAu4EPnMVcSdIa6rOivxpYqKpnquokcD+wZ3hAVT1fVUeBH53pXEnS2uoT9JuA40Pbi11bH73nJtmbZC7J3NLSUs/dS5JW0ifoM6ateu6/99yqOlRVs1U1OzMz03P3kqSV9An6RWDL0PZm4Nme+1/NXEnSBPQJ+qPA9iTbklwEXA8c7rn/1cyVJE3Aijc1q6pTSW4FHgI2APdU1bEk+7r+g0leA8wBrwJ+nOR2YEdVfX/c3DV6LZKkMXrdvbKqjgBHRtoODj1/jsFpmV5zJUnnjlfGSlLjDHpJapxBL0mNM+glqXEGvSQ1zr8ZK+mCNH/5FavexxVfm59AJWvPFb0kNc6gl6TGeepG58xf/s7bVjX/PX/3qQlVIl1YXNFLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMY1d5virfs/var537zzrROqRJLWB1f0ktQ4g16SGtcr6JPsSvJ0koUk+8f0J8ldXf+TSa4a6vvDJMeSfCXJJ5K8fJIvQJJ0eisGfZINwAFgN7ADuCHJjpFhu4Ht3WMvcHc3dxPwbmC2ql4PbACun1j1kqQV9VnRXw0sVNUzVXUSuB/YMzJmD3BvDTwCXJLksq5vI/CTSTYCFwPPTqh2SVIPfYJ+E3B8aHuxa1txTFV9G/gA8C3gBPBCVX123EGS7E0yl2RuaWmpb/2SpBX0CfqMaas+Y5K8msFqfxvw88Arkrxj3EGq6lBVzVbV7MzMTI+yJEl99Hkf/SKwZWh7My89/bLcmDcD36iqJYAkDwK/Bnz8bAvWuXFg38Or3sctB6+ZQCWSVqvPiv4osD3JtiQXMfhl6uGRMYeBG7t33+xkcIrmBINTNjuTXJwkwLXA/ATrlyStYMUVfVWdSnIr8BCDd83cU1XHkuzr+g8CR4DrgAXgReDmru/RJA8AjwOngC8Dh9bihUiSxut1C4SqOsIgzIfbDg49L+CWZeb+GfBnq6hRGmtx/7+ueh+b7/z1CVQirW9eGStJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxvYI+ya4kTydZSLJ/TH+S3NX1P5nkqqG+S5I8kORrSeaT/OokX4Ak6fRWDPokG4ADwG5gB3BDkh0jw3YD27vHXuDuob6/AT5TVZcDbwTmJ1C3JKmnPiv6q4GFqnqmqk4C9wN7RsbsAe6tgUeAS5JcluRVwG8AHwaoqpNV9b3JlS9JWkmfoN8EHB/aXuza+ox5LbAEfCTJl5N8KMkrxh0kyd4kc0nmlpaWer8ASdLp9Qn6jGmrnmM2AlcBd1fVlcB/Ay85xw9QVYeqaraqZmdmZnqUJUnqo0/QLwJbhrY3A8/2HLMILFbVo137AwyCX5J0jvQJ+qPA9iTbklwEXA8cHhlzGLixe/fNTuCFqjpRVc8Bx5O8rht3LfDVSRUvSVrZxpUGVNWpJLcCDwEbgHuq6liSfV3/QeAIcB2wALwI3Dy0i3cB93XfJJ4Z6ZMkrbEVgx6gqo4wCPPhtoNDzwu4ZZm5TwCzZ1+iJGk1vDJWkhpn0EtS4wx6SWqcQS9Jjev1y1hN1hs+9oZVzX/qpqcmVImkC4EreklqnCt6aQ19/uFfXPU+rr3mPyZQiS5kruglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuO8TXED5i+/YtX7uOJr8xOoRNJ6ZNBLQ+64446pzm/GHT+9yvkvTKYOAZ66kaTmGfSS1LheQZ9kV5Knkywk2T+mP0nu6vqfTHLVSP+GJF9O8qlJFS5J6mfFoE+yATgA7AZ2ADck2TEybDewvXvsBe4e6b8N8Ld9kjQFfVb0VwMLVfVMVZ0E7gf2jIzZA9xbA48AlyS5DCDJZuCtwIcmWLckqac+Qb8JOD60vdi19R3z18AfAT8+3UGS7E0yl2RuaWmpR1mSpD76BH3GtFWfMUneBjxfVY+tdJCqOlRVs1U1OzMz06MsSVIffYJ+EdgytL0ZeLbnmDcBb0/yTQanfK5J8vGzrlaSdMb6BP1RYHuSbUkuAq4HDo+MOQzc2L37ZifwQlWdqKr3VdXmqtrazXu4qt4xyRcgSTq9Fa+MrapTSW4FHgI2APdU1bEk+7r+g8AR4DpgAXgRuHntSpYknYlet0CoqiMMwny47eDQ8wJuWWEf/wz88xlXKElaFe91I51nXvOFJ1Y1/7nf/OWJ1HGuveFjb1jV/KduempClSzvwL6HVzX/loPXTKiS/89bIEhS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS43oFfZJdSZ5OspBk/5j+JLmr638yyVVd+5YkX0gyn+RYktsm/QIkSae3YtAn2QAcAHYDO4AbkuwYGbYb2N499gJ3d+2ngPdU1RXATuCWMXMlSWuoz4r+amChqp6pqpPA/cCekTF7gHtr4BHgkiSXVdWJqnocoKp+AMwDmyZYvyRpBX2CfhNwfGh7kZeG9YpjkmwFrgQeHXeQJHuTzCWZW1pa6lGWJKmPPkGfMW11JmOSvBL4JHB7VX1/3EGq6lBVzVbV7MzMTI+yJEl99An6RWDL0PZm4Nm+Y5K8jEHI31dVD559qZKks9En6I8C25NsS3IRcD1weGTMYeDG7t03O4EXqupEkgAfBuar6oMTrVyS1MvGlQZU1akktwIPARuAe6rqWJJ9Xf9B4AhwHbAAvAjc3E1/E/BO4KkkT3Rtf1JVRyb6KiRJy1ox6AG6YD4y0nZw6HkBt4yZ90XGn7+XJJ0jXhkrSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcb2CPsmuJE8nWUiyf0x/ktzV9T+Z5Kq+cyVJa2vFoE+yATgA7AZ2ADck2TEybDewvXvsBe4+g7mSpDXUZ0V/NbBQVc9U1UngfmDPyJg9wL018AhwSZLLes6VJK2hVNXpByS/Deyqqj/ott8J/EpV3To05lPAnVX1xW7788AfA1tXmju0j70MfhoAeB3w9Ope2rIuBb6zRvueFGucjPOhRjg/6rTGyVjLGn+hqmbGdWzsMTlj2ka/Oyw3ps/cQWPVIeBQj3pWJclcVc2u9XFWwxon43yoEc6POq1xMqZVY5+gXwS2DG1vBp7tOeaiHnMlSWuozzn6o8D2JNuSXARcDxweGXMYuLF7981O4IWqOtFzriRpDa24oq+qU0luBR4CNgD3VNWxJPu6/oPAEeA6YAF4Ebj5dHPX5JX0t+anhybAGifjfKgRzo86rXEyplLjir+MlSSd37wyVpIaZ9BLUuMumKA/H27FkGRLki8kmU9yLMlt065pOUk2JPlydw3FupPkkiQPJPla9/H81WnXNCrJH3af568k+USSl6+Dmu5J8nySrwy1/UySzyX5evfvq6dZY1fTuDr/ovt8P5nkH5JcMsUSx9Y41PfeJJXk0nNRywUR9OfRrRhOAe+pqiuAncAt67ROgNuA+WkXcRp/A3ymqi4H3sg6qzXJJuDdwGxVvZ7BmxWun25VAHwU2DXSth/4fFVtBz7fbU/bR3lpnZ8DXl9VvwT8O/C+c13UiI/y0hpJsgV4C/Ctc1XIBRH0nCe3YqiqE1X1ePf8BwzCadN0q3qpJJuBtwIfmnYt4yR5FfAbwIcBqupkVX1vqkWNtxH4ySQbgYtZB9eYVNW/AN8dad4DfKx7/jHgt85lTeOMq7OqPltVp7rNRxhctzM1y3wsAf4K+COWuXh0LVwoQb8JOD60vcg6DNBhSbYCVwKPTrmUcf6awX/UH0+5juW8FlgCPtKdXvpQkldMu6hhVfVt4AMMVnUnGFx78tnpVrWsn+uui6H792enXE8fvwf807SLGJXk7cC3q+rfzuVxL5Sg730rhvUgySuBTwK3V9X3p13PsCRvA56vqsemXctpbASuAu6uqiuB/2Z9nG74P9157j3ANuDngVckecd0q2pDkvczOA1637RrGZbkYuD9wJ+e62NfKEHf5zYO60KSlzEI+fuq6sFp1zPGm4C3J/kmg1Ng1yT5+HRLeolFYLGq/venoQcYBP968mbgG1W1VFU/Ah4Efm3KNS3nv7q70dL9+/yU61lWkpuAtwG/W+vvIqFfZPCN/d+6r5/NwONJXrPWB75Qgv68uBVDkjA4rzxfVR+cdj3jVNX7qmpzVW1l8HF8uKrW1Uq0qp4Djid5Xdd0LfDVKZY0zreAnUku7j7v17LOfmE85DBwU/f8JuAfp1jLspLsYnDX3LdX1YvTrmdUVT1VVT9bVVu7r59F4Kru/+uauiCCvvsFzf/eimEe+Pt1cCuGcd4EvJPBKvmJ7nHdtIs6T70LuC/Jk8AvA38+3XL+v+6njQeAx4GnGHwtTv0S/iSfAL4EvC7JYpLfB+4E3pLk6wzeLXLnNGuEZev8W+CngM91XzsH12GN06ll/f10I0mapAtiRS9JFzKDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXufwAY1w4csHb6DgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQKUlEQVR4nO3df6zddX3H8edrrUTBOVyoE9tmraYBGpnSNKxKZjLQpAVC98eSwaYwtqVpUhSNxtWZTP5aSOackjU0DaIQiWxBljXaiQY0m4kQyg/BWph3yOyFMq4xQyPJauN7f5yvy/Fy2vtt77k99376fCQnPd/Pj+95n3t7X/dzv+d8vydVhSSpXb826QIkSQvLoJekxhn0ktQ4g16SGmfQS1Ljlk+6gFHOOeecWrNmzaTLkKQl45FHHvlRVa0Y1bcog37NmjXs379/0mVI0pKR5L+O1eehG0lqnEEvSY0z6CWpcb2CPsnmJE8nmUqyc0T/+Um+neR/k3zkROZKkhbWnEGfZBmwC9gCrAeuSbJ+1rAfAx8APnkScyVJC6jPiv5iYKqqnqmqI8DdwNbhAVX1YlU9DPz8ROdKkhZWn6BfCRwa2p7u2vroPTfJtiT7k+yfmZnpuXtJ0lz6BH1GtPW9tnHvuVW1p6o2VtXGFStGvudfknQS+gT9NLB6aHsV8HzP/c9nriRpDPqcGfswsC7JWuA54Grgj3vufz5zF4ebfmMM+3hp/vuQpJM0Z9BX1dEkNwD3AcuA26vqQJLtXf/uJG8E9gOvA36R5IPA+qr6yai5C/RcJEkj9LrWTVXtA/bNats9dP8FBodles2VJJ06nhkrSY0z6CWpcQa9JDXOoJekxhn0ktS4RfkJU5JOnTU7vzLvfTx78xVjqEQLxRW9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjfN69JJOSwfPv2De+7jgqYNjqGThuaKXpMa5otcp83d/dOW85n/4H788pkqk04sreklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJalyvoE+yOcnTSaaS7BzRnyS3dP1PJNkw1PehJAeSfDfJF5O8epxPQJJ0fHMGfZJlwC5gC7AeuCbJ+lnDtgDruts24NZu7krgA8DGqnorsAy4emzVS5Lm1GdFfzEwVVXPVNUR4G5g66wxW4E7a+BB4Owk53Z9y4HXJFkOnAk8P6baJUk99An6lcChoe3prm3OMVX1HPBJ4IfAYeClqvraqAdJsi3J/iT7Z2Zm+tYvSZpDn6DPiLbqMybJ6xms9tcCbwLOSvLeUQ9SVXuqamNVbVyxYkWPsiRJffQJ+mlg9dD2Kl55+OVYY94N/KCqZqrq58C9wDtPvlxJ0onqE/QPA+uSrE1yBoMXU/fOGrMXuLZ7980mBodoDjM4ZLMpyZlJAlwGLI2PZJGkRsz5wSNVdTTJDcB9DN41c3tVHUiyvevfDewDLgemgJeB67u+h5LcAzwKHAUeA/YsxBORJI3W6xOmqmofgzAfbts9dL+AHceY+wngE/OoUZI0D54ZK0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS43q9j34pWbPzK/Oa/+zNV4ypEklaHFzRS1LjDHpJapxBL0mNa+4YvU4f0zv/fd77WHXz742hEmlxc0UvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapwnTGmkXdsfmPc+duy+dAyVSJovV/SS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjvNbNBFx4x4Xzmv/kdU+OqRJJp4NeK/okm5M8nWQqyc4R/UlyS9f/RJINQ31nJ7knyVNJDiZ5xzifgCTp+OYM+iTLgF3AFmA9cE2S9bOGbQHWdbdtwK1DfZ8BvlpV5wNvAw6OoW5JUk99Dt1cDExV1TMASe4GtgLfGxqzFbizqgp4sFvFnwv8DHgX8KcAVXUEODK+8qXF7f4H3jLvfVx26X+OoRKdzvoculkJHBranu7a+ox5MzADfC7JY0luS3LWqAdJsi3J/iT7Z2Zmej8BSdLx9Qn6jGirnmOWAxuAW6vqIgYr/Fcc4weoqj1VtbGqNq5YsaJHWZKkPvoE/TSwemh7FfB8zzHTwHRVPdS138Mg+CVJp0ifoH8YWJdkbZIzgKuBvbPG7AWu7d59swl4qaoOV9ULwKEk53XjLuNXj+1LkhbYnC/GVtXRJDcA9wHLgNur6kCS7V3/bmAfcDkwBbwMXD+0i/cDd3W/JJ6Z1SdJWmC9Tpiqqn0Mwny4bffQ/QJ2HGPu48DGky9RkjQfXgJBkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG+cEj0pCbbrppovOlheCKXpIaZ9BLUuM8dCNJY7Jr+wPzmr9j96VjquRXuaKXpMYZ9JLUOINekhpn0EtS43wxtgEHz79g3vu44KmDY6hEp8Ibv/H4vOa/8PtvH0sdWjpc0UtS4wx6SWqcQS9JjfMYvaTxu+k35jn/pfHUIcAVvSQ1z6CXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc4zYyUtCRfeceG85j953ZNjqmTpcUUvSY0z6CWpcQa9JDWuV9An2Zzk6SRTSXaO6E+SW7r+J5JsmNW/LMljSb48rsIlSf3MGfRJlgG7gC3AeuCaJOtnDdsCrOtu24BbZ/XfCPhZdZI0AX1W9BcDU1X1TFUdAe4Gts4asxW4swYeBM5Oci5AklXAFcBtY6xbktRTn6BfCRwa2p7u2vqO+TTwUeAXx3uQJNuS7E+yf2ZmpkdZkqQ++gR9RrRVnzFJrgRerKpH5nqQqtpTVRurauOKFSt6lCVJ6qNP0E8Dq4e2VwHP9xxzCXBVkmcZHPK5NMkXTrpaSdIJ6xP0DwPrkqxNcgZwNbB31pi9wLXdu282AS9V1eGq+lhVraqqNd28B6rqveN8ApKk45vzEghVdTTJDcB9wDLg9qo6kGR7178b2AdcDkwBLwPXL1zJkqQT0etaN1W1j0GYD7ftHrpfwI459vFN4JsnXKEkaV48M1aSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrXK+iTbE7ydJKpJDtH9CfJLV3/E0k2dO2rk3wjycEkB5LcOO4nIEk6vjmDPskyYBewBVgPXJNk/axhW4B13W0bcGvXfhT4cFVdAGwCdoyYK0laQH1W9BcDU1X1TFUdAe4Gts4asxW4swYeBM5Ocm5VHa6qRwGq6qfAQWDlGOuXJM2hT9CvBA4NbU/zyrCec0ySNcBFwEOjHiTJtiT7k+yfmZnpUZYkqY8+QZ8RbXUiY5K8FvgS8MGq+smoB6mqPVW1sao2rlixokdZkqQ++gT9NLB6aHsV8HzfMUlexSDk76qqe0++VEnSyegT9A8D65KsTXIGcDWwd9aYvcC13btvNgEvVdXhJAE+Cxysqk+NtXJJUi/L5xpQVUeT3ADcBywDbq+qA0m2d/27gX3A5cAU8DJwfTf9EuB9wJNJHu/a/qqq9o31WUiSjmnOoAfognnfrLbdQ/cL2DFi3rcYffxeknSKeGasJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1rlfQJ9mc5OkkU0l2juhPklu6/ieSbOg7V5K0sOYM+iTLgF3AFmA9cE2S9bOGbQHWdbdtwK0nMFeStID6rOgvBqaq6pmqOgLcDWydNWYrcGcNPAicneTcnnMlSQsoVXX8AckfApur6i+67fcBv1tVNwyN+TJwc1V9q9u+H/hLYM1cc4f2sY3BXwMA5wFPz++pHdM5wI8WaN/jYo3jsRRqhKVRpzWOx0LW+NtVtWJUx/IekzOibfZvh2ON6TN30Fi1B9jTo555SbK/qjYu9OPMhzWOx1KoEZZGndY4HpOqsU/QTwOrh7ZXAc/3HHNGj7mSpAXU5xj9w8C6JGuTnAFcDeydNWYvcG337ptNwEtVdbjnXEnSAppzRV9VR5PcANwHLANur6oDSbZ3/buBfcDlwBTwMnD98eYuyDPpb8EPD42BNY7HUqgRlkad1jgeE6lxzhdjJUlLm2fGSlLjDHpJatxpE/RL4VIMSVYn+UaSg0kOJLlx0jUdS5JlSR7rzqFYdJKcneSeJE91X893TLqm2ZJ8qPs+fzfJF5O8ehHUdHuSF5N8d6jtN5N8Pcn3u39fP8kau5pG1fm33ff7iST/nOTsCZY4ssahvo8kqSTnnIpaTougX0KXYjgKfLiqLgA2ATsWaZ0ANwIHJ13EcXwG+GpVnQ+8jUVWa5KVwAeAjVX1VgZvVrh6slUB8Hlg86y2ncD9VbUOuL/bnrTP88o6vw68tap+B/gP4GOnuqhZPs8rayTJauA9wA9PVSGnRdCzRC7FUFWHq+rR7v5PGYTTyslW9UpJVgFXALdNupZRkrwOeBfwWYCqOlJV/zPRokZbDrwmyXLgTBbBOSZV9W/Aj2c1bwXu6O7fAfzBqaxplFF1VtXXqupot/kgg/N2JuYYX0uAvwc+yjFOHl0Ip0vQrwQODW1PswgDdFiSNcBFwEMTLmWUTzP4j/qLCddxLG8GZoDPdYeXbkty1qSLGlZVzwGfZLCqO8zg3JOvTbaqY/qt7rwYun/fMOF6+vgz4F8nXcRsSa4Cnquq75zKxz1dgr73pRgWgySvBb4EfLCqfjLpeoYluRJ4saoemXQtx7Ec2ADcWlUXAT9jcRxu+H/dce6twFrgTcBZSd472arakOTjDA6D3jXpWoYlORP4OPDXp/qxT5eg73MZh0UhyasYhPxdVXXvpOsZ4RLgqiTPMjgEdmmSL0y2pFeYBqar6pd/Dd3DIPgXk3cDP6iqmar6OXAv8M4J13Qs/91djZbu3xcnXM8xJbkOuBL4k1p8Jwm9hcEv9u90Pz+rgEeTvHGhH/h0CfolcSmGJGFwXPlgVX1q0vWMUlUfq6pVVbWGwdfxgapaVCvRqnoBOJTkvK7pMuB7EyxplB8Cm5Kc2X3fL2ORvWA8ZC9wXXf/OuBfJljLMSXZzOCquVdV1cuTrme2qnqyqt5QVWu6n59pYEP3/3VBnRZB371A88tLMRwE/mkRXIphlEuA9zFYJT/e3S6fdFFL1PuBu5I8Abwd+JvJlvOrur827gEeBZ5k8LM48VP4k3wR+DZwXpLpJH8O3Ay8J8n3Gbxb5OZJ1gjHrPMfgF8Hvt797OxehDVOppbF99eNJGmcTosVvSSdzgx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1Lj/A5jUilGOM1HmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(20):\n",
    "    for i, item in enumerate(sita_hats[j][0]):\n",
    "        # print(i, item.item())\n",
    "        plt.bar(i, item.item())\n",
    "        # plt.savefig('' + str(i) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1396, 0.0239, 0.0263,  ..., 0.0221, 0.1890, 0.0608],\n",
       "        [0.0582, 0.0133, 0.0250,  ..., 0.0225, 0.2440, 0.0666],\n",
       "        [0.0729, 0.0186, 0.0256,  ..., 0.0234, 0.2535, 0.0801],\n",
       "        ...,\n",
       "        [0.0858, 0.0352, 0.0254,  ..., 0.0125, 0.2291, 0.0738],\n",
       "        [0.1421, 0.0133, 0.0249,  ..., 0.0042, 0.3370, 0.0403],\n",
       "        [0.1147, 0.0223, 0.0327,  ..., 0.0232, 0.1612, 0.0616]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "z = torch.softmax(z_train, dim=1).detach()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "loss = nn.MSELoss()\n",
    "# input = torch.randn(1, 5, requires_grad=True)\n",
    "input = torch.tensor([1.0, 2.0, 3.0])\n",
    "target = torch.tensor([1.0, 2.0, 3.0])\n",
    "# target = torch.tensor([2, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d5a93703cc22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "target = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "input = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "loss = nn.MSELoss()\n",
    "loss(input, target)\n",
    "# a = torch.cat((target, input), 0)\n",
    "# torch.softmax(a, dim=1)\n",
    "# target.add_(input)\n",
    "# target + input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/08dvs7wn6t5_jn5sxxsly14c0000gn/T/ipykernel_6248/503755249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "target = torch.randn(1,1000)\n",
    "nn.MSELoss(target, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10635299, 0.12664911, 0.04766772, 0.02757354, 0.05697789,\n",
       "       0.05699783, 0.03527545, 0.11439677, 0.07075627, 0.04713076,\n",
       "       0.05768883, 0.12192362, 0.02987717, 0.03248456, 0.06824748])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_z = []\n",
    "for i in range(topic_num):\n",
    "    term_z.append(sum(z[:,i]))\n",
    "term_z = np.array(term_z)\n",
    "# z = torch.softmax(z, dim=1).detach().cpu().numpy()\n",
    "# z_a\n",
    "term_z/sum(term_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11836473, 0.13411619, 0.0479399 , 0.        , 0.06292735,\n",
       "       0.06392176, 0.02147614, 0.12511938, 0.0829907 , 0.04677167,\n",
       "       0.06448372, 0.13043394, 0.00752335, 0.01465324, 0.07927793])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#\n",
    "ms = MinMaxScaler()\n",
    " \n",
    "#\n",
    "ms.fit_transform(term_z.T).T[0]/sum(ms.fit_transform(term_z.T).T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\err09\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\err09\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_graph.grid_graph(z_train, topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\err09\\OneDrive\\Project\\Study\\01_topicmodel\\FLmodel_NTM\\trainer.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.data = np.array(data)\n",
      "c:\\Users\\err09\\OneDrive\\Project\\Study\\01_topicmodel\\FLmodel_NTM\\trainer.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n",
      "c:\\Users\\err09\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP(test) = 16427.260\n"
     ]
    }
   ],
   "source": [
    "dataloader_test  = trainer.DataLoader(test_data, bow_vocab, batch_size, shuffle=False)\n",
    "pp_test = trainer.compute_perplexity(ntm_model, dataloader_test)\n",
    "print(\"PP(test) = %.3f\" % (pp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ./topwords_e9999.txt\n",
      "Topic 0:          \n",
      "Topic 1:          \n",
      "Topic 2:          \n",
      "Topic 3:          \n",
      "Topic 4:          \n",
      "Topic 5:          \n",
      "Topic 6:          \n",
      "Topic 7:          \n",
      "Topic 8:          \n",
      "Topic 9:          \n",
      "Topic 10:          \n",
      "Topic 11:          \n",
      "Topic 12:          \n",
      "Topic 13:          \n",
      "Topic 14:          \n",
      "[[ 7.1993372e-06 -1.6985834e-06  1.4108712e-06 ... -2.9334951e-05\n",
      "  -4.0255563e-05 -2.6012287e-05]\n",
      " [ 8.6775499e-06  9.9357158e-07 -7.0794440e-06 ...  1.0917112e-05\n",
      "  -6.3899970e-06  3.1294971e-05]\n",
      " [ 1.8935904e-06 -9.3974234e-07 -4.7839849e-06 ... -2.4753861e-05\n",
      "  -2.1822005e-05 -3.7616854e-05]\n",
      " ...\n",
      " [ 1.6902081e-06 -2.5305806e-06  3.5194039e-06 ... -6.1711791e-05\n",
      "  -3.5940255e-05 -2.2357799e-05]\n",
      " [-8.5179781e-06 -4.4525768e-06  6.6510302e-06 ...  3.5564182e-05\n",
      "   4.3075372e-05  4.3023956e-05]\n",
      " [-9.3602875e-07 -1.3951710e-05  3.7516397e-06 ... -5.1770667e-06\n",
      "   2.0776852e-05 -1.0609367e-05]]\n"
     ]
    }
   ],
   "source": [
    "logdir = \"./\"\n",
    "ntm_model.print_topic_words(bow_vocab, os.path.join(logdir, 'topwords_e%d.txt' % 9999))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== # 1, Topic : 13, p : 14.7881 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                               rupture disc          \n",
      "\n",
      "===== # 2, Topic : 5, p : 13.1272 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                              m                                                                         e                                                                            a  b c  d   f g      mm            \n",
      "\n",
      "===== # 3, Topic : 13, p : 36.0295 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :          a                                                     a                                                                                                                                                                 \n",
      "\n",
      "===== # 4, Topic : 13, p : 30.4343 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                 \n",
      "\n",
      "===== # 5, Topic : 5, p : 17.2528 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                             \n",
      "\n",
      "===== # 6, Topic : 5, p : 14.8859 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                            \n",
      "\n",
      "===== # 7, Topic : 13, p : 32.8914 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                )                                                                                                                                                                             ) )                                                                  \n",
      "\n",
      "===== # 8, Topic : 13, p : 26.7727 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                                                                                                                             \n",
      "\n",
      "===== # 9, Topic : 9, p : 14.9461 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                 mm                                                               \n",
      "\n",
      "===== # 10, Topic : 13, p : 28.9964 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                          mm          mm                                                    mm                                                                      \n",
      "\n",
      "===== # 11, Topic : 13, p : 16.5901 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                    m       )                                      mm                                                                                    FlowMole Ltd  UTILX Corporation           cm           q kPa       \n",
      "\n",
      "===== # 12, Topic : 6, p : 11.7688 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                             m  m             sec                        kg             cm                                                                                         \n",
      "\n",
      "===== # 13, Topic : 13, p : 45.0019 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                          \n",
      "\n",
      "===== # 14, Topic : 13, p : 50.8215 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                     \n",
      "\n",
      "===== # 15, Topic : 13, p : 23.4859 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                 \n",
      "\n",
      "===== # 16, Topic : 5, p : 11.8566 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                        (              o                                 \n",
      "\n",
      "===== # 17, Topic : 13, p : 22.4723 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                           m                                              \n",
      "\n",
      "===== # 18, Topic : 13, p : 21.6669 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                               \n",
      "\n",
      "===== # 19, Topic : 9, p : 12.3452 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                         vol.(                                                                            vol.                                o                     \n",
      "\n",
      "===== # 20, Topic : 13, p : 13.0970 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                             \n",
      "\n",
      "===== # 21, Topic : 13, p : 38.5442 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                           \n",
      "\n",
      "===== # 22, Topic : 13, p : 24.9896 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                     \n",
      "\n",
      "===== # 23, Topic : 13, p : 26.6438 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                \n",
      "\n",
      "===== # 24, Topic : 9, p : 16.9661 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                                                         H                                                                 Global Positioning System                 \n",
      "\n",
      "===== # 25, Topic : 13, p : 33.5510 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                        \n",
      "\n",
      "===== # 26, Topic : 13, p : 33.8143 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                   \n",
      "\n",
      "===== # 27, Topic : 13, p : 41.9110 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                          IVIV                                                                                                                                                                            \n",
      "\n",
      "===== # 28, Topic : 7, p : 23.1904 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                \n",
      "\n",
      "===== # 29, Topic : 6, p : 14.6896 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                                           \n",
      "\n",
      "===== # 30, Topic : 13, p : 20.9745 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                         \n",
      "\n",
      "===== # 31, Topic : 13, p : 32.8408 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                \n",
      "\n",
      "===== # 32, Topic : 13, p : 25.6339 %\n",
      "Topic words : , , , , , , , , , \n",
      "Input :                                                                                                                                                                                                      \n"
     ]
    }
   ],
   "source": [
    "trainer.lasy_predict(ntm_model, dataloader_test, bow_vocab, num_example=50, n_top_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "55ee831d6af1ae9706634b6e60f3ce072bf89ecf7e73a290d2e6ee7cdf841cc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
