{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTM\n",
    "author : nakamuraleon  \n",
    "email : nakamuraleon0552@gmail.com  \n",
    "\n",
    "### 1. Table of contents\n",
    "\n",
    "### 2. Dataset\n",
    "\n",
    "### 3. Reference\n",
    "https://qiita.com/m3yrin/items/3a8157f65eb9862ac21e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import grid_graph\n",
    "import gensim\n",
    "import glob\n",
    "import trainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2, 3. Loading File (Load dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfile(period):\n",
    "    f = open('./Dataset/'+str(period)+'_docsfile.txt', 'rb')\n",
    "    dicts = pickle.load(f)\n",
    "    return dicts\n",
    "\n",
    "def splitdata(dicts):\n",
    "    test_valid_size = int(len(dicts) * 0.1)\n",
    "    test_data  = dicts[:test_valid_size]\n",
    "    valid_data = dicts[test_valid_size : test_valid_size*2]\n",
    "    train_data = dicts[test_valid_size*2 :]\n",
    "    return train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 2000\n",
    "dicts = loadfile(period)\n",
    "dicts = dicts[:500]\n",
    "bow_vocab = gensim.corpora.Dictionary(dicts)\n",
    "bow_vocab_size = len(bow_vocab)\n",
    "train_data, valid_data, test_data = splitdata(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:199: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  torch.nn.init.kaiming_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 1000 \n",
    "topic_num = 15\n",
    "batch_size = 32\n",
    "ntm_model = trainer.NTMEstimator(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num, l1_strength=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:211: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.data = np.array(data)\n",
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:212: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1  ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 2188.0782\n",
      "Test epoch : 1 Average loss: 2495.7283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/08dvs7wn6t5_jn5sxxsly14c0000gn/T/ipykernel_7796/2372893990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, valid_data, bow_vocab, batch_size, n_epoch)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_test_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mpp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PP(train) = %.3f, PP(valid) = %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py\u001b[0m in \u001b[0;36mcompute_perplexity\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdata_bow_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bow_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m#loss += loss_function(recon_batch, data_bow, mu, logvar).detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/ntm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/PredNTM/ntm.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0me1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc21\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ntm_model, z_train, z_valid = ntm_model.fit(train_data, valid_data, bow_vocab, batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vocab = gensim.corpora.Dictionary(dicts)\n",
    "bow_vocab_size = len(bow_vocab)\n",
    "hidden_dim = 1000\n",
    "topic_num = 15\n",
    "batch_size = 32\n",
    "ntm_model = trainer.NTMEstimator(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num, l1_strength=0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:170: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  torch.nn.init.kaiming_uniform(m.weight)\n",
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:179: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.data = np.array(data)\n",
      "/Users/nakamurareon/Library/CloudStorage/OneDrive-Personal/PredNTM/trainer.py:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n",
      "/Users/nakamurareon/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1759e-11, 1.0140e-18, 2.0393e-19, 1.0418e-24, 1.2098e-15, 9.9998e-01,\n",
      "         2.8157e-23, 4.8728e-29, 6.0457e-20, 8.8956e-16, 8.0873e-18, 7.6977e-14,\n",
      "         1.9210e-05, 5.1432e-17, 2.1127e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0425, 0.0611, 0.0674, 0.0385, 0.0333, 0.0478, 0.0371, 0.1147, 0.1018,\n",
      "         0.0984, 0.0464, 0.0936, 0.0469, 0.1000, 0.0704]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 2101.4947\n",
      "Test epoch : 1 Average loss: 1719.2348\n",
      "PP(train) = 20830.188, PP(valid) = 19495.021\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0420e-20, 1.3733e-43, 4.1421e-17, 7.0960e-27, 1.8266e-03, 1.1954e-11,\n",
      "         2.4577e-30, 0.0000e+00, 2.4632e-25, 1.8964e-25, 4.1880e-32, 4.5594e-19,\n",
      "         5.1087e-05, 9.4366e-31, 9.9812e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0602, 0.0669, 0.1076, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 2058.0023\n",
      "Test epoch : 2 Average loss: 1717.0748\n",
      "PP(train) = 20445.207, PP(valid) = 19022.256\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3682e-42, 0.0000e+00, 2.6625e-43, 0.0000e+00, 1.6135e-12, 9.9987e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6587e-36, 8.1008e-17,\n",
      "         1.3472e-04, 4.0600e-41, 9.5433e-21]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0425, 0.0611, 0.0674, 0.0385, 0.0333, 0.0478, 0.0371, 0.1147, 0.1018,\n",
      "         0.0984, 0.0464, 0.0936, 0.0469, 0.1000, 0.0705]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1991.7610\n",
      "Test epoch : 3 Average loss: 1716.3109\n",
      "PP(train) = 19532.725, PP(valid) = 18374.672\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0681e-28, 0.0000e+00, 8.1275e-43, 0.0000e+00, 4.5363e-01, 5.4607e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5274e-43, 2.6790e-27, 2.9890e-04,\n",
      "         5.0620e-08, 0.0000e+00, 2.6031e-31]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0449, 0.0564, 0.0562, 0.0395, 0.0555, 0.0569, 0.0618, 0.1152, 0.0725,\n",
      "         0.0994, 0.0566, 0.0789, 0.0477, 0.0830, 0.0755]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1954.3791\n",
      "Test epoch : 4 Average loss: 1718.2908\n",
      "PP(train) = 18585.238, PP(valid) = 17807.355\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2425e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9137e-01, 1.7741e-17,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0656e-33, 8.3965e-03, 4.3145e-19,\n",
      "         5.1855e-17, 0.0000e+00, 2.3573e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0483, 0.0422, 0.0381, 0.0957, 0.0657, 0.1058, 0.1082, 0.0451,\n",
      "         0.0933, 0.0675, 0.0600, 0.0455, 0.0624, 0.0769]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1936.9195\n",
      "Test epoch : 5 Average loss: 1724.6527\n",
      "PP(train) = 17851.922, PP(valid) = 17864.287\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.8726e-01, 0.0000e+00, 4.7644e-44, 0.0000e+00, 1.4797e-10, 5.1951e-14,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2750e-41, 1.2740e-02, 3.1097e-11,\n",
      "         1.2205e-21, 0.0000e+00, 8.4709e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0912, 0.0660, 0.0299, 0.1016, 0.0439, 0.0453, 0.0375, 0.1025, 0.0680,\n",
      "         0.0767, 0.0775, 0.1095, 0.0679, 0.0482, 0.0344]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1932.0570\n",
      "Test epoch : 6 Average loss: 1725.5029\n",
      "PP(train) = 17248.998, PP(valid) = 17726.939\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0554e-14, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9998e-01, 6.0367e-25,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8455e-37, 3.1135e-22, 5.7946e-16,\n",
      "         1.7314e-05, 0.0000e+00, 8.6400e-16]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1926.3991\n",
      "Test epoch : 7 Average loss: 1721.7955\n",
      "PP(train) = 16795.553, PP(valid) = 17436.537\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0061e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1793e-01, 1.9275e-13,\n",
      "         3.1666e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3730e-07, 1.1830e-13,\n",
      "         1.6611e-02, 0.0000e+00, 5.6546e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0642, 0.0471, 0.0725, 0.0458, 0.0741, 0.0677, 0.1081, 0.1004, 0.0541,\n",
      "         0.0688, 0.0651, 0.0585, 0.0498, 0.0683, 0.0556]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1921.0188\n",
      "Test epoch : 8 Average loss: 1720.6809\n",
      "PP(train) = 16527.105, PP(valid) = 17588.014\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9677e-10, 0.0000e+00, 2.8026e-44, 0.0000e+00, 9.9314e-01, 5.9192e-15,\n",
      "         1.4015e-40, 0.0000e+00, 0.0000e+00, 5.2829e-43, 6.5858e-10, 2.4978e-09,\n",
      "         6.8571e-03, 0.0000e+00, 1.4347e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0423, 0.0382, 0.0955, 0.0657, 0.1062, 0.1082, 0.0452,\n",
      "         0.0940, 0.0671, 0.0601, 0.0457, 0.0621, 0.0769]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1915.9267\n",
      "Test epoch : 9 Average loss: 1716.4574\n",
      "PP(train) = 16289.788, PP(valid) = 17434.773\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0520e-15, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2201e-03, 2.4839e-19,\n",
      "         1.2336e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5070e-21, 3.5341e-13,\n",
      "         5.2291e-02, 0.0000e+00, 9.4449e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0798, 0.0458, 0.1028, 0.0509, 0.0593, 0.0671, 0.1042, 0.0921, 0.0603,\n",
      "         0.0543, 0.0615, 0.0557, 0.0516, 0.0708, 0.0439]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1912.3042\n",
      "Test epoch : 10 Average loss: 1713.3371\n",
      "PP(train) = 16089.124, PP(valid) = 17318.045\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.5993e-17, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.5156e-01, 2.1121e-19,\n",
      "         8.0461e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4406e-13, 2.6221e-14,\n",
      "         1.9143e-01, 0.0000e+00, 5.7010e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0478, 0.0527, 0.0505, 0.0430, 0.0808, 0.0667, 0.0952, 0.1028, 0.0512,\n",
      "         0.0924, 0.0638, 0.0594, 0.0489, 0.0640, 0.0807]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1910.5552\n",
      "Test epoch : 11 Average loss: 1712.9490\n",
      "PP(train) = 15989.591, PP(valid) = 17410.000\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5938e-18, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9970e-01, 3.0007e-21,\n",
      "         9.3796e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9200e-18, 1.6201e-10,\n",
      "         2.9878e-04, 0.0000e+00, 1.5456e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00006\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1909.0962\n",
      "Test epoch : 12 Average loss: 1711.0834\n",
      "PP(train) = 15823.288, PP(valid) = 17245.445\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9240e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4359e-08, 1.6717e-23,\n",
      "         3.2181e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1931e-28, 2.0909e-21,\n",
      "         5.5265e-01, 0.0000e+00, 4.4735e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0625, 0.0604, 0.0897, 0.0570, 0.0500, 0.0672, 0.0752, 0.0867, 0.0690,\n",
      "         0.0740, 0.0552, 0.0554, 0.0563, 0.0688, 0.0727]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00012\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1907.2707\n",
      "Test epoch : 13 Average loss: 1710.7424\n",
      "PP(train) = 15696.229, PP(valid) = 17154.998\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.1334e-23, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2835e-01, 1.5487e-28,\n",
      "         1.1940e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5209e-25, 2.8444e-16,\n",
      "         1.6455e-01, 0.0000e+00, 1.0710e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0492, 0.0520, 0.0521, 0.0431, 0.0809, 0.0668, 0.0971, 0.1030, 0.0513,\n",
      "         0.0899, 0.0642, 0.0594, 0.0489, 0.0645, 0.0776]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00021\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1905.0743\n",
      "Test epoch : 14 Average loss: 1711.7523\n",
      "PP(train) = 15566.596, PP(valid) = 17248.539\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3002e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7547e-08, 8.1672e-27,\n",
      "         3.7956e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8643e-25, 3.6607e-18,\n",
      "         2.6131e-08, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00038\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1902.7668\n",
      "Test epoch : 15 Average loss: 1709.7150\n",
      "PP(train) = 15332.947, PP(valid) = 16978.971\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7266e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5320e-11, 5.5057e-35,\n",
      "         9.8700e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7732e-37, 2.2473e-31,\n",
      "         2.5931e-01, 0.0000e+00, 7.4069e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0726, 0.0516, 0.0979, 0.0536, 0.0555, 0.0675, 0.0915, 0.0902, 0.0641,\n",
      "         0.0620, 0.0591, 0.0559, 0.0538, 0.0703, 0.0543]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00068\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1900.4382\n",
      "Test epoch : 16 Average loss: 1709.6600\n",
      "PP(train) = 15140.922, PP(valid) = 16975.975\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6211e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8672e-02, 3.5865e-33,\n",
      "         1.1550e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5034e-36, 5.8138e-21,\n",
      "         2.6967e-05, 0.0000e+00, 9.3130e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0788, 0.0448, 0.0985, 0.0495, 0.0624, 0.0672, 0.1080, 0.0938, 0.0585,\n",
      "         0.0547, 0.0626, 0.0562, 0.0509, 0.0706, 0.0434]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00122\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1897.7465\n",
      "Test epoch : 17 Average loss: 1709.6625\n",
      "PP(train) = 14967.579, PP(valid) = 17019.094\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7515e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8397e-12, 6.8782e-32,\n",
      "         3.7050e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0831e-35, 6.7943e-28,\n",
      "         1.8778e-02, 0.0000e+00, 9.8122e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0811, 0.0449, 0.1039, 0.0505, 0.0598, 0.0670, 0.1063, 0.0922, 0.0597,\n",
      "         0.0530, 0.0618, 0.0556, 0.0512, 0.0708, 0.0423]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00219\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1895.3601\n",
      "Test epoch : 18 Average loss: 1707.4363\n",
      "PP(train) = 14701.714, PP(valid) = 16771.863\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2166e-12, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4920e-39,\n",
      "         2.4903e-11, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00395\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1892.0874\n",
      "Test epoch : 19 Average loss: 1707.2936\n",
      "PP(train) = 14489.999, PP(valid) = 16792.398\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3050e-11, 3.0651e-39,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4013e-45, 9.8139e-36,\n",
      "         5.1962e-10, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.00711\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1888.7631\n",
      "Test epoch : 20 Average loss: 1706.0420\n",
      "PP(train) = 14242.773, PP(valid) = 16676.805\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0323e-11, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8278e-36,\n",
      "         1.6947e-02, 0.0000e+00, 9.8305e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0811, 0.0448, 0.1039, 0.0504, 0.0598, 0.0670, 0.1065, 0.0922, 0.0597,\n",
      "         0.0529, 0.0618, 0.0556, 0.0512, 0.0708, 0.0422]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.01281\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1885.8359\n",
      "Test epoch : 21 Average loss: 1704.4707\n",
      "PP(train) = 13938.165, PP(valid) = 16510.975\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6934e-16, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8055e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.02305\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1880.8666\n",
      "Test epoch : 22 Average loss: 1704.2910\n",
      "PP(train) = 13678.657, PP(valid) = 16519.227\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3313e-17, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.7455e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.04150\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1877.1095\n",
      "Test epoch : 23 Average loss: 1701.6236\n",
      "PP(train) = 13330.747, PP(valid) = 16242.552\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.6132e-16, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.6259e-09, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.07473\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1871.7318\n",
      "Test epoch : 24 Average loss: 1701.5578\n",
      "PP(train) = 13011.266, PP(valid) = 16254.235\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3458e-28, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6402e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.13455\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1867.0996\n",
      "Test epoch : 25 Average loss: 1698.8779\n",
      "PP(train) = 12639.498, PP(valid) = 16013.519\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0576e-16, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8410e-09, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.24223\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1861.9121\n",
      "Test epoch : 26 Average loss: 1696.8432\n",
      "PP(train) = 12272.278, PP(valid) = 15817.661\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1004e-20, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.1961e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.43609\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1855.4301\n",
      "Test epoch : 27 Average loss: 1695.3453\n",
      "PP(train) = 11874.525, PP(valid) = 15679.666\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5171e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.5102e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 0.78519\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1849.0216\n",
      "Test epoch : 28 Average loss: 1693.4998\n",
      "PP(train) = 11448.701, PP(valid) = 15512.622\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2603e-15, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1563e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 1.41372\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1841.7444\n",
      "Test epoch : 29 Average loss: 1690.2756\n",
      "PP(train) = 11042.518, PP(valid) = 15237.918\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7814e-29, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.5637e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 2.54544\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1834.2807\n",
      "Test epoch : 30 Average loss: 1688.7900\n",
      "PP(train) = 10599.947, PP(valid) = 15095.722\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6534e-27, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.0276e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 4.58280\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1826.8731\n",
      "Test epoch : 31 Average loss: 1685.4627\n",
      "PP(train) = 10168.596, PP(valid) = 14805.678\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5383e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6581e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 8.25125\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1819.3389\n",
      "Test epoch : 32 Average loss: 1681.8975\n",
      "PP(train) = 9789.102, PP(valid) = 14519.961\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3770e-27, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.5783e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 14.85641\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1811.5040\n",
      "Test epoch : 33 Average loss: 1679.9506\n",
      "PP(train) = 9375.649, PP(valid) = 14319.901\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2842e-29, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.6507e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 26.74652\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1803.3906\n",
      "Test epoch : 34 Average loss: 1677.1236\n",
      "PP(train) = 9024.741, PP(valid) = 14104.724\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1936e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7210e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 48.15530\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1796.5294\n",
      "Test epoch : 35 Average loss: 1674.4018\n",
      "PP(train) = 8680.160, PP(valid) = 13863.026\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9994e-26, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.3696e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 86.69641\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1789.4618\n",
      "Test epoch : 36 Average loss: 1670.8920\n",
      "PP(train) = 8371.889, PP(valid) = 13617.457\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2786e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1412e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 156.09438\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1783.2331\n",
      "Test epoch : 37 Average loss: 1668.4740\n",
      "PP(train) = 8075.081, PP(valid) = 13413.537\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9707e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7916e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 281.02982\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1778.1406\n",
      "Test epoch : 38 Average loss: 1666.2186\n",
      "PP(train) = 7816.246, PP(valid) = 13253.258\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3896e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.4500e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 505.94394\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1774.3486\n",
      "Test epoch : 39 Average loss: 1664.0371\n",
      "PP(train) = 7569.144, PP(valid) = 13049.229\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3456e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2023e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 910.97644\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1771.0354\n",
      "Test epoch : 40 Average loss: 1661.3160\n",
      "PP(train) = 7355.713, PP(valid) = 12877.352\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5932e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6712e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 1640.24817\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1772.7461\n",
      "Test epoch : 41 Average loss: 1660.5707\n",
      "PP(train) = 7158.698, PP(valid) = 12833.854\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3693e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8124e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 2953.11621\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1780.5304\n",
      "Test epoch : 42 Average loss: 1658.1051\n",
      "PP(train) = 6972.018, PP(valid) = 12645.503\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8382e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.5093e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 5316.18652\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1797.4161\n",
      "Test epoch : 43 Average loss: 1657.5754\n",
      "PP(train) = 6815.937, PP(valid) = 12635.027\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.8067e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.8915e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 9569.68750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1833.8361\n",
      "Test epoch : 44 Average loss: 1657.6178\n",
      "PP(train) = 6679.115, PP(valid) = 12643.767\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0722e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.5413e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.002, l1 strength = 17223.79688\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1902.7788\n",
      "Test epoch : 45 Average loss: 1653.2791\n",
      "PP(train) = 6542.993, PP(valid) = 12300.939\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4795e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.8281e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.003, l1 strength = 30993.64258\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 2026.6666\n",
      "Test epoch : 46 Average loss: 1653.4344\n",
      "PP(train) = 6419.488, PP(valid) = 12317.702\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4723e-38, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.2826e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.005, l1 strength = 55737.91016\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 2253.1480\n",
      "Test epoch : 47 Average loss: 1651.7803\n",
      "PP(train) = 6304.787, PP(valid) = 12204.433\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2204e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.7959e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.008, l1 strength = 100130.45312\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 2654.3393\n",
      "Test epoch : 48 Average loss: 1645.2766\n",
      "PP(train) = 6219.039, PP(valid) = 11783.297\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6681e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.013, l1 strength = 179471.18750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 3356.3447\n",
      "Test epoch : 49 Average loss: 1644.0855\n",
      "PP(train) = 6119.650, PP(valid) = 11715.785\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.1900e-36, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.0856e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.020, l1 strength = 320575.68750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 4558.4498\n",
      "Test epoch : 50 Average loss: 1639.1436\n",
      "PP(train) = 6045.090, PP(valid) = 11386.842\n",
      "Writing to ./topicwords/0-topwords_e50.txt\n",
      "Topic 0: 河川汚濁 重量構造物 案内ローラ 鉄筋篭 ケミカル物質 周辺支持地盤 通路方向 掘削方向後方 側壁部分 セルフレベリング形下地形成組成物\n",
      "Topic 1: 縦孔 ＡＥ減水剤種類 セグメント継手面 戸建住宅 箇所数 回転羽 添加材 化学物質粒子 組立形態 セメント構成物\n",
      "Topic 2: 参照 配置 構造 位置 形態 実施形態 作動 側 斜視図 最大\n",
      "Topic 3: ネジ鉄筋 技術的課題 立面図 摺動 角度 端面 機械加工 瞬時 中高層建築物 重度\n",
      "Topic 4: 上方 発明 力 ｃ 一般 利点 性能 底面 アンカー 間隔\n",
      "Topic 5: 排水Ｗ 土木学会 自動システム 重し材 ＢＥＴ値 高性能減水剤 キー型枠片 進入領域 バルコニーＵ 該円フランジ\n",
      "Topic 6: 摺動面 円弧部 規則類 耐力増強 発行 該試験片 カッター駆動部 ユーティリティ階 modulus 前記縦部材\n",
      "Topic 7: － Ａ ２つ 柱 下部 上記構成 位置 剛性 多角形 スパン\n",
      "Topic 8: 砂 効果 側方 端部 ｂ 金属板 費用 貫通孔 単一 製造\n",
      "Topic 9: 鉛直荷重 貫入開始 音漏れ ステンレス板 前記構造体 遅れ サッシ等 中間部セグメント タンク 高強度モルタル\n",
      "Topic 10: アンモニアガス発濃度 ターンテーブルｄ 基準 ガスクロマトグラフ 電磁シールド空間 乾燥スケジュール 電磁波遮蔽機能 ビット 直角 補強部\n",
      "Topic 11: 各種装置 最適充填性 什器 受渡し 定着長 遮へい材カバー チューブ周辺地 傾胴角度範囲 道路路盤 セグメント分割\n",
      "Topic 12: 施工 内側 Ａ 設置 幅 現場 説明図 図面 エア ねじ部\n",
      "Topic 13: 内側ベース型枠 断熱性 スイッチ 攪乱状態 鉄筋部分 荷台 平行リンク運動 中空部Ｓ 漏水発生位置検出方式 中空ＰＣａ柱製造\n",
      "Topic 14: 所定 他 等 種 荷重 接合構造 符号 工期 両側 鋼材\n",
      "\n",
      "===== # 1, Topic : 14, p : 18.3313 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 鋼管矢板 継手部材 鋼管矢板本管 鋼管矢板Ａ 鋼管矢板基礎 実施形態 継手 継手部 継手空間Ｃ 継手空間 Ｐ－Ｐ型継手部材 図 矢板 貫通孔 せん断耐力 継手部材表面 係止部 コンクリート 上記継手部材 請求項 円形鋼管 充填材 空間 Ｐ－Ｐ型継手 Ｐ－Ｐ 矢板同士 付着強度 鋼管矢板本管外面 基礎 矢板Ａ 発明 継手部材同士 上記 上記実施形態 先端部 形態 鋼管矢板側 鋼管矢板同士 表面 鋼管矢板Ｂ 管軸方向 上記鋼管矢板本管 連結 鋼管矢板Ａ側 鋼管矢板Ａ同士 当該鋼管矢板Ａ 連結構造 規模橋梁基礎 部材等 実施 該鋼管矢板 充填 内部空間 凹凸 継手構造 閉鎖形状 継手空間Ｃ側 構成 継手部構造 当該継手空間Ｃ せん断強度 例 Ａ 小径鋼管 Ｐ－Ｔ 突起 規模 凹凸模様 剛性 コンクリート注入 充填作業 部分 面 組側 当該空間 該継手部材 せん断力 圧縮強度 頂版コンクリート 排土 位置 孔 隙間 先端 該継手空間 当該表面 単位継手部材長当たり スリット 外面 状態 特徴 地盤 効果 橋梁基礎 Ｔ形 板 スリット付き小径鋼管 実施例 閉鎖状態 洗浄 課題 m 先端部同士 管軸 帯板 上記コンクリート充填作業 土砂 モルタル tf 複数個 図面 組 パイプ 適用 所定 符号 別 Ｌ－Ｔ 平面図 側面図 貫通孔形成部分Ｃ 基本構成 試験体 コンクリート注入作業 せん断強度向上 付着 方向 上記形状 説明 掘削 mm ～ 一方向 ｂ 左側 右側 先端位置 同士 程度 複数 外 コンクリート打設 コンクリート打設作業 円形 小径 位置Ｂ 横方向 横節状 上記課題 模様 部分Ｂ 曲線状 チェッカー状 スパイラル状 対向面側 円弧状等 水平方向 上下方向 突出方向 付着切れ 位置合わせ 反対側 対称位置 外力 本数 手段 存在 設時 片側 各組 同一 他 作用 ａ 開口断面形状 スリット付きボックス 当該隙間 板厚 内外面 洗浄作業 粘着力 摩擦力 小判形 互い所定間隔 技術分野 技術 複数本 互い 外径 ずれ変形 簡略化 向上 可能性 室内 室内実験 m程度 沈設回数 ずれ 矩形 施工 一般 kgf cm 低下 度合い 方策 方法 範囲 漏れ 溶接 全面 つまり 目的 各々 回数 左右 両側 他方 両方 列 スタッドジベル ガイド 距離 許容 変位 所望 アングル 設位置 記載\n",
      "\n",
      "===== # 2, Topic : 14, p : 17.4352 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 搬送ベルト コン コン受入台車 側 トンネル形成装置 切羽側 上側ベルト 図 請求項 トンネル 口側 トンネル形成方法 形成方法 駆動モータ コン装置 スチールフォーム形成装置 ベルト延伸装置 破砕機 段差部 下側ベルト トンネル形成 受取装置 装置 形成装置 移動フレーム 掘削機 ズリ テール装置 形成 トンネル内面 下側 ローラ 部分 駆動装置 ガイドレール 口 搬送 端 発明 流側 端部 上記請求項 トンネル内壁 ベルト洗浄装置 側面図 モータ 搬入 当該搬送ベルト コン搬入側部分 機枠 上側 搬送路 ｂ コン搬入 ～図 実施形態 切羽側側部分 外 断面図 概略側面図 上側部分 方法 ズリ搬出 ベルト洗滌装置 垂装置 縦断面図 実施 形態 上記 尺 状態 搬出 破砕装置 案内ローラ 反対側 後端側 拡大側断面図 掘削装置 洗浄装置 駆動 上昇端 部分断面図 コンクリート 効率 図示左側 新規 課題 手段 後流側 左方側 右方側 搬送位置 掘削機等 受け取り 移動 説明 アジテータカー 目的 転 特徴 － イ ロ ゲート 搬送路横 コン受け取り 拡大断面図 掘削 部分正面図 傾斜部分 トンネル外 付着生コン 逆転 上方 左端 左側 左端部 所定部分 図示左側下部 技術 資材 別 符号 進行 次 ブレード 図面 右端 泥 表面 ａ 壁下地 上記実状 昇り傾斜面 風管等 発明者等 図示右側 支持ローラ コンクリート面 コンクリート壁面 壁パネル 効率アップ 効率化 壁材 利用分野 安全通路 新鮮空気 排気ガス 量 運搬量 空間 壁面 上方空間 右側 水平状 可能性 位置 産業 施工 外部 口外 トロッコ 軌道 人 排出 送風 設備 落石 原因 補強 物資 完成 検討 コンベア 片側 下地 残留 直近 通常 変化 中心 外側 山 発破 岩盤 先端 関係 式 作業 往き 戻り 上下 除去 複数 １つ 一方向 内部 両側 上面 程度 他 機能 ストーレッジ 間隔 ジグザグ 距離 分 他所 タンク 箇所 ワイヤー 効果 通り 構成 作動 格納\n",
      "\n",
      "===== # 3, Topic : 14, p : 18.5964 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 型枠 内型枠 外型枠 Ｕ型ＰＣ部材 ＰＣ梁型枠 型枠装置 コンクリート 底板 下端部 外側板 上端部 上記内側板 内側板 内側板 側板 間隔 底部 Ｕ コンクリートＣ 閉止部 Ｕ型空間 断面逆Ｕ字状 間隔可変手段 断面Ｕ字状 上記内型枠 上記底板 上記型枠装置 外型枠設置工程 図 開口部側 開口部 逆Ｕ字状 平板型枠 上記外側板 梁 ＰＣ 型枠設置工程 型工程 端部 開放部分 上記ＰＣ梁型枠 ＰＣ部材 スターラップ 隙間 該内型枠 実施形態 外側板間 サポート部材 型枠作業 Ｌ形型枠 該ＰＣ梁型枠 Ｕ字状 木口板 外側板 可変 板 補助筋 充填性 型枠費 内型枠設置工程 上記閉止部 側方 下端部間距離 流動性 部材 型作業 製造方法 該Ｕ型ＰＣ部材 上端開口部 状態 Ｕ型内部 ＰＣ製品 幅 凹凸部 所定間隔 拡大断面図 発明 定盤 上記間隔可変手段 上端開放部分 成形端部 上記隙間 Ａ部 Ｂ部 脚部 コンクリート打設工程 上記 上端 下端部間隔 折曲代 ＲＣ梁 底側 上記延長部材 梁端 製造 断面図 打設コンクリート 内側 延長部材 製品 上記スターラップ 空間部 対向面 ジャッキ アングル材 伸縮ロッド 梁幅 脱型工程 方向 折曲部分 所定間隔Ｓ 部分 梁床 盤 上記弾性発泡体 設置 先端部 下端 下端主筋 上方 下方 リブ 外方 両端部 必要性 生産性 Ｈ型鋼 幅寸法 上記木口板 流動性コンクリート 上面 方法 梁せい 梁施工 下端部分 当該下端部間距離 該コンクリート 一般部分 充填 外側 上記凹部内側 該コンクリートＣ 成形端部位置 上記外側板間 流動抵抗 隙間δ 設置作業 配筋 斜視図 距離 凹凸板 脱 上下逆 凹部内側 設空間Ｓ 該サポート部材 課題 他方 手段 構造部材 複数種類 肉厚 複数 補強リブ 実施 形態 凹凸状 上記ジャッキ 空間 該延長部材 柱部材 工程 弾性発泡体 作業床 説明 転用 現場 左右 上側 ａ ｂ 設 許容性 付着性 床面 一定 面 所定幅Ｓ 該繋ぎ部材 当該部分 所定 成形方法 鉄筋作業 等 先端部下面 ～図 床スラブ 寸法変化 側面 養生 一定寸法 両側 底辺 設側 境界面 目的 特徴 設量 各種 硬化 例 表面 裏面 置部材 代わり コストダウン 効果 構造体 押圧部材 チェッカープレート等 天井クレーン等 ＲＣ造建物 壁等 打撃等 一般 種類 構造 通路抵抗 実用新案登録 耐アルカリ耐油性 剛性ゴム系 閉塞状態 上下方向 セット状態 鉄筋組立 技術分野 技術 柱 コストアップ 圧力差 内部 コスト 添付図面 フランジ 上方フランジ 圧力 セット 結束線 位置 挿通部分 ストックヤード 組立 図面 プレキャストコンクリート 底面 一体 数 １つ 号 レベル 設始点 終点 巣 設作業 空気 一体化 段階 設時 ウェブ ｃ 内方 下面 荷重 力 送り込み 外周 両者 衝撃 剥離 下部 最後 余裕 構成 該底板 解体 ワンタッチ 別 場所 種々 隅々 符号\n",
      "\n",
      "===== # 4, Topic : 12, p : 13.5700 %\n",
      "Topic words : 施工, 内側, Ａ, 設置, 幅, 現場, 説明図, 図面, エア, ねじ部\n",
      "Input : アンモニアガス セメント アンモニアガス発生量 コンクリート アンモニアガス発生 アンモニアガス発生セメント アンモニアガス濃度 アンモニアガス発生コンクリート 発生 骨材 請求項 実施形態 発生量 アンモニアガス発生ポテンシャル コンクリート試験体 アンモニア発生量 シリカフューム 活性ガス 図 アンモニアガス濃度試験 アンモニアガス量 窒素ガス ガス発生量 石膏 添加 添加用 アンモニガス発生量 ｇ アンモニア発生レベル 普通ポルトランドセメント 細骨材 アンモニアガス低減レベル アンモニウムイオン 鉄滓スラグ 試験体 環境 水 発明 シリカフューム添加 セメントペースト セメント量 混和材 アンモニア濃度 アルカリ環境下 柱試験体 種類 セメント添加用 アンモニアガス発生試験装置 低減効果 イオン 試験 方法 表 μｇ 使用量 量 イオン交換材 構成材料 アンモニアガス試験装置 影響 溶出量 アンモニウムイオン溶出量 Ｅ アンモニア溶出レベル セメント試験体 アンモニアガス低減効果 効果 発生条件 アンモニガス発生 アンモニアガス発濃度 コンクリート工事 添加材 添加鉄滓スラグ総量 ポルトランドセメント ～ 窒化物 密閉容器 実施 形態 重量比 セメント重量 砕砂 Ａ 養生 コンクリート表面 建設工事 セメントペースト試験体 材令 ＡＥ減水剤 材料レベル 程度 チタン石膏 アンモニアガス吸着シート 一定量 粉体シリカフューム 蒸気養生 コンクリート材料 製造工程 高性能減水剤 美術館 総量 ガス濃度予測曲線 単位重量 評価方法 Ｆ工場 低減 重量比率 セメントＡ 美術品 評価 原料 特徴 ｍ ポテンシャル グラフ コンクリート用構成材料 セメント添加用石膏種類 水セメント比 角柱試験体 説明 セメント構成物 石灰岩砕砂 期間 窒素量 減水剤 比表面積 添加物 含有量 細骨材種類 容器 シリカフューム添加分 セメントＢ セメントＣ セメント質量 窒素含有量 含有窒素量 天然 Ｂ Ｃ 技術 差 目的 記載 他 関係 反応 環境条件 建築物 アンモニア成分 カルシウムイオン コンクリート内部 粘土原料 微粒分 吸着材料 イオン交換物質 低減技術 ＡＥ減水剤種類 ケミカルフィルタ シリカ系 検知管 混和材料 説明図 陽イオン 石灰岩砕砂)、アンモニウムイオン レベル 含有レベル 使用 通常使用量 長期 現状 排 一般 変化 メーカー 対象 Ｄ ｐｐｂ 気 低減対策 イオン調整作用 対策 強度 通常 ｄａｙ 空気質環境 アルカリ溶液下 アルカリ溶液 ｇ程度 一定レベル 養生条件 単位重量当たり 水処理 製造 シリカフューム自身 美術館建設 製造工場 工場 石灰岩 条件 吸着処理 製造原料 天然石膏 純水 建設コスト 早強ポルトランドセメント カルシウム水和物 微量 博物館 課題 特性 別 特定 要因 ℃ 実験 傾向 範囲 デシケータ 分 インピンジャ 水中 所定 部屋 分解 タイプ 段階 抑制効果 適用効果 測定方法 測定装置 焼成工程 調合条件 石灰岩砕石 フィルタ 測定 処理 砂 表面積 技術分野 乾燥程度 耐久性 発現性 可能性 ｄａｙ程度 美術館運営上 海砂 山砂 換気設備 設備 物質 硬化 問題点 硬化熱 産業副産物 岩種 上限値 設計強度 工期短縮 内部 調合 抑制 油絵 空気 存在 確立 所要 観点 開館 壁紙 ボード 意匠 機能 塗料 実状 手段 原因 配慮 ａ ｂ ｃ ｄ ｅ 凝結 各種 実情 設後 原材料 精度 キーポイント 知見 川砂 有機物 水洗い Ｓ Ｇ ボンベ ポンプ 流量 弁 一緒 純粋 イオンクロマト ｍｍｇ 上述 ナトリウム カリウム 性質 常温 ℃、 Ｘ Ｙ ％～ 進行 図面 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 12.2896 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : ケミカルフィルタ ケミカル物質 酸性ガス 袋体 アルカリ性ガス 吸着除去対象 活性炭 吸着剤 図 空気清浄装置 実施形態 通気性 仕切材 複数種類 種類 通気方向 発明 ケミカルフィルタユニット 酸性ガス除去専用 アルカリ性ガス除去専用 アルカリ性ガス除去用 トレー 収容体 請求項 吸着効率 吸着除去 ｂ 圧力損失 分解斜視図 酸性ガス除去用 形態 双方 空気 多段 ガス状 収容体内 状態 微粒子除去 ケミカル物質専用 イオン交換繊維 寿命 上記 小型化 メッシュ状 空調機 送風動力 樹脂製 活性炭量 収容形態 クリーンルーム 清浄度 セルボックス 厚み寸法 部品点数 厚み イオン物質 前記吸着剤 前記収容体 説明 実施 自体 コストダウン 既存 性能フィルタ ＨＥＰＡフィルタ 方向 製造工場等 液晶パネル製造工場 等 活性炭等 液晶パネル 仕切板 半導体 一般 フィルタチャンバ 課題 同等 ｍｍＡｑ 単独 送風機 他 設備費 樹脂成型品 有機物等 布等 小形化 簡略化 透視図 寸法 量 運転費とも 中和化学反応 アニオン成分 カチオン成分 所要スペース 設置スペース 技術分野 技術 微粒子 汚染対策 汚染 直列 直列多段 対策 使用枚数 処理条件 容量アップ 最少限 周知 平形 ファンチャンバ 後段 逆 手段 分 範囲 実験 削減 変更 効果 図面 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 15.7209 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 梁型枠 床型枠 金属板 型枠工法 込型枠 ランナー金物 フィラー金物 鋼製 鋼製打込型枠 前記金属板 鋼製折板製 床型枠どうし 請求項 発明 電磁波 鋼製折板 型枠解体 合板型枠 込型枠工法 遮蔽機能 上記実施形態 遮蔽体 実施形態 上記 隙間 鋼板製 接続部 図 遮蔽効果 該金属板 縁部 金属製メッシュ 上部 どうし 梁 実施 キーストンプレート 床 形態 施工性 状態 内法寸法Ｗ 接続 フラットタイプデッキ 面接触状態 タップビス 現場 金属薄板等 隙間Ｓ コストアップ 説明 ｃ 遮蔽対策 電磁波遮蔽機能 外法寸法Ｗ 有用性 ｃ部 コンクリート 符号 建物 課題 ｂ 間隙 他方 鋼板等 溝部 上面 鋼板 効果 導電性材料 上記事情 メッシュ状 ビス止め 携帯電話等 施工精度 厚みＷ 施工手間 Ｕ字状 技術分野 技術 下面側 補強リブ 天然資材 使用節減 電子機器 特殊用途 拡大図 チャンネル材 下面 手間 スラブ 組み立て 機運 パソコン 普及 天井 壁 観点 支障 改善 手段 ａ 相互 工場 所定 規格 余地 変更 他 銅板 自体 逆 おかず 増大 作業 図面\n",
      "\n",
      "===== # 7, Topic : 14, p : 17.7473 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 防波ブロック 固定端部 杭 防波板部 図 コンクリート板体 貫通孔 前記防波ブロック 杭頭 上下寸法 進行方向 前記防波板部 上下 前記固定端部 防波堤 コンクリート板 構造 水平方向 状態 岸側 接続ブロック 側断面図 略Ｔ字状 固定端部同士 沖側 部分 側端縁 カーテンウォール 同士 海底面 断面図 ＩＩ－ＩＩ線矢視断面図 ＩＩＩ－ＩＩＩ線矢視断面図 ＩＶ－ＩＶ線矢視断面図 発明 波浪 請求項 カーテンウォール 構成 ｘ方向 貫通孔同士 両側端縁 上下方向 前記杭 寸法 側 突出状態 固定端部Ｌ 左右対称 実施 施工性 前記コンクリート板体 側方 形状 コンクリートＣ 形態 地盤Ｇ 部分拡大図 海底地盤Ｇ 該コンクリート板体 前記貫通孔 鋼矢板 複数 交互 水表面 Ｉ－Ｉ線矢視断面図 接続部分 進行方向側 堤体 ａ 接合部 施工現場 施工費用 耐久性 上方 隙間 参照 海底地盤 低減化 上記実施 平面視略ロ字状 該貫通孔 符号 Ｌ リングジョイント 防食対策 該防波ブロック 上下寸法Ｌ 反対方向 説明 特徴 記載 空間 鋼管 周囲 上述 技術 課題 施工 平面図 軟弱地盤 面 正面図 前記波浪 カーテンウォール式 接合構造 等 手段 上部 図面 ｂ 下方 中央 組み合わせ 効果 信頼性 水底面 直交状態 重量構造物 港湾等 軽量化 上記課題 工場等 クレーン等 収縮モルタルＭ 符号Ａ 符号Ｂ 技術分野 外力条件 重量 挿通スペース 構築方法 支持層 タイロッド ターンバックル 種 沈下 懸念 腐食 強度 弱点 事情 該杭 該上下 プレキャストコンクリートブロック 四つ 状況 下部 下端 捨て石 最初 下段 向き 手順 個々 凹凸 他 例\n",
      "\n",
      "===== # 8, Topic : 14, p : 15.9815 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 汚染土壌 処理装置 地下躯体 地下水 集水口 集水槽 浄化処理 孔パネル 図 地下階 浄化施設 建物 請求項 地上階 汚染物質 地盤 発明 実施形態 孔 浄化 処理 上記実施形態 該地下階 透水性充填材 浄化方法 浄化設備 集水 形態 前記建物 前記処理装置 既存建物 土壌 該集水槽 躯体 処理水 上記 ～図 概要図 実施 浄化対象 鋼管 地下躯体自体 屋外 浄化処理作業 微生物処理 前記集水口 該建物 任意 用途 パネル 地下階部分 施工手順 施設 流動性 水溶性 説明 上記施設 透水性樹脂等 拡大断面図 前記 ボーリング孔 要部断面図 地上 有機塩素系化合物 要部拡大図 有機塩素系化合物等 上記構成 コンクリート製パネル 方法 等 平面図 化学物質 課題 符号 掘削 ピット 外部 ～ 位置 管路 吸引管 案内管 重金属等 平板状 珪砂等 メッシュ状 格子状 スリット状 対象 基礎 構成例 ベタ基礎 杭基礎 技術分野 技術 油類 詳細後述 活性炭吸着 貫通状態 背面側 フィルタ層 例 杭 目詰まり 周知 溶出 揚水 費用 規模 方策 手段 ｂ ｃ ｄ とも 工場 事務所 側面 底面 排水 公知 曝気 径程度 管状 一体 土砂 閉塞 砕石 先端 ネジ キャップ 目的 所定 構造 最適 効果 完了 早期 支障 広がり 範囲 流入 図面\n",
      "\n",
      "===== # 9, Topic : 14, p : 17.8182 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 掘削孔 切羽面 トンネル 地 チューブ 改良体 図 トンネル断面 地山 モルタル充填式長尺フェイスボルト 掘削 断面 安定化構造 注入材 樹脂材 注入材Ｍ 工法 壁工法 尺 進方向 立断面図 トンネル掘削 改良効果 安定化工法 尺フェイスボルト 切羽 トンネルＴ 前記チューブ 安定化 効果 発明 内部 掘削工法 先進工法 Ｇ 前記地山 ｘ方向 前記切羽面 前記トンネル 口径 強度改良効果 短縮化 ロックボルト 適用性 構造 尺モルタル充填式フェイスボルト 残長 ボルト長 せん断強度 充填材Ｍ 設置等 天端面 方向 断面トンネル等 進方向前方 前記注入材 該掘削孔内部 引き抜き耐力 変位抑制効果等 上半 実施 切羽断面積 トンネル断面Ｓ 径 形態 モルタル 掘削方向後方 縦断図 軟弱地山 安定化方法 搬送作業 側断面図 方法 技術 課題 地山性状 行掘削 ～ 下 部分 工期 隙間 軟弱地 粘着力 摩擦力 モルタル等 拡大立断面図 注入式フォアボーリング トンネル支保構造体 工事費 請求項 前方 力学特性 先進掘削坑 安定性 前記チューブ内部 Ａ方向 掘削対象箇所 壁部 基端部 説明 程度 壁 分 作業 性状 基端部側 チューブ周辺 水抜き効果 補助工法 チューブ周辺地 適用 天井部 岩機等 後 通常 参照 坑内作業 作業環境 手段 記載 特徴 図面 樹脂製 位置 状態 増強 本数 進行 変更 工費 削減 自立性 搬送 加背数 上記課題 上記実施 技術分野 地質 形状 コンクリートｃ 地質確認 事前補強 配分回数 扁平形状 施工 施工機械 盤下げ インバートコンクリートＩＣ 浸透領域 後方 中点線 所定寸法 周辺 周知 一般 限界 区間 右 左 ｂ 上述 撤去 二つ 応力 悪影響 移動 事情 mm 底面 複数 直径 コーティング ポンプ パターン 構成 両者 工程 例 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 17.5189 %\n",
      "Topic words : 所定, 他, 等, 種, 荷重, 接合構造, 符号, 工期, 両側, 鋼材\n",
      "Input : 屋根 タンク屋根 タンク内壁 防護部材 周縁端部 タンク内壁側 スカート部 図 エアレイジング方法 タンク セグメント部材 エアレイジング用 屋根骨 屋根板 略半円筒状 周面 タンク底部 実施形態 シール部材 請求項 タンクＴ 側 水平方向 発明 底部 タンク上部 部材 タンク開口部 コーキング剤 外方 曲面 先端 上昇 ｂ 周縁 ドーム状 軸方向 略円筒状 方向 周縁下部 状態 接触 タンク底部付近 技術 形状 半円状 概略側面図 多角筒状 エアレイジング 拡大図 略多角筒状 継ぎ目部 損傷 付近 継ぎ目部分 予備浮上 先端付近 液密 周方向 パイプ部材 金属部材 説明 空気圧 架台 複数 同士 空間 バランス 実施 形態 セグメント 穴 両端縁部 ～ 課題 摩擦 抵抗 所定 略円筒形状 水平 Ｒ状 布状 メッシュ状 径方向外方 外径 Ａ－Ａ矢視図 Ｔ 多角形状 下側 一端側 他端側 セグメント縦目地 空気 浮上 継ぎ目 壁面 値 距離 延 効果 ～図 下部外周 部分 程度浮上 技術分野 下 気密 問題点 上記課題 気密性能 形鋼 ＳＴＫ鋼材 所定間隔 一端 柔軟性 程度 摩擦抵抗 一般 関係 溝 移動 恐れ 確認 目的 手段 建造 角形 配慮 間隙 板材 骨組み 上面 ８つ 継目 溶接 角 強度 コスト 平板 ボルト 材質 球状 引っ掛かり 図面 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1809e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0939e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.029, l1 strength = 569729.68750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 5962.1815\n",
      "Test epoch : 1 Average loss: 1191.4539\n",
      "PP(train) = 3691.721, PP(valid) = 3850.420\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.3856e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.041, l1 strength = 1006166.81250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 9279.7498\n",
      "Test epoch : 2 Average loss: 1180.9759\n",
      "PP(train) = 3546.057, PP(valid) = 3693.820\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6052e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5003e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.056, l1 strength = 1762721.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 14652.9801\n",
      "Test epoch : 3 Average loss: 1179.1876\n",
      "PP(train) = 3377.658, PP(valid) = 3587.145\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5390e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.072, l1 strength = 3057251.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 23214.6508\n",
      "Test epoch : 4 Average loss: 1172.4840\n",
      "PP(train) = 3261.032, PP(valid) = 3471.989\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0790e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7428e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.090, l1 strength = 5242904.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 36636.7195\n",
      "Test epoch : 5 Average loss: 1170.7272\n",
      "PP(train) = 3199.507, PP(valid) = 3421.341\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0039e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.3250e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.110, l1 strength = 8879121.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 57297.3547\n",
      "Test epoch : 6 Average loss: 1170.5021\n",
      "PP(train) = 3155.331, PP(valid) = 3391.418\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.3693e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.132, l1 strength = 14831203.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 88462.0234\n",
      "Test epoch : 7 Average loss: 1168.6116\n",
      "PP(train) = 3147.729, PP(valid) = 3377.153\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6052e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.8572e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.155, l1 strength = 24398830.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 134406.5656\n",
      "Test epoch : 8 Average loss: 1169.1614\n",
      "PP(train) = 3114.715, PP(valid) = 3367.228\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0065e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6563e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.178, l1 strength = 39496500.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 200571.3031\n",
      "Test epoch : 9 Average loss: 1168.3104\n",
      "PP(train) = 3124.604, PP(valid) = 3370.846\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.1210e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.199, l1 strength = 62925176.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 293757.1312\n",
      "Test epoch : 10 Average loss: 1168.5010\n",
      "PP(train) = 3123.687, PP(valid) = 3376.982\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.3055e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.221, l1 strength = 98784704.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 422515.8187\n",
      "Test epoch : 11 Average loss: 1168.7874\n",
      "PP(train) = 3143.943, PP(valid) = 3397.637\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6520e-33, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.244, l1 strength = 152766640.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 596317.3875\n",
      "Test epoch : 12 Average loss: 1169.4292\n",
      "PP(train) = 3175.650, PP(valid) = 3429.975\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.7947e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.267, l1 strength = 232463008.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 824542.5750\n",
      "Test epoch : 13 Average loss: 1171.7021\n",
      "PP(train) = 3184.014, PP(valid) = 3455.130\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.6744e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.289, l1 strength = 348290688.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1117128.1250\n",
      "Test epoch : 14 Average loss: 1172.4016\n",
      "PP(train) = 3228.599, PP(valid) = 3494.538\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0256e-36, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.312, l1 strength = 513951200.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1482708.4000\n",
      "Test epoch : 15 Average loss: 1173.9201\n",
      "PP(train) = 3281.472, PP(valid) = 3546.416\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9236e-44, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.334, l1 strength = 746026816.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1924364.8500\n",
      "Test epoch : 16 Average loss: 1176.4342\n",
      "PP(train) = 3304.353, PP(valid) = 3585.606\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.9567e-43, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.358, l1 strength = 1066925184.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 2445156.4000\n",
      "Test epoch : 17 Average loss: 1178.5924\n",
      "PP(train) = 3356.005, PP(valid) = 3643.260\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.383, l1 strength = 1500606976.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 3034558.2500\n",
      "Test epoch : 18 Average loss: 1181.0197\n",
      "PP(train) = 3426.418, PP(valid) = 3711.745\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.411, l1 strength = 2074655616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 3675308.7000\n",
      "Test epoch : 19 Average loss: 1183.8939\n",
      "PP(train) = 3492.959, PP(valid) = 3781.629\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.445, l1 strength = 2812308736.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 4329778.5500\n",
      "Test epoch : 20 Average loss: 1187.6315\n",
      "PP(train) = 3543.924, PP(valid) = 3841.020\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.469, l1 strength = 3722843136.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 4936755.6000\n",
      "Test epoch : 21 Average loss: 1191.0399\n",
      "PP(train) = 3646.917, PP(valid) = 3944.299\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.496, l1 strength = 4847098880.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 5484389.7000\n",
      "Test epoch : 22 Average loss: 1195.0349\n",
      "PP(train) = 3745.258, PP(valid) = 4044.186\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.522, l1 strength = 6193662976.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 5921829.3000\n",
      "Test epoch : 23 Average loss: 1199.8309\n",
      "PP(train) = 3843.611, PP(valid) = 4147.489\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.548, l1 strength = 7777174528.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 6217343.8000\n",
      "Test epoch : 24 Average loss: 1205.5751\n",
      "PP(train) = 3986.913, PP(valid) = 4299.753\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.572, l1 strength = 9590194176.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 6342585.4000\n",
      "Test epoch : 25 Average loss: 1212.2033\n",
      "PP(train) = 4159.689, PP(valid) = 4485.142\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.605, l1 strength = 11628182528.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 6286984.6000\n",
      "Test epoch : 26 Average loss: 1219.7102\n",
      "PP(train) = 4313.225, PP(valid) = 4650.401\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.623, l1 strength = 13782609920.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 6021768.6000\n",
      "Test epoch : 27 Average loss: 1227.9625\n",
      "PP(train) = 4527.015, PP(valid) = 4876.071\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.640, l1 strength = 16133785600.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 5634001.8000\n",
      "Test epoch : 28 Average loss: 1236.6383\n",
      "PP(train) = 4805.577, PP(valid) = 5169.146\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.671, l1 strength = 18660048896.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 5146683.7000\n",
      "Test epoch : 29 Average loss: 1245.9209\n",
      "PP(train) = 5073.465, PP(valid) = 5451.735\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.701, l1 strength = 21120004096.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 4570977.5000\n",
      "Test epoch : 30 Average loss: 1256.1041\n",
      "PP(train) = 5383.941, PP(valid) = 5784.367\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.719, l1 strength = 23419119616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 3955105.4000\n",
      "Test epoch : 31 Average loss: 1266.3172\n",
      "PP(train) = 5789.340, PP(valid) = 6205.140\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.741, l1 strength = 25641385984.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 3376672.9500\n",
      "Test epoch : 32 Average loss: 1276.0822\n",
      "PP(train) = 6267.415, PP(valid) = 6696.744\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.776, l1 strength = 27657787392.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 2840127.9000\n",
      "Test epoch : 33 Average loss: 1285.5446\n",
      "PP(train) = 6802.644, PP(valid) = 7233.494\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.805, l1 strength = 29118582784.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 2330700.1500\n",
      "Test epoch : 34 Average loss: 1294.4546\n",
      "PP(train) = 7346.683, PP(valid) = 7778.181\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.843, l1 strength = 30036213760.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1895993.5000\n",
      "Test epoch : 35 Average loss: 1302.0822\n",
      "PP(train) = 7901.883, PP(valid) = 8333.398\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.862, l1 strength = 30189176832.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1518521.0000\n",
      "Test epoch : 36 Average loss: 1308.2620\n",
      "PP(train) = 8456.940, PP(valid) = 8882.014\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.886, l1 strength = 29937135616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1207429.5250\n",
      "Test epoch : 37 Average loss: 1313.4686\n",
      "PP(train) = 8985.015, PP(valid) = 9405.302\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.906, l1 strength = 29197051904.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 966058.8875\n",
      "Test epoch : 38 Average loss: 1317.7053\n",
      "PP(train) = 9454.980, PP(valid) = 9871.242\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.915, l1 strength = 28084510720.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 772603.8125\n",
      "Test epoch : 39 Average loss: 1320.9455\n",
      "PP(train) = 9841.614, PP(valid) = 10252.601\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.945, l1 strength = 26849177600.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 619414.9875\n",
      "Test epoch : 40 Average loss: 1323.4844\n",
      "PP(train) = 10155.512, PP(valid) = 10561.895\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.961, l1 strength = 25143134208.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 500576.4188\n",
      "Test epoch : 41 Average loss: 1325.3848\n",
      "PP(train) = 10410.868, PP(valid) = 10814.684\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.965, l1 strength = 23288903680.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 403472.6688\n",
      "Test epoch : 42 Average loss: 1326.7063\n",
      "PP(train) = 10609.403, PP(valid) = 11013.312\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.973, l1 strength = 21511426048.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 322612.7062\n",
      "Test epoch : 43 Average loss: 1327.6624\n",
      "PP(train) = 10764.993, PP(valid) = 11167.432\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.983, l1 strength = 19756115968.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 256673.7594\n",
      "Test epoch : 44 Average loss: 1328.2255\n",
      "PP(train) = 10869.898, PP(valid) = 11270.399\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.987, l1 strength = 18011334656.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 207613.5719\n",
      "Test epoch : 45 Average loss: 1328.6509\n",
      "PP(train) = 10958.768, PP(valid) = 11355.198\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6726e-42, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.992, l1 strength = 16380030976.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 166663.3406\n",
      "Test epoch : 46 Average loss: 1328.7734\n",
      "PP(train) = 11018.796, PP(valid) = 11414.012\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 14842259456.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 137352.0719\n",
      "Test epoch : 47 Average loss: 1328.7972\n",
      "PP(train) = 11054.013, PP(valid) = 11448.909\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.7998e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 13427365888.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 111994.4406\n",
      "Test epoch : 48 Average loss: 1328.6748\n",
      "PP(train) = 11065.921, PP(valid) = 11462.183\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.7698e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 12137481216.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 90792.7125\n",
      "Test epoch : 49 Average loss: 1328.5350\n",
      "PP(train) = 11063.916, PP(valid) = 11461.854\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 10964709376.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 74566.7906\n",
      "Test epoch : 50 Average loss: 1328.2069\n",
      "PP(train) = 11060.129, PP(valid) = 11460.128\n",
      "Writing to ./topicwords/1-topwords_e50.txt\n",
      "Topic 0: 河川汚濁 重量構造物 通路方向 案内ローラ 孔 特定 別 周辺支持地盤 繰り返し 粘性\n",
      "Topic 1: 縦孔 ＡＥ減水剤種類 セグメント継手面 上述 環境汚染 治具 部 土砂 所定量排出 組立形態\n",
      "Topic 2: 参照 配置 位置 構造 実施形態 形態 斜視図 上部 最大 Ｄ\n",
      "Topic 3: ネジ鉄筋 端面 粉体 質量 継ぎ面 減圧 固化処理杭造成工法 曲線 モルタル類充填完了状態 断面コ字状\n",
      "Topic 4: 上方 ｃ 発明 参照 力 利点 一般 性能 底面 間隔\n",
      "Topic 5: 排水Ｗ 土木学会 左右方向 ２つ 引き抜き動作 相互 次 重量 投入分解工程 段階\n",
      "Topic 6: 破損 地震 開口 円弧部 内面 構造 各階 複数孔 省力化 ユーティリティ階\n",
      "Topic 7: － Ａ 位置 ２つ 柱 上記構成 下部 剛性 床構造 把持部材\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 量 費用 金属板\n",
      "Topic 9: 鉛直荷重 特開昭 貫入開始 種々 音漏れ 中間部セグメント 鉛直方向剛性 部屋 滑り出し 号\n",
      "Topic 10: 基準 ターンテーブルｄ アンモニアガス発濃度 直角 電磁シールド空間 ガスクロマトグラフ 乾燥スケジュール 電磁波遮蔽機能 ビット 最適\n",
      "Topic 11: 什器 各種装置 受渡し 最適充填性 耐震性能 端面 連結 ニ 下流側 ディルドリン等\n",
      "Topic 12: 施工 内側 Ａ 図面 説明図 設置 現場 幅 範囲 目的\n",
      "Topic 13: 所定間隔 技術分野 別 前端 限界 平行案内フレーム 作用 自然電位 ヤシ繊維 後端内壁面\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 22.0825 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 被覆材 軸材 緩衝材 図 複合材 前記被覆材 実施形態 前記内側軸材 外力低減材 軸方向 継ぎ目地 外力 内側軸材 付着応力度 Ａ 比較例 プラスチック製ダンボール 柱 コンクリート Ｂ ｍｍ ひび割れ 式 製造方法 被覆 構造物 継ぎ目地間隔 ｆ 試験体 型枠 競技場 鉛直方向 実施例 下部構造 発明 τ 外側 力 網目状部材 付着応力度τ ポリエチレンシート ワイヤーメッシュ 応力 ｈ 鋼製 水平方向 試験 間隔 － 屋根 強度 円形鋼管 参照 付着強度試験 工程 Ａ－Ａ断面図 評価 Ｂ－Ｂ断面図 試験方法 前記軸方向 梁 乾燥収縮 前記型枠 φ 上記 発生 t 棒状体 付着応力 Ａ～図 付着応力度試験 溝状 値 Ｃ プラスチック Ｐ 意匠性 強度未満 特許文献 設計基準強度 前記目地 模式図 目的 Ｎ ｍｍ 構成 Ｓ 縁切り効果 c 例 ００７ シート 形態 セメント系硬化体 フロー図 概念図 説明図 丸鋼 説明 外周 目地 材料 強度ｆ 円形状 スタンド 表 回数 数 前述 建物 製造工程 金属製 実施例参照 競技フィールド 断面円形 継ぎ目 応力伝達 管状部材 拘束力 耐火性 伝達 下側 シーリング材等 プラスチック製段ボール 水平断面図 特徴 形成 位置 効果 相当 ｔ π 上記実施形態 Ｃ参照 直径φ 継高 登録商標 鋼 ダンボール 継ぎ目部分 Ｂ参照 引抜き試験 向上 記載 課題 内部 図面 ｄ h ｔ×ｆ 外力等 安全側評価 圧縮強度 抑制 片持ち状 荷重ｆｃ 力ｆｂ 下部 鋼管 斜視図 荷重Ｐ 実験式 次式 積雪荷重ｆａ ＪＳＣＥ－Ｇ 等 部分 競技フィール Ｈ形鋼 先行技術文献 荷重ｆｃ 技術分野 背景技術 等価物 複数 見た目 状態 部位 縁切り ↑/↑ テフロン フッ素シート 地震力 金属等 外周部分 耐久性 角形鋼管 境界部分 露出部分 モルタル等 直径 溝 矢印 丸棒 内面形状 白色矢印 黒色矢印 中空形状 外周面 耐力Ｐ 樹脂材料 無機材料 特開平 号公報 所定範囲 土間コンクリート 外側端 樹脂 内面 使用予定 耐力 境界 次 ひび割れ抑制 適用対象 地震 黒色 中空 概要 手段 規模 施設 基礎 雪 表面 模様 伸び 経年 中心 反作用 剛性 厚み ポリプロピレン 縁 後述 上端 下端 左側 右側 番線 ガムテーブ 外面 中間 Ｄ Ｅ 継ぎ 条件 水準 無し 現場 調合 鉄筋 案 荷 最大 径 釣り合い 設回数 滑り 伸縮 理解 趣旨 変更 他 セラミック 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 23.1633 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 防水シート 展張装置 走行台車 展張方向 防水シート展張装置本体 傾動アーム トンネル用防水シート レール トンネル周方向 トンネル方向 走行方向 側 展張 前記走行台車 支保工面 前記押え棒 走行 図 前記傾動アーム 移動台 押え棒 支保工面Ｓ 前記防水シート シート端部 前記防水シート展張装置本体 装置 溶着接合部 配向方向 裏面緩衝材 先端側アーム トンネル スライド用ジャッキ 端部 支保工面側 保持軸 請求項 走行用レール部 付き防水シート 前記レール 防水シート張り装置 駆動部 防水シート用 基端側アーム 表面側 左右 作業架台 展張要領 前記移動台 コンクリート釘 上面側 前記保持軸 面 シートホルダー部 部 前記防水展張装置 傾動用ジャッキ 上記請求項 面ファスナー 連結部 該シート保持部 片側方向屈曲部 作業 発明 駆動装置部 切断作業 電磁誘導溶着 シート保持部 屈曲 積層シート材 トンネル用防水シート布設装置 傾動ジャッキ 溶着材 レール材 前記傾動ジャッキ 展張作業 電磁誘導溶着用 ガイド走行レール レール面 平面視 上下方向軸周り 先端 特許文献 シート押えローラアーム部 端側アーム 幅方向 矢印方向 前記作業架台 シートロール 構造 前記傾動用ジャッキ アーム コンクリート 状態 周回り方向 時計回り方向 施工手順図 溶着材付きディスク 傾斜角 施工精度 接合部 工コンクリート 方法 布 組ローラ ガイドレール 作業台車 下記特許文献 トンネル内壁 レール走行構造 ローラー部 連結材 説明 記載 プラスチック製 台車 作業員 先端同士 Ｂ 傾動アーム同士 Ｈ型形鋼 前記面ファスナー トンネル中央側 前記スライド用ジャッキ 前記左右 前記スペーサ 前記チェーン 走行台車等 上記シートロール 溝型形綱 図示 前記電磁誘導溶着 所定 ローラ 山岳トンネル工事 中間地点 展張位置 レール走行構造例 該吹付面 円弧面 止め装置 トンネル断面 一対 特開 号公報 離隔距離 複数本 油圧モータ ヒンジ継手 変化 支持板 ディスク 円盤 配向方向変化 縦断方向 垂直方向 傾斜角度 屈曲ストッパー 前記小アーム B Ａ 基端部同士 下側 右端側 左端側 Ｂ側 Ａ側 動作説明図 チェーン ～ 横断面図 該トンネル 修正 ～図 角度変化 前記接着剤 高周波電流発生コイル 前記組ローラ 上面 プラスチック 軸周り － 課題 A 軸部材 該着貼部 軸受部 中間点 Ｈ型形綱 調整 離間 一端 回転 上下 事前 敷設作業 人手作業 溝型形鋼 車輪部材 長代 進行方向前側 施工手順 Ｓ 接着剤 張設作業 幅 スペーサ 背面合わせ ボイド管 伸縮操作 等 裏面 接着剤付きディスク 断面図 平面図 要部拡大断面図 スプロケット 図面 工場 上記 形態 軸受 平面展開図 接着剤等 傾斜角±θ 傾斜角（±θ 水平配向 同士 熱コテ式溶着機 先行技術文献 揺動シリンダ 要領 ロール状 回動 変形例 技術分野 背景技術 揺動制御 設前 吹付 人力 程度 該一対 均衡 上記課題 ズレ インバートＨ 中央 複数 フランジ モータ ピン θ ディスクウエルダー 図示例 形態例 動作 内壁 アーチ状 長部分 弾性変形 シリンダ後端 －θ 直線状 A)(B 押圧ヘッド 該押圧ヘッド 地山 湧水 大型化 労務負担 圧着線 遊び分 衝突状態 通過空間 位置 進行 重機類 安全機構 衝突 圧着 アイソレーション 設 厚み 増大 開発 間隔 両端 吹付面 後部 遊端 レバー 概要 手段 特徴 ライン 効果 詳説 実施 車両 階段 踊り場 場所 全長 現場 前端 ピストン 傾き 上側 通常 取付け 下向き 固定 種々 他 支障 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 22.8644 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 基礎 浮き基礎体 ばね 振動発生機械 基礎体 杭 水平移動規制部材 皿ばね 振動 振動発生源 底板コンクリート 実施形態 図 部 ガイド部 浮き基礎型式 基礎構造 水平移動 コンクリート基礎 連結部材 固有周期 Ｂ Ａ 振動数 前記浮き基礎体 地盤 発明 パイルキャップ 閉塞部 地震発生 係止部 部分拡大断面図 移動 特許文献 前記ばね マット基礎増厚部 振動低減効果 上面 上端面 固有振動数 ばね材 出隅部 壁部 ジャッキスペース 杭体 機器基礎 マット基礎 前記杭 拡大断面図 当該浮き基礎体 隙間 上下 直列 構造 ジャッキ 振動機器 振動系 基礎側 機械 段差部 力 効果 下面 Ａ－Ａ断面図 スリーブ 周縁部 水平力 後述 天井面 周面 内部 部分 直列ばね形式 水平方向 位置 上述 式 荷重 ＰＨＣ杭 号公報 － 壁面 自由度 幅 貫通孔 ｆ 当該ばね 形態 平面図 上端 構造物 前記実施形態 外周面 略Ｌ字形状 側面図 先端 Ｃ部分拡大断面図 説明 錘 ボルト ｓ ばね特性 機器 荷重Ｗ 前記連結部材 コンクリート製 水平荷重 既製コンクリート杭 方向 地震 上端部 筒状 複数 面 面積Ａ 破線Ｃ 特徴 ｐ 上下方向 周辺地盤 質点系 円筒形状 振動伝達率 鋼製 弾性体 ばね定数ｋ 当該杭 下面周縁部 特開 建設コスト 質点 パラメータ解析 円環状 外側近傍 Ｈｚ コスト 近傍 実施例 固有周期帯 支柱 特許 課題 調整 １つ 程度 杭体内 杭上面 水平変位 負担面積Ａ 前記地盤 構成 重量 先端支持力 地震荷重 支持地盤 先端面 構造安全性 下面周縁 内壁面 外壁面 Ｃ ～図 先行技術文献 ロッキング力 モーメント力 壁 鉛直荷重 長期許容支持力Ｒ 略直方体形状 技術分野 背景技術 矩形ブロック状 材 内側 配置 当該 挙動 図面 符号 下 枚数 鍔状 直線状 直下 単体 重心 対角線 下方 単位面積当り 円柱形状 外側 側面 平面視 重力加速度Ｇ 共振現象 減衰率 共振曲線 横軸 縦軸 操作性 ～ 発明者 寸法比 構成要件 表面 複数本 鉄筋コンクリート造 表面処理 摩擦抵抗 フェールセーフ機構 外径 ｋＮつまり 当り 重量ｍ ｍ 鉄筋コンクリート 比重Ｓ Ｓ ｄＢ 既製品 特性 改良等 コイルスプリング 参照 中空 頭部 四隅 装置 取り付け 概要 手段 マッシブ 空間 省略 直上 Pretensioned Spun High Strength Concrete Piles 周波数 目標 注油 状態 想定 メンテナンス φ Ｎ 値 内径 量 ｔ Ｒｐ ｈ ｂ ｌ ｃ ｃｍ 距離 交換 増減 厚み 設計 中間 ）～（ 一対 目的 範囲 変形 風雨 カバー\n",
      "\n",
      "===== # 4, Topic : 14, p : 23.9521 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : プレハブ構造 斜材 パネル材 型枠 埋設型枠 鋼材 実施形態 コンクリート 圧縮材 型枠構造 圧縮力 頂版 力 コンクリート構造物 補強材 図 前記圧縮材 チャンネル材 配筋作業 下側 前記斜材 取付部 前記主鋼材 コンクリート製 接続鋼材 部材 隅角部 剛性 アングル材 発明 プレキャスト部材 張力調整機構 ネジ鋼 コンクリート打設時 プレハブ構造等 孔 上下 ボルト 鉛直方向 方向 ｃ トラス構造 前記型枠 両端部 軸部 調整 張力 側壁 ボルト等 支保工 荷重 圧縮力ｃ 支持材 作業 略鉛直方向 形鋼 格点部 ｂ 構造物 構成 上部 下部 構造部材 特許文献 ボックスカルバート 構築方法 当該孔 補強筋 例 前記 等 平面 力ｂ コンクリート強度 形態 側 中間部 上端部 配筋 平面保持 一体性 図示 先端 荷重ａ 略水平方向 鋼製リブ材 支点 上側 ナット 柱材等 繊維補強コンクリート等 設時 コンクリート打込み コンクリート打設後 配力用鋼材 説明 両側 ａ 中間壁 技術 実施形態]（ 補強鉄筋 下 CT形鋼等 L形鋼等 平鋼等 せん断補強鋼材 繊維補強コンクリート板 特開 号公報 強度 初期剛性 曲げ 取付作業 課題 施工 上記 位置 架設 確保 中央 鉄筋等 図面 鋼板 下端部 柱等 ターンバックル側 トンネル等 鋼板等 図等 先行技術文献 線Ａ－Ａ 線Ｂ－Ｂ 技術分野 背景技術 撤去作業 解体作業 構築 傾斜方向 発生リスク ターンバックル 複合ハーフプレキャスト部材 設置 状態 特徴 自体 発現 断面 箇所 変形 隙間 符号 スパン ひび割れ発生リスク 技術的思想 技術的範囲 支持 鉛直面同士 Ｌ字状 変更例 修正例 接合部分 効果 座屈 低減効果 同士 添付図面 プレキャストコンクリート 現場 概要 規模 重量 経済 外力 損傷 目的 手段 】[ 周囲 両者 溶接 片側 組合せ 自重 双方 地上 元 組立 負担 代わり 仮設 利点 背中合わせ 別 ～ ターンバックル 段階 複数 表面 凹凸 橋梁 間隔 添接板 業者 本願 範疇 各種\n",
      "\n",
      "===== # 5, Topic : 14, p : 18.0947 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 飛行体 避難者Ｐ 制御部 避難者 制御器 位置 飛行制御部 放水ヘッド 防火装置 構造物 水 飛行制御処理 飛行制御 制御処理 図 霧雨状 放水 避難者認識部 火災 飛行指令 飛行指令信号 放水制御部 防火 実施形態 通信部 水粒子 飛行 制御 火災感知器 放水制御 水幕 散水部 火炎Ｆ 距離検出器 放水量 発進指令 避難 水Ｗ 自動飛行制御 放水部 説明 放水制御信号 Ｓ 制御指令 防火効果 熱暴露 カメラ 制御ユニット 火災感知信号 前記飛行体 記憶部 発明 移動 装置 スパイラル状 目標位置 飛行経路 前記避難者 周囲 ロータ 検出信号 遠隔操作 制御機器 回転制御 放水指令 内部構造データ 火炎 上方 モータ ポンプ 避難誘導 構造 信号 本体 設置数 Ｐ 火災発生 場所 形態 駆動制御信号 感知信号 位置停止飛行 複数 移行制御処理 説明図 飛行状態 防火対象 位置座標値 駆動信号 ｂ 画像 映像 距離 認識 タンク スプリンクラーヘッド 前記放水部 範囲 手動操作 概要説明図 火災場所 ｃ 作動指令 データ ブロック図 スパイラル部分 放水機能 特許文献 存在 熱 鉛直方向 回転軸 移動方向 火災検知器 放水用 特開 号公報 スプリンクラー 流路 Ｆ 防火機能 通信機器 撮像信号 電気的構成 動作 下部 外部 バッテリ 一つ 手法 収容器 防火支援 位置情報 前方位置 待機位置 遠隔操作機器 水等 水補給 効果 図面 センサ ～ 情報 Ｗ 熱暴露傷害 作動信号 霧状 機器 火災範囲 対流熱 概要図 個人識別信号 通信機 回転 撮像機器 撮像データ 技術分野 背景技術 距離センサ 先行技術文献 － 概要 課題 フローチャート 同一 符号 ｄ プログラム ジャイロセンサ 否 有無 便宜 物体 状況 四つ 六つ 下方 遮断効果 ＧＰＳ モニタ 上述 コントローラ 送水機器 回転翼 作動 手動操縦 電気部品 レーザ式 レーダ式 音波式 特許請求 防災管理センタ 電力源 電力供給 ステップＳ ～Ｓ 傷害 天井 初期段階 変形例 添付図面 積載能力 表面形状 監視カメラ 変形 天井付近 高熱 高熱化 輻射熱 火傷 消火 目的 手段 要素 無人 航空機 マルチコプター ドローン 左右 上下 ホバリング ＣＰＵ ＲＯＭ ＲＡＭ コンピュータ 通常 見取り図 送受信 入出力 アクチュエータ 同数 ａ 既存 火災報知器 アーム 中心 外側 ガイド 中央 未満 逆 分布 程度 水量 工場 ビル 人員 側 マーカ スッテプ 判定 GlobalPositioning System 加速度 空間 図示 実行 一連 他 記載 要旨 上部 熱気 効率 マニュアル\n",
      "\n",
      "===== # 6, Topic : 14, p : 19.0435 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 検査管理情報 検査 検査情報作成支援システム 検査者 自己位置 前記検査管理情報 管理サーバ 図 ＧＮＳＳ観測情報 検査位置 管理 移動局 タブレット端末 情報 検査項目 測位サーバ ＧＮＳＳ補正観測情報 地図情報 位置情報 自己位置特定システム 受信機Ｒ 検査履歴情報Ｄ 該自己位置 検査記録画面 位置 該検査管理情報 検査処理部 前記管理サーバ 測位計算部 水シート スパーク検査 前記移動局 検査済み位置 検査者名 前記測位サーバ 前記タブレット端末 サーバ 品質管理項目 検査写真 圧検査 前記ＧＮＳＳ観測情報 加圧検査ボタンＢ 検査情報作成支援 検査日 検査状態 ＧＮＳＳアンテナＡ 前記検査者 ネットワークＮ 検査済み領域情報Ｄ 現場写真 検査地域 検査値 検査済み領域 スパーク検査ボタンＢ 受信機 制御部 履歴 前記地図情報 圧検査ボタンＢ 撮像部 発明 加圧検査 表示画面Ｅ 計画情報 管理項目 入出力部 平面図ＤＭ 検査作業 ＧＮＳＳアンテナ 検査記録 履歴管理 作成 検査対象領域Ｄ 基準局 自己位置特定処理手順 水シート工事 検査現場地域Ｅ 処理 前記ＧＮＳＳ補正観測情報 検査対象 廃棄物処分場 記録 自己位置特定処理 支援処理 地図 計算支援 前記自己位置 工事 ＧＮＳＳ 情報携帯端末 検査済み領域情報ＤＭ 構成 地図情報Ａ 自己位置Ｒ ＧＮＳＳ衛星Ｓ ネットワーク 各種情報 当該検査 検査続行 検査日時 バキュームボックス検査 検査経路 検査種別 検査軌跡 検査記録内容 前記受信機 形態 ＲＴＫ測位方式 自己位置等 ブロック図 精度位置特定 模式図 計算 計画情報受信ステップ 自己位置Ｐ 現場 上記 もと ＧＮＳＳアンテナＢ 前記現場 前記 写真 ステップＳ 発生位置 特許文献 前記地図 品質管理項目抽出ステップ 測位方式 特徴 検査進捗状況 アップロードボタンＢ 前記ネットワーク 終了ボタンＤ 前記現場写真 前記撮像部 表示画面 実施 合格 整数値バイアス 携帯端末 表示画面Ｎ 選択 精度 概要構成 表示部 平面図Ｅ 項目名 合否 側 イメージ 水シート間溶着部 終了ボタンＢ５ 端末装置 有線接続 干渉測位方式 収束解 対象 内容 接合部 前記基準局 説明 課題 程度 表示 完了 衛星Ｓ 導電シート 箇所 入力 登録 搬送波 携帯 スパーク 制御 撮像装置 ネットワークＰ 式融着 石鹸水 Ａ 選択受付ステップ 入力受付ステップ 敷設工事 敷設工事終了 補修工事 登録要求ステップ 先行技術文献 技術分野 背景技術 融着箇所 手間 目的 フローチャート 信号 波数 誤差 ヘルメット 空気 証憑 画像 携帯性 図示 装置 分散 入出力インターフェース 銅線等 動線 構成要素 アップロードボタンＢ５ 概要 ウェアラブル装置 無線接続 ポンプ等 接合船 処置内容 使用状況 特開 号公報 基準点 図面 添付図面 搬送波位相 バッグパック エアーチャンネル 圧力低下 クリープ現象 種類ごと 圧力 真空箱 ごと 日時 現象 各種 手作業 紙 記入 － 手段 効果 Real Time Kinematic GPS ５つ Global Navigation Satellite System ～ インターネット 決定 推測 人 装着 ベルト 身体 気圧 ストレス 漏れ 裏面 ブラシ 地盤 電圧 有無 施設 タッチパネル 色 形状 組み合わせ プロット エンティティ 入れ物 数値 事象 場所 ブザー 右側 起動 時点 部分 他 所望 リアルタイム 影響 生成 クラウドコンピューティング クラウドサーバ 送受信 タイムラグ 統合 任意 単位 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 20.5914 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : フラップ ２つ 枠部材 外フラップ 状態 運搬かご 前記２つ 筒状部材 案内部材 前記仮 養生蓋 前記枠部材 図 仮 床スラブ 開口 矩形状 前記運搬かご 前記矩形 前記 コンクリートスラブ 板状 支持部材 上下方向 矩形 底板 レール 外 横棒 周縁 複数 案内作用 直立状態 板体 図示 平面形状 頭付ピン 前記コンクリートスラブ 前記正面 斜視図 例 ｂ 板片 先端部 水平状態 テーパ状 上方 他 開放状態 発明 前記前記仮 コンクリート建築物 前記角筒状 筒部材 前記直立 伸長方向 下方 双方 前記直立状態 角筒状 前記養生蓋 作業者 辺 リブ 物 下階 仮設材 養生 下 上下 作業 前記コンクリート建築物 前記建築物 特許文献 間隔 一対 下面 他方 階 面 ～図 前記直立状態維持手段 接触 前記揺動 前記複数 前記他 該枠部材 上階 可能性 直立状態維持手段 補助板 建設途上 落下 形成 形成予定 手段 外周縁 該内フラップ 閉鎖状態 直列状態 平面図 説明 支保工 保護 発生 周り ｄ 同一 蝶番 維持 水平方向 移動 事故 直列 形状 先端部近傍 上端部 下端部 昇降作業 底面図 断面図 横断面形状 先行技術文献 後記特許文献 技術分野 背景技術 揺動制限 周囲 参照 梁 揺れ 課題 不慮 囲い 内方 該仮 末広がり 受け入れ 符号 ａ 設置 上面 ｃ 該水平 側 伸び 空間 側板 正面 ３つ 水平移動 頂面 昇降 安全性 落下等 落下防止 昇降装置 号公報 接触事故 形態 実施形態 上端 軸線 挿通可能 取り付け数量 永久磁石 磁気的吸着 損傷 開平 － 概要 事情 該開口 接近 未然 最小限 程度 図面 長方形 部分 外力 ストッパ 働き 合し レベル 長 ４つ 四隅 共通 全長 和 挿通 手作業 ウインチ ロープ 直方体 背面 ネット 荷物 同士 区間 態 障害 一体 方\n",
      "\n",
      "===== # 8, Topic : 14, p : 21.4089 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : トラス架構 軸方向 連結材 実施形態 ユニット 屋根架構 幅方向 図 上部材 Ｙ方向 下部材 束材 トラス構造 斜材 Ｘ方向 特許文献 上記実施形態 アーチ状 請求項 方向 上弦材 下弦材 横材 同士 発明 水平方向 間隔 繋ぎ機構 トラス屋根架構 Ｚ方向 構造 長手方向 整数倍 前記軸方向 技術 トラス架構及びこのトラス架構 コスト Ｘ字状 木質 号公報 部材 屋根 屈曲架構ユニット 集成材 トラス梁 軸線Ｇ 連結部材 ユニット同士 Ａ 断面 二つユニット 前記ユニット同士 斜視図 コストアップ 上側凸 前記連結材 形態 トラス梁等 直線状 床組 特開 複数 参照 引きボルト 木製トラス梁ユニット 建築物 屋根組等 記載 仰角 Ｂ ブレース 個数 各種屋根構造 梁組等 接合方法 接着剤 鉛直方向 上下方向 ドリフトピン 断面積 逆Ｖ字状 構造物 平板状 Ｌ字状 矢印Ｙ 逆Ｖ字 説明 剛性 強度 効果 三つ 寄棟屋根 半切妻屋根等 矢印Ｘ 平面構造物 ベント梁 木造格子梁 引きボルト等 上記 梯子状 平面視Ｕ字形状 先行技術文献 矢印Ｚ 平面視 特開昭 技術分野 背景技術 木造建築 空間 課題 体育館等 別 観点 作用 接合 ラグスクリュー等 特開平 拡大図 中間部 上端部 下端部 端部 金物 ピン 尺平板形状 接合状態 接着工程 定着金物 接続金物 製作期間 施工期間 単位角材 金属製 ラグスクリュー 角材 重ね合せ － 概要 目的 手段 図面 内部 ホゾパイプ 側面 山 外側 位置 中央 一つ 角度 前述 ビス 板金 構成 四つ 他 斜め 要旨 範囲 態様 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 20.2750 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 測定 地盤 ＲＩ計器 締固め度 Ｄスキャナ 地盤締固め度測定システム 状態 凹凸測定領域ＧＡ ステップＳ 測定可否 空隙ＡＳ 領域ＧＡ 凹凸測定部 凹凸 実施形態 領域 図 地盤状態測定部 測定精度 体積 凹凸測定領域取得機構 体積Ｖ 地盤締固め度測定方法 ＲＩ測定領域ＧＡ 凹凸測定部変位機構 測定可 判定部 地盤締固め度 中性子計数率 中性子 測定地点 密度 地点 上記実施形態 γ線計数率 測定ステップ 距離Ｄ Ｖ 含水量 車両 Ｖ２ γ線 補正部 測定不可 熱中性子検出部 γ線検出部 熱中性子 発明 体積情報 程度 位置情報 算出部 仮想空間ＩＳ 精度 情報 中性子線源 ＲＩ測定可否 制御信号 Ｓ 仮想面ＩＰ 位置 頂点ＴＰ 移動部 測定器 測定対象 コントローラ ～ステップＳ 制御信号送信部 形態 効率 距離 特許文献 変位センサ 動作 拡大断面図 モータ 変位量 制御部 参照 角度センサ γ線源 判定ステップ 具合 試験 数 表面 判定 表面形状 体積Ｖ２ エリア 所定 回数 角度 空隙ＡＳ 構成 バリアングル機構 チルト機構 初期位置 不可 γ線密度 込部 説明 システム 影響 グラフ 別 間隔 空隙 方法 駆動源 中性子水分計 側面図 フローチャート 車体 可否 中性子水分計等 造成工事 進行方向 複数 車輪 ａ 下方 ３つ ＣＰＵ 傾向 否 地盤品質 ＲＩ計器 相関 先行技術文献 技術分野 背景技術 － 課題 目視 整地 図面 散乱 図示 内部 性質 高速 ｂ 底面 ｍｅｍｏｒｙ 等 同一 符号 中心 機器 ００７ ブロック図 移動 技術的範囲 含水量等 エンジン等 ｒｅａｄ－ｏｎｌｙ 光切断法 作業者 作業員 処理 管理 管理項目 自動化 走行車 特開平 号公報 効果 案内レール 出力軸 相関関係 関係 演算処理 演算 作用効果 前方上部 前方 適用例 写真測量 具体的構成 一つ 人手 労力 概要 確認 現状 目的 手段 ワイヤ 便宜 ２つ 場所 ｃｅｎｔｒａｌ ｐｒｏｃｅｓｓｉｎｇ ｕｎｉｔ プログラム ＲＯＭ ＲＡＭ ｒａｎｄｏｍ ａｃｃｅｓｓ 任意 傾き 左右 広範囲 趣旨 透過 曲線 他 橇 乗り物\n",
      "\n",
      "===== # 10, Topic : 14, p : 22.8340 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 摩擦領域 床免震システム 免震床 床 免震機構 減衰材 床面 床免震装置 滑り免震 滑り免震機構 滑り床板 免震 滑り面 床免震 摩擦滑り材 摩擦 構造床 荷重支持部 床スラブ 材 滑り素材 機能床免震システムＡ 地震 床体 免震領域 機能床免震 滑り性 図 床免震技術 免震装置 面 摩擦係数 床免震方法 機構 衝撃 免震床システム ｂ 免震建物 免震システム 作業床 スラブ 鋳鉄材荷重支持部 上部筐体 歩廊床体 通常床体 床免震機構 床免震構造 特許文献 摩擦性 下部筐体 弾性体 機能免震 機能床免震システム 機器 実施態様 滑り性素材付着機構 滑り性素材 黒鉛潤滑 部分床免震システム 免震床周り 鋼板等 滑り機構 滑り面機構 号公報 床板 床領域 部分免震 床免震システム構成 免震性能 発明 ｃ 滑り床機構 固定領域 機能免震床システム部屋 下部鋼板等 免震方式 免震対象 機能床免震システムＢ 性素材 筐体 グリス摩擦調整機構 特開 摩擦面 地震エネルギー システム ゲル体 例 素材 摩擦性能 機能免震床システム部屋Ｂ エンボス機構 剛滑り支承 機器類 減衰機構 構成 滑り床下方 黒鉛混入モルタル層 機能 グリス ｄ 摩擦機構 黒鉛膜層 仕上げ床 材料 クリアランス ビーズ 天然ゴム 空隙 部屋 部分 鋼板 特徴 発泡体 面機構 作業員等 摩擦板 － 記載 減衰力 床や仕上げ床面 装置類 黒鉛塗膜 高摩擦領域 滑り性能 作業員用 エンボス層 減衰効果 減衰体 樹脂製ゴム 摩擦減衰力 設備 下面 減衰機能 水平方向 作業 ゴムバンド 摩擦部材 ゴム体 作業性 取付構造 潤滑剤 支承 移動 滑り材料 上部 ゴム弾性体 装置 機器直下 袋等 方法 複数 梁 ）～（ フラット 機材 上部鋼板等 特開平 ガラス繊維充填プラスチック 黒鉛混入モルタル 弾性発泡体 エンボス 利用性 可能性 抵抗性 吸収 調整 建物 黒鉛 ａ 上下鋼板等 薬品庫 建造物 減衰部材 所定変位 変位 差 立面図 転動性 接合体 説明 変形 柱 上面 圧縮 個所 ポリアミド ポリイミド ウレタンアクリレート 停止 範囲 機器操作用 突起部 地震衝撃 ウレタンゴム エンボス機構 微小凹凸形成鋼板 ウレタン 上下 測定装置 樹脂製 鋼製 粗面化処理 原点復帰機能 固定板 先行技術文献 固定 精密機器類 中央部 取り合い部 半導体製造装置 中小地震 地震対策 点接触用突起 ｂ等 技術分野 背景技術 隅角部 地震対応措置 作業床や通路 性能 通常 Ａ 作業スペース 固定状態 水平移動 点接触状態 工場 参照 上方 基礎 種類 凹部 階高 壁 段差 課題 手段 ダメージ 衝撃吸収 下 突起 図示 外側 帯板状 平面視方形状 化エチレン 空隙部分 平面図 吸収差 復帰作用 Ｈ形鋼 模式図 横方向 効果 建築物 付加剤 薬品棚 多段階 耐震建物 移動余裕 微小振動 レベル差 試験設備 加速度 振動 応答加速度 コイルばね 下方 周り 間隙 当該間隙 表面加工 通路 中規模程度 レベル 耐震 設置環境 スペース 対象 抵抗 程度 原点 提案 支承部 オイルダンパー 同士 相互 復元 増幅 棒状 一端 他端 側面 ポリ 騒音 まわり 概要 用途 目的 組合せ 揺れ 巾 振幅 移行 一つ ダンパー 同一 低下 塗料 球体 損傷 検査 図面 形態 次 側 段階 能力 ４つ 粘性 小径 上と下 他 反発 押圧 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1044e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 9898746880.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 60619.3805\n",
      "Test epoch : 1 Average loss: 1347.7310\n",
      "PP(train) = 11725.834, PP(valid) = 11786.882\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3961e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.4984e-39, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 8933801984.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 49814.5477\n",
      "Test epoch : 2 Average loss: 1347.0616\n",
      "PP(train) = 11705.276, PP(valid) = 11768.473\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4574e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 8060758016.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 41039.3406\n",
      "Test epoch : 3 Average loss: 1346.4330\n",
      "PP(train) = 11665.059, PP(valid) = 11732.165\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2612e-44, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.1550e-37, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 7271508992.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 34051.9039\n",
      "Test epoch : 4 Average loss: 1345.8435\n",
      "PP(train) = 11612.923, PP(valid) = 11685.534\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8653e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 6558798336.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 28020.8840\n",
      "Test epoch : 5 Average loss: 1345.1701\n",
      "PP(train) = 11560.006, PP(valid) = 11638.135\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0145e-38, 0.0000e+00,\n",
      "         0.0000e+00, 1.0761e-41, 1.4300e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0194e-33, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 5915181568.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 23288.0949\n",
      "Test epoch : 6 Average loss: 1344.5181\n",
      "PP(train) = 11499.146, PP(valid) = 11583.368\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0538e-29, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.5405e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 5334408192.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 19651.6848\n",
      "Test epoch : 7 Average loss: 1343.7681\n",
      "PP(train) = 11442.382, PP(valid) = 11532.599\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.1520e-42, 0.0000e+00, 6.6636e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.4161e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 4810295808.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 16695.9195\n",
      "Test epoch : 8 Average loss: 1342.9147\n",
      "PP(train) = 11385.322, PP(valid) = 11480.671\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6052e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0391e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 4337363968.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 14080.2271\n",
      "Test epoch : 9 Average loss: 1342.0971\n",
      "PP(train) = 11325.393, PP(valid) = 11425.927\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3022e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.8709e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1748e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 3910855680.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 11907.7191\n",
      "Test epoch : 10 Average loss: 1341.3303\n",
      "PP(train) = 11259.710, PP(valid) = 11365.188\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8755e-42, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 3526202368.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 10248.5584\n",
      "Test epoch : 11 Average loss: 1340.5592\n",
      "PP(train) = 11192.626, PP(valid) = 11303.413\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2540e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8841e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 3179322112.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 8762.0396\n",
      "Test epoch : 12 Average loss: 1339.7377\n",
      "PP(train) = 11128.329, PP(valid) = 11244.397\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.1494e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 2866472960.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 7493.7900\n",
      "Test epoch : 13 Average loss: 1338.9060\n",
      "PP(train) = 11064.391, PP(valid) = 11185.370\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7431e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.4492e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.9929e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 2584366848.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 6487.4632\n",
      "Test epoch : 14 Average loss: 1338.1080\n",
      "PP(train) = 10998.879, PP(valid) = 11124.753\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6309e-38, 0.0000e+00, 5.1624e-22, 0.0000e+00,\n",
      "         0.0000e+00, 5.8159e-40, 5.7857e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.8900e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2329992960.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 5656.4725\n",
      "Test epoch : 15 Average loss: 1337.3157\n",
      "PP(train) = 10932.640, PP(valid) = 11063.441\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1158e-31, 0.0000e+00, 2.2500e-30, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0639e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5204e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2100634112.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 4990.1344\n",
      "Test epoch : 16 Average loss: 1336.5554\n",
      "PP(train) = 10864.831, PP(valid) = 11000.380\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2612e-44, 0.0000e+00, 1.2415e-38, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.7664e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1893827456.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 4431.8427\n",
      "Test epoch : 17 Average loss: 1335.7543\n",
      "PP(train) = 10799.224, PP(valid) = 10939.419\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8882e-27, 0.0000e+00, 2.3871e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.4178e-21, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.2954e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1707371648.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 3956.3056\n",
      "Test epoch : 18 Average loss: 1334.9572\n",
      "PP(train) = 10733.275, PP(valid) = 10878.205\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2675e-42, 0.0000e+00,\n",
      "         0.0000e+00, 1.7607e-41, 3.9965e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0459e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1539265024.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 3547.0721\n",
      "Test epoch : 19 Average loss: 1334.1174\n",
      "PP(train) = 10668.778, PP(valid) = 10818.294\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0898e-33, 0.0000e+00, 3.6873e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.0845e-32, 5.2576e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.5557e-12, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1387695232.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 3208.0313\n",
      "Test epoch : 20 Average loss: 1333.2349\n",
      "PP(train) = 10605.974, PP(valid) = 10759.820\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.8212e-30, 0.0000e+00, 1.1820e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.5320e-35, 1.1166e-23, 4.2894e-42, 0.0000e+00, 0.0000e+00,\n",
      "         5.6717e-01, 5.0447e-44, 4.3283e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0620, 0.0609, 0.0893, 0.0571, 0.0497, 0.0671, 0.0744, 0.0864, 0.0692,\n",
      "         0.0746, 0.0550, 0.0553, 0.0564, 0.0687, 0.0738]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1251046912.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 2927.5840\n",
      "Test epoch : 21 Average loss: 1332.3958\n",
      "PP(train) = 10542.241, PP(valid) = 10700.479\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1457e-22, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.7099e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0237e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1127848576.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 2687.3851\n",
      "Test epoch : 22 Average loss: 1331.5555\n",
      "PP(train) = 10478.462, PP(valid) = 10641.099\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0197e-28, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.1968e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1016782272.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 2488.9457\n",
      "Test epoch : 23 Average loss: 1330.7169\n",
      "PP(train) = 10415.168, PP(valid) = 10581.575\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.2706e-42, 0.0000e+00, 4.0221e-40, 0.0000e+00,\n",
      "         0.0000e+00, 1.5993e-36, 8.9683e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1730e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 916653376.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 2318.9877\n",
      "Test epoch : 24 Average loss: 1329.9246\n",
      "PP(train) = 10350.510, PP(valid) = 10521.100\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2500e-42, 0.0000e+00, 8.2275e-33, 0.0000e+00,\n",
      "         0.0000e+00, 4.0918e-43, 4.0186e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.3710e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 826382592.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 2172.8865\n",
      "Test epoch : 25 Average loss: 1329.1346\n",
      "PP(train) = 10286.713, PP(valid) = 10461.298\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2992e-42, 0.0000e+00, 1.0821e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.1062e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8205e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 744995584.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 2049.9332\n",
      "Test epoch : 26 Average loss: 1328.2989\n",
      "PP(train) = 10224.772, PP(valid) = 10403.117\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2111e-38, 0.0000e+00, 6.9601e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.2144e-39, 3.6884e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.7413e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 671622208.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1945.9314\n",
      "Test epoch : 27 Average loss: 1327.5064\n",
      "PP(train) = 10161.800, PP(valid) = 10344.036\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.5451e-40, 0.0000e+00, 8.5391e-30, 0.0000e+00,\n",
      "         0.0000e+00, 3.5639e-30, 2.5786e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3116e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 605473664.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1855.9654\n",
      "Test epoch : 28 Average loss: 1326.6923\n",
      "PP(train) = 10099.851, PP(valid) = 10285.938\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8768e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.5666e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.2180e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 545838656.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1779.5522\n",
      "Test epoch : 29 Average loss: 1325.8623\n",
      "PP(train) = 10039.312, PP(valid) = 10229.229\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2972e-34, 0.0000e+00, 2.2400e-19, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.2019e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9635e-15, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 492077280.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1713.5242\n",
      "Test epoch : 30 Average loss: 1325.0373\n",
      "PP(train) = 9978.633, PP(valid) = 10172.106\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.8669e-36, 0.0000e+00, 7.0704e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.0187e-42, 2.0115e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.9697e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 443609824.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1656.0240\n",
      "Test epoch : 31 Average loss: 1324.2143\n",
      "PP(train) = 9918.633, PP(valid) = 10115.640\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8026e-43, 0.0000e+00, 3.5916e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.3461e-41, 3.3004e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.8055e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 399916192.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1606.9162\n",
      "Test epoch : 32 Average loss: 1323.4065\n",
      "PP(train) = 9858.777, PP(valid) = 10059.381\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.4806e-41, 0.0000e+00, 7.8869e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 2.0985e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6204e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 360524288.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1564.2543\n",
      "Test epoch : 33 Average loss: 1322.6303\n",
      "PP(train) = 9798.777, PP(valid) = 10002.958\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0638e-44, 0.0000e+00, 9.6755e-25, 0.0000e+00,\n",
      "         0.0000e+00, 5.0033e-38, 2.6566e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.5526e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 325012480.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1526.9400\n",
      "Test epoch : 34 Average loss: 1321.8589\n",
      "PP(train) = 9739.097, PP(valid) = 9946.520\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.7610e-35, 0.0000e+00, 2.1535e-32, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8007e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 292998624.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1494.4554\n",
      "Test epoch : 35 Average loss: 1321.0294\n",
      "PP(train) = 9682.051, PP(valid) = 9892.673\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.0593e-32, 0.0000e+00, 3.2133e-32, 0.0000e+00,\n",
      "         0.0000e+00, 7.4411e-40, 2.1934e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.8170e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 264138128.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1466.3688\n",
      "Test epoch : 36 Average loss: 1320.1977\n",
      "PP(train) = 9625.376, PP(valid) = 9839.043\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1827e-18, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.4790e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.3810e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 238120416.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1441.7290\n",
      "Test epoch : 37 Average loss: 1319.3892\n",
      "PP(train) = 9568.559, PP(valid) = 9785.424\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7399e-28, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.6950e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7457e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 214665456.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1420.0832\n",
      "Test epoch : 38 Average loss: 1318.5966\n",
      "PP(train) = 9511.305, PP(valid) = 9731.048\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.3809e-38, 0.0000e+00, 8.2911e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.4512e-25, 3.6758e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7451e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 193520816.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1401.2268\n",
      "Test epoch : 39 Average loss: 1317.8196\n",
      "PP(train) = 9454.250, PP(valid) = 9677.075\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6493e-34, 0.0000e+00, 6.1745e-23, 0.0000e+00,\n",
      "         0.0000e+00, 4.6798e-27, 4.2710e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9022e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 174458928.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1384.4280\n",
      "Test epoch : 40 Average loss: 1317.0402\n",
      "PP(train) = 9397.913, PP(valid) = 9623.642\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.1763e-40, 0.0000e+00, 1.4868e-34, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0928e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2847e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 157274656.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1369.3602\n",
      "Test epoch : 41 Average loss: 1316.2247\n",
      "PP(train) = 9343.979, PP(valid) = 9572.857\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.8942e-41, 0.0000e+00, 3.4979e-32, 0.0000e+00,\n",
      "         0.0000e+00, 4.7363e-26, 2.0941e-24, 9.8091e-45, 0.0000e+00, 0.0000e+00,\n",
      "         5.8704e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 141783040.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1356.3272\n",
      "Test epoch : 42 Average loss: 1315.4516\n",
      "PP(train) = 9288.371, PP(valid) = 9520.099\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.0829e-44, 0.0000e+00, 1.5612e-31, 0.0000e+00,\n",
      "         0.0000e+00, 2.2342e-36, 8.9543e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2763e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 127817352.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1344.4615\n",
      "Test epoch : 43 Average loss: 1314.7049\n",
      "PP(train) = 9232.917, PP(valid) = 9467.531\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.4078e-45, 0.0000e+00, 4.0217e-43, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.1528e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 115227288.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1334.1940\n",
      "Test epoch : 44 Average loss: 1313.9187\n",
      "PP(train) = 9179.239, PP(valid) = 9416.699\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4677e-42, 0.0000e+00, 1.3492e-34, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 3.0561e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.3751e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 103877352.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1324.8000\n",
      "Test epoch : 45 Average loss: 1313.1246\n",
      "PP(train) = 9126.221, PP(valid) = 9366.507\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4975e-39, 0.0000e+00, 3.4222e-31, 0.0000e+00,\n",
      "         0.0000e+00, 6.4509e-32, 8.7468e-38, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         6.0530e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 93645392.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1316.4671\n",
      "Test epoch : 46 Average loss: 1312.3752\n",
      "PP(train) = 9072.704, PP(valid) = 9315.756\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2262e-35, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1033e-38, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 84421280.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1308.8594\n",
      "Test epoch : 47 Average loss: 1311.6106\n",
      "PP(train) = 9019.459, PP(valid) = 9265.254\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7637e-37, 0.0000e+00, 1.3273e-25, 0.0000e+00,\n",
      "         0.0000e+00, 2.6377e-35, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.4254e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 76105744.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1302.0609\n",
      "Test epoch : 48 Average loss: 1310.8418\n",
      "PP(train) = 8967.440, PP(valid) = 9216.168\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8516e-39, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 7.5390e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8605e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 68609112.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1296.1288\n",
      "Test epoch : 49 Average loss: 1310.0829\n",
      "PP(train) = 8915.841, PP(valid) = 9167.141\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.5036e-31, 0.0000e+00, 1.8754e-19, 0.0000e+00,\n",
      "         0.0000e+00, 9.2792e-38, 3.1118e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.4930e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 61850920.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1290.5800\n",
      "Test epoch : 50 Average loss: 1309.3146\n",
      "PP(train) = 8864.835, PP(valid) = 9118.635\n",
      "Writing to ./topicwords/2-topwords_e50.txt\n",
      "Topic 0: 上昇ストローク グラウトポンプ等 破砕物 氷 水シート下 天井部分ａ ピニオンギヤ アングル 保管 系統図\n",
      "Topic 1: 扉枠 連続螺旋 ハンマー 製 ＡＭＤ 分割部分 上方空間 リフター 前記圧入口 矢印方向\n",
      "Topic 2: 参照 配置 位置 構造 ボルト 電気エネルギー 据付場所 保護膜 ニ－矢視図 結合部材\n",
      "Topic 3: 海底面 上部フロア 都市地下工事 右折モード 外径ｄ 一般防水層 音域 強度バランス 先行破壊 前記鉄骨枠材\n",
      "Topic 4: 上方 ｃ 発明 参照 力 符号ａ 柱剪断力 プレキャスト部材 上記水平方向規制部材 コンクリート構造体内部\n",
      "Topic 5: 一元管理 特殊用途 Ｌ字 隙間δ 周波域 カッタドラム 重量測定工程 チ－ 補強用コンクリート板 外建物\n",
      "Topic 6: 配置図 根太受け 該超音波パルス 加圧 排土板 扉枠開口部 判定基準値 吸着効果 コ 施工条件\n",
      "Topic 7: － Ａ 位置 ２つ 柱 鉄筋コンクリート造 前記本体部 °、シリンダ 電気的回路 柱面\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 台車 煙 山間部等\n",
      "Topic 9: セットロス セグメントリング相互 増し締め 耐摩鋼 トラックレール 幅広 切り換え 上記説明 真綿 櫓\n",
      "Topic 10: 点線表示 前記可動板 前記空気塊 基本コンクリート配合 孔機械 前部側 トンネル断面方向 鋼板どうし 有機系材料 詳細斜視図\n",
      "Topic 11: 孔発破 側断面 充填性 鉄筋同士 左側壁 埋立 避難口 トンネル形成方法 前記建物 トンネル径\n",
      "Topic 12: 施工 内側 Ａ 図面 計測対象 Ｇ ・・・チエーンブロック 塩ビタイル 排土板Ｈ ストレート管\n",
      "Topic 13: 脇 処理技術 有機系発泡剤 下部保持部材 前記掘削機 最新版管理 係止凹部 スラリー添加 連結状況 フロアパネル単体\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 11.0400 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : アンカー定着剤 アンカー定着剤アンプル アンカー 剤 質量部 アンカー定着方法 定着剤 液体成分 固体成分 アルミナセメント アンプル 定着 凝結遅延剤 質量 母材 発明 水 ～ 孔 界面活性剤 石灰類 調整剤 該アンカー定着剤 ポルトランドセメント 強度 添加剤 アンカー定着剤成分 膨張剤 量 実施例 アンカー挿入 成分 セメント 質量部～ アンカー定着剤自体 上記アンカー定着剤 使用量 軽質マグネシア アンカー定着剤容量 定着剤配合 上記 剤等 アンカー孔 ナトリウム塩 必須成分 減水剤 高性能減水剤 ガラスアンプル 混合物等 挿入性 方法 アンカー接着剤アンプル アンカー破断 挿入 骨材 セメント成分 ドデシルベンゼンスルホン酸塩 外管 硬化剤 分散剤 ＡＥ剤 増量剤 引抜耐力 炭酸カルシウム粒子 混合物 合計量 管 母材表面 コンクリート躯体 ドデシルベンゼンスルホン酸ソーダ グルコン酸ソーダ 混合性 アンカー自体 反応性 コンクリート ガラス カリウム塩 該アンカー 凝結 成分量 塩 表面 portland cement 混合 内管 揺変剤 等 リン酸塩 ハンマードリル 一軸圧縮強度 形態 割合 セメント組成物 セメント自体 特徴 消石灰 増 グルコン酸 アルミナ溶融セメント等 アルミン酸塩セメント アンプル自体 ホウ酸等 ポリカルボン酸等 ドリル等 岩盤 硬化 シリカヒューム 添加材 固体成分全量 オキシカルボン酸 任意成分 凹凸 カルボキシメチルセルロース スラリー 該孔 合計質量 凝結性 太平洋セメント製 アンプル素材 炭酸カルシウム コンクリート等 ガラス等 特許文献 存在下 質量比 Ｍ－ ハンマードリル等 使用 作業性等 法等 使用方法 結晶水等 酸化亜鉛等 上記実施例 範囲 直径 硫酸塩 結物 金属酸化物 岩石 低下 表 耐硫酸塩ポルトランドセメント 凝結反応 ドロマイトプラスター等 コスト等 たんぱく質等 ベントナイト等 シリカヒューム等 石膏等 繊維等 角度等 カップ等 岩盤等 レンガ等 カルシウムイオン 菱光石灰製 施工性 プラスチックバッグ等 熱ポルトランドセメント 分 使用割合 使用形態 水練り 通り水 混合比 孔内部分 合計 ウレタン結合含有ポリエーテル等 強度不足 強度発現 最終強度 花王製 使用状態 前記水分量 宇部マテリアル製 固形分 中庸熱ポルトランドセメント 天然 課題 目的 効果 市販 high early strength heat 逆 酸化銅 特性 所望 高炉水砕スラグ Ｐａ 原料 注入 破壊 容器 サイズ 掘削孔 径 夫 肉 支障 早強ポルトランドセメント 小野田製ハイパーエクスパン 普通ポルトランドセメント 上記割合 炭酸マグネシウム 固形分含量 － 技術分野 背景技術 海水法 カオリン 内壁表面 土木建築資材 工業化適正 単純円筒形 封入形態 ボルト ねじボルト 異形棒鋼 特開 号公報 発明者 不足 配位 ショ糖 程度 焼成温度 結 破片状 ガム質 ｓ程度 ｓ プラスチック 破片 人工岩石 商品名 ＨＦ 引抜試験 下記表 経時低下 配合 説明 水ガラス 粘性 濃度 開示 手段 研究 母体 最良 normal ultra moderate low sulfate resisting 両者 ムラ 材料 後述 封鎖 機能 有機物 無機物 事前 両方 light burned magnesia 製法 ℃、 ℃ 他 砕石 硅砂 砂 石英 白墨 粒状 繊維状 セルロース エトリンガイト 打撃 設 破砕 散逸 抑制 吸収 公知 隙間 該孔内 悪化 表面積 あたり 体積 容積 通常 マイティ 下表 数値 設置 ベース 管内 内部 確認 タイプ φ 試験\n",
      "\n",
      "===== # 2, Topic : 14, p : 15.0329 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : エネルギー吸収面体 止水面体 漂流物捕捉面体 吸収 エネルギー 孔体 貫通孔 面体 漂流物 海側 エネルギー吸収材料 防潮施設 内陸側 エネルギー吸収材 枠体 高潮 図 面状体 発明 方向 捕捉 実施例 複数 水 孔 地表面 構造物 繊維 面 千鳥状 施設 水平方向 陸側 防潮堤 複合体 説明 止水シート 面状 説明図 位置 柔軟性 板 実施 枠 構成 合成樹脂製 材料 多孔質繊維 連続孔 壁体 支持体 貫通孔群 波 引き違い戸 巻き取り式 透過率 壁 支柱 開口 地上 周囲 止水材料 止水材 特許文献 開口部 膜体 枠体ごと地上面 構造 複数段 海 分割状 ブロック状 格子状 海面 海水 技術 課題 延長 水辺 災害 図面 対象 上昇 レール 壁面 衝撃 合成樹脂 開口率 技術分野 背景技術 先行技術文献 台風 気圧 潮位 現象 民家 刑務所 次 目隠し 人々 エアバッグ 力 支持力 特徴 面積 効果 通常 形態 津波 水位 中間 意味 鋼材 ガレージ 商店 シャッター 曲面 穴 抵抗 効率 船舶 丸太 被害 機能 引き出し 空隙率 透水率 水平移動 鋼製 プラスチック製 アラミド繊維 炭素繊維 段 シート 擁壁 支持部材 衝撃力 波力 海岸 特公昭 号公報 本件発明 地球規模 異常気象 明細書 打ち抜き鋼板 コンクリート 鋼板 コンクリート板 海岸線 原因 風 満潮 川 状態 費用 制約 構想 － 概要 水域 住民 生活 文化 提案 両端 手段 上記 環境 襲来 参照 地震 豪雨 浸水 他 角度 同一 便宜 海底 網 一定 通過 隙間 減衰 両側 通行 風景 障害 段階 地形 数 寸法 組み合わせ 帯 公知 一般 素材 陸上 ヨット 漁船 貯木場 コンテナ 家屋 人体 各地 ベルト 自体 名称 目的 学校 校門 門扉 平常 三重 視界 作業 利点 雨戸 湾内 小型 損害 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 14.5552 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 画像 デジタルカメラ 図 カメラ 距離 レーザーポインタ装置 光 リフレクタ板 レーザー距離 前記デジタルカメラ 撮影 リフレクタ手段 模式図 請求項 該デジタルカメラ 表示パターン 広角撮影装置 ストロボ手段 カメラ光軸 前記カメラ光軸 装置 位置 広角 被写体 方位 発明 カメラ光軸Ｂ 画像処理方法 支持手段 前記カメラ光軸Ｂ レーザーポインタ支持手段 符号 壁面 前記ストロボ手段 方位磁石 被写体Ａ 方位撮影装置 b パノラマ画像 レーザー距離計 Ａ a 撮影装置 立坑 Ｂ 画像処理 距離ｄ ストロボ デジタル画像データ 壁面Ａ 上側リフレクタ 手段 写真撮影 カメラ位置 特許文献 ｄ 角度 距離測定 立坑壁面 再生画像 広角画像 ストロボ装置 参照 撮影領域 斜視図 前記再生画像 a)(b 東西南北 方位撮像レンズ 該カメラ光軸 光学系 方位撮影用レンズ トンネル 画像データ 周り 下側リフレクタ 縦線 構造 周画像 前記レーザー距離 回転台 前記レーザーポインタ装置 作業 一定寸法 寸法 該カメラ光軸Ｂ トンネル壁面Ａ 線 該レーザー距離 複数 特徴 下側回転台 断面図 回転 前記リフレクタ板 上側回転台 広角写真 シャッター 状態 θ 方向 該レーザー距離計 偏心 実施例 正面図 側面図 平面図 角度調整作業 角度調整 縮尺 c 仮想線 揺動角 揺動方向 該広角撮影装置 照射領域 坑軸 前記被写体Ａ 放射状 ｂ 前記被写体 格子状 光照射領域 作業者 一定 偏心位置 該再生画像 回転位置 d トンネル等 軸心 方位撮像用レンズ 上述 １つ 作動 ａ 横線 基準 坑壁 方位写真 該パターン パソコンＰ 方位レンズ 符号Ａ 前記光学系 ストロボライト 前記仮想線 写真 号公報 略 利用効率 取り付け状態 照明ポイント 格子 回転移動 該上側回転台 説明 記載 周囲 他 図面 効果 北 断面 ４つ ｃ アルミ板 略円板状 中心 補正 符号Ａ参照 該表示パターン 方法 上側 符号ｄ参照 a)～(d a)～(c データ 扇形状 符号θ サンプル点 円形断面 半円断面 該方位磁石 治具 ＰＡＬＮＯＮレンズ 略円筒状部材 外観 オートジャイロ ～ 観察 パソコン ° － 課題 影響 目的 一対 相互 煙突 短縮 操作 最良 形態 様子 円 頂点 スケッチ 三脚 上方 測定 サンプル点θ 符号α 上記治具 坑底 技術分野 背景技術 ひび割れ等 立山システム研究所製 偏心補正 パソコンＰＣ 倒立円錐形 特開 特開平 ひび割れ 岩種 均一縮尺 円形 α 内側縁 登録商標 中心付近 一般 判別 程度 コンピュータ 開示 対策 括弧 番号 要素 記述 好適 該壁面 該距離 菱形 表面 範囲 近傍 該画像 種々 発光 市販 鏡 元 下部 別 所定 蝶番 図示 株式会社 台座 該台座 作用 ３つ 等間隔 色 タブレット スライタスペン\n",
      "\n",
      "===== # 4, Topic : 14, p : 13.8147 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : エネルギー吸収面体 止水面体 面体 吸収 漂流物捕捉面体 エネルギー 孔体 貫通孔 地表面 漂流物 防潮施設 エネルギー吸収材料 海側 図 面状体 枠体 内陸側 エネルギー吸収材 実施例 高潮 発明 面 説明図 漂流物補足面体 水 孔 方向 複数 構造物 千鳥状 繊維 説明 施設 地上 陸側 複合体 防潮堤 面状 止水シート 支柱 構成 板 地表 柔軟性 実施 上昇 位置 海岸線 窓枠 障害物 重量物 壁面 合成樹脂製 材料 多孔質繊維 連続孔 壁体 支持体 貫通孔群 状態 波 巻き取り式 透過率 止水材料 壁 延長 開口 周囲 平常 面体自体 水平移動 水平方向 止水材 膜体 開口部 構造 複数段 枠体ごと地上 海 分割状 ブロック状 格子状 海面 海水 課題 水辺 災害 通常 図面 対象 補足 遊歩道 衝撃 障害 溝 合成樹脂 開口率 台風 気圧 潮位 現象 海岸 民家 技術 刑務所 次 目隠し 人々 エアバッグ 力 支持力 特徴 面積 効果 地中 形態 津波 水位 他 中間 意味 鋼材 ガレージ 商店 シャッター 曲面 穴 抵抗 効率 組み合わせ 船舶 被害 機能 下降 上方 屋根 人力 空隙率 透水率 鋼製 プラスチック製 アラミド繊維 炭素繊維 可能性 段 シート 技術分野 背景技術 擁壁 支持部材 衝撃力 波力 複合動作 コンクリート板 保護板 本件発明 地球規模 異常気象 明細書 打ち抜き鋼板 コンクリート 鋼板 自体 担当者 縦溝 動作 設置状態 原因 風 満潮 川 費用 制約 構想 開示 水域 住民 生活 文化 提案 両端 手段 上記 環境 最良 参照 地震 豪雨 浸水 角度 同一 便宜 海底 網 一定 通過 隙間 減衰 両側 通行 風景 段階 地形 数 寸法 帯 公知 空間 一般 素材 陸上 ヨット 漁船 家屋 人体 各地 ベルト 名称 目的 雨よけ 湾内 丸太 小型 損害 下方 電気 油圧 レール 学校 校門 引き出し 門扉 作業 利点 ジャッキ 作動 軽量 襲来 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 12.9823 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 光 反射板 反射光 反射 導光部 採光装置 色温度 採光部 調光部 導光部床面 角度 前記主反射板 光調節機構 色温度変換フィルター 照明物 照射角度 特定地域 発明 前記調光部 反射面 色温度調節機構 透過板 請求項 前記採光部 前記導光部 図 前記反射光 調節 照射方向 光部 光強度 照明 太陽光 ｂ 照射角度調節機構 ｄ ｃ 説明 強度 ＮＤフィルター 実施形態 記載発明 光ダクト 日差し 前記 照射 反射板制御手段 前記照射角度調節機構 太陽 ～ 側面説明図 照射物 記載 透過光 分光エネルギー分布 列 パンチングメタル製 追尾手段 緯度地域 ｄ導光部床面 複数 ｐ 前記色温度 方法 Ｏｆｆ 特許文献 導光効率 状態 自動車 フィルター 前記光強度 自然光 光ファイバ 製品 Ｏｎ 所定方向 透過率 太陽追尾手段 色 前記採光装置 補助 ｓ 特徴 ａ 方向 高緯度地域 ｐ反射 － 地域 帯 所定 位置 緯度 設計室 穴 ｄ列 前記特定地域 照明効果 人工太陽灯 特定 ｅ列 孔 上記採光装置 パンチングメタル 金属板 ～（ｃ 真上 機構 物面鏡 可能性 角度用 号公報 ハーフミラー 電動式 製 ｅ 平面説明図 ＮＤフィルター ｃ列 種々 下方 サイズ 数 部分 平面透視説明図 地域固有 発明者等 手段 形態 方法等 開発製品 製品開発 前記太陽光 開発製品等 等 Ｏｎ－Ｏｆｆ 可変ＮＤフィルター ｂ列 使用方法 人口太陽灯 商品開発者 開発コスト 列～ Ｏｆｆ状態 Ｏｎ－Ｏｆｆ切り替え 奥方向 左右方向 使用方法例 ｂ等 室内 内面 課題 世界 検討 要素 目的 向き 前方 場所 スペース 残り 代わり 幅 使用 気象条件等 ショールーム等 自動車等 平面鏡 使用例 Ｏｎ状態 効果 高緯度 特開昭 家具調度品 技術分野 背景技術 デザイン性 特開平 展示品 ファッション用品 見え方 複数種類 適確 光量割合 紙面手前 光量 割合 ｓ補助 開示 利点 関心 物品 実現 提供 試験 品質 スピードアップ 陰り 影響 最良 通常 開口 経路 特性 一辺 側壁 ° モータ 一軸 軸 先 両者 θ °）。【 Ｓ 朝夕 別 雰囲気 横 後方 奥行き 独立 斜線 両方 構成 使い方 図面 符号 射光\n",
      "\n",
      "===== # 6, Topic : 14, p : 15.2124 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : カッタビット ゲート部材 水密箱 スライド式開閉ゲート ゲート部 カッタビット支持レール 装填 図 シール部材 進機 カッタビット装填 カッタスポーク カッタビット装填用ゲート機構 既設カッタビット 円筒部 カッタビット装着溝 カッタビット装填機構 前記ゲート部材 支持アーム カッタビット交換方法 該カッタビット装填用ゲート機構 カッタヘッド ゲート開閉用 ゲート Ａ 交換用 ジャッキ カッタビット交換 カッタビット供給孔 供給用水密箱 ゲート下部材 特許文献 支持レール 固定板 ｃ 水密箱支持具 前記カッタヘッド中心円筒部 機外側 カッタビット収容室 駆動装置 連通 支持突部 カッタヘッド中心円筒部 水密箱等 機内側 支点部材 該ゲート部材 側 ゲート上部材 前端 流体噴射ノズル カッタビット交換用 カッタビット装填用開閉ゲート機構 押し出しジャッキ 中心部 開閉扉 端面 状態 箱 土砂等 開口部 Ｂ 前記支持レール 回転カッタ 後端側 面 回転体 前記カッタビット装填用ゲート機構 前端部 進 発明 形態 交換装置 係止部 交換 カッタビット装填装置 面側 カッタビット交換装置 カッタビット押出手段 ビット装填用ゲート機構 説明図 支持レール側 前記カッタスポーク側 最後位置 実施 ゲート部材ガイド 前記駆動装置 説明 水 拡大図 径外方向 本体側 カッタヘッド中心側 装 カッタ駆動モータ 固定ジャッキ 後方 下 断面 動作 ビット交換用 該ジャッキ 該カッタビット装着溝 領域Ａ部 掘削土砂 回転側 構造 カッタビット追加機構 端部 該カッタビット収容室 交換方法 円筒状 方向 回転 上端面 接面 一つ 特徴 参照 記載 上記 スキンプレート 前記開閉動作 中心部回転体内 開閉口 方法 機内 位置 機内側端部 筒状 水密状態 固定側 進機前端 水密化 径方向中心側 工程 進機本体 筒 支点 回転カッタ外周側 破損等 最前位置 前記 進機外 コスト 土砂 前記回転体 前記カッタスポーク 回転カッタ前面 複数 スペース 前方 符号 前面側 上部 Ｂ側 Ｈ線 Ｊ線 可能性 接触状態 胴部 噴射力 掘削機内部 文献 特開 号公報 請求項 接触 既設 介在物 チェーンブロック 交換頻度増加 連通位置 進機軸方向 板状体 動作説明図 最前端 シャッタ 目的 姿勢 サポート ｂ 破線 該支持レール 図参照 径方向 機軸方向 下端面 係止力 当該カッタスポーク ジャッキ台 軸方向断面図 ～図 開口 放射方向 単一 相対 － 課題 前述 製作 他方 内側 技術 放射状 全開 ロッド 空間 設置 構成 隔壁 側面 ボルト 閉動作 他 下方 シールドトンネル掘削工事 内筒 治具 手段 ビット 上端 技術分野 背景技術 参照)。　 困難性 外径部 外方 外径 丸Ｂ 長距離化 該回転体内 当該図面 維持管理 挿入圧力 工事 製造コスト 問題点 ～ 本明細書 油圧系統 ロータリージョイント スクリューコンベア 谷形 内部 装入 圧力 地山 規格通り ずれ ずれ防止 図面 対策 種々 開示 同数 数 使用 利点 意味 コストパフォーマンス ペース 複雑 上下 ）～（ 効果 最良 コピーカッタ ギア 図示 両者 凹 形状 後述 移動 実線 理由 流入 周り 低下 よう テーパ アール Ｒ 曲面 閉 タイミング 操作 同等 下面 山谷 ピッチ バルブ 梃子 腕 先端 上方 置後 部分 凹陥部 ｆ\n",
      "\n",
      "===== # 7, Topic : 14, p : 12.8808 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 透光性樹脂製折板 上記透光性樹脂製折板 折板屋根 金属折板 取付用下地金物 上記取付用下地金物 透光性樹脂 台形状 屋根 樹脂製 樹脂 該透光性樹脂製折板 台形状タイトフレーム 金物 取付用押え金物 外端縁部 図 取付方法 上記台形状タイトフレーム 折版屋根板 端縁部 開口部 折板状 突設ボルト 該台形状タイトフレーム 金属製折版屋根材 採光用 外側端部 屋根梁 上記金属製折板 発明 樹脂製折版屋根材 上記折版屋根板 説明図 金属製 端部 折板形状 特許文献 該金属折板 上記開口部Ｂ 上底部分 前記開口部 構造 取付構造 金属折板側 ポリカーボネート製折版屋根材 頂部 下端部 該下端部 採光用開口部 ｂ ユーピロン・サンガード折版 取付用押え金物Ｂ 上記特許文献 該取付用下地金物 説明 明り採り部材 端縁部同士 補強材 金属製キャップ 左右端部 開口部Ｂ 上部 特徴 明かり取り ステップ 係合端縁部 上記開口部両側 上記ボルト 材料 施設 上記突設ボルト 実施例 通常タイトフレーム 固定手段 利用可能性 ハゼ部 接続部 中間部 両端部 特許 等 新設 左右 技術 漏水 原因 手段 三菱エンジニアリングプラスチックス株式会社製 フッ素樹脂焼付塗装 穴 熱膨張 伸縮率 号公報 既存建物 止水 上記固定手段 上記通常タイトフレーム 上記実施例 ボルト等 課題 工場 傷 亀裂 ナット 取り付け方法 発明方法 参照 タイトフレーム 差 表面 延命 上方 下記 シーリング メンテナンス 台風 外方 ｃ キン 駅 アーケード 増設 傾斜側面部分 ナット等 明り取り部材 亀裂等 ワッシャ等 建物 技術分野 背景技術 挟圧し 穴断面 挟圧状態 穴明け 上面形状 略同形 留め具 経年劣化 強度低下 傷付き 爆風破断 改修 ポリカーボネート 商品名 雨漏り防止 断面 摩擦活性 改修工事 追加新設 太陽光 シールパッキン 進行 公昭 － 開示 既設 効果 利点 現場 固着 拘束 替え 懸念 応用 設置 シール ビス スパナ 最良 形態 手前 上側 形成 ＦＲＰ 素材 スペーサー 風 バタツク 暴風雨 程度 スポンジ 加工 構成 産業 用途 上下 風圧 屋外 上屋 図面 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 13.0293 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 反応材 粉体浄化材 反応材注入ホース 注入ノズル 地盤改良機 注入管 粉体浄化材供給ライン 注入攪拌装置 上記反応材注入ホース 反応材注入手段 注入ホース保護管 上記注入ノズル 上記反応材注入手段 管 地盤改良システム 粉体浄化材供給路α 攪拌軸 上記注入ホース保護管 上記攪拌軸 ノズルカバー 給路 上記内側ホース 内側管 反応材供給ライン 上部軸受 上記注入攪拌装置 請求項 排出路β 外側ホース 外側管 図 地盤改良 上記粉体浄化材 上部 内側ホース 上記 上記ブレ止めスペーサ 上記地盤改良機 供給 気検知槽 上記外側ホース ブレ止めスペーサ 上記上部軸受 上記攪拌翼 上記ベント管 排出口 上記反応材 発明 浄化材供給機 浄化材噴出孔 ベースマシン 該注入ホース保護管 ガイド管 上記注入管 応材送給ホース 外側 周辺地盤 保護外管 ベント管 上記粉体浄化材供給路α 粉体浄化材排出路β 上記リーダ 管構造 リーダ 上記排出路β 撹拌軸 攪拌翼 防護圧送ホース ホースジョイント 記載 分岐管 内管 該内側ホース ベント管接続端側 側端部 排出路 鋼製 気検知部 上記地盤改良システム 反応材圧送ポンプ 性 該注入攪拌装置 上記反応材圧送ポンプ 該浄化材供給機 反応材収容槽 ｂ ｃ 土詰まり 給ライン 地盤改良工法 地盤改良工事 上記保護外管 環状空処 噴出孔 保護被覆ホース 上記地盤改良機Ａ 微細鉄粉 該漏気検知槽 下端 上記防護圧送ホース 上記ガイド管 上記ノズルカバー 下部 供給口 上記分岐管 上記内管 発明地盤改良システム 程度 水 側 要部拡大図 拡大側面図 上下部 端部 サニーホース フラットホース 該注入ノズル リン酸 内部 座屈 実施例 所要 圧送 上記システム 地中 土壌 上端 ゴム管 電線管 エフレックス管 管状 該排出路β 上記撹拌軸 正面図 側奥部 上部軸受付近 上記環状空処 破損 噴出 周辺環境 反対側端 上流側端 下流側端 引上げ 土砂 先端 部分 手段 汚染物質 耐薬品性 要部 特許文献 側面図 下側 説明 過酸化水素 摩擦 i ii 水中 前面 ビクトリックジョイント 内径 エア 上端部 状態 下降 攪拌軸挿通部 α 周辺 土 等 混合液等 筒状 混合液 詰まり防止 構成図 該撹拌軸 課題 周囲 ～ 下方 スイベル 肉厚合成樹脂製 鋼線 中空筒状 トリクロロエチレン等 サンドブラストホース等 タイロンホース等 付近 技術分野 背景技術 汚染土壌 地中所要 所要位置 ネジジョイント 螺旋状 耐摩耗性 混合ミキサ 触媒酸化法 μｍ程度 相対摺動 下端付近 位置 工法 特開 複数本 貫入下降 泥濘化 クローラ式 連結板 固定金具 径 粒径 終端 防止 終端開口 箇所 屈曲状態 破損箇所 圧送注 各種 参照 一つ － 開示 通り バックホウ 走行 作業 尺 小径 被害 提供 目的 iii 効果 飛散 最良 形態 一対 直上 両端 弧状 通路 始端 昇降 次 自体 外力 内面 悪化 泡 図面 符号 入管\n",
      "\n",
      "===== # 9, Topic : 14, p : 12.6260 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 鋼管柱 口部ユニット 梁 鋼管部材 エンドプレート 開口部 ボルト挿通孔 上記鋼管柱 ボルト 柱梁接合構造 口部鉄骨 柱 高力ボルト 梁ユニット 梁ブラケット 外方側 連結手段 鋼管 鉄骨造 上記 上記エンドプレート 内方側 内側ダイヤフラム 上記鉄骨柱 請求項 柱梁架構 上記梁ブラケット 接合部 ダイヤフラム 外側ダイヤフラム 鉄骨柱 上下端部 端部 構造部材 上記ダイヤフラム 下端部 柱梁 梁等 部材 応力伝達 応力 上記梁ユニット 発明 上記鋼管部材 架構構造 先端部 架構 上記外方側 外径 解体 ｂ 図 上下部フランジ 連結部 ｃ 記載 ナット フランジ 外周側 当該鋼管柱 上記開口部 継ぎ手板 複数 上記柱梁架構 口部ユニット側 せん断応力 当該鉄骨柱 外方 接合 当該開口部 連結 挿通孔 中心部 要部 ａ 該柱梁架構 内側 上記作業 周側 利用 角形鋼管 当該ボルト 内径 上記構成 建築物 構成 上記構造部材 中間位置 梁成 外周 口 中間 側 当該ボルト挿通孔 特許文献 周方向複数箇所 円周方向 上記外周側 口径 特徴 人 双方 ｄ 下記特許文献 上記フランジ フランジ同士 位置 手段 実施形態 Ｈ形鋼 課題 ～ 上記実施 作業 形態 ブラケット 接合強度 当該応力伝達 建築廃棄物 処分量 特開 号公報 円環状 断面形状 上記連続位置 建設廃棄物 上記課題 産業廃棄物 説明 剛性 組立 寸法 ウエブ 該外方側 建築物自体 窒素酸化物等 当該解体作業 鉄筋コンクリート造 角形等 多角形 建設残土等 － 規模 耐力 内法 同等 本数 最良 符号 φ 等間隔 互い 状態 外壁 内壁 内部 扉等 建築現場 近傍位置 ～図 分解図 平面図 正面図 断面 形状 技術分野 背景技術 リサイクル性 耐震性 可能性 各種形態 軸線 軸力 有効利用 地球環境 リサイクル 問題点 近傍 肉厚 階高 平面 保守用 資源 二酸化炭素 排出 負荷 低減 動き コンクリートガラ 鉄材 発生 材料 試み 地震 変形 コンクリート 設 溶接 手間 原型 利点 開示 両者 虞 事情 効果 所望 スパン 力学 円形 金物 汎用 面 並び 各々 他方 挿入 間隔 上方 点検 図面\n",
      "\n",
      "===== # 10, Topic : 14, p : 13.1264 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ライン状照明装置 性部材 光 光伝送路 照明 図 光透過性部材 中空状 字状 中空部材 鏡面 スリット 照明装置 壁面 上記中空部材 光透過部材 光源 天井面 上記中空状 上記 フレーム 上記ライン状照明装置 中空体 スリット状 実施例 取付部材 開口 上記スリット 縁 ライン状 性 特許文献 パネル 発明 部材 ハードケース 開放端 ｂ 端 上記光透過性部材 面 長手方向 壁 照明等 上記光透過部材 内面 ａ 部分 上記鏡面 字状取付部材 コ 導光シート 光伝送部分 筒状 アルミ 保護パイプ 天井内部 製造コスト 建築物 Ｌ字状 開口部 光源光 上記中空体 該可撓性部材 アルミ鏡面 両端 構成 方向 施工性 構成例 両端開口 中空体内 場所 丸パイプ 外 上記実施例 アルミ材 設置手順 上記光源 天井 断面図 号公報 断面 押し縁 アクリル板 上記壁面 端部 説明 外部 平面 両側 一対 天井内部等 課題 形状 樹脂 突起 ビス 上記課題 上記パネル 上記コ 上記フレーム 発光面 壁面等 鏡面反射機能 他方端 光源部分 反射シート 発光部 トンネル 状態 － 外周 外側 効果 隙間 設置 幅 径 孔部 樹脂等 ビス等 配管等 突起部 紙面左右方向 手順 技術分野 背景技術 設置場所 取り付け部分 特開平 特開 範囲 展示場 マイクロプリズム 問題点 配管 楕円形状 アラノッド社 登録商標 紙面 白熱電球 蛍光灯 応用範囲 面積 位置 道路 一般 建物 廊下 空間 領域 通路 外装 記載 前記 片端 開示 構造 目的 手段 次 アルミニウム 最良 形態 ミロ シルバー 表面 木材 厚み 金属 φ ～ 程度 役割 内側 側 ロス 参照 メタルハライドランプ ＬＥＤ 下 裏側 手 ねじ ハードカバー 向上 拡大 取付け 図面 部品 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4018e-41, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4666e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9213e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 55758432.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1113.3629\n",
      "Test epoch : 1 Average loss: 1040.5516\n",
      "PP(train) = 9326.139, PP(valid) = 9396.067\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2025e-39, 0.0000e+00, 1.4312e-31, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.4416e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9970e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 50266068.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1108.9252\n",
      "Test epoch : 2 Average loss: 1039.8685\n",
      "PP(train) = 9283.979, PP(valid) = 9358.105\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1247e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.1642e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6737e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 45314720.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1104.7792\n",
      "Test epoch : 3 Average loss: 1039.2618\n",
      "PP(train) = 9238.836, PP(valid) = 9319.079\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.0110e-39, 0.0000e+00, 6.5244e-33, 0.0000e+00,\n",
      "         0.0000e+00, 2.2042e-42, 1.5865e-15, 5.3508e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.5143e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 40851092.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1101.1774\n",
      "Test epoch : 4 Average loss: 1038.7344\n",
      "PP(train) = 9189.198, PP(valid) = 9276.978\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5137e-36, 1.1771e-43, 6.8363e-30, 2.8026e-45,\n",
      "         0.0000e+00, 3.6921e-33, 6.6323e-33, 1.0229e-43, 0.0000e+00, 5.4934e-40,\n",
      "         3.5591e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 36827144.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1098.0652\n",
      "Test epoch : 5 Average loss: 1038.2565\n",
      "PP(train) = 9136.418, PP(valid) = 9232.389\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1461e-24, 0.0000e+00, 1.3822e-32, 0.0000e+00,\n",
      "         0.0000e+00, 9.1949e-33, 1.8835e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.3346e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 33199566.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1094.8295\n",
      "Test epoch : 6 Average loss: 1037.7920\n",
      "PP(train) = 9084.106, PP(valid) = 9188.549\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2163e-36, 0.0000e+00, 8.1490e-29, 0.0000e+00,\n",
      "         0.0000e+00, 4.2642e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0168e-11, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 29929316.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1092.0337\n",
      "Test epoch : 7 Average loss: 1037.2927\n",
      "PP(train) = 9033.206, PP(valid) = 9145.947\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.7514e-26, 0.0000e+00, 1.4420e-37, 0.0000e+00,\n",
      "         0.0000e+00, 1.3000e-30, 4.5718e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.9338e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 26981194.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1089.5112\n",
      "Test epoch : 8 Average loss: 1036.7802\n",
      "PP(train) = 8983.216, PP(valid) = 9104.245\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.2884e-37, 0.0000e+00, 1.6890e-20, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.4927e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2453e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 24323470.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1087.0796\n",
      "Test epoch : 9 Average loss: 1036.2472\n",
      "PP(train) = 8935.121, PP(valid) = 9064.091\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.2397e-39, 0.0000e+00, 2.7350e-30, 0.0000e+00,\n",
      "         0.0000e+00, 4.5003e-35, 4.5250e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.6173e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 21927540.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1084.8997\n",
      "Test epoch : 10 Average loss: 1035.7117\n",
      "PP(train) = 8887.716, PP(valid) = 9024.397\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.3149e-32, 0.0000e+00, 1.0267e-27, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0308e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.4255e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 19767616.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1082.7629\n",
      "Test epoch : 11 Average loss: 1035.1723\n",
      "PP(train) = 8841.220, PP(valid) = 8985.546\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.5464e-41, 0.0000e+00, 1.0001e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.1034e-39, 7.6711e-28, 1.5471e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.4607e-07, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 17820450.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1080.8744\n",
      "Test epoch : 12 Average loss: 1034.6363\n",
      "PP(train) = 8794.724, PP(valid) = 8946.547\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6648e-38, 0.0000e+00, 1.8911e-24, 0.0000e+00,\n",
      "         0.0000e+00, 3.2835e-38, 5.6177e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0044e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 16065086.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1079.1114\n",
      "Test epoch : 13 Average loss: 1034.1310\n",
      "PP(train) = 8747.655, PP(valid) = 8907.062\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, 1.6689e-42, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.8319e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.1722e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 14482630.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1077.5869\n",
      "Test epoch : 14 Average loss: 1033.6425\n",
      "PP(train) = 8699.627, PP(valid) = 8866.441\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.2164e-29, 6.7262e-44, 3.8335e-20, 0.0000e+00,\n",
      "         0.0000e+00, 2.1845e-28, 1.4404e-29, 9.8091e-45, 0.0000e+00, 0.0000e+00,\n",
      "         1.6418e-13, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 13056050.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1076.0246\n",
      "Test epoch : 15 Average loss: 1033.1623\n",
      "PP(train) = 8652.035, PP(valid) = 8826.283\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.5526e-34, 0.0000e+00, 1.8146e-19, 0.0000e+00,\n",
      "         0.0000e+00, 7.0065e-45, 4.5369e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.6406e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 11769992.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1074.6062\n",
      "Test epoch : 16 Average loss: 1032.6893\n",
      "PP(train) = 8604.868, PP(valid) = 8786.391\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.4008e-41, 0.0000e+00, 7.6506e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.6854e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.8639e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 10610615.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1073.1935\n",
      "Test epoch : 17 Average loss: 1032.1839\n",
      "PP(train) = 8559.713, PP(valid) = 8748.197\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2339e-27, 2.9660e-34, 1.3445e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.2278e-39, 1.6342e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.0425e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 9565440.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1071.9678\n",
      "Test epoch : 18 Average loss: 1031.6701\n",
      "PP(train) = 8514.876, PP(valid) = 8710.270\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.2320e-26, 3.5796e-41, 1.5135e-06, 1.9945e-40,\n",
      "         2.8026e-45, 1.3851e-28, 3.4205e-16, 1.5465e-40, 5.0574e-35, 1.9648e-36,\n",
      "         5.1981e-01, 6.2965e-39, 4.8019e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0636, 0.0594, 0.0907, 0.0567, 0.0506, 0.0673, 0.0769, 0.0871, 0.0684,\n",
      "         0.0726, 0.0557, 0.0555, 0.0561, 0.0690, 0.0704]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 8623217.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1070.9102\n",
      "Test epoch : 19 Average loss: 1031.1667\n",
      "PP(train) = 8470.654, PP(valid) = 8672.754\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7757e-26, 1.5770e-39, 6.8066e-22, 0.0000e+00,\n",
      "         0.0000e+00, 8.1213e-33, 1.8935e-26, 1.5728e-37, 0.0000e+00, 0.0000e+00,\n",
      "         7.3987e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7773806.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1069.6264\n",
      "Test epoch : 20 Average loss: 1030.6626\n",
      "PP(train) = 8426.581, PP(valid) = 8635.252\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.8282e-44, 0.0000e+00, 3.4857e-30, 0.0000e+00,\n",
      "         0.0000e+00, 2.9916e-41, 2.1242e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.9971e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7008064.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1068.4507\n",
      "Test epoch : 21 Average loss: 1030.1687\n",
      "PP(train) = 8382.434, PP(valid) = 8597.617\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.1012e-40, 0.0000e+00, 2.6806e-20, 0.0000e+00,\n",
      "         0.0000e+00, 8.9311e-35, 2.7017e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2027e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6317750.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1067.5108\n",
      "Test epoch : 22 Average loss: 1029.6635\n",
      "PP(train) = 8339.241, PP(valid) = 8560.872\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2872e-40, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0249e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0991e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5695434.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1066.4610\n",
      "Test epoch : 23 Average loss: 1029.1624\n",
      "PP(train) = 8296.058, PP(valid) = 8524.050\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6062e-36, 2.9574e-32, 1.9670e-16, 0.0000e+00,\n",
      "         0.0000e+00, 4.4775e-37, 2.9501e-20, 3.6174e-35, 0.0000e+00, 5.6052e-44,\n",
      "         1.9344e-11, 8.7124e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5134417.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1065.5935\n",
      "Test epoch : 24 Average loss: 1028.6832\n",
      "PP(train) = 8252.351, PP(valid) = 8486.632\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.4203e-33, 0.0000e+00, 8.4046e-20, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.8310e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.4089e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4628663.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1064.5226\n",
      "Test epoch : 25 Average loss: 1028.2119\n",
      "PP(train) = 8209.288, PP(valid) = 8449.798\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.8468e-27, 0.0000e+00, 7.1011e-19, 0.0000e+00,\n",
      "         0.0000e+00, 1.1706e-27, 1.5278e-21, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.6707e-26, 1.3705e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4172726.75000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1063.7013\n",
      "Test epoch : 26 Average loss: 1027.7301\n",
      "PP(train) = 8166.411, PP(valid) = 8412.995\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.5532e-41, 0.0000e+00, 1.0192e-26, 0.0000e+00,\n",
      "         0.0000e+00, 2.9427e-44, 3.5356e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0949e-33, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3761701.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1062.7419\n",
      "Test epoch : 27 Average loss: 1027.2338\n",
      "PP(train) = 8124.650, PP(valid) = 8377.191\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8026e-45, 0.0000e+00, 1.3704e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.0774e-34, 9.4200e-29, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2875e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3391163.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1061.9443\n",
      "Test epoch : 28 Average loss: 1026.7458\n",
      "PP(train) = 8083.040, PP(valid) = 8341.461\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.3438e-31, 0.0000e+00, 9.6267e-20, 1.0496e-42,\n",
      "         0.0000e+00, 8.4585e-35, 3.3241e-35, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         7.9493e-11, 1.2444e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3057124.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1061.1535\n",
      "Test epoch : 29 Average loss: 1026.2439\n",
      "PP(train) = 8042.349, PP(valid) = 8306.587\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4902e-30, 3.6434e-44, 4.2632e-30, 0.0000e+00,\n",
      "         0.0000e+00, 1.9772e-37, 1.0145e-35, 0.0000e+00, 0.0000e+00, 7.7975e-39,\n",
      "         7.0251e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2755988.75000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1060.5193\n",
      "Test epoch : 30 Average loss: 1025.7667\n",
      "PP(train) = 8001.046, PP(valid) = 8271.000\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0940e-40, 0.0000e+00, 6.8076e-26, 0.0000e+00,\n",
      "         0.0000e+00, 4.2039e-45, 2.8537e-24, 2.8026e-45, 0.0000e+00, 0.0000e+00,\n",
      "         8.8459e-12, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2484516.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1059.4622\n",
      "Test epoch : 31 Average loss: 1025.2800\n",
      "PP(train) = 7960.426, PP(valid) = 8235.910\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.9466e-43, 0.0000e+00, 5.6955e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.3748e-33, 3.6402e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.9577e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2239784.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1058.8300\n",
      "Test epoch : 32 Average loss: 1024.7796\n",
      "PP(train) = 7920.436, PP(valid) = 8201.432\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 2.5424e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.6859e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2804e-17, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2019159.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1058.0931\n",
      "Test epoch : 33 Average loss: 1024.2884\n",
      "PP(train) = 7880.706, PP(valid) = 8167.183\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.5464e-39, 0.0000e+00, 1.3463e-40, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.8757e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.9838e-35, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1820266.37500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1057.2083\n",
      "Test epoch : 34 Average loss: 1023.7913\n",
      "PP(train) = 7841.901, PP(valid) = 8133.697\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.7728e-36, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.2250e-27, 1.5669e-41, 0.0000e+00, 0.0000e+00,\n",
      "         5.4638e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1640965.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1056.5887\n",
      "Test epoch : 35 Average loss: 1023.3058\n",
      "PP(train) = 7802.602, PP(valid) = 8099.746\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7344e-36, 0.0000e+00, 7.5710e-32, 0.0000e+00,\n",
      "         0.0000e+00, 2.4063e-29, 7.7068e-28, 0.0000e+00, 0.0000e+00, 2.3738e-42,\n",
      "         5.3983e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1479325.37500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1055.9440\n",
      "Test epoch : 36 Average loss: 1022.8402\n",
      "PP(train) = 7763.130, PP(valid) = 8065.453\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1334e-13, 1.1996e-29, 1.4571e-10, 1.5554e-42,\n",
      "         0.0000e+00, 3.0251e-38, 1.3858e-33, 1.5982e-38, 0.0000e+00, 1.4013e-45,\n",
      "         3.9753e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1333607.62500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1055.2942\n",
      "Test epoch : 37 Average loss: 1022.3667\n",
      "PP(train) = 7723.777, PP(valid) = 8031.220\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.3168e-35, 1.9618e-44, 1.4542e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.4153e-33, 2.5514e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6164e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1202243.50000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1054.5850\n",
      "Test epoch : 38 Average loss: 1021.8990\n",
      "PP(train) = 7684.660, PP(valid) = 7997.232\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.4592e-39, 0.0000e+00, 4.9883e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.2877e-40, 7.1564e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1893e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1083819.12500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1053.9218\n",
      "Test epoch : 39 Average loss: 1021.4286\n",
      "PP(train) = 7646.441, PP(valid) = 7963.957\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.0166e-43, 0.0000e+00, 2.1074e-32, 0.0000e+00, 2.4722e-14, 2.8026e-45,\n",
      "         0.0000e+00, 7.0127e-34, 2.8698e-20, 9.2598e-42, 0.0000e+00, 0.0000e+00,\n",
      "         1.1627e-09, 9.9226e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 977059.87500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1053.3307\n",
      "Test epoch : 40 Average loss: 1020.9626\n",
      "PP(train) = 7608.325, PP(valid) = 7930.881\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0601e-31, 0.0000e+00, 7.7071e-19, 0.0000e+00,\n",
      "         0.0000e+00, 1.3089e-29, 1.2654e-19, 1.6956e-43, 0.0000e+00, 5.6052e-45,\n",
      "         8.5673e-16, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 880816.75000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1052.7229\n",
      "Test epoch : 41 Average loss: 1020.4954\n",
      "PP(train) = 7570.154, PP(valid) = 7897.574\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2612e-44, 0.0000e+00, 1.7846e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.4946e-39, 2.4531e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0136e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 794053.81250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1051.9710\n",
      "Test epoch : 42 Average loss: 1020.0217\n",
      "PP(train) = 7532.533, PP(valid) = 7864.774\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.8437e-34, 0.0000e+00, 1.1210e-44, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 3.8079e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.1314e-23, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 715837.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1051.3343\n",
      "Test epoch : 43 Average loss: 1019.5415\n",
      "PP(train) = 7496.122, PP(valid) = 7832.984\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.9727e-26, 4.4490e-35, 1.6500e-20, 1.1980e-41,\n",
      "         0.0000e+00, 9.4950e-27, 7.4965e-22, 6.7725e-42, 0.0000e+00, 3.8886e-42,\n",
      "         1.1801e-09, 8.0575e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 645325.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1050.7035\n",
      "Test epoch : 44 Average loss: 1019.0622\n",
      "PP(train) = 7459.396, PP(valid) = 7800.980\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.2599e-43, 1.6686e-26, 3.6968e-28, 4.3260e-14, 2.8026e-45,\n",
      "         1.4013e-45, 3.5161e-26, 1.3338e-18, 1.4401e-35, 0.0000e+00, 1.1403e-38,\n",
      "         8.5371e-04, 2.4271e-32, 9.9915e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 581758.87500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1050.2476\n",
      "Test epoch : 45 Average loss: 1018.6085\n",
      "PP(train) = 7422.262, PP(valid) = 7768.507\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6199e-30, 4.0194e-40, 3.8053e-16, 0.0000e+00,\n",
      "         0.0000e+00, 9.4410e-34, 9.3939e-27, 1.8693e-42, 0.0000e+00, 0.0000e+00,\n",
      "         1.7584e-12, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 524454.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1049.4820\n",
      "Test epoch : 46 Average loss: 1018.1637\n",
      "PP(train) = 7385.066, PP(valid) = 7735.940\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7606e-40, 0.0000e+00, 7.0065e-45, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.1923e-37, 0.0000e+00, 0.0000e+00, 3.3631e-44,\n",
      "         8.7984e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 472793.81250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1049.1428\n",
      "Test epoch : 47 Average loss: 1017.7146\n",
      "PP(train) = 7348.358, PP(valid) = 7703.790\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3355e-38, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.2459e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.5222e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 426222.28125\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1048.2135\n",
      "Test epoch : 48 Average loss: 1017.2350\n",
      "PP(train) = 7312.997, PP(valid) = 7672.804\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0377e-30, 0.0000e+00, 2.3930e-29, 0.0000e+00,\n",
      "         0.0000e+00, 4.8899e-38, 2.1571e-38, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         8.2508e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 384238.18750\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1047.6744\n",
      "Test epoch : 49 Average loss: 1016.7495\n",
      "PP(train) = 7278.133, PP(valid) = 7642.278\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8026e-44, 0.0000e+00, 6.5096e-32, 0.0000e+00,\n",
      "         0.0000e+00, 5.6052e-44, 1.2170e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.9062e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 346389.65625\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1047.1801\n",
      "Test epoch : 50 Average loss: 1016.2896\n",
      "PP(train) = 7242.405, PP(valid) = 7610.923\n",
      "Writing to ./topicwords/3-topwords_e50.txt\n",
      "Topic 0: 掘削土砂 背景技術 搬出 カッタチャンバ 字状 処理装置 位置決め手段 配合条件 技術手段 炭素繊維\n",
      "Topic 1: 酸性 高温 係止 質量 解体 圧送ポンプ 転倒 角 未満 悪影響\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 技術 発明 図面 手段 課題\n",
      "Topic 3: 他端 一端 セグメントリング 建築物 構造体 欠点 空間 地盤 管 破損\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 手段 説明 技術 形態\n",
      "Topic 5: 水圧 スペース 寿命 透水性 セメント 回転速度 側面部 推進ジャッキ 圧送ポンプ 支承\n",
      "Topic 6: 空気量 杭 資源 カッタヘッド 滞留 施工機 塗装 硬度 開放側端部 供給\n",
      "Topic 7: － Ａ 位置 ２つ 柱 手段 技術分野 説明 課題 発明\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 課題 発明\n",
      "Topic 9: 鉛直方向 切羽 リング 動力 鉄骨造 監視 上記実施例 組合せ ベルトコンベア 管路\n",
      "Topic 10: 平面計画 壁状 容積 コスト化 熱硬化性樹脂 試料 剛 出願人 口 リブ\n",
      "Topic 11: 地盤Ｇ ｃｍ 足場 アングル材 所期 有効利用 前記パイプ 付与 ℃ 珪酸カルシウム板\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 手段 発明 説明 技術 課題\n",
      "Topic 13: Ｇ 設計通り 傾斜角度 主体 添加量 付き 粉体 通過 軌跡 日\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 12.7804 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 組立柱 地下躯体 コアパネル アングル アングル材 柱 材 請求項 上部アングル 杭 杭頭 上部躯体 鉄筋コンクリート柱 構築方法 芯材 床構造 連結材 図 鉄筋コンクリート造 位置 構芯柱 建物 前記地下躯体 鉄骨柱 鉛直材 前記組立柱 施工精度 構造 上部連結材 施工 床付け面 地盤Ｇ 固定アングル 施工コスト 上端部 端部 地盤 コスト 鋼材組立体 塞ぎ板 発明 工法 上方 受けアングル 記載 構成 鉄筋 設置対象位置 施工誤差 複数階 上部 施工方法 該床構造 ラチス Ｈ型鋼 杭頭付近 該構芯柱 用 実施 鋼板組立体 複数 前記連結材 上部材 該地下躯体 立設時 平面視十字状 下端部 下方 特徴 コンクリート 形態 前記鉛直材 寸法 前記柱 前記アングル材同士 施工性 施工期間 前記コアパネル 前記構芯柱 位置変更 周囲 Ｉ－Ｉ線矢視断面 ＩＩ－ＩＩ線矢視断面 梁取付 短縮化 前記杭 受け材 Ｉ－Ｉ線矢視断面図 ＩＩ－ＩＩ線矢視断面図 前記鉛直材同士 課題 前記鋼材組立体 所定 低減化 上記実施 所定範囲 範囲 付近 鉄骨造 該鋼材組立体 説明 採用 梁 ｂ 間隙 所定位置 梁取付位置 鉄骨 要部拡大断面図 都市部 中間部 精度 立面図 平面視略ロ字状 前記正方形 コスト削減 敷地 内部 手段 該杭 該柱 図面 四面 上述 余裕 吸収 工期短縮 上記事情 上記課題 同士 技術分野 技術 問題点 近傍部分 正方形 幅寸法 部分 近傍 周知 制限 現場 中心 通常 要求 仮設 設 該受 状況 頂点 四辺 各面 大 形成 各階 レベル 空間 上下 等 趣旨 他 掘削 効果 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 10.2380 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 柱頭固定構造 柱 管状部材 鉛直部材 Ｈ型鋼 固定部材 柱頭部 下端部 図 上面パネル 前記鉛直部材 請求項 側方突出部材 コンクリート充填鋼管柱 コンクリートＣ 突出状態 構造 側方 鋼管 貫通孔 下面パネル 柱頭 プレート材 前記柱 蓋体 前記管状部材 発明 上面 内部 鋼管柱 建物 ディテール 上面図 実施 孔径 立断面図 記載 形態 ダイヤフラム 外観斜視図 前記柱頭部 短縮化 低減化 複雑化 角型断面 課題 プレート部材 上方 構成 支持柱 施工性 前記貫通孔 コンクリート 下端 板厚 前記蓋体 該管状部材 説明 工期 コスト 上端 特徴 上述 塔屋 鋼製 状態 中心部 屋上 手段 図面 スタッドボルト ボルト 簡易 止め 径 安定性 平面視八角形状 上記課題 上記実施 技術分野 技術 冠壁 塔屋等 当該建物 周知 円形 同一 付着 フランジ ｂ サイズ 変更 代わり スタッドジベル 効果 径寸法 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 11.7243 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : クロス部ジョイント金物 板状金物 下部筋 Ｘ型配筋 鉄筋 前記上部筋 上部筋 前記クロス部ジョイント金物 配筋 該クロス部ジョイント金物 該板状金物 該Ｘ型配筋 配置構造 Ｘ 前記下部筋 端部 前記板状金物 構造 Ｘ状 前記突起棒 クロスジョイント金物 ジョイント金物 梁 柱 機械式継ぎ手 断面 鉄筋同士 構造計算 筋 図 連結用ピン ねじ継ぎ手 突起棒 Ｘ形状 前記Ｘ型配筋 断面径 発明 請求項 中心点 該Ｘ配筋 辺 幅寸法 前記鉄筋 前記機械式継ぎ手 軸方向 Ｘ型配筋自身 継ぎ手手段 縦方向 方向 鉛直軸方向 配筋方法 配筋構造自体 左右 構造物 配筋作業 配筋設計 連結用 面側 該突起棒 配置 施工性 同軸状 前記ねじ継ぎ手 クリアランス 工期短縮 連結ピン 同士 該上部筋 該下部筋 幅寸法Ｗ 幅 径 該ねじ継ぎ手 長方形 ＸＹ方向 面 状態 ねじ節鉄筋等 工費削減 前記柱 通り長 断面方向 異形鉄筋 説明 中央 コンクリート 記載 特徴 厚み 位置 偏心 同軸 ねじ節付き 施工 ａ ｄ 課題 直径 並列 手段 半径 該鉄筋 部分 符号 所望 形状 残り 単独 値 施工方法 掘削土等 柱自身 産業廃棄物 複雑性 自身 量 現場等 コスト削減 必要量 偏心量 掘削汚泥 技術分野 技術 左右梁 上記事情 実施 形態 実施形態 せん断力 ＲＣ 両者とも 設計 コスト 上記 基礎杭 取り付け位置 ｂ 内部 該柱 ずれ 該梁 前面 コストダウン 目的 同一 任意 ｃ 角度 上述 構成 簡潔 混雑 程度 設量 厚 重量 本件 効果 図面\n",
      "\n",
      "===== # 4, Topic : 14, p : 11.4109 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : プローブ 絶縁体 ゴム系材料 図 内部導体 収納管 構造 導体 プローブ先端 外部導体 材料 同軸構造 縦断面図 金属体 プラズマ破砕装置用プローブ 断面構造例 正面図 プローブ自体 波付き金属管 発明 ゴム 先端 衝撃力 放電 装置 絶縁材料 プローブ構造 絶縁性能 構造例 プラズマ破砕装置 例 製造 破砕 プローブ装置 平行導体 衝撃波 岩石等 線導体 構成 電流 面 力 弾性 同軸導体構造プローブ プローブ材料 粒状体 パルスパワー電源 内部 平行導体構造プローブ Ａ Ｂ 方法 方向 実施態様 破砕対象 効果 プラスチック材料 ゴム材料 プラズマ破砕装置プローブ 技術 製造方法 平行導体構造 波付き金属管導体 波付き構造 中心軸 断面構造 衝撃吸収性能 柔軟性 絶縁体自体 中心導体 割れ 欠け 軸直角方向 波付き円筒金属管 線構造 プローブ中心軸 プラスチック プラスチック絶縁材料 パルス電流 当該プローブ 放電破砕 ゴム弾性 ケーブル 絶縁性 収納管自体 円柱構造 プローブ先端表面 プローブ先端側 ゴム粒子分散複合系 プローブ製造方法 プローブ後端 通常金属材料 弾性変形 ゴム補強プラスチック 強度 使用 プラズマ破砕技術 該内部導体 衝撃波伝達物質 説明 往路 複数 上述 ｂ 変形 ポリエチレン 軸 シリコンゴム 天然ゴム 粒状物 金属棒 金属パイプ 衝撃吸収力 該絶縁体 コスト増 接着剤 複数本 当該材料 号公報 対象 電力ケーブル 衝撃 高分子材料 課題 上記 周囲 空隙 軸方向 合金材料 衝撃吸収効果 エチレンプロピレン共重合ゴム 性能 接着力 概念図 ポリエチレン等 破壊用電極 圧縮力 岩石 実施 電圧パルス電流 プラズマ化 代表例 抵抗力 スイッチ等 ブーム等 接続部等 － 様子 損傷 塗布 残り 部分 銅 アルミ 他 複合 セラミックス 砂 ２つ 液状 圧縮方向 導電性能 可能性 耐衝撃性 当該放電エネルギー 高分子量ポリエチレン 強度補強 略円筒状 ガラス繊維強化エポキシ樹脂 物質 特表平 技術分野 複数層 伸縮変形 使用寿命 寿命延長 コスト低減 特開平 スチロール樹脂 ＰＢＴ コンデンサバンク 外傷保護 表面 反発作用 作業車 ブーム 固定点 制動手段 作業 手段 本願 本願発明 芯材 パイプ 音響インピーダンス ＰＢＴ 岩盤 穴 役割 後方 瞬時 角度 ケース 双方 ポリエポキシ ポリウレタン ポリカーボネイト 対策 実現 付加 要因 径 工程 増加 通り 一つ 圧入 遙 形態 ａ 主体 一般 鉄 ステンレス エチレンプロピレンゴム クロロプレンゴム 適用 ナイロン 乱反射 差 岩 かけ 分割 数 限定 特質 間隙 コストダウン 破損 図面 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 13.6796 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : コンクリート 測定線 状態 抵抗 コンクリート状態測定センサ 抵抗値 測定 コンクリート状態測定システム 抵抗測定装置 測定データ 測定端子 開口部 コンクリート構造物 抵抗測定 値 抵抗値Ｚ コンクリート状態 治具抵抗Ｒ ｋΩ コンクリート打設時 コンクリート上面 測定コンクリート データ 短絡抵抗Ｒ 図 コンクリート状態測定 データベース管理装置 流動性 被覆材 抵抗線 工コンクリート 測定コンクリート状態測定システム 状態データ コンクリート打設位置 一対 合成抵抗値Ｒ データベース コンクリート状態測定システムコンクリート状態測定システム 短絡部 導電性 コンクリート測定センサ コンクリート状態測定センサコンクリート状態測定センサ 管理 型枠 レベルＳ コンクリート状態測定方法 抵抗Ｚ 管理コンクリート コンクリート打設中 合成抵抗 初期状態 コンクリート面 経過状態 抵抗率 コンクリート測定方法 Ｒ 充填 結コンクリート 前記コンクリート状態測定システム 構造物 配置コンクリート状態測定センサ 抵抗値Ｒ 発明 測定原理 コンクリート構造物コンクリート構造物 抵抗Ｒ ｙ 短絡状態 前記コンクリート測定センサ 合成抵抗値 充填状態 構造 測定方法 短絡抵抗 地山 コンクリート量 流動性コンクリート 治具抵抗 テープ状基板 測定対象 測定場所 コンクリート打設後 コンクリート打設領域 コンクリート打設量 コンクリート打設前 コンクリート打設状況 Ｃ 位置 ｉ 内部状態 データベース管理ソフト 維持管理 コンクリート内部 Ｌ 式 線状 データ収集装置 セントル 結 初期データ 十分流動性 合成抵抗Ｒ 程度 設抵抗測定装置 直列抵抗 抵抗器 コンクリート表面 算出コンクリート 測定原理図 導電率 工コンクリート打設 設後 水 ステップ状 防水性 直線性 経時変化 ≦ｙ≦Ｈ 特徴 保護管 一端 Ｓ 予測値 工コンクリート外側 工 設時 コンクリート断面上方 コンクリート残量分 例 逆打 トンネル テープ状 実施 変化 データベースデータベース 接続端子 充填部 端部 説明 個所 面状 経年変化 固定具 イ ロ 設位置 空洞 引き出し線 関係 劣化 経過 短絡 ｋΩ程度 経過データ 充填量 設中 セントル方式 周囲地山 ～（Ｅ 比較検討 テープ状物体 データ処理 絶縁性 構築 リアルタイム ハ 両面 ニ 表 引出線 枠 継部 α 地山断面 上面 ｋα 設予定残量 初期充填状況 ｋ番目 充填個所 充填終了 粘着テープ 追加量 ｙ軸 実施例 天端部分 天端上部 長期 形態 図面 等間隔 内部 浸入 Ｂ 種類 並列 ホ 方法 形 ｘ ｆ ｇ 効果 シミュレーション 地山側 上記実施例 連続地中壁施工 Ｅ 複数 程度未満 説明図 構成図 複数位置 複数箇所 設残量分 ボックスカルバート施工 利用分野 上部 箇所 取付部材 配置 １つ下 ｉ↓－↓ ｎ個 設量 全量Ｖ 全量 算出 下 崩落原因 設 コールドジョイント 産業 技術 目的 手段 設状態 空間 他方 Ａ 周囲 孔 間隙 均一 フック 片面 Ｄ 各面 側面 下部 ～ 間隔 下方 種々 ２つ グラフ 考察 水平面 上昇 リニアー 推定 奥行き ｗ ヘ 時点 判定 ト 日付 ＴＤ ＳＴＡ コメント 監視 評価 左右 外部 減少 計測 所定 期間 浸水 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 13.0241 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 鋼矢板 地下水 改良地盤 鋼矢板護岸 埋立地 水 矢板護岸 護岸 詰め材 水位 水栓 圧縮空気 支持地盤 水底地盤 矢板 地盤 矢板式護岸 側 透水性 鋼管矢板 図 水路 最高地下水位 構造 支持層 実施 護岸構造 通路 地下水位 継手部 軟弱粘土層 発明 海底地盤 水位検出手段 形態 圧縮空気供給手段 埋立地側 外水域 号公報 水底面 特開平 水平支持力 混合物 透水係数 水位計 透水 セメント系固化材 汚染物質 前記鋼矢板 管材 軟弱地盤 支持力 詰め材料 改良土 廃棄物処理場 送り込み鋼矢板 地盤改良 浸水 砂杭 拡散防止護岸 安定性 部分 水圧 一端側 他端側 公報 汚水 エアコンプレッサ 工法 説明図 断面図 説明 例 貯水槽 位置 水平抵抗力 検出値 前記埋立地 継手 手段 砂礫層 海底面 外海 － 水ポンプ 海底 詰め 埋立地用 構造材 水頭 砂 sec 粘性土 廃棄物処分場 処理場 ↑-↑ cm 間隔 上記 状態 砂礫 断面 セメント 圧力 地下水 廃棄物処分場建設予定地 問題点 ＳＣＰ ＳＣＰ工法 沿岸域廃棄物処理場 水平抵抗 外側 上部 上面 土砂 幅 条件 上記構造 締固め改良 空気吐出配管 技術 コスト 配管 前記特開平 拡散防止 砂礫杭 水域 必要性 砂杭周辺 上記実施 海側 海洋汚染 外周構造体 主成分 特徴 他 砕石 課題 拡散 目的 該通路 該水路 境界部 Sand Compaction Pile 作用 外部 領域 水質 等 鋼管 上記実施形態 浚渫土 前記水路 ポンプ 汚水貯留池 土留め壁 内部空管内 排水ポンプ 鉛直方向 長手方向 技術分野 環境保全 止水性 気象変動 設備コスト 工事費 送り込み sec程度 浸水量 構築方法 泥状 有効利用 目視確認 自動制御 観点 内外 礫 付近 流れ 費用 多量 大雨 容量 性状 側部 対策 強度 剛性 有無 下 自体 下面 下方 一般 前提 心配 構成 港湾 河川 周囲 施工 現地 資源 次 井戸 一定 数 設計 効果 図面 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 12.8237 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ＰＣ板 スラブ材 接合部 床スラブ ＣＦＲＰ板 スラブ 繊維強化部材 図 プレキャストコンクリート板 ＰＣ板間 応力 繊維強化プラスチック板 板 ＰＣ板同士 補強方法 発明 実施形態 モルタル 梁 床スラブパネル 両端部近傍 前記プレキャストコンクリート板 下面 複数 重量 スラブ下面 端部 中空スラブ板 スラブ重量 Ａ－Ａ断面図 Ｂ－Ｂ断面図 炭素繊維強化プラスチック板 量 接合力 施工面 下面図 接着剤 荷重作用 荷重 重量等 ＰＣ板ごと 繊維補強部材 自重 隙間 ｍｍ 平面図 東レ（株）製 モルタル等 課題 上記 炭素繊維 方向 面 軽量 補強 可能性 補強部材 ＣＦＲＰ板用エポキシ系接着剤 炭素繊維重量 アラミド繊維 請求項 トレカラミネートＴＬ 登録商標 ヤング係数 説明 ひび割れ 強度 ｋＮ 増加 接着力 エポキシ系接着剤 上面 下面側 実施 形態 屋根部材 保持部材 施工性 施工 日本シーカ（株）製 下面全面 一体 剛性 目的 特徴 差異 工事 シーカデュア 性能 Ｎ 置荷重等 効率 ゴムローラ等 孔等 反対側 上面側 対辺側 耐久性 平面形状 単位面積あたり 技術分野 技術 上記課題 添付図面 枠状 商品名 トレカＴ 付け硬質 補修作業 図面 重み 虞 各々 雨漏り 原因 手段 発生 ～ 矩形 ＡＬＣ Autoclaved light weight concrete 上下 Ｓ ｍ 比重 幅 帯状 圧縮 コンクリート 部位 間隔 他方 通り 自身 現場 既存 所定 位置 アンカー ボルト 取り扱い 鉄筋コンクリート 増し 鋼材 養生 専用 効果 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 12.4037 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 揺動カッター カッターヘッド 開口部 開口部面積 カッター 面板 開口率 センターカッター 外周フレーム材 実施形態 上部 中間フレーム材 カッターヘッド上部 進機 中央部 円形カッター カッタービット等 下部 本願発明 地山 面積 発明 外周部 揺動カッター上部 カッター面板 掘削土砂 土砂 図 形態 スポーク 掘削断面積 揺動角度 掘削土 カッターヘッド中心部 カッタービット 回転型カッターヘッド 開口率差 揺動型カッターヘッド 正面図 範囲 中心 回動角度 揺動カッター下部 揺動掘削 揺動中心位置Ｖ 面板開口部 取込み 該カッターヘッド 差 揺動中心位置 カッター面板前 該揺動カッター 実施 ° 前記開口部面積 周方向 円形 地 回動方向 保持 左右部 上下部 扇形面積 半円面積 下方 半径方向 支持フレーム等 進 所定 交互 例 放射状 掘削土砂取込み 扇形 中間 異形断面 楕円断面 土砂取込み 説明 構成 複数 中央 方向 中央上部 掘削効率 技術 左右 上下 効果 横長長円断面 土砂速度 発明者 回転方向 土圧 中央下部 回転 角度範囲 チャンバー 真横方向 排土口 チャンバー下 課題 前面 後方 上方 チャンバー 設計 図面 程度 側部 下側 回転運動 シールド本体側 号公報記載 前記条件 技術分野 特開 切削具 駆動装置 異形 山止め効果 図示略 早期排出 異常上昇 中立状態 状態 崩壊防止 排出 チャンバ 重力 切羽 スピード 出願 種 － 手段 ％～ トンネル 付着 結 推力 トルク トラブル 一方向 同一 等間隔 上側 ～ 要旨 種々 変更 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 13.0495 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ボイラ 解体方法 解体 ボイラ解体作業 ボイラ解体方法 解体作業 建屋躯体 架台 状態 図 トップガーダー 解体工程 実施形態 方法 建屋 作業 前記ボイラ 概略図 側方 架台設置工程 ボイラ切離し工程 架工程 ｂ 解体用重機 発明 ジャッキ 上記 効率 下方 上記ボイラ切離し工程 工程 建屋上部 前記 下げ式ボイラ 仮設支柱 降下 材 上方 解体作業開始 火力発電所 前記トップガーダー 設置 宙吊り状態 概略 安全性 停止操作 左右両端 アーム 架面 底面側形状 ～図 止材 完了 ボルト 降下量分 対向状態 課題 荷重 実施 形態 盛土 頂面 支持盤 底面 説明 所定 間隔 下部 交互 先 作業効率低下 架完了 重量 手間 揺れ 鋼材 ともども 部分 あと 転び止材 直立安定性 技術分野 技術思想 熱膨張 内部応力 耐用年数 ジャッキ付き ボルト締め 相違点 大型 収縮 発生 規定 後者 止め 廃材 次 要因 手段 特徴 早期 両者 目視 程度 間隙 溶接 時点 最初 中心 本 構築 コスト 位置 効果 図面 他 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 12.1954 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 厚み 複層構造体 計測 濃度 Ｘ線 透過濃度 部材 複層構造 透過濃度画像 部材厚み 方法 計測物体 厚みＨ Ａ材 厚み計測 Ｈ ステップＳ Ｘ線計測方法 Ｂ材 厚みａｉ 構造材 厚み非破壊計測方法 厚み計測方法 前記複層構造体 図 計測装置 濃度ｄｍ 計測演算部 参照情報 計測演算 Ｘ線照射装置 厚みデータ 画像 発明 Ｘ線検出部 計測台 Ｘ線検出手段 透過Ｘ線 Ｘ線照射手段 非破壊 厚みｂｉ Ｘ線計測装置 Ｘ線 計測演算手段 ａ 関係 情報 計測アルゴリズム 配管 曲線 Ｘ線源 ｈ 標準物体 透過 ａｉ 計測部 Ｈ－Ｈ 実施 走査制御部 中空配管 画像情報 Ｘ線照射 検出器 既知 Ａ 関係近似曲線 Ｘ線撮影 方法等 物体 データ記憶部 撮影 例 形態 前記参照情報 Ｂ 記憶手段 計測値 標準厚み 装置 手段 組み合わせ Δ 複数 横軸 検出データ 電磁力 フイルム ａｉ 濃度ｄ Ｘ線検出器 減衰 ｂ 前記Ｘ線検出手段 厚みｂ 組み合わせ厚み 前記Ｘ線照射手段 出力部 測定装置 Ａ部材 関係曲線 透過物体 音波 処理 建築現場等 中空部分 Ｂ部材 部分 複数点 縦軸 説明 放射線 精度 － 上記 ｂｉ 判定 基準値 ピーク部分 データ ａｉ－Δ 上記実施 透過視覚情報 ＸＹ方向 同一面 透過物 ＸＹ駆動 場所 現場 内径 差 設定 透過線量 測定部 基準物体 変化 課題 点 中空 号公報参照 腐食部 中心部 境界部 点状 取得情報 検査装置 傾向 周囲 ライニング等 ″ 基準 逆 未知 特徴 図面 概要 他方 内壁 スケール等 Ｐ点 ノギス等 方向 刻み 方向撮影 両側 真下 程度 スプライン曲線 積層板状 走査位置 上記課題 変状 励磁電流 渦電流 析出物 積層状態 技術分野 技術 交流電磁石 内部 発明者 特開平 直線補間 外径 スケール 誤差範囲 誤差 範囲 減衰特性 線量 表示画面 印刷用紙 外 内部変化 腐食 付着状況 運送路 反射 傷 面積 広がり 人力 平滑 空間 平板 管状 条件下 信号 人間 五感 加工 塗装 メッキ 欠陥 有無 寸法 検体 工場 施設 欠点 ＣＴ 断面 性能 搬 小型 関数 該関数 最低 単一 フィルム カーブ 非線形 性質 材料 使用 真横 繰り返し 否 角度 縁 Ｈ２ 先 種々 変形 効果 外見 比較 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.1272e-36, 9.8622e-37, 3.1893e-26, 0.0000e+00,\n",
      "         0.0000e+00, 4.0504e-27, 9.3212e-25, 4.3160e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.5671e-09, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 312269.31250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1163.8823\n",
      "Test epoch : 1 Average loss: 1223.5025\n",
      "PP(train) = 7310.320, PP(valid) = 7378.892\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0346e-22, 0.0000e+00, 1.2755e-25, 0.0000e+00,\n",
      "         0.0000e+00, 3.7835e-44, 4.0219e-18, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6231e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 281509.90625\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1162.9532\n",
      "Test epoch : 2 Average loss: 1222.8771\n",
      "PP(train) = 7272.693, PP(valid) = 7345.331\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.4848e-29, 2.1510e-42, 1.4648e-16, 0.0000e+00,\n",
      "         0.0000e+00, 6.6447e-28, 4.2165e-26, 8.1840e-29, 0.0000e+00, 1.4013e-45,\n",
      "         2.8672e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 253780.39062\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1162.4774\n",
      "Test epoch : 3 Average loss: 1222.2336\n",
      "PP(train) = 7232.825, PP(valid) = 7310.898\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.0835e-31, 0.0000e+00, 1.7417e-37, 0.0000e+00,\n",
      "         0.0000e+00, 1.7236e-36, 3.0268e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6530e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 228782.31250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1161.8604\n",
      "Test epoch : 4 Average loss: 1221.5761\n",
      "PP(train) = 7191.501, PP(valid) = 7275.489\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8511e-31, 7.0065e-45, 2.5956e-25, 0.0000e+00,\n",
      "         0.0000e+00, 4.1926e-36, 1.0901e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.5904e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 206246.60938\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1160.9727\n",
      "Test epoch : 5 Average loss: 1220.9037\n",
      "PP(train) = 7150.142, PP(valid) = 7240.333\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1036e-38, 0.0000e+00, 1.8430e-33, 0.0000e+00,\n",
      "         0.0000e+00, 9.7482e-33, 7.0065e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.3511e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 185930.73438\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1160.1352\n",
      "Test epoch : 6 Average loss: 1220.2338\n",
      "PP(train) = 7109.040, PP(valid) = 7205.520\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0197e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.7061e-30, 1.8964e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.6314e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 167616.03125\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1159.3454\n",
      "Test epoch : 7 Average loss: 1219.5733\n",
      "PP(train) = 7067.959, PP(valid) = 7170.696\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4204e-41, 0.0000e+00,\n",
      "         0.0000e+00, 9.1488e-38, 3.3586e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0611e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 151105.37500\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1158.5958\n",
      "Test epoch : 8 Average loss: 1218.8954\n",
      "PP(train) = 7028.027, PP(valid) = 7136.864\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.3297e-39, 0.0000e+00, 2.4543e-34, 0.0000e+00,\n",
      "         0.0000e+00, 5.7373e-33, 9.3331e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6885e-15, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 136221.06250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1157.7117\n",
      "Test epoch : 9 Average loss: 1218.2131\n",
      "PP(train) = 6989.307, PP(valid) = 7104.133\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6444e-32, 1.4013e-45, 1.3349e-33, 0.0000e+00,\n",
      "         0.0000e+00, 6.8777e-21, 1.1537e-29, 1.6873e-32, 0.0000e+00, 0.0000e+00,\n",
      "         2.9829e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 122802.90625\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1157.0260\n",
      "Test epoch : 10 Average loss: 1217.5506\n",
      "PP(train) = 6949.772, PP(valid) = 7070.555\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2012e-25, 2.4383e-43, 2.2807e-15, 0.0000e+00,\n",
      "         0.0000e+00, 3.4065e-08, 1.8229e-28, 7.6776e-34, 1.4013e-45, 4.7662e-33,\n",
      "         8.8370e-08, 7.0394e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 110706.47656\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1156.4185\n",
      "Test epoch : 11 Average loss: 1216.9285\n",
      "PP(train) = 6908.669, PP(valid) = 7035.598\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1538e-35, 0.0000e+00, 5.8546e-37, 0.0000e+00,\n",
      "         0.0000e+00, 3.0406e-30, 6.5827e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3435e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 99801.57812\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1155.5543\n",
      "Test epoch : 12 Average loss: 1216.3098\n",
      "PP(train) = 6867.809, PP(valid) = 7000.754\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.9271e-37, 0.0000e+00, 1.1886e-30, 0.0000e+00,\n",
      "         0.0000e+00, 4.6407e-27, 6.2639e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6969e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 89970.84375\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1154.7768\n",
      "Test epoch : 13 Average loss: 1215.6547\n",
      "PP(train) = 6828.702, PP(valid) = 6967.358\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.9735e-40, 0.0000e+00, 2.7413e-27, 0.0000e+00,\n",
      "         0.0000e+00, 5.3975e-33, 1.0346e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0022e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 81108.46094\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1154.0697\n",
      "Test epoch : 14 Average loss: 1214.9868\n",
      "PP(train) = 6790.917, PP(valid) = 6935.173\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4477e-39, 0.0000e+00,\n",
      "         0.0000e+00, 6.1540e-37, 2.1019e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3515e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 73119.04688\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1153.1220\n",
      "Test epoch : 15 Average loss: 1214.2997\n",
      "PP(train) = 6754.412, PP(valid) = 6904.079\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.0858e-36, 7.0065e-45, 3.8734e-25, 0.0000e+00,\n",
      "         0.0000e+00, 3.9618e-33, 8.2016e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5920e-24, 1.4013e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 65916.61719\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1152.7651\n",
      "Test epoch : 16 Average loss: 1213.6414\n",
      "PP(train) = 6717.212, PP(valid) = 6872.238\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8481e-30, 0.0000e+00,\n",
      "         0.0000e+00, 7.6389e-35, 6.4092e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.9024e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 59423.64453\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1151.7668\n",
      "Test epoch : 17 Average loss: 1213.0187\n",
      "PP(train) = 6679.032, PP(valid) = 6839.436\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.7688e-24, 3.0604e-42, 5.2514e-35, 0.0000e+00,\n",
      "         0.0000e+00, 4.4312e-22, 1.1890e-41, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "         5.1144e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 53570.25000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1151.2044\n",
      "Test epoch : 18 Average loss: 1212.4019\n",
      "PP(train) = 6641.050, PP(valid) = 6806.823\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7347e-40, 0.0000e+00, 6.5768e-37, 0.0000e+00,\n",
      "         0.0000e+00, 5.4739e-32, 1.1181e-38, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2705e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 48293.42969\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1150.1665\n",
      "Test epoch : 19 Average loss: 1211.7669\n",
      "PP(train) = 6604.298, PP(valid) = 6775.237\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1718e-32, 0.0000e+00, 2.4012e-40, 0.0000e+00,\n",
      "         0.0000e+00, 3.4245e-40, 8.7016e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.6445e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 43536.39062\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1149.6737\n",
      "Test epoch : 20 Average loss: 1211.1183\n",
      "PP(train) = 6568.626, PP(valid) = 6744.486\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2768e-39, 0.0000e+00, 2.0726e-30, 0.0000e+00,\n",
      "         0.0000e+00, 5.8932e-23, 3.6691e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.5287e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 39247.93359\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1148.8669\n",
      "Test epoch : 21 Average loss: 1210.4655\n",
      "PP(train) = 6533.758, PP(valid) = 6714.471\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3960e-37, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 5.2304e-31, 2.0347e-42, 0.0000e+00, 0.0000e+00,\n",
      "         1.3513e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 35381.90234\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1148.0925\n",
      "Test epoch : 22 Average loss: 1209.8157\n",
      "PP(train) = 6498.895, PP(valid) = 6684.385\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4119e-33, 0.0000e+00, 2.0405e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.8625e-34, 6.3240e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3658e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 31896.68555\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1147.3571\n",
      "Test epoch : 23 Average loss: 1209.2005\n",
      "PP(train) = 6462.912, PP(valid) = 6653.272\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1968e-33, 8.4078e-45, 1.2249e-26, 0.0000e+00,\n",
      "         0.0000e+00, 2.0897e-36, 1.9267e-35, 6.4988e-40, 0.0000e+00, 0.0000e+00,\n",
      "         3.4336e-22, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 28754.77148\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1146.7751\n",
      "Test epoch : 24 Average loss: 1208.5990\n",
      "PP(train) = 6426.906, PP(valid) = 6622.134\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, 7.7751e-30, 0.0000e+00,\n",
      "         0.0000e+00, 1.3198e-35, 1.2120e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.5636e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 25922.34570\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1146.0256\n",
      "Test epoch : 25 Average loss: 1207.9911\n",
      "PP(train) = 6391.778, PP(valid) = 6591.688\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0393e-37, 4.5122e-43, 4.0446e-32, 0.0000e+00,\n",
      "         0.0000e+00, 8.3420e-33, 7.3341e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2963e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 23368.92188\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1145.3200\n",
      "Test epoch : 26 Average loss: 1207.3663\n",
      "PP(train) = 6357.728, PP(valid) = 6562.196\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7685e-40, 0.0000e+00, 2.5453e-19, 0.0000e+00,\n",
      "         0.0000e+00, 8.3895e-26, 3.0817e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.2979e-03, 2.8673e-33, 9.9270e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0446, 0.1042, 0.0503, 0.0600, 0.0669, 0.1071, 0.0923, 0.0595,\n",
      "         0.0526, 0.0619, 0.0556, 0.0511, 0.0708, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 21067.01758\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1144.6782\n",
      "Test epoch : 27 Average loss: 1206.7434\n",
      "PP(train) = 6323.870, PP(valid) = 6532.830\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.6070e-35, 6.1697e-39, 1.2730e-31, 0.0000e+00,\n",
      "         0.0000e+00, 2.7361e-20, 2.9165e-21, 6.4018e-33, 0.0000e+00, 0.0000e+00,\n",
      "         3.0442e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 18991.85742\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1143.9979\n",
      "Test epoch : 28 Average loss: 1206.1221\n",
      "PP(train) = 6290.487, PP(valid) = 6503.843\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2340e-37, 2.8026e-45, 1.6367e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.3418e-36, 1.2267e-29, 6.0256e-44, 0.0000e+00, 0.0000e+00,\n",
      "         5.6587e-13, 1.1598e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 17121.10547\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1143.3160\n",
      "Test epoch : 29 Average loss: 1205.5093\n",
      "PP(train) = 6257.185, PP(valid) = 6474.904\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0597e-40, 0.0000e+00, 5.8368e-30, 0.0000e+00,\n",
      "         0.0000e+00, 1.0233e-22, 5.8030e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.1389e-36, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 15434.62793\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1142.4066\n",
      "Test epoch : 30 Average loss: 1204.8954\n",
      "PP(train) = 6224.326, PP(valid) = 6446.285\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.8874e-36, 0.0000e+00, 1.6576e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.0172e-39, 3.7849e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.2766e-24, 2.5307e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 13914.27344\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1141.7689\n",
      "Test epoch : 31 Average loss: 1204.2654\n",
      "PP(train) = 6192.241, PP(valid) = 6418.313\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.3290e-39, 0.0000e+00, 4.3657e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.8124e-30, 1.0463e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.4458e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 12543.67871\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1141.0462\n",
      "Test epoch : 32 Average loss: 1203.6551\n",
      "PP(train) = 6159.864, PP(valid) = 6390.035\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.3160e-38, 0.0000e+00, 1.3172e-42, 0.0000e+00,\n",
      "         0.0000e+00, 1.1102e-29, 4.5253e-35, 1.5414e-44, 0.0000e+00, 0.0000e+00,\n",
      "         1.2134e-34, 2.2070e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 11308.09082\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1140.6108\n",
      "Test epoch : 33 Average loss: 1203.0739\n",
      "PP(train) = 6126.406, PP(valid) = 6360.729\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8427e-37, 0.0000e+00,\n",
      "         0.0000e+00, 2.1875e-40, 1.0317e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8310e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 10194.21191\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1139.6436\n",
      "Test epoch : 34 Average loss: 1202.4945\n",
      "PP(train) = 6093.438, PP(valid) = 6331.854\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1263e-39, 0.0000e+00, 4.1779e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.4013e-45, 2.4264e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.6107e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 9190.05371\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1138.8080\n",
      "Test epoch : 35 Average loss: 1201.8650\n",
      "PP(train) = 6062.734, PP(valid) = 6305.027\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4443e-41, 1.4235e-19, 0.0000e+00,\n",
      "         0.0000e+00, 9.0064e-26, 1.8097e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3680e-25, 2.8026e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 8284.80762\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1138.4679\n",
      "Test epoch : 36 Average loss: 1201.2408\n",
      "PP(train) = 6032.363, PP(valid) = 6278.404\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2093e-31, 0.0000e+00, 7.8285e-24, 0.0000e+00,\n",
      "         0.0000e+00, 6.9392e-26, 2.8920e-29, 3.4584e-42, 0.0000e+00, 9.1785e-43,\n",
      "         1.2191e-24, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7468.73096\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1137.8532\n",
      "Test epoch : 37 Average loss: 1200.6479\n",
      "PP(train) = 6001.268, PP(valid) = 6251.083\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.2083e-33, 0.0000e+00, 6.4068e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.0806e-33, 5.2283e-28, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.1813e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6733.04004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1137.0040\n",
      "Test epoch : 38 Average loss: 1200.0547\n",
      "PP(train) = 5970.505, PP(valid) = 6224.100\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5862e-37, 0.0000e+00, 3.3058e-33, 0.0000e+00,\n",
      "         0.0000e+00, 1.8302e-26, 1.3572e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.2809e-20, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6069.81641\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1136.1772\n",
      "Test epoch : 39 Average loss: 1199.4544\n",
      "PP(train) = 5940.392, PP(valid) = 6197.606\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5821e-36, 1.4013e-45, 4.3878e-29, 0.0000e+00,\n",
      "         0.0000e+00, 1.8719e-28, 8.7411e-36, 3.0381e-40, 0.0000e+00, 0.0000e+00,\n",
      "         3.4035e-22, 5.7061e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5471.92236\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1135.7667\n",
      "Test epoch : 40 Average loss: 1198.8695\n",
      "PP(train) = 5909.766, PP(valid) = 6170.601\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0934e-32, 3.9334e-42, 1.5701e-18, 0.0000e+00,\n",
      "         0.0000e+00, 4.2824e-34, 7.3188e-22, 2.0456e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.2596e-22, 5.8127e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4932.92285\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1135.0438\n",
      "Test epoch : 41 Average loss: 1198.2958\n",
      "PP(train) = 5879.073, PP(valid) = 6143.491\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.1935e-33, 0.0000e+00, 1.1001e-30, 0.0000e+00,\n",
      "         0.0000e+00, 4.5531e-26, 1.2987e-22, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.1182e-31, 5.0447e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4447.01611\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1134.2674\n",
      "Test epoch : 42 Average loss: 1197.7165\n",
      "PP(train) = 5849.042, PP(valid) = 6116.949\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6879e-41, 0.0000e+00, 2.0030e-31, 0.0000e+00,\n",
      "         0.0000e+00, 3.2111e-32, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.6319e-32, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4008.97241\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1133.6509\n",
      "Test epoch : 43 Average loss: 1197.1181\n",
      "PP(train) = 5820.213, PP(valid) = 6091.556\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1210e-44, 0.0000e+00, 1.0171e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.4230e-24, 2.3009e-42, 4.8485e-43, 0.0000e+00, 0.0000e+00,\n",
      "         2.6502e-32, 6.7893e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3614.07739\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1133.0803\n",
      "Test epoch : 44 Average loss: 1196.5298\n",
      "PP(train) = 5791.399, PP(valid) = 6066.088\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.0638e-43, 2.5697e-25, 0.0000e+00, 2.4509e-20, 0.0000e+00,\n",
      "         0.0000e+00, 2.5599e-23, 1.1062e-20, 1.7937e-40, 0.0000e+00, 6.2330e-42,\n",
      "         7.5958e-08, 5.5057e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3258.08057\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1132.4278\n",
      "Test epoch : 45 Average loss: 1195.9450\n",
      "PP(train) = 5762.899, PP(valid) = 6040.900\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.7006e-25, 0.0000e+00, 9.7075e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.7048e-27, 4.4226e-22, 1.0005e-42, 0.0000e+00, 0.0000e+00,\n",
      "         3.5767e-27, 2.8727e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2937.15039\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1131.7857\n",
      "Test epoch : 46 Average loss: 1195.3654\n",
      "PP(train) = 5734.287, PP(valid) = 6015.553\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.1563e-37, 2.7225e-38, 3.5313e-31, 0.0000e+00,\n",
      "         0.0000e+00, 4.6455e-19, 2.4120e-29, 1.8037e-40, 0.0000e+00, 0.0000e+00,\n",
      "         6.9118e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2647.83276\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1131.0377\n",
      "Test epoch : 47 Average loss: 1194.8008\n",
      "PP(train) = 5705.001, PP(valid) = 5989.616\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0923e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.3507e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.2034e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2387.01367\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1130.2635\n",
      "Test epoch : 48 Average loss: 1194.2172\n",
      "PP(train) = 5676.584, PP(valid) = 5964.433\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7187e-36, 1.1897e-39, 4.6286e-22, 0.0000e+00,\n",
      "         0.0000e+00, 2.2448e-23, 5.2992e-11, 3.4682e-42, 0.0000e+00, 8.1107e-42,\n",
      "         1.6931e-18, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2151.88599\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1129.7702\n",
      "Test epoch : 49 Average loss: 1193.6442\n",
      "PP(train) = 5648.681, PP(valid) = 5939.678\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 8.6841e-34, 0.0000e+00, 2.9853e-35, 0.0000e+00,\n",
      "         0.0000e+00, 2.2055e-27, 1.4237e-23, 8.5186e-41, 0.0000e+00, 0.0000e+00,\n",
      "         7.1978e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1939.91919\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1129.1041\n",
      "Test epoch : 50 Average loss: 1193.0720\n",
      "PP(train) = 5621.145, PP(valid) = 5915.292\n",
      "Writing to ./topicwords/4-topwords_e50.txt\n",
      "Topic 0: 欠点 イ 外方 補強材 ロ 破壊 複数個 植物 剥離 どうし\n",
      "Topic 1: 欠点 イ ロ 外方 補強材 破壊 複数個 剥離 上記実施 どうし\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 形態 手段 説明 課題 図\n",
      "Topic 3: 架設 緊張力 横断面図 運搬 両端側 既設 定着 完了 改善 水溶性\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 手段 形態 説明 課題\n",
      "Topic 5: 欠点 外方 イ ロ 破壊 複数個 補強材 どうし 剥離 μｍ\n",
      "Topic 6: 欠点 イ 外方 補強材 ロ 破壊 複数個 剥離 上記実施 建築\n",
      "Topic 7: － Ａ 位置 ２つ 柱 技術分野 手段 形態 説明 課題\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 手段 形態\n",
      "Topic 9: イ 欠点 ロ 外方 破壊 複数個 補強材 植物 評価 剥離\n",
      "Topic 10: イ 欠点 外方 ロ 破壊 複数個 補強材 剥離 大 上記実施\n",
      "Topic 11: イ 欠点 ロ 外方 複数個 破壊 補強材 剥離 植物 上記実施\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 手段 形態 説明 課題 図\n",
      "Topic 13: イ ロ 欠点 外方 破壊 補強材 複数個 上記実施 剥離 施設\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 10.7438 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 残渣 焼成物 コンクリート構造物 中和物 コンクリート 酸性 骨材 残渣及び 人工骨材 セメント 質量 レアアース 原料 焼成物製造用原料 アルカリ性固化材 固化体 等 方法 質量部 処理方法 泥 固化材 含有率 製造 代替物 コンクリート構造物構築工程 地盤 発明 上記 水酸化ナトリウム 乾燥質量 圧壊強度 細骨材 残渣等 セメント系固化材 ペレット 上記コンクリート 上記コンクリート構造物構築工程 実施例 人工 酸 混合物 石灰系固化材 成分 カルシウム含有原料 水分含有率 加熱手段 セメントクリンカ 沈澱方式 処理 加熱温度 模擬試料 地盤造成工程 ～ 該酸性 該コンクリート構造物 電気炉 上記酸性 モルタル 消石灰等 遠心分離方式 石灰 離島 造粒物－強度試験方法 人工骨材製造用工場 人工骨材製造工場 － 用途 観点 コンクリート等 圧壊強度試験方法 セメント製造用工場 マグネシア系固化材 アルカリ性材料 分級 セメント製造工場 マグネシア系固化材等 比較例 埋め立て地等 手段 粘土等 成形方法 レアアースメタル成分 太平洋 目的 表 本土 建造物 該焼成物 生成物 深海 アルカリ性 工程 酸化マグネシウム 防波堤等 海水電気分解工場 セメント等 特許文献 配合量 該ペレット ℃ 工場等 粘土状物 工場 製品製造工場等 埋め立て資材 記載 領域 中和処理 固形分 装置 光学ガラス汚泥 加熱炉等 鉄含有原料 方式 加熱 水酸化ナトリウム等 乾燥状態 ＣａＯ成分 有効成分 酸化マグネシウム等 課題 各種 電気 レアアース採鉱事業 ケイ素含有原料 アルミニウム含有原料 タンク等 海底 供給源 ］～［ 生石灰 ｐＨ 利用 中性 海域 通常 Ｎ 中和 連続式 バッチ式 上記地盤造成工程 エコセメント等 温度 発電設備 熱ポルトランドセメント等 石灰等 ガス等 埋め立て資材等 混合セメント 加圧脱水方式等 希酸 深海底 質量基準 海水 設備 水酸化マグネシウム 残渣以外 水素等 成形体 ポルトランドセメント 該地盤造成工程 製品工場 ウラン等 シリカ等 カンター等 離島等 ロータリーキルン等 篩等 コーンクラッシャ等 オフィス等 フライアッシュセメント等 メガフロート等 マイクロ波加熱装置等 割合 多量 他 ℃、 強度 ａ ｂ 種類 島 高炉セメント 処理液 鉄ケーキ等 押出し成形機等 加圧脱水方式 フィルタープレス等 元素 資材 排水処理装置 焼成条件 塩工場 圧壊温度 軽焼酸化マグネシウム 燃料電池等 サンドコンパクション等 海中備蓄タンク等 ゼオライト分離 試料 皿形造粒機 技術分野 背景技術 先行技術文献 炭酸マグネシウム 最高温度 上記課題 上記目的 光学ガラス研磨 太平洋セメント社製 量 中庸熱ポルトランドセメント 洗浄工程 遠隔地 焼却炉 典型例 該成形体 ℃（焼結温度 普通ポルトランドセメント 各種ポルトランドセメント メガフロート 軽焼マグネシア 波力 鉄 最先端技術産業 特徴 程度 上述 ｍ 水分 石灰石 Ｏ 促進 消石灰 水 該温度 エネルギー ＪＩＳ Ｚ 該粉末 周辺 開発 材料 主成分 ブロック状 実施形態 塩酸 エネルギー源 エタノール 円柱状 測定 バインダー 海洋温度差等 経済性 破砕手段 水素ガス ハンドプレス 資源量 汚泥 防波堤 製品 水素 輸出奨励政策 規制強化政策 放射性元素 供給不足 鉄滓 発電施設 自然エネルギー 周期律表 作業員宿舎 事情下 状況下 潮力 ボロン磁石 ＬＥＤ電球 寡占的産出国 価格高騰 特開平 号公報 アルカリ剤 発明者 形態 液 資源 Ａｌ 沖縄本島 燃料 ロータリーキルン 浮防波堤 海洋 市販品 商品名 化学組成 直径方向 αデンプン 石炭灰 説明 ネオジム 需要 中国 方針 確保 トリウム 利点 硫酸 鉛 バリウム 沈殿 該沈殿 溶液 概要 効果 希塩酸 範囲 族 ランタロイド Ｌａ ランタン Ｌｕ ルテチウム Ｓｃ スカンジウム Ｙ イットリウム 海 層状 負荷 容器 上澄み スクリュー コスト 簡易 脱水 順 珪石 ＳｉＯ Ｆｅ 珊瑚 貝殻 日本 北海道 本州 四国 九州 効率 埋立 粉末 スラリー 所望 形状 ロールクラッシャ ジョークラッシャ ポンツーン 土地 面 空港 運搬 頻度 ジオセット 低温 ℃） 粒状 該工場 一連 風力 太陽光 抽出 文中 試薬 φ 昇 荷重\n",
      "\n",
      "===== # 2, Topic : 14, p : 12.5608 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 分離性コンクリート コンクリート プレキャストブロック 一体性 台座コンクリート コンクリート構造物 図 水中 型枠 実施形態 脚部 構造体 水中コンクリート 本体 コンクリート構造物自体 筒状 プレキャストブロック等 内側 付着性 分離性 発明 水 外側 目 コンクリート塊 温度上昇 特許文献 前記プレキャストブロック 構築方法 トレミー管 ａ ｂ 残留水隙等 前記コンクリート ひび割れ 前記本体 水等 斜視図 設 平面 耐久性 下層 ｃ 締切用 円柱状 高め一体性 － 底面 断面 鉛直方向断面 水和反応 水和熱 コンクリート内部 コンクリート外側 堤体 設量 上記 傾斜 孔 例 形態 筒 前記 位置 流動距離 設置 底部 前記型枠 状態 表面 設箇所 順 台座 接触箇所 充填 形状 付着面積 工事 面積 等 型枠脱 技術的範囲 実施形態]　図 重量物 号公報 内外両面 丸印 水みち 説明 発生 縁 上向き 水底 ダム 矢印 基部等 平面隅部 温度応力 線Ａ－Ａ 線Ｂ－Ｂ 線Ｃ－Ｃ 堤体近傍 周方向 先行技術文献 技術分野 背景技術 方法 アンカー等 不純物等 矩形状 ひび割れ指数等 工事箇所 温度応力解析 温度差 規模 恐れ 手法 課題 目的 特徴 設時 効果 図面 様子 上下 事前 ～図 設前 設後 接触 接触面積 手順 別 離岸堤 添付図 特許請求 充填箇所 剥離箇所 円形平面 平面形状 技術的思想 範囲 特開昭 ダム工事 橋脚基部 特開平 角度θ tanθ 変更例 修正例 重量 高め 問題点 内部 補強筋 骨材 設置面 矢印Ｄ 層分 間隔 °間隔 多角形 周囲 水域 浮力 役割 相応 マス セメント 低温 空間 漏水 概要 強度 注意 手段 所定 製作 水平面 下 エア 要因 品質 ２つ 次 最後 設時間 利点 広範囲 工程 短縮 数量 最初 陸上 海岸 】[ 三角形 数 下部 業者 範疇 各種 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 11.3715 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 鉄骨架構 鉄骨柱 柱用ＰＣ版 梁用ＰＣ版 鉄骨梁 コンクリート版 内側ＰＣ版 柱脚部 側壁部 解体方法 構造物 前記鉄骨架構 実施形態 図 足場 ＰＣ版 柱用コンクリート版 線材 梁用コンクリート版 上記実施形態 フランジ部 内側 工程 Ａ 前記コンクリート版 Ｂ 反対側 請求項 解体 重機 壁部 柱脚部解体工程 断面Ｃ字状 断面Ｌ字状 外周部 縦断面図 結束工程 発明 特許文献 取り外し作業 外壁 当該鉄骨柱 外周架構 ウェブ部 外壁部 端部 遮蔽パネル 該鉄骨架構 方法 ＰＣ版取外し工程 Ｈ形鋼 状態 外側 下側 一対 例 等 結束 上記 両端部 手間 記載 ワイヤーロープ等 前記重機 上下方向 断面Ｃ字形状 断面Ｌ字形状 粉塵等 結束部材 解体作業 形態 締付器具 下端部 先端部 該柱脚部 参照 左右 Ｂ線断面図 結束方法 特開 号公報 可能性 Ａ同士 上下 変形例 幅方向両端部 形鋼 説明 障害 騒音 内部 両側 形状 前記足場 － 仕上げ材等 火入れ方法 形成方法 先行技術文献 枠組足場等 横方向 平板状 技術分野 背景技術 アングル等 可動フォーク等 課題 効果 図面 拡大図 側面図 周囲 縛 具 隙間 先 粉塵 Ｂ－ 矢印ＩＮ 矢印ＯＵＴ 延出し レバーブロック 登録商標 複数台 種 クレーン 概要 目的 手段 構成 種々 鋼管 開口 上側 スラブ 本数 配置 アーム 下部 支点 部位 対策 鋼材 ボルト 各種 要旨 範囲 態様 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 12.0034 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 防潮構造物 構造 既設構造物 構造物 既設構造物Ａ 上部壁 壁ブロック 防潮堤 鋼構造 防潮壁 基礎 柱 図 護岸構造物 構造物同士 鉄筋コンクリート構造 支持杭 前記既設構造物 門型構造体 既設防潮構造物 止水部材 施工方法 防波構造物 プレストレストコンクリート構造 一段目 上部壁ブロック 当該防潮構造物 止水板 津波 防潮構造物)　 実施形態 鉄骨鉄筋コンクリート構造 堤体 発明 防潮壁堤 地下貯水槽 組立壁 規模 施工 ,… ケーソン基礎 地盤面下 支持力 特許文献 ゴム製 技術 通常 a 目 両側 止水材 慣性力 金属性 正面図 地中連続壁基礎 フーチング 複数 止水性能 該門型構造体 杭 重量 相対変位 海岸線 フーチング基礎 鋼管 死荷重 津波荷重 上部壁間 門型構造体単独 コンクリート系 変位追随性 鋼管杭 場所 復旧 側 B 地震 施工手順 高力ボルト接合 断面 面 荷重 礎杭 使用 コスト プレキャスト部材 前記柱 a)～(c クレーン等 鉄筋コンクリート http://kotobank word/%E 貯水槽 瀝青材 高潮 短期間 海 上側 両端 延長方向 施工性 イ－イ線断面図 ロ－ロ線断面図 ハ－ハ線断面図 ニ－ニ線断面図 陸地側 ボルト接合 特開 号公報 延長 下 E スポンジシール c 建設機械 鉄筋コンクリート等 断面性能 海岸線方向 方法 説明 課題 海水 部材 補強 最小限 強度 台 接合方法 柱下 高力ボルト等 b 形態 装置 法面 現場施工 施工段階 下側 上部工自重 コンクリート等 防波機能 先行技術文献 地盤 方向 技術分野 背景技術 当該鋼管 水性緩衝材 利用可能性 襲来 一般 陸 用地 形状 期間 目的 HYPERLINK jp 流入 地上 最初 寸法 架台 台車 間隔 中央 数量 構築 省力化等 緩衝装置 海岸保全施設 a),(b 上下方向 想定 建設 所定間隔ごと ラーメン架構体 手延桁 想定外 台形状 版状 想定規模 上端部 下端部 東日本大震災 太平洋沿岸 堤防本体 複数本 自重 太径 作業ヤード 工場製作 中心 余震 再来 台風 大潮 浸水 頻度 実現 種 一定 勾配 コンクリートブロック 状態 該堤体 概要 前者 工事 長期 後者 工費 手段 特徴 A 陸上 新設 左右 外力 大型 効果 費用 図面 水門 下方 構成 両方向 工法 スパン 両方 現地 品質 向上 溶接 接着 最終 影響 剛性 例 産業 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 12.1383 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 融雪水 融雪水槽 水 路 冷水 熱交換器 雪室 融雪水供給路 水領域 冷水往路 弁Ｖ 雪エネルギー供給システム 熱交換 冷熱 空調機 雪類 空調システム 冷熱利用系 温度 雪 雪Ｓ 雪類エネルギー供給システム 温水領域 温度センサ 前記融雪水槽 流入口 前記熱交換器 側 熱源機 流路 供給 システム 前記冷水往路 冷却用熱媒体 熱媒体 連通口 Ｖ 冷水ポンプＰ 水温 排水路 所定温度 ｂ 領域 融雪水温度 空調用冷水 系 融雪水温度 冷水ポンプ 供給状態 前記冷熱利用系 状態 発明 開 通水 前記融雪水供給路 吐出口 熱交換器循環状態 ポンプ 所定 空調用冷水流路切替装置 空調装置 熱源機循環状態 制御部 融雪水供給手段 融雪水温度センサ 排水ポンプＰ 系外 散水ノズル 熱源機側往路 冷水空調機 前記雪室 空調 水位 水位センサ バイパス路 前記融雪水温度センサ 吸込口 雪エネルギーシステム 前記雪類エネルギー供給システム 雪類エネルギーシステム 構成 供給口 融雪水温 形態 当該熱交換 雪エネルギー供給方法 前記空調用冷水 前記連通口 弁 前記 実施 実施形態 信号線 位置 図 冷水温度センサ 当該空調用冷水 前記空調用 熱源機往路 循環流路 Ｐ 前記空調用冷水流路切替手段 特許文献 冷熱利用系等 前記熱源機 閉 熱源機側往路Ｖ 水熱源ヒートポンプユニット 隔壁 低温 融雪水流路切替装置 前記融雪水流路切替装置 冷熱利用運転 当該雪室 ｂ側 当該流路上 当該熱媒体 連通路 排水ポンプ 室 前記冷却用媒体 前記流入口 上述 ヘッダー 床面 運転 空調設備 散水 側端側 情報 排水口 つらら等 所定温度未満 他 一端 特開 水路 図示 上方 冷熱源 構成図 冷熱負荷 冷房運転 ターボ冷凍機 冷房 号公報 手段 自然気象 生成物 一端側 設置側 反対側 他側 高位側 低位側 開度制御 説明 冬季 技術 稼働 底面 前記隔壁 夏季 課題 室内 空気 ℃） 端部 制御方法 設備等 当該箇所 パンチング板等 先行技術文献 所定間隔 所定範囲 技術分野 背景技術 角部 当該送水 － 目的 氷点下 内部 図面 放水 後述 床 グレーチング板 多孔板 ゴミ等 後者 夏季等 一連 前述 中間貯留槽 略矩形状 Ｕ字状 中間位置 槽 中間 平面図 固体状 蓄熱槽 近接箇所 保温性 保温材 堆積位置 ℃）未満 上記課題 水冷方式 下方 下方近傍 近傍 釜場 室内空気 フロートスイッチ 積雪 概要 状況 該雪室 効率 効果 需要 用途 材質 順 エアハンドリングユニット 機器 閉状態 下部 側壁 全長 上部 床下 人 金網 下り 勾配 距離 前者 終端 ａ ２つ 配置 逆 容積 流通 最初 通常 外部 ℃ 融解 溢水 予兆 水中 流れ 形状 部分 指令 ℃、 量 相当 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 11.9535 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 孔管材 スリット 閉塞部材 スリット部 パイプ 土砂堆積部 土砂 管 図 加圧空気 複数 実施態様 水 目詰まり カップリング部材 地盤 位置 開口端部 外管 有孔管材 略縦断面図 内管 特許文献 孔 パイプ部材 空気 飽和砂地盤 液状化対策 発明 前記パイプ部材 管頭 カップリング 所定 前記スリット部 前記複数 号公報 所定量 前記土砂堆積部 マーカー 下方 状態 特開 中空部 開口部 中空 スクリーン管 管体 略斜視図 前記中空部 空気圧 初期位置 方法 上方 Ｏリング 閉塞 連結部 水圧 特許 井戸用管 ｂ 外管側 前記カップリング部材 容積 空気圧力 鉛直方向 水部 円形孔 － シール部材 シール材 ａ ｃ ｄ 目的 別 工程 ステップ 網部 先端部 下端部 目詰まり防止対策 三重構造 空気注入パイプ 洗浄用 説明 シルト 粘土 ブラシ 表面 和 一定 ｅ オス 範囲 井戸内洗浄 高圧洗浄水 先行技術文献 止水用 特許請求 開口面積 地下水 管内洗浄 技術分野 背景技術 空気注入 内部 材料 mm 課題 力 キャップ 特徴 効果 ｆ 体積 空間 コスト メス 下方位置 連結 空断面積 細粒分 汚染物質 矩形状 解消策 毛先 洗浄水 条件 管内 矢印Ａ 参照符号 地盤条件 符号 土壌 対象 地中 幅 概要 設時 外側 事態 圧力 上述 手段 内面 法 上面 図面 形態 金属 ゴム コルク ウレタン 万が一 下面 ホース 手間 侵入 選択 部分 他 ｇ 種々 変更\n",
      "\n",
      "===== # 7, Topic : 14, p : 12.3813 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 鋼矢板Ｐ 爪部 チャック装置 鋼矢板 パイルチャック部 杭圧入機 オーガケーシングＫ ケーシングチャック部 杭 パイルチャック本体 爪移動部 継手Ｊ サブチャック部 前記鋼矢板 爪駆動部 クランプ装置 図 オーガケーシング 駆動 油圧シリンダ 既設杭Ｐ 方向 継手 オーガ装置 可動爪 駆動部 前記チャック装置 バンド駆動部 圧入方向 ｂ 固定爪 矢板本体 杭圧入施工 ガ装置 ｃ 圧入 クランプ爪 長手方向 実施形態 バンド 上端部 既設 リーダーマスト ケーシングチャック 前記 請求項 外周面 可動クランプ爪 杭圧入装置 圧入施工 状態 圧入装置 移動手段 スライドベース 既設杭 発明 硬質地盤 前記パイルチャック部 ケーシングチャック部本体 前記ケーシングチャック部 ｅ 継手部 杭列 前記継手 ｄ 歯部 フランジ部 ～図 鈎状 Ｕ形鋼矢板 地中 側 上端側 固定クランプ爪 オーガスクリューＳ Ｐ 位置 形状 装置 当該杭圧入機 両側 面 先端側 地盤 Ｕ形鋼矢板等 説明図 メイン油圧シリンダ 記載 Ｋ オーガ併用工法 幅方向 特許文献 前記オーガケーシング 先端部 説明 サドル 動作 基部 円筒状 モーター駆動 前記オーガ装置 機械本体 力 Ｊ 前端部 カーブ状 特徴 向き 構成 曲面 シリンダ スライド移動機構 駆動モーターＭ等 抵抗力 位置形状 曲率 昇降範囲 形態 両端 部分 凹部 目 課題 内面 外面 他方 過程 複数 先 ｆ ガ併用工法 断面図 上記実施形態 固定手段 幅方向両側 手段 略円筒状 先行技術文献 縦断面図 ～ 概略図 背面側 前面側 反対側 ｂ側 技術分野 背景技術 III－III線 参照 － 図面 左右 周知 凹凸 上下 内側 保管環境等 配置切替手段 使用環境 ｃ等 機械 列 範囲 柔軟性 耐破断性 先行掘削 特開平 号公報 変形例 前面 オーガヘッドＨ 強化繊維 帯状部材 製造基準 バラつき 外部要因 変形 ストローク分 挟持 配置 下部 岩盤 概要 種類 目的 効果 周囲 公知 一対 下面 前方 前側 先頭 内部 図示 螺 万力 樹脂 皮革 公差 誤差 機能 下方 上方 解除 動 係 単体 妨げ 適用 趣旨 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 11.2464 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 天端棒 天端ポイント 天端ポイントＡ 天端位置 前記天端棒 収容具 横筋Ｘ 発明 コンクリート 鉄筋 作業員 前記収容具 下側鉄筋 設コンクリート 特許文献 横筋 図 螺子孔 作業 前記上板 上板 ハンドル 螺合 該天端棒 収容部 前記螺子孔 本体 設置 使用方法 螺合動作 螺合溝 下板 部材 基準具 状態 下端 略垂直方向 締結用ボルト 長手方向 コ字状 前記 上側鉄筋 説明 取付機構 実施例 棒材 取付作業 本願発明 本願 収容空間 前記特許文献 課題 側板 目印 Ａ 前記ハンドル 設置作業 位置 側方 前記下板 ハンドルＸ 取付位置 作業効率性 前記本体 連結具 設 回転操作用 取付方法 先行技術文献 方法 型枠用 垂直起立 コンクリート構造物 技術分野 背景技術 設空間 上下 上端 特徴 効果 領域 分 図面 固定 通り 使用状態 ネジ孔 回転動作 水平方向 分解斜視図 実施形態 平板状 長手部材 通常通り左官仕上げ 安定性 直立状態 説明図 設置範囲 特開 号公報 組立完了 結束線 複数箇所 形態 構成 構成要素 平板 カラーテープ 空洞部分 構築 記載 － 概要 改善 余地 手作業 人 場所 スラブ 設中 難 製品 目的 手段 任意 １つ 維持 内部 セパレータ 既成 側壁 形状 谷 表示 棒状 溶接 縦横 ａ 下方 ｂ 工程 確認 付与 所定 取り外し 利用 まとめ 拡大 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 12.0628 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 耐震壁厚肉部 耐震壁 床スラブ厚肉部 ラーメン構造 耐震壁薄肉部 床スラブ薄肉部 構造部 構造 共用廊下 床スラブ 架構面 側 梁間方向 前記耐震壁厚肉部 柱 前記耐震壁 前記床スラブ厚肉部 桁行方向 バルコニー 梁 前記構造部 実施例 壁状 建物 スラブ状 戸境壁 壁厚 前記ラーメン構造 開口部 住空間 壁式構造 前記共用廊下 スラブ 住戸 床構造 剛性 連層耐震壁 図 集合住宅 前記柱 端部 薄肉 ｂ 前記バルコニー 自由度 前記耐震壁薄肉部 室内空間 共用廊下側 複数 壁厚内 位置 補強構造体 特許文献 境界部分 前記複数 床 平面図 壁柱 メガブレース構造 基準階 戸境 免震装置 廊下側 複数層 ～ 発明 端部位置 構成 該複数 該耐震壁 ラーメン架構面 夫 程度 板状 空間 該共用廊下 構造体 強度 構造物 前記ラーメン構面 架構 境 集合住宅建物 構造部材 室内 他方 間取りプラン 相対関係 水平剛性 散点ハッチング 耐震補強架構 採光 構造種別 耐力 該梁間方向 Y方向 手方向 上記 説明 接合部 端面 中央部 Ｄ 居室 鉄筋コンクリート造 該構造部 外側端部 内側端部 外側 他側 該バルコニー 断面図 側面図 説明図 ～図 せん断補強筋 眺望 通常 領域 前記建物 平面形 板状集合住宅 下層階 プラン 梁形状 屋外 等 幅 下面 上面 建築物 遮音性能 左右方向 横方向 Ｘ方向 長手方向 メガブレース 特開 号公報 配管スペース 構成要素 前記住戸 部分 補強部材 制約 課題 技術 突出 形態 ａ バルコニー側夫 線状 ブロック状 内側 屋内空間 内部空間 鉄骨鉄筋コンクリート造 鉄骨造 鉄骨コンクリート造 建物等 先行技術文献 中央部分 コンクリート量等 平面視 技術分野 背景技術 ＳＲＣ造 ＲＣ造 ＳＣ造 中高層 目的 特徴 記載 耐力壁 － 使い勝手 各層 方 主筋 確保 メイン 全長 範囲 遮音性 増大 高層 効率 図面 骨組 辺 上辺 壁面 符号 スパン 扁平梁 断面 部材 他 採光性 剛接合 耐振動性 妻面 ホテル等 開放感 請求項 必要強度 地震力 スターラップ 材料コスト グレード向上 グレード 相違点 Ｌ字形 窓 天井 概要 手段 帯状 外形 組合せ フープ ハンチ 効果 重量 バランス 構築 プライバシー 間口 奥行き 矩形 手摺 同一 要旨 付加 事務所 用途\n",
      "\n",
      "===== # 10, Topic : 14, p : 11.4821 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 壁材 定着筋 定着板 本体プレート 補強筋 実施形態 図 プレート部 側 建築躯体 脚部 端部 前記壁材 建物躯体 定着金物 連結構造 前記本体プレート 壁材コンクリート Ｙ方向 Ａ ファスナー 室内側 本体プレート周辺 連結構造Ａ 内面 支持ボルト 上記実施形態 応力 下方 接続部 前記定着板 ｂ側 縦筋 溶接 Ｚ方向 連結ボルト 直線状 ネジ孔 ｂ 外面 孔 壁部 ボス部 板状 溶接等 正面図 発明 側断面図 定着性能 例 ２つ 前記壁材側 一対 プレキャストコンクリート製 構造 Ｂ 配筋態様 Ｘ方向 Ｉ－Ｉ線断面図 上部 ボルト 特許文献 せん断力 発生 横筋 斜め下方 周辺 並び 応力伝達 補強リブ 前記建物躯体 定着力 エア抜き孔 天板部 矢印Ｘ 内側 挿通部 説明図 ネジ棒部分 滑り材 コ字 前記建物躯体側 定着金具 力 方向 形態 面 位置 鉄筋 下端 外側 構成 伝達 せん断 調整座金 調整台 溝部 連結 Ｙ 上端部 開口部 配筋量 説明 垂直姿勢 座金 取付台座 曲げ 外面側 内面側 室外側 頭部側 側方 下側 字状 － 溝部 １つ 下部 矢印Ｙ 上下 等 矢印Ｚ 接合部近傍 ネジ孔ｈ 上下方向 鉛直方向 水平方向 厚み方向 法線方向 左右方向 面方向 貫通孔 前記ファスナー 調整 ～図 先行技術文献 円弧状 Ｌ字 溝形鋼等 逆Ｕ字 隅肉溶接 技術分野 背景技術 課題 地震等 ひび割れ 効果 鋼板 ナット ｃ 自重等 荷重 上面 下面 ３つ 一体 空間 支持耐力 機能 リブ 形状例 フック形状 床や梁 鉄骨梁 延設方向 特開 号公報 自重 ～（Ｄ 建入れ 外径 径 固着位置 合計２つ Ｄ 方法 種類 概要 風 要因 仕組み 目的 手段 特徴 図面 複数 図示 段差 製造 鋼材 中央 上方 周囲 先端 位置決め b 支圧 分断 引き抜き ａ 板材 基調 組立て コストダウン 品質 他 両端 Ｃ 上側\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.5915e-26, 1.5692e-37, 8.5865e-20, 0.0000e+00,\n",
      "         0.0000e+00, 6.7502e-21, 1.6676e-29, 9.7060e-40, 1.3387e-40, 3.4332e-43,\n",
      "         8.0822e-14, 1.2470e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1748.83167\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1149.9243\n",
      "Test epoch : 1 Average loss: 1156.1174\n",
      "PP(train) = 5939.491, PP(valid) = 5876.007\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4347e-26, 1.1953e-39, 9.1561e-24, 0.0000e+00,\n",
      "         0.0000e+00, 4.0807e-19, 5.5691e-35, 5.0867e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.2300e-18, 9.8091e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1576.56677\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1149.2495\n",
      "Test epoch : 2 Average loss: 1155.5561\n",
      "PP(train) = 5912.246, PP(valid) = 5851.525\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.5961e-42, 7.0065e-45, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.1986e-34, 4.0966e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0199e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1421.27051\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1148.5109\n",
      "Test epoch : 3 Average loss: 1154.9800\n",
      "PP(train) = 5884.206, PP(valid) = 5827.239\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2251e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.4153e-43, 2.8320e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.3739e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1281.27136\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1147.8102\n",
      "Test epoch : 4 Average loss: 1154.4122\n",
      "PP(train) = 5855.952, PP(valid) = 5803.303\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 2.9006e-38, 1.1542e-34, 5.6199e-33, 0.0000e+00,\n",
      "         0.0000e+00, 5.9624e-32, 4.1019e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.7060e-34, 1.2705e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1155.06250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1147.4173\n",
      "Test epoch : 5 Average loss: 1153.8571\n",
      "PP(train) = 5826.655, PP(valid) = 5778.562\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0439e-33, 0.0000e+00, 1.2769e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.3423e-33, 3.5093e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.2745e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1041.28564\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1146.6109\n",
      "Test epoch : 6 Average loss: 1153.3076\n",
      "PP(train) = 5797.356, PP(valid) = 5753.898\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.7458e-18, 8.5185e-42, 4.0301e-31, 0.0000e+00,\n",
      "         0.0000e+00, 9.2799e-25, 2.6383e-27, 3.1215e-34, 0.0000e+00, 0.0000e+00,\n",
      "         1.9349e-15, 2.7544e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 938.71606\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1146.0107\n",
      "Test epoch : 7 Average loss: 1152.7554\n",
      "PP(train) = 5768.546, PP(valid) = 5729.662\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2192e-34, 0.0000e+00, 1.8597e-31, 0.0000e+00,\n",
      "         0.0000e+00, 2.2943e-32, 2.5226e-33, 1.9989e-39, 0.0000e+00, 0.0000e+00,\n",
      "         1.4690e-19, 4.5220e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 846.24988\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1145.3019\n",
      "Test epoch : 8 Average loss: 1152.1948\n",
      "PP(train) = 5740.464, PP(valid) = 5706.230\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.2512e-42, 3.9862e-40, 1.2939e-33, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.5543e-36, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.6434e-44, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 762.89191\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1144.5267\n",
      "Test epoch : 9 Average loss: 1151.6284\n",
      "PP(train) = 5713.119, PP(valid) = 5683.469\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.8106e-31, 1.2546e-38, 2.0133e-26, 0.0000e+00,\n",
      "         0.0000e+00, 7.1550e-35, 9.0453e-30, 3.5104e-38, 0.0000e+00, 0.0000e+00,\n",
      "         1.1305e-28, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 687.74493\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1143.9054\n",
      "Test epoch : 10 Average loss: 1151.0732\n",
      "PP(train) = 5685.345, PP(valid) = 5660.231\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1019e-43, 2.0665e-32, 0.0000e+00,\n",
      "         0.0000e+00, 5.8632e-28, 1.0840e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.5333e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 620.00012\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1143.3221\n",
      "Test epoch : 11 Average loss: 1150.5224\n",
      "PP(train) = 5657.776, PP(valid) = 5637.041\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.1850e-31, 2.2041e-31, 3.6217e-24, 0.0000e+00,\n",
      "         0.0000e+00, 6.1134e-29, 4.8825e-35, 1.4610e-38, 0.0000e+00, 0.0000e+00,\n",
      "         2.9188e-34, 2.2421e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 558.92834\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1142.7250\n",
      "Test epoch : 12 Average loss: 1149.9838\n",
      "PP(train) = 5629.987, PP(valid) = 5613.640\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.8252e-36, 1.4013e-45, 3.2109e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.4144e-23, 6.5109e-41, 8.7119e-42, 0.0000e+00, 0.0000e+00,\n",
      "         9.5580e-25, 1.4599e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 503.87231\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1142.0698\n",
      "Test epoch : 13 Average loss: 1149.4580\n",
      "PP(train) = 5601.837, PP(valid) = 5589.830\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.8251e-41, 1.1153e-22, 4.4793e-38, 8.6991e-23, 0.0000e+00,\n",
      "         0.0000e+00, 1.1470e-31, 9.9678e-18, 0.0000e+00, 4.2039e-45, 0.0000e+00,\n",
      "         1.3572e-16, 6.7014e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 454.23947\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1141.4811\n",
      "Test epoch : 14 Average loss: 1148.9327\n",
      "PP(train) = 5574.131, PP(valid) = 5566.359\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0727e-29, 2.8630e-37, 7.2441e-23, 0.0000e+00,\n",
      "         0.0000e+00, 2.5794e-29, 1.0775e-31, 7.2998e-41, 0.0000e+00, 0.0000e+00,\n",
      "         1.0124e-20, 3.2880e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 409.49561\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1140.7793\n",
      "Test epoch : 15 Average loss: 1148.3874\n",
      "PP(train) = 5547.525, PP(valid) = 5543.971\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0041e-34, 0.0000e+00,\n",
      "         0.0000e+00, 6.1685e-37, 2.0237e-25, 1.0454e-42, 0.0000e+00, 0.0000e+00,\n",
      "         6.3386e-20, 4.2039e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 369.15915\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1140.2664\n",
      "Test epoch : 16 Average loss: 1147.8279\n",
      "PP(train) = 5522.157, PP(valid) = 5522.694\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7040e-42, 0.0000e+00, 6.6007e-27, 0.0000e+00,\n",
      "         0.0000e+00, 2.4645e-23, 9.5062e-22, 2.6014e-34, 0.0000e+00, 0.0000e+00,\n",
      "         2.0287e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 332.79593\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1139.3046\n",
      "Test epoch : 17 Average loss: 1147.2789\n",
      "PP(train) = 5496.595, PP(valid) = 5501.152\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4545e-42, 2.7470e-37, 1.0168e-29, 0.0000e+00,\n",
      "         0.0000e+00, 5.7269e-24, 6.4443e-26, 4.5228e-35, 0.0000e+00, 0.0000e+00,\n",
      "         8.9534e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 300.01459\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1138.8852\n",
      "Test epoch : 18 Average loss: 1146.7360\n",
      "PP(train) = 5470.928, PP(valid) = 5479.460\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.4389e-24, 1.8780e-41, 4.6046e-20, 0.0000e+00,\n",
      "         0.0000e+00, 3.4362e-22, 2.4002e-17, 4.9551e-34, 0.0000e+00, 7.0690e-37,\n",
      "         2.6660e-16, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 270.46231\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1138.2028\n",
      "Test epoch : 19 Average loss: 1146.2021\n",
      "PP(train) = 5444.928, PP(valid) = 5457.384\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2307e-41, 9.9488e-28, 1.2326e-33, 4.0989e-18, 0.0000e+00,\n",
      "         0.0000e+00, 3.6236e-33, 3.7072e-33, 1.3658e-40, 0.0000e+00, 7.4549e-43,\n",
      "         1.2527e-18, 3.6135e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 243.82101\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1137.7246\n",
      "Test epoch : 20 Average loss: 1145.6765\n",
      "PP(train) = 5419.032, PP(valid) = 5435.242\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7295e-39, 5.0852e-22, 1.8233e-33, 9.7137e-19, 0.0000e+00,\n",
      "         4.1823e-36, 1.3163e-20, 8.8946e-15, 1.3909e-32, 0.0000e+00, 0.0000e+00,\n",
      "         2.1960e-10, 8.0591e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 219.80396\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1137.0021\n",
      "Test epoch : 21 Average loss: 1145.1530\n",
      "PP(train) = 5393.454, PP(valid) = 5413.433\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.6845e-36, 2.6625e-44, 2.3274e-40, 0.0000e+00,\n",
      "         0.0000e+00, 4.7994e-35, 4.6301e-37, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.7531e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 198.15265\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1136.3790\n",
      "Test epoch : 22 Average loss: 1144.6312\n",
      "PP(train) = 5368.079, PP(valid) = 5391.788\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.9569e-30, 6.0299e-32, 4.4602e-17, 0.0000e+00,\n",
      "         0.0000e+00, 4.1153e-15, 2.2091e-28, 2.4806e-28, 0.0000e+00, 0.0000e+00,\n",
      "         6.9658e-21, 3.2139e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 178.63405\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1135.8427\n",
      "Test epoch : 23 Average loss: 1144.1094\n",
      "PP(train) = 5343.047, PP(valid) = 5370.424\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.8399e-34, 9.8091e-45, 1.9122e-18, 0.0000e+00,\n",
      "         0.0000e+00, 2.8785e-23, 4.3893e-27, 5.0010e-38, 0.0000e+00, 4.2179e-43,\n",
      "         9.7318e-22, 1.2378e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 161.03809\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1135.0782\n",
      "Test epoch : 24 Average loss: 1143.5831\n",
      "PP(train) = 5318.675, PP(valid) = 5349.691\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4187e-32, 2.5442e-40, 3.5842e-22, 0.0000e+00,\n",
      "         0.0000e+00, 6.5332e-18, 1.7790e-24, 4.1598e-37, 0.0000e+00, 0.0000e+00,\n",
      "         3.2226e-22, 7.9085e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 145.17538\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1134.5399\n",
      "Test epoch : 25 Average loss: 1143.0590\n",
      "PP(train) = 5294.389, PP(valid) = 5328.972\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 2.5658e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.8110e-38, 5.0261e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         6.8842e-30, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 130.87520\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1133.8009\n",
      "Test epoch : 26 Average loss: 1142.5367\n",
      "PP(train) = 5270.520, PP(valid) = 5308.682\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3660e-39, 2.0874e-32, 1.1699e-35, 1.0510e-18, 0.0000e+00,\n",
      "         0.0000e+00, 8.0417e-12, 5.8799e-21, 5.4126e-34, 0.0000e+00, 2.3822e-44,\n",
      "         3.4943e-27, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 117.98362\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1133.3289\n",
      "Test epoch : 27 Average loss: 1142.0172\n",
      "PP(train) = 5246.549, PP(valid) = 5288.182\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4920e-38, 9.4269e-40, 1.2051e-26, 0.0000e+00,\n",
      "         0.0000e+00, 8.4845e-22, 1.2331e-19, 8.0898e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.3311e-18, 5.4651e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 106.36190\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1132.6846\n",
      "Test epoch : 28 Average loss: 1141.5030\n",
      "PP(train) = 5222.388, PP(valid) = 5267.520\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1705e-27, 0.0000e+00,\n",
      "         0.0000e+00, 3.9113e-36, 2.6092e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.3230e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 95.88496\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1131.9944\n",
      "Test epoch : 29 Average loss: 1140.9806\n",
      "PP(train) = 5198.898, PP(valid) = 5247.424\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1463e-31, 1.4321e-39, 6.2012e-29, 0.0000e+00,\n",
      "         0.0000e+00, 6.2432e-39, 3.9306e-29, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.5374e-16, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 86.44002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1131.3888\n",
      "Test epoch : 30 Average loss: 1140.4576\n",
      "PP(train) = 5176.077, PP(valid) = 5227.999\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.9041e-38, 3.1962e-23, 2.2452e-26, 3.1957e-24, 0.0000e+00,\n",
      "         0.0000e+00, 2.5008e-15, 8.8883e-23, 3.9429e-27, 4.7364e-43, 0.0000e+00,\n",
      "         7.8494e-21, 3.0791e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 77.92543\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1130.9290\n",
      "Test epoch : 31 Average loss: 1139.9466\n",
      "PP(train) = 5152.913, PP(valid) = 5208.158\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.9492e-44, 7.5494e-23, 1.8813e-36, 1.3201e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.0786e-21, 9.6179e-27, 2.8136e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.4848e-16, 5.1150e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 70.24956\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1130.2272\n",
      "Test epoch : 32 Average loss: 1139.4502\n",
      "PP(train) = 5128.821, PP(valid) = 5187.215\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.9162e-20, 5.7283e-36, 1.2671e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.0955e-16, 7.7094e-16, 2.4949e-35, 0.0000e+00, 3.9797e-43,\n",
      "         1.2958e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 63.32978\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1129.7436\n",
      "Test epoch : 33 Average loss: 1138.9561\n",
      "PP(train) = 5105.188, PP(valid) = 5166.824\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4836e-41, 4.4438e-33, 0.0000e+00,\n",
      "         0.0000e+00, 6.9240e-27, 3.4252e-28, 1.4314e-40, 0.0000e+00, 1.5274e-43,\n",
      "         1.8087e-18, 3.9797e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 57.09161\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1129.0583\n",
      "Test epoch : 34 Average loss: 1138.4540\n",
      "PP(train) = 5082.385, PP(valid) = 5147.196\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.3855e-37, 0.0000e+00, 8.1181e-31, 0.0000e+00,\n",
      "         0.0000e+00, 3.4976e-32, 1.6753e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.2467e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 51.46793\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1128.4623\n",
      "Test epoch : 35 Average loss: 1137.9429\n",
      "PP(train) = 5060.403, PP(valid) = 5128.327\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1126e-41, 1.1730e-21, 0.0000e+00, 2.2165e-13, 6.4880e-43,\n",
      "         0.0000e+00, 1.6053e-15, 8.3583e-18, 4.0322e-32, 0.0000e+00, 7.1707e-41,\n",
      "         3.2520e-14, 1.6453e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 46.39819\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1127.9863\n",
      "Test epoch : 36 Average loss: 1137.4326\n",
      "PP(train) = 5038.512, PP(valid) = 5109.455\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0263e-22, 1.6036e-37, 2.0334e-17, 0.0000e+00,\n",
      "         0.0000e+00, 1.0186e-19, 5.5474e-21, 8.6079e-36, 0.0000e+00, 1.3311e-41,\n",
      "         3.3831e-18, 1.6913e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 41.82784\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1127.3349\n",
      "Test epoch : 37 Average loss: 1136.9319\n",
      "PP(train) = 5016.428, PP(valid) = 5090.472\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2543e-40, 2.6322e-35, 4.0605e-30, 6.0987e-12, 0.0000e+00,\n",
      "         0.0000e+00, 1.7255e-16, 1.0412e-22, 1.9688e-41, 4.2039e-45, 0.0000e+00,\n",
      "         2.8085e-22, 1.8466e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 37.70768\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1126.7031\n",
      "Test epoch : 38 Average loss: 1136.4305\n",
      "PP(train) = 4994.737, PP(valid) = 5071.820\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1644e-28, 5.7958e-34, 2.1109e-34, 0.0000e+00,\n",
      "         0.0000e+00, 5.3749e-17, 3.0902e-30, 3.4661e-35, 0.0000e+00, 8.2249e-39,\n",
      "         2.8003e-22, 2.8005e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 33.99337\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1126.1168\n",
      "Test epoch : 39 Average loss: 1135.9287\n",
      "PP(train) = 4972.869, PP(valid) = 5052.910\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3428e-35, 9.5114e-21, 4.8307e-39, 9.0594e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.5849e-16, 3.7393e-19, 2.7688e-30, 0.0000e+00, 2.7620e-42,\n",
      "         4.3060e-18, 3.4700e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 30.64492\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1125.6030\n",
      "Test epoch : 40 Average loss: 1135.4401\n",
      "PP(train) = 4950.539, PP(valid) = 5033.517\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.4287e-41, 0.0000e+00, 5.3719e-32, 0.0000e+00,\n",
      "         0.0000e+00, 1.5598e-28, 1.2101e-31, 1.0229e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.1321e-26, 2.4024e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 27.62631\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1125.0986\n",
      "Test epoch : 41 Average loss: 1134.9550\n",
      "PP(train) = 4928.395, PP(valid) = 5014.300\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4821e-33, 6.6744e-40, 4.0337e-25, 0.0000e+00,\n",
      "         0.0000e+00, 4.9048e-27, 9.9018e-37, 1.7252e-40, 0.0000e+00, 0.0000e+00,\n",
      "         3.6662e-21, 2.7465e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 24.90504\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1124.4622\n",
      "Test epoch : 42 Average loss: 1134.4648\n",
      "PP(train) = 4907.163, PP(valid) = 4995.871\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.0958e-36, 3.1371e-30, 3.4124e-41, 1.2073e-24, 0.0000e+00,\n",
      "         0.0000e+00, 3.1213e-23, 8.7199e-27, 1.2661e-34, 0.0000e+00, 2.2421e-44,\n",
      "         2.2849e-08, 4.2126e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 22.45183\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1123.9603\n",
      "Test epoch : 43 Average loss: 1133.9641\n",
      "PP(train) = 4886.769, PP(valid) = 4978.278\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.7804e-27, 8.7675e-40, 1.8202e-26, 0.0000e+00,\n",
      "         0.0000e+00, 3.4924e-20, 3.2360e-30, 0.0000e+00, 2.6260e-41, 0.0000e+00,\n",
      "         8.0560e-26, 1.6272e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 20.24026\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1123.3179\n",
      "Test epoch : 44 Average loss: 1133.4711\n",
      "PP(train) = 4866.505, PP(valid) = 4960.799\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-44, 4.0186e-25, 5.6239e-37, 2.8409e-32, 0.0000e+00,\n",
      "         0.0000e+00, 2.6622e-17, 1.2827e-21, 4.6243e-44, 1.3593e-43, 1.4503e-42,\n",
      "         7.8909e-22, 1.1911e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 18.24663\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1122.8259\n",
      "Test epoch : 45 Average loss: 1132.9844\n",
      "PP(train) = 4845.591, PP(valid) = 4942.661\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2281e-43, 4.2806e-23, 3.1228e-34, 1.6280e-17, 0.0000e+00,\n",
      "         2.3724e-42, 2.7934e-07, 2.4658e-14, 5.5733e-23, 0.0000e+00, 2.2701e-43,\n",
      "         5.9248e-26, 1.7315e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 16.44942\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1122.2560\n",
      "Test epoch : 46 Average loss: 1132.5049\n",
      "PP(train) = 4824.565, PP(valid) = 4924.354\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.6572e-38, 0.0000e+00, 1.7068e-22, 0.0000e+00,\n",
      "         0.0000e+00, 1.3065e-27, 2.0223e-26, 4.1521e-40, 0.0000e+00, 1.4013e-45,\n",
      "         1.8370e-28, 8.1945e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 14.82931\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1121.5306\n",
      "Test epoch : 47 Average loss: 1132.0244\n",
      "PP(train) = 4803.813, PP(valid) = 4906.269\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1123e-42, 2.5263e-37, 4.1234e-36, 9.1780e-33, 0.0000e+00,\n",
      "         0.0000e+00, 2.1369e-22, 2.0349e-29, 1.5695e-43, 0.0000e+00, 0.0000e+00,\n",
      "         1.9758e-24, 2.1356e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 13.36876\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1120.9866\n",
      "Test epoch : 48 Average loss: 1131.5387\n",
      "PP(train) = 4783.693, PP(valid) = 4888.843\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2612e-44, 5.0767e-29, 1.1210e-44, 3.0654e-28, 0.0000e+00,\n",
      "         0.0000e+00, 6.5210e-20, 2.1543e-23, 3.0818e-39, 0.0000e+00, 0.0000e+00,\n",
      "         7.4162e-19, 4.0152e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 12.05206\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1120.3822\n",
      "Test epoch : 49 Average loss: 1131.0532\n",
      "PP(train) = 4764.016, PP(valid) = 4871.855\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.7386e-38, 1.7065e-18, 2.5047e-26, 1.9055e-27, 0.0000e+00,\n",
      "         2.0739e-43, 4.4991e-08, 3.6034e-36, 1.3263e-40, 0.0000e+00, 9.8846e-39,\n",
      "         1.3649e-24, 1.7849e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 10.86504\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1120.0119\n",
      "Test epoch : 50 Average loss: 1130.5794\n",
      "PP(train) = 4744.018, PP(valid) = 4854.486\n",
      "Writing to ./topicwords/5-topwords_e50.txt\n",
      "Topic 0: 並行 建築 補強材 植物 軸線方向 組合せ 緑化 定着構造 粉砕 Ｉ\n",
      "Topic 1: 並行 建築 補強材 軸線方向 植物 緑化 組合せ 前部 定着構造 粉砕\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 3: 機 定着板 前記センサ 帯 有無 建造物 円周方向 短期間 継手 ガイドローラ\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 手段 形態 説明 課題\n",
      "Topic 5: 建築 並行 補強材 軸線方向 緑化 植物 どうし 組合せ 定着構造 粉砕\n",
      "Topic 6: 補強材 建築 並行 軸線方向 緑化 植物 定着構造 組合せ 粉砕 Ｉ\n",
      "Topic 7: － Ａ 位置 ２つ 柱 技術分野 手段 形態 説明 発明\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 手段 形態\n",
      "Topic 9: 並行 建築 植物 軸線方向 ｘ 補強材 緑化 添加剤 前部 組合せ\n",
      "Topic 10: 補強材 並行 建築 軸線方向 植物 緑化 組合せ ｘ 定着構造 粉砕\n",
      "Topic 11: 並行 建築 軸線方向 緑化 組合せ 植物 補強材 定着構造 前部 粉砕\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 手段 形態 説明 発明 課題\n",
      "Topic 13: 建築 並行 軸線方向 組合せ 緑化 ｘ 補強材 Ｉ 定着構造 粉砕\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 10.7826 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 伝熱部材 室温熱平衡システム 伝熱 図 熱 熱移動 金属板 部材 室温 居室Ａ 部屋 接続用金属板 天井裏空間 天井仕上げ面 熱移動媒体 例 端部 前記伝熱部材 Ｂ 部屋相互 ｂ 熱伝導率 熱移動経路 発明 自身端部 熱的快適性 居室 上記実施形態 Ａ 伝熱部材周囲 放射冷暖房 天井 天井裏空間Ｔ 複数 脱衣室 熱平衡状態 天井放射パネル 部分 熱的平衡状態 特許文献 ボタンスイッチ 床暖房 居間Ｃ 金属 ａ システム 模式図 前記金属板 状態 伝熱媒体 断熱材 縦棒 設定温度 体感温度 金属板表面 管状伝熱部材 伝熱速度 相互 住戸内室温 ｂ相互 金属製 接続 接面 移動 熱移動サイクル 熱収支 伝熱的連通状態 温熱 棒状 前記天井仕上げ面 平面図 イメージ ｃ 実施形態 快適性 病院等 ヒートショック等 ガス等 石膏ボード 居間 部材相互 住戸 技術 Ｗ/（ｍ 低温部 要部 床面 居住施設 端部相互 分割構造 態様 熱的体感 拡大図 宿泊施設 適用例 変形例 熱伝達性能 均一化 天井面 室温設定 健康被害 省エネ 作動液 効果 トイレ Ｚ部分 上記室温熱平衡システム 放射冷暖房技術 遮断 エアコン等 模様等 暖房 夏季 冬季 ヒートパイプ 風呂 上記放射冷暖房技術 端部同士 部分Ｍ 居室相互 Ａ－Ａ断面図 分割部 居住空間 空気 アルミニウム 床暖房等 室温分布 住宅 － レスポンス 洋室 板状 天井全面 軽量鉄骨 蒸発潜熱 ステンレス 加熱部 居室Ｂ Ｂ相互 床 形態 斜視図 加温部 図左右方向 図左方向 図右方向 説明 節電 向上 構成 夏 冬 Ｋ ℃ 位置 上部 圧操作 接続状態 温風暖房 冷暖房機器 冷暖房器具 床暖房とも 課題 中空 ステンレス等 廊下等 温度検知手段 先行技術文献 情勢等 空気温度 カム面 廊下部分 ボタン背面 技術分野 背景技術 コールドドラフト等 ハンダ付け等 居住 接 Ｂ側 上記課題 生産性 毛細管構造 ｂ)） 符号Ｚ 参照 燃料 冷却 熱源 手段 電気 下地 蒸気 図面 遮断状態 デザイン 二つ 変形 矢印 日差し 機構 背面側 スロープ ステンレス製 高齢化 顕在化 一体構造 日本建築学会環境系論文集 アルミニウム製 周囲 蒸発 符号 換気手段 制御手段 反対側 中空棒状体内 毛細管現象 ａ)） 符号Ｙ 冷媒 冷媒管 特開平 号公報 号 加熱 問題点 接触配置 方法 管状 壁面近傍 垂直下向き 壁面 固定方法 離隔距離 原発 稼働 年間 家庭 高め 低め 方策 対象 研究 ｐｐ 概要 窓際 所定 種 通路 記載 目的 特徴 両端 内壁 吸収 凝縮 放出 上述 外観 凹凸 印刷 材料 趣旨 種々 ３つ 日当たり 冷房 台数 形状 帯状 網状 概念 切替 下部 水平面 下方 斜め\n",
      "\n",
      "===== # 2, Topic : 14, p : 10.4457 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 土壌 重金属 ヒ素含有重金属汚染土壌 不溶化材 鉄 含有量 汚泥 前記汚泥 ヒ素 溶出 ヒ素濃度 汚染土壌 上記土壌溶出量試験 溶出量 量 硫酸アルミニウム 不溶化 硫酸 水酸化鉄 Ｌ 鉄バクテリア 基準不適合土壌 炭酸カルシウム 水 質量 粒子径 方法 実施例 前記 土壌汚染対策法 土壌溶出基準 不溶化方法 ヒ素等 溶出基準 ヒ素等重金属 土壌溶出量 発明 重金属溶出量 ヒ素重金属含有汚染土壌 地下水浄化 鉄含有量 重金属含有量基準 汚染状態 土壌含有量試験 浄水場 ヒ素含有重金属 土壌溶出量基準 ヒ素含有量 浄水場等 ヒ素等重金属汚染土壌 前記ヒ素含有重金属汚染土壌 浄化 土壌溶出量試験 試料量 重金属等 カドミウム 鉛 溶出基準値 土壌含有量測定試験 表層土壌 特許文献 土壌溶出量試験方法 原位置不溶化 前記重金属 自然由来重金属等含有岩石 材 不溶化措置 試料 能力 溶出基準不適合 特定有害物質 関連条例 等 ミキサー等 土壌試料 μｍ 分析方法 不溶化能力 不溶化効果 溶解鉄 汚染程度 カドミウム等 フッ素 汚泥添加量 施設 特徴 記載 基準値 試料液 前記不溶化材 乾燥温度 Ｌ未満 スタビライザー等 性能 特開 号公報 平均粒子径 添加量 カドミウム濃度 混合 濃度 山砂 分析 試験操作 鉛濃度 フッ素濃度 コスト 比較例 ）～（ 含水率 重金属類 － 効果 作業性 平均粒径 上記浄水場等 前記浄水場 対策 撹拌 液 不溶化処理 前記汚泥単独 孔径 産業廃棄物 前記汚泥単体 乾燥コスト等 容器 ～ 程度静置後 汚泥単体 装置 鉄化合物系 鉄系化合物 浄化施設等 地下水中 上澄み液 検液 カルシウム 当該汚泥 未満 攪拌方法 前記比較例 質量体積比 経済性 粒径 乾燥方法 硫酸アルミニウム単独 クロム等 回数毎分 不溶化措置費用 群 種類 溶媒 機 ｋｇ 対応マニュアル 補完マニュアル 混合方法 上記 化合物 有害物質 混合液 建設工事 ポリ容器 有効利用 浄化装置 地下水等 粉砕機等 混合装置 発生物 自然乾燥 注入 性状 ごと 技術 塩酸 割合 ℃） 調整 測定 バックホウ 攪拌 μｍ程度 定量 処理方法 室温乾燥 水素イオン濃度指数 レーザー回折散乱式粒子径分布測定装置 添加 山砂単独 組成物 粒径調整 定量分析 先行技術文献 ＬＡ－ Ｓ字ブレンダー 深層混合工法 層混合工法 誘導結合発光分光分析装置 ＡＥＳ 水素化物発生装置 技術分野 背景技術 粉体 スーパーミキサー ハイスピードミキサー 薬剤 課題 圧 幅 メンブランフィルター ＪＩＳ Ｋ 砒素 ℃ 観点 混練機 表 Ｖ字ブレンダー 流動性 機械的乾燥 材料コスト 粉体同士 質量減少率 岩石 スタビライザー 分 凝集沈殿剤 園芸用肥料 増量剤 リガク製 ＪａｒｒｅｌｌＡｓｈ製 ＨＹＤ－ 環境大臣 化学的合成品 発明者 農業用水 量り採り 室温 製鉄所 海底ヘドロ 説明 調査 範囲 土地 変更 参照 市場 主成分 マグネシウム 提供 実態 精 概要 目的 手段 原料 見出し 形態 姿 塊 ＨＯＲＩＢＡ 溶液 リテラ リボンブレンダー タンブラー コーンブレンダー ナウターミキサー ヘンシェルミキサー 結 ハンドリング 深部 マンガン 亜鉛 銅 トンネル 鉱山 理解 常温 回転 ＩＣＰ ＣＩＲＯＳ ＣＣＤ\n",
      "\n",
      "===== # 3, Topic : 14, p : 11.7238 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : タイル 緩衝層 試験 コンクリート タイル先付けプレキャストコンクリート部材 コンクリート試験体 梁 試験体 ポリマーセメントモルタル 圧縮試験 タイル目地 補強領域Ａ 実施形態 タイル先付け工法 ひび割れ 柱 躯体 ひび割れ追従性 試験体Ａ 図 コンクリート下地 一般領域Ｂ 領域 配合量 コンクリート部材 追従性 前記タイル 試験体Ｃ 粉体 ポリマー Ａ 骨材 前記緩衝層 目地材 試験体Ｂ 下地コンクリート タイルシート 施工性 層厚 柱梁接合部 塗着 目地 発明 硬化体 接合面 比較例 前記柱 表面 前記梁 砂骨ローラ 前記ポリマーセメントモルタル ～ 条件 ポリマー配合量 工程 層単独 地震 層 特許文献 配合 剥離 型枠 ひび割れ幅 養生工程 コンクリート打設面 確認試験 先付けプレキャストコンクリート部材 粉体量 範囲 程度 変形追従性 前記実施形態 タイル先付工法 性 小型コンクリート試験体 割れ 前記 ａ プレキャストコンクリート部材 補強領域 凹凸 区間 ＭＮ試験機 プレキャスト部材 最大粒径 コンクリート構造物 強度 タイル面 区間長Ｌ ｂ 表 準備工程 配置工程 細骨材率 圧縮強度 試験体Ａ～Ｃ 下地ひび割れ追従性 モルタル 梁せいＤ コンクリート面 断面図 ポリマー濃度 裏面 接合性 弾性率 Ｂ タイルモルタル 付着力 作業性 強度コンクリート コンクリートカーテンウォール部材 目的 μｍ パターンローラ等 前記接合面 粉体比 ローラ 梁せい 破断面 実施例 － 裏側 タイル裏面 タイルパック タイル付着性能 形態 ローラ塗 プレキャスト材 ひび割れ誘発目地 Ｃ 斜視図 水セメント比 塗付 課題 側面 形状 付着性 前記型枠 形成方法 揺れ 種類 性能 ↑－↑ タイル先付プレキャスト版用タイルモルタル組成物 変形速度 取付状況 角柱状 接合部周囲 裏足 作業 πゲージ 技術 コンクリート供試体 区間長 型枠側 混和材 説明 仕様 施工 特徴 設工程 グラフ Ｎ ｍｍ 所定 比率 セメント 美観性 流動性 接着性 着 粒径等 一軸圧縮 前記課題 ブチルゴム粘着材 平均粒径 側端面 養生 先行技術文献 角部 ベット面 垂直面 下端面 上端面 製品名ＡＥコート 幅 硬化 軸力等 技術分野 背景技術 技術思想 上下方向中間部 地震力 荷重方法 施工法 施工費 近傍 未満 状況 効果 平面図 側面図 設計基準強度 温度 mm 位置 剥落 箇所 底面 設面 中央 鉄筋 当該ポリマーセメントモルタル 使用目的等 正面視Ｔ字状 張り付け方法 壁等 設定方法 防水用 工期短縮化 硬化収縮 Ｄ 形成 荷重 ゲージ 性能評価 取り付け状況 ポゾラン系反応粒子 省力化 低減化 カーテンウォール仕上げ 温冷繰り返し 株式会社イーテック製 市販既調合 荷重Ｐ 特開平 号公報 緩和能力 ヘアークラック程度 問題点 壁 平均 周囲 ～（ｄ 形状保持 経年変化 評価 本件発明 ｄ 構成要素 構成 保護 向上 設 傾向 外力 対象 ディファレンシャル・ムーブメント 概要 厚み 設可能 曝露 設時 混練水 手段 長期 手間 図面 背面 他 一対 参照 上方 周 上面 下面 検討 性質 鏝塗 観点 好適 複数 鏝 距離 同士 隙間 ｃ 公知 関係 一定 建物 前述 趣旨 変更 一体 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 10.6353 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 隔離室 開口部 扉 配膳用 図 オープンスペース側 防音性能 精神科病院 枠 発明 オープンスペース 前記隔離室 隔離室側 エアタイト材 防音材 側 防錆性能 患者 金属製 面積 前記扉 前記小扉 気密性 隔離室等 特許文献 前記オープンスペース 扉内部 床面 前記オープンスペース側 上部側 ポリカーボネート製 看護師等 前記配膳用 保護室 正面図 施設 課題 構造 強度 戸 錆 前記精神科病院 例 内部 枠性能 説明 大声 食事 隙間 前記枠 気密性能 木製開口枠 － 効果 病院 断面図 Ｔ－ 縦断面図 横断面図 Ａ 遮音性能 先行技術文献 防音効果 Ａ－Ａ線 縦断面図 ～Ｔ－ ステンレス製 鋼製 ゴム製 合成樹脂製 技術分野 背景技術 ロックウール等 ストッパー 材質 形状 幅 出し入れ 医師 窓 平面図 周囲 程度 金属製材料 金属材料 セミエアタイト等 グラスウール 利用可能性 先行例 感染症対応 音響透過損失 実施例 保護施設 特開 号公報 取付け部分 上記課題 ～ 中心周波数 Ｈｚ 食器類 専用 提案 仕様 概要 手段 目的 要旨 図面 位置 Ｂ 形態 キン ヒンジ 開き 下部 騒音 ｄＢ 密度 ｍ 強化 様子 便宜 産業 類似 部屋 符号 カウンター\n",
      "\n",
      "===== # 5, Topic : 14, p : 10.3108 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 地盤改良体 地盤 山留め壁 地下構造物 応力度 静止側圧 力 地盤改良山留め モーメント 側圧 底部地盤改良体 地盤改良 せん断力 鋼材 図 前記地盤改良体 Ｄ ｋＮ 水平せん断 当該地盤改良体 せん断応力度τｈ σｔａｓ σｈｓ 前記実施形態 厚み 転倒モーメント 実施形態 式 応力度σｈｓ 実施例 Ｐｋｏ 許容せん断応力度 応力度σｔａ σｃａ 断面係数 τｈ σｈａ 転倒 許容せん断応力度τａ τａ 水圧 前記地下構造物 せん断応力度 発明 ｍ 当該地下構造物 許容圧縮応力度 σｔａ 厚みＤ せん断力Ｐｗ せん断力Ｐｋｏ 転倒モーメントＭｐａ 応力度σｈａ 改良等 応力度σｔａｓ 断面係数Ｚ 抵抗モーメント 特許文献 反対側 鉛直方向 検討 構築方法 施工費 働側圧 働側圧Ｐａ 単位幅当たり 応力度σｈｓ 応力度σｈａ 回転中心Ｏ ａ ｋＮ 付着力 摩擦力 Ｈ 側圧係数 地下構造物側 比較例 計算 抵抗モーメントＭｗ Ｐａ 単位体積重量 耐力 滑動 間隔 縦断面図 外側つまり地下構造物 周囲 後述 壁面 直下 材 深層混合処理工法 σｔａ Ｐｗ 断面係数Ｚｓ 側圧係数ｋｏ γ ピッチ Ｌ Ｈｗ Ｈ－ 体積 工期 当たり 下端 Ｍｐａ ｋＮｍ 静止側圧Ｐｋｏ 平面図 安全性 安全率 水圧Ｐｗ モーメントＭｐｗ モーメントＭｐｋｏ モーメントＭ 単位幅 － 号公報 請求項 Ｍｗ Ｚ 形態 回転中心 Ｚｓ 説明 効果 沈下 上端 分布 Ｆｓ 水位 重量 Ｐｓ Ｍｐｗ Ｍｐｋｏ Ｍ 単位体積重量γ 水平方向 抵抗 台形状 先行技術文献 技術分野 背景技術 課題 目的 記載 特徴 工程 図面 ＰａＨ 上述 ４つ 仕様 比較 ｋｏ Ｏ 略台形状 Ｈ鋼 三角形状 所定間隔おき 特開 特開平 発電所 加重Ｐｓ ピッチＬ 加重 参照 概要 プロセス 土 手段 手順 ２つ 位置 ）～（ 数値 Ｚｓ 影響 範囲 変形 符号 Ｚs\n",
      "\n",
      "===== # 6, Topic : 14, p : 12.8190 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 杭圧入装置 チャック装置 杭 補助装置 Ｐ用チャック 圧入施工 杭Ｐ クランプ装置 周辺機器 杭圧入施工 杭圧入機 ウォータージェット併用圧入 単独圧入 ステップＳ パイルオーガ併用圧入 既設杭Ｐ 動作条件 構成機器 油圧ホースリール 打ち抜き動作条件 動作 リーダーマスト 工法 施工作業 鋼矢板 判別 補助装置取付部 装置 当該杭圧入装置 Ｐ－単独モード 信号出力 鋼矢板用クランプ装置 出力条件 杭圧入工法 コントローラー モード オーガ装置 Ｐ 装置本体 Ｐ－ＰＡ併用モード Ｐ－ウォータージェット併用モード Ｐ－パイルオーガ併用モード 併用モード 動作制御手段 操作 停止操作 オーガ装置駆動用 既設杭 ケーブル 鋼矢板用 ガ装置駆動用 作業者 Ｐ－ＷＪ併用モード 発明 図 動作制御処理 杭列 単独モード 施工 鋼矢板サイズ ウォータージェットリール 既設 上端部 － 周辺機器等 スライドベース 機器 判別処理 チャック部 駆動 パイルオーガ ウォータージェット併用モード 制御部 各種補助装置 操作性 判別手段 ＰＡ サイズ ｂ ガ装置 施工環境 パイルオーガ併用モード ウォータージェット噴出用ホース ウォータージェット 制限 判断 構成 状態 所定 上端側 処理 各種動作モード 地中 タイプ 地盤条件 特許文献 各種周辺機器 概略図 サドル メイン油圧シリンダ 設定 他 各種工法 自動 油圧シリンダ 入力操作 昇降範囲 ＷＪ 各種制御プログラム 説明 力 課題 技術 負担 手段 先 周知 ＣＰＵ モード切替 起動操作 形態 回転駆動 各種 前記判別手段 演算処理等 前端部 先行技術文献 技術分野 背景技術 固定手段 図面 フローチャート 複数 左右 ワークエリア等 データ等 ＲＯＭ等 否 表 細部構造等 背面側 前面側 地盤 列 回路パターン 電気回路 設定入力 協働 特公昭 号公報 上記 請求項 実施形態 前面 ストローク分 挟持 上記各部 各部 接続状態 巻き取り 図示 図示省略 実施 下部 位置 参照 概要 目的 記載 特徴 効果 下面 前方 前側 先頭 向き 方向 形状 解除 動 ＲＡＭ 組み合わせ 上下動 識別 例 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 11.6060 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 固定型構造部 柱脚部 構造 柱 柱頭部 柱頭柱脚 構造部 変形 図 柱Ｃ 最大層間変形角 固定モデル 固定 各層最大層間変形角 柱脚 解析 実施形態 最大値 建築構造 建物構造 柱脚部ＣＬ 変形角 固定端 固定型構造 ヒンジ部 テーパ部 降伏機構分離型構造 構造解析モデル 解析対象 断面 柱端 柱梁 時刻歴 層間変形角 方向 最大変形角 ヒンジ 断面図 変形モード 梁端部 主筋Ｒ 柱頭柱脚固定モデル 各層 層間変形 柱頭部ＣＵ 地震 柱脚固定 柱頭部ＣＵ 地震応答解析 建物 下層柱脚 変形分布 せん断補強筋Ｒ 地盤増幅特性 柱端部 応答変形 コーナ部 降伏機構 弾性変形角 建築構造物 建物脚部 降伏モーメント 回転角 固定型支持部 柱断面 応答変形制御設計 c線方向 d線方向 柱頭 前記柱脚部 柱脚部ＣＵ 形態 変形モデル せん断力－層間変形関係 ａ ｂ 構造特性 変形概念図 コンクリート 梁 柱頭固定 連層壁 モーメント 上部構造 上層柱頭 固定支持部 架構 モデル 発明 ELCENTRO波 TAFT波 HACHINOHE波 最下層柱脚 柱頭部ＧＰ 中間層 降伏せん断力 c d 作用モーメント 上部スラブ 回転変形 最大応答値 テーパ部Ｃ 前記柱頭部 主筋 下部スラブ 地震動 グラフ 弾性変形 上記建物構造 応答改善 柱ＣＬ スラブ 関係 方法 ｃ 地盤 左側梁端部 右側梁端部 建築物 構造設計法 層 応答値 ｄ 表 構造計画研究所建築構造解析プログラムRESP 技術 断面積 梁断面 告示波 弾性変形角θｅ 加速度応答スペクトル 中層部 特許文献 ΣcMu θｐ 目標クライテリア 他 強度 梁ヒンジ 層間変位角Ｒｉ 復元力モデル 例 ΣbMu 変形一様化 テーパ状 前記 動的解析 動的弾塑性応答解析 部分 エネルギー 四隅 節点 設計建物 Masing型バイリニアモデル 復元力特性 固有値解析 解析例 降伏ヒンジ 曲点位置 建設省告示 傾斜角 回転角θｐ 回転角θｅ 降伏 地震増幅特性 応答性状 合計 号 魚骨形 クライテリア 設計 仮定 上層 町田 静的弾塑性解析 日本建築学会構造系論文集 地震応答予測 必要復元力 ダンパー等 複数本 必要復元力特性 下層 コンクリート内部 断面性能 階 θｅ 断面変更 断面寸法 断面縮小 断面形状 説明 釣り合い 】　( 上述 早期 鉄筋 富津 損傷 m 質点系モデル 付着除去区間Ｒ 骨組モデル RCモデル 武田モデル 多層建物 式 斜め水平力 区間ＧＰ 曲点 付着除去区間ＧＰ 方向IVA 鉛直方向 非線形特性 ΣcMu≧ 鉄筋コンクリート造 剛性 文献 塑性化 観測波 降伏剛性低下率 上記文献 先行技術文献 平面図 位相特性 ～図 設計用地震動 弾性 ELCENTRO TAFT HACHINOHE 技術分野 背景技術 主筋Ｇ 合計ΣbMu ひび割れモーメント 地震波形 研究 下記 課題 分布 範囲 算出 ダンパー 通常 メカニズム 図面 別 上端 下端 図示 本数 ハッチング 影響 SHAKE 東京 代表 種類 耐力 同一 建て鉄筋コンクリート造無限均等ラーメン 複数階 支配面積 初期剛性 円錐台状 上記目的 上記平成 空隙ＧＰ 非線形性能 性能 スペクトル 付着 単位面積当たり 技術的思想 ダンパー量 免震装置 作成）(e 平石久廣 上下階 特開 号公報 次 作成 固有周期 骨格曲線 質点 立体フレーム 骨組 目的 近傍手前 次式 平面 当たり 解放工学的基盤 平成 金子雅子 平塚高広 Ｒｉ 位相 傾向 三つ 】( 概要 手順 a b 行い 等価 変換 )～( 手間 ダンパ 支承部 手段 特徴 効果 所定 ピッチ 入力 圧壊 周 増大 状態 左右 比 下 株 F T m× 重量 kN ｈ Ｂ New 増分 かなり 差異 領域 千葉 機能 周囲 各種 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 10.2942 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 事前設置ドレーン 鉛直方向ドレーン材 埋め立て土 埋め立て地盤 埋め立て予定地 埋め立て レーン材 地盤 浮体 実施形態 埋め立て方法 方向 設置ドレーン 図 補強材 補強 水平方向ドレーン材 前記 鉛直方向 水中埋め立て 前記浮体 スライド機構 袋体 立体格子状 格子枠状 自重圧密 表層補強 充填材 上下方向 レーン 埋め立て地 錘 前記埋め立て土 前記埋め立て予定地 前記工程 排水 地盤改良 シート 事前設置 設置段数 格子 軟弱土 接続材 格子状枠 ｂ 発明 ロール状部分 施工性 水中 土運船 所定 間隔 圧密 棒材 埋め立て施工 形態 チェーン 立体格子 下端 工程 例 状態 設置位置 離隔管理 上端 施工条件 支え材 ａ 水面 安定性 改良方法 効果 埋め立て済み ｃ 袋体内 前記埋め立て土中 特許文献 粘性土 埋立地 水 参照 所定間隔 上方 軟弱地盤 実施 造成 浚渫土 荷等 波等 モルタル等 透水性 前記袋体 立体形状 上面 剛性 気 内部 覆土 連続性 土圧等 軟弱粘土 線状体 バーチカルドレーン 位置 地盤造成 排砂管 板状 説明 圧密促 目的 底 方法 施工 張力 図示 最後 ００７ 前記袋体内 技術的範囲 改良 プラスチック材 排水距離 浚渫土砂 埋立 土砂 同士 土木シート 表層補強効果 表層改良 効率 表層 負担 注入 構成 程度 移動 制約 下方 次 堤体等 鉄筋棒等 沈下 図等 前記錘 把持機構 技術分野 背景技術 鉄筋棒 軟弱粘性土中 チェーン等 竹等 ポンプ等 波浪等 破断等 鉄板等 先行技術文献 特許 隅部 排水層 複数 課題 粘性土地盤 重量 記事 時点 軽量 初期 形 布 図面 部材 交点 水底 全域 運搬 配置 上昇 下 形状 符号 水深 手順 最初 上下 平面隅部 技術的思想 特許請求 距離 範囲 透水係数 層厚 筒形状 添付図 バーチカルドレーン打設 上端同士 余長 余裕長 把持力 ｂ同士 変更例 修正例 特開平 号公報 足場確保 問題点 木材類 沈下量 固定具 含水状態 脱水設備 設備 号 建設 サンドドレーン 地表 短縮 工期 下部 上部 － 概要 重機 進入 荷重 機械 注意 手段 特徴 過度 投入 表面 完了 縦横 比重 既知 付近 値 ～ 周囲 通り 単位 並置 水量 別 箇所 要因 一定 恐れ 落ち 簡易 喫水線 ウィンチ 板材 記載 自立 関係 ソケット ｄ 形成 上記 浮力 業者 範疇 各種\n",
      "\n",
      "===== # 9, Topic : 14, p : 10.1551 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 金属粉末焼結層 摺動板 バック材 焼結密度領域 結密度 結 結用合金粉末 金属板 潤滑性樹脂 基本レール 潤滑性 摺動板固定用溝 基本レール固定用溝 レールプレス 分岐器用床板 耐衝撃性 耐荷重性 耐摩耗性 補強部材 混合粉末 レールプレス固定用溝 粉末 縞鋼板 上面 固体潤滑剤 焼結密度 鋼板 図 板厚 孔 結条件 パンチングメタル 基板 板 実施 高焼結密度領域 摺動性能 結用合金 メッシュ構造 潤滑油 側 金属部分 等 潤滑性重視 Ａ 部分 板厚ｔ 発明 既製品 性能 形態 鋼板等 上記実施 面 配列パターン 完成体 摺動支持 含浸率 エキスパンドメタル等 耐摩耗性重視 凹凸 自己潤滑 前記バック材 凸部 底部 上面図 中間体 トングレール 側面 摺動用途 クラック等 特許文献 接合強度 金属 ボルト支持穴 レベラー加工 開孔率 圧延条件等 耐摩耗性等 圧延 前記基本レール 調整 所望 形状 可能性 複数 Ａ－Ａ断面図 反対側 固体潤滑剤等 割合 表面 サイズ 長手方向 金属メッキ 空隙部 種類 縞模様 溝 ボルト穴 接合 一般構造用圧延鋼板ＪＩＳ 正面図 黒鉛 床板 つぎ ＰＴＦＥ 上述 凹部 Ｂ 他方 底面 青銅系 ～ 重量 ｔ 方法 前記補強部材 設置ポイント 複数種類 配置位置 接触面積 青銅合金系 熱間圧延軟鋼板ＪＩＳ 領域ごと 方法等 両端部 説明 高圧 低圧 － メッシュ 下面 状態 銅メッキ 黄銅メッキ等 課題 錫 幅方向両端部 硫化モリブデン等 ビス止め等 製造条件 専用パンチプレス機 配列 階段等 外力等 黒鉛等 積層構造 滑り軸受等 方向Ｄ 上記構成 分岐器 上面側 銅 塑性変形等 凹凸部 中央部 縁部 長手方向Ｌ 強度 エキスパンドメタル 縞鋼鈑 面側 接合方法 ニッケルメッキ 錫メッキ 先行技術文献 上記事情 上記課題 滑り止め用途 技術分野 背景技術 特注品 硫化モリブデン ＰＦＡ ＰＡＩ ＰＩ 耐候性 凹部上 図面 置 鉤部 腹部 Ｗ 露出部分 Ｇ 例 素材 空間部分 前記基板 残部銅 パンチメタル 銅めっき ごと 配置 床面 構成材料 保守点検作業 すみ肉溶接 製造コスト ひし形状 円形状 部位ごと 特開平 号公報 既製品 くさび作用 転倒防止 材料 ニッケル 列車通過 部位 ろう付け めっき 頻度 ポリテトラフルオロエチレン パーフルオロアルコキシアルカン ポリアミドイミド ポリイミド 概要 記載 通常 メーカ 管理 通路 目的 手段 当該 効果 厚み 点線 鉄道 線路 斜め 間隔 ＳＰＨＣ 後述 ＭｏＳ リン ＰＥ ポリエチレン ＰＰ ポリプロピレン はつぎ 所定 使用 共通 ＳＳ 全面 代わり 網目 継ぎ目 一体 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 11.4787 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 型枠部材 側壁部補助型枠部材 補助型枠部材 天端部補助型枠部材 工用型枠装置 型枠 移動式型枠 断面半径 外周面 天端部 下端部 トンネル 工部材 工 両端部 側壁部 両端部支承部 内壁面 図 半径 防水シート 最小断面半径部分 部材 妻側両端部 結束部材 型枠装置 発泡プラスチック材 断面形状 下部支承部 型枠形状 側壁部型枠部材 実施形態 金属シート 延長方向 半径方向 部分 ｂ 工用 薄板状 天端 横断面図 充填材 コンクリート 工コンクリート 下側 鋼製 装置 発明 円周方向 枠組 鋼製ワイヤー 斜視図 周面 トンネル内空断面形状 最小断面半径 縦断面図 最下部 Ｈ型鋼 トンネル本体部 特許文献 技術 挿通孔 空断面 ａ コンクリート等 複数 セントル 工作業 充填圧力 アーチ形状 横断面方向 コスト 横断面形状 エアージャッキ等 説明 油圧ジャッキ 同一面 説明図 １つ 手間 構成 強度 平滑化施工法 充填圧 工コンクリート等 トンネル延長方向 方向 紐状部材 トンネル形状 横断面径 当該防水シート 性 パイプ 当該枠組 アーチ状 可能性 形状 内壁 号公報 形態 地下水 充填圧力等 等 当該トンネル Ｈ鋼等 トンネル内壁面 製作 運搬 組み立て 解体 台車 展張 鋼鉄製 本体部分 ポリプロピレン製 同一 注入装置 駆動装置 進退装置 山岳トンネル 施工箇所 板状 トンネル地山 施工方法 ～図 特許 空隙部分 先行技術文献 剥離等 圧力測定手段 ＮＡＴＭ工法等 塩化ビニル等 技術分野 背景技術 既存 種々 概要 課題 全長 目的 成型 軽量 隙間 品質 外観 図面 レール 状態 厚み 表面 拡幅アーチ構造体 箇所 耐久性 複数箇所 特開 手段 中心点 スライドフォーム 圧送装置 駆動 拡径部分 応用例 方法 空隙 専用 参照 工程 変形 破壊 範囲 － 要因 事情 特徴 効果 単位 一対 入管 板材 モータ 機構 外側 環境 補強 一連 上方 程度 傷 他 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.3938e-37, 5.9401e-42, 1.2422e-26, 0.0000e+00,\n",
      "         0.0000e+00, 5.5057e-38, 6.4187e-21, 1.8108e-38, 0.0000e+00, 0.0000e+00,\n",
      "         5.0135e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 9.79494\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1155.7562\n",
      "Test epoch : 1 Average loss: 1085.2525\n",
      "PP(train) = 4883.764, PP(valid) = 4840.363\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.2246e-29, 7.9975e-38, 3.1753e-26, 0.0000e+00,\n",
      "         0.0000e+00, 7.3928e-37, 1.2471e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.8966e-29, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 8.83023\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1155.2938\n",
      "Test epoch : 2 Average loss: 1084.7866\n",
      "PP(train) = 4862.845, PP(valid) = 4822.692\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.7829e-32, 1.4928e-40, 1.8089e-34, 0.0000e+00,\n",
      "         0.0000e+00, 2.8466e-27, 9.5656e-37, 1.3238e-41, 0.0000e+00, 0.0000e+00,\n",
      "         9.6942e-28, 2.3822e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7.96055\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1154.7136\n",
      "Test epoch : 3 Average loss: 1084.3150\n",
      "PP(train) = 4840.871, PP(valid) = 4804.867\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2762e-35, 1.4151e-36, 7.7303e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.2108e-25, 1.1283e-27, 5.4138e-40, 0.0000e+00, 0.0000e+00,\n",
      "         6.4213e-31, 4.2040e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 7.17653\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1154.1032\n",
      "Test epoch : 4 Average loss: 1083.8441\n",
      "PP(train) = 4818.537, PP(valid) = 4787.166\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3687e-42, 2.2708e-19, 9.8841e-30, 1.3074e-11, 0.0000e+00,\n",
      "         0.0000e+00, 1.9884e-09, 2.3944e-22, 5.9389e-25, 4.2039e-45, 5.1848e-44,\n",
      "         5.7788e-09, 2.3994e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 6.46973\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1153.5677\n",
      "Test epoch : 5 Average loss: 1083.3713\n",
      "PP(train) = 4796.278, PP(valid) = 4769.776\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4572e-37, 5.3429e-21, 8.3991e-30, 8.4434e-05, 0.0000e+00,\n",
      "         0.0000e+00, 5.7088e-09, 1.7831e-22, 3.9969e-23, 1.3725e-37, 6.9287e-32,\n",
      "         2.4887e-16, 1.4340e-30, 9.9992e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5.83253\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1152.8920\n",
      "Test epoch : 6 Average loss: 1082.9090\n",
      "PP(train) = 4773.362, PP(valid) = 4751.974\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.1001e-40, 4.0200e-28, 6.5167e-34, 5.8083e-21, 0.0000e+00,\n",
      "         0.0000e+00, 4.2799e-32, 1.5432e-22, 4.9373e-41, 0.0000e+00, 0.0000e+00,\n",
      "         6.4401e-18, 1.3217e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 5.25810\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1152.2358\n",
      "Test epoch : 7 Average loss: 1082.4391\n",
      "PP(train) = 4751.153, PP(valid) = 4734.802\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.6167e-36, 0.0000e+00, 1.6045e-25, 0.0000e+00,\n",
      "         0.0000e+00, 4.9092e-24, 8.5222e-30, 6.8699e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.0877e-27, 8.5050e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4.74024\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1151.5405\n",
      "Test epoch : 8 Average loss: 1081.9646\n",
      "PP(train) = 4729.467, PP(valid) = 4718.002\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5600e-40, 2.4474e-17, 7.5645e-33, 5.0040e-09, 0.0000e+00,\n",
      "         0.0000e+00, 3.8576e-20, 2.0066e-16, 1.2221e-32, 0.0000e+00, 4.4554e-38,\n",
      "         2.8810e-10, 1.0238e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 4.27338\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1150.8149\n",
      "Test epoch : 9 Average loss: 1081.4953\n",
      "PP(train) = 4707.643, PP(valid) = 4701.137\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2240e-37, 2.3989e-26, 9.0787e-38, 1.2161e-18, 0.0000e+00,\n",
      "         0.0000e+00, 1.6262e-07, 2.3683e-17, 1.8345e-34, 1.5210e-41, 3.0548e-43,\n",
      "         8.5035e-16, 1.5310e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3.85250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1150.2756\n",
      "Test epoch : 10 Average loss: 1081.0310\n",
      "PP(train) = 4685.650, PP(valid) = 4684.015\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5414e-42, 2.9996e-29, 4.1899e-43, 2.9019e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.6755e-25, 4.0162e-34, 1.2014e-37, 0.0000e+00, 0.0000e+00,\n",
      "         1.1326e-27, 2.8026e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3.47308\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1149.6345\n",
      "Test epoch : 11 Average loss: 1080.5712\n",
      "PP(train) = 4663.819, PP(valid) = 4666.960\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4682e-28, 1.0706e-37, 7.9953e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.0415e-28, 9.3356e-30, 1.9864e-35, 0.0000e+00, 0.0000e+00,\n",
      "         2.2210e-30, 3.9236e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 3.13108\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1149.0393\n",
      "Test epoch : 12 Average loss: 1080.1128\n",
      "PP(train) = 4642.133, PP(valid) = 4650.024\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.6066e-32, 2.2615e-17, 1.0628e-37, 2.8751e-21, 0.0000e+00,\n",
      "         0.0000e+00, 3.3146e-17, 1.4112e-17, 4.7544e-34, 0.0000e+00, 2.0145e-39,\n",
      "         1.4615e-17, 2.6181e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.82278\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1148.4788\n",
      "Test epoch : 13 Average loss: 1079.6555\n",
      "PP(train) = 4620.689, PP(valid) = 4633.239\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0636e-42, 1.6007e-18, 1.8292e-38, 1.7696e-23, 0.0000e+00,\n",
      "         0.0000e+00, 1.0170e-12, 2.2366e-26, 9.9728e-35, 0.0000e+00, 1.9732e-40,\n",
      "         1.4953e-24, 6.7889e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.54486\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1147.7426\n",
      "Test epoch : 14 Average loss: 1079.1978\n",
      "PP(train) = 4599.722, PP(valid) = 4616.778\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.6129e-37, 2.7865e-33, 5.9238e-28, 0.0000e+00,\n",
      "         0.0000e+00, 9.0843e-15, 1.3849e-24, 2.5004e-28, 0.0000e+00, 0.0000e+00,\n",
      "         1.6284e-14, 1.1797e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.29432\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1147.1279\n",
      "Test epoch : 15 Average loss: 1078.7419\n",
      "PP(train) = 4579.077, PP(valid) = 4600.639\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0949e-26, 1.7207e-36, 1.0812e-21, 0.0000e+00,\n",
      "         0.0000e+00, 2.3094e-28, 1.9000e-24, 1.6408e-39, 0.0000e+00, 2.3730e-41,\n",
      "         8.3211e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 2.06845\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1146.5418\n",
      "Test epoch : 16 Average loss: 1078.2945\n",
      "PP(train) = 4558.024, PP(valid) = 4584.044\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 4.1884e-37, 2.0417e-32, 7.4306e-31, 0.0000e+00,\n",
      "         0.0000e+00, 3.2518e-17, 1.7565e-21, 1.2900e-36, 0.0000e+00, 0.0000e+00,\n",
      "         9.1029e-26, 1.4707e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 1.000, l1 strength = 1.86482\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1145.8664\n",
      "Test epoch : 17 Average loss: 1077.8433\n",
      "PP(train) = 4537.579, PP(valid) = 4567.972\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.0334e-16, 6.7188e-22, 6.2267e-16, 0.0000e+00,\n",
      "         0.0000e+00, 1.0864e-12, 1.4808e-14, 9.9518e-33, 0.0000e+00, 7.3875e-27,\n",
      "         1.3265e-14, 5.4810e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.68124\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1145.2484\n",
      "Test epoch : 18 Average loss: 1077.3897\n",
      "PP(train) = 4517.363, PP(valid) = 4552.060\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2855e-42, 2.6831e-27, 2.2507e-38, 0.0000e+00,\n",
      "         0.0000e+00, 1.6973e-26, 1.6679e-22, 7.3503e-39, 0.0000e+00, 0.0000e+00,\n",
      "         1.7282e-25, 2.6625e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.51575\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1144.6268\n",
      "Test epoch : 19 Average loss: 1076.9374\n",
      "PP(train) = 4497.658, PP(valid) = 4536.588\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.8736e-37, 3.8665e-21, 1.3817e-42, 5.8213e-22, 0.0000e+00,\n",
      "         0.0000e+00, 8.1622e-19, 4.8266e-27, 8.1709e-36, 0.0000e+00, 0.0000e+00,\n",
      "         1.8065e-18, 3.2193e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.36655\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1144.0285\n",
      "Test epoch : 20 Average loss: 1076.4900\n",
      "PP(train) = 4478.082, PP(valid) = 4521.165\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.9745e-24, 3.3619e-35, 1.8859e-10, 0.0000e+00,\n",
      "         0.0000e+00, 1.5993e-11, 3.8960e-28, 3.9330e-32, 5.6052e-45, 4.4097e-41,\n",
      "         4.6910e-17, 5.2838e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.23205\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1143.4240\n",
      "Test epoch : 21 Average loss: 1076.0445\n",
      "PP(train) = 4458.569, PP(valid) = 4505.750\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1911e-43, 4.3535e-31, 9.0804e-43, 3.0690e-19, 0.0000e+00,\n",
      "         4.4421e-43, 7.7938e-07, 1.3228e-25, 3.1222e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.0697e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.11080\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1142.9165\n",
      "Test epoch : 22 Average loss: 1075.6062\n",
      "PP(train) = 4438.583, PP(valid) = 4489.867\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.0638e-44, 9.1476e-34, 4.4634e-35, 4.8900e-12, 0.0000e+00,\n",
      "         0.0000e+00, 1.2163e-24, 1.8727e-26, 2.3822e-44, 3.3103e-40, 1.4446e-38,\n",
      "         3.6765e-23, 4.2844e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 1.00149\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1142.3791\n",
      "Test epoch : 23 Average loss: 1075.1681\n",
      "PP(train) = 4418.863, PP(valid) = 4474.167\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.1213e-36, 1.7256e-27, 2.6016e-26, 1.0179e-19, 0.0000e+00,\n",
      "         1.6675e-43, 4.3137e-21, 3.5196e-22, 4.7604e-33, 0.0000e+00, 0.0000e+00,\n",
      "         2.9189e-20, 1.6135e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.90294\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1141.7398\n",
      "Test epoch : 24 Average loss: 1074.7330\n",
      "PP(train) = 4399.533, PP(valid) = 4458.743\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6287e-36, 4.5566e-28, 1.4154e-39, 8.5278e-27, 0.0000e+00,\n",
      "         0.0000e+00, 3.7381e-12, 1.0187e-16, 5.3353e-33, 3.4920e-42, 3.5803e-39,\n",
      "         1.4065e-26, 1.6989e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.81410\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1141.1952\n",
      "Test epoch : 25 Average loss: 1074.3019\n",
      "PP(train) = 4380.358, PP(valid) = 4443.436\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.3822e-44, 5.2240e-35, 2.5526e-37, 3.8765e-28, 0.0000e+00,\n",
      "         0.0000e+00, 1.9306e-28, 3.5047e-26, 1.5094e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.2637e-25, 6.3988e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.73400\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1140.5179\n",
      "Test epoch : 26 Average loss: 1073.8691\n",
      "PP(train) = 4361.326, PP(valid) = 4428.156\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1210e-44, 4.1444e-19, 6.7341e-39, 3.7882e-25, 0.0000e+00,\n",
      "         0.0000e+00, 3.1362e-20, 1.3077e-31, 2.2427e-31, 0.0000e+00, 0.0000e+00,\n",
      "         2.6433e-27, 3.7196e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.66179\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1139.8973\n",
      "Test epoch : 27 Average loss: 1073.4330\n",
      "PP(train) = 4342.926, PP(valid) = 4413.493\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.1657e-44, 4.2104e-37, 8.6347e-37, 6.4882e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.2407e-26, 5.1589e-20, 1.6447e-34, 3.0418e-41, 5.8611e-41,\n",
      "         1.2333e-36, 7.6055e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.59670\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1139.3321\n",
      "Test epoch : 28 Average loss: 1072.9973\n",
      "PP(train) = 4324.815, PP(valid) = 4399.092\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3142e-38, 2.0939e-19, 3.3155e-34, 1.9555e-13, 0.0000e+00,\n",
      "         0.0000e+00, 1.6636e-07, 7.5164e-15, 5.4568e-37, 4.0638e-44, 2.3140e-40,\n",
      "         5.3995e-22, 8.4035e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.53800\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1138.7525\n",
      "Test epoch : 29 Average loss: 1072.5664\n",
      "PP(train) = 4306.730, PP(valid) = 4384.659\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5749e-41, 8.1170e-24, 1.4162e-33, 7.7196e-14, 0.0000e+00,\n",
      "         0.0000e+00, 3.1182e-22, 5.5800e-20, 3.1194e-31, 0.0000e+00, 2.1019e-44,\n",
      "         1.5306e-21, 5.5448e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.48509\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1138.2408\n",
      "Test epoch : 30 Average loss: 1072.1408\n",
      "PP(train) = 4288.328, PP(valid) = 4369.936\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.4466e-40, 1.0060e-25, 1.2137e-24, 1.7945e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.0752e-13, 2.2320e-28, 1.3539e-33, 0.0000e+00, 8.4078e-45,\n",
      "         3.4275e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.43738\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1137.6000\n",
      "Test epoch : 31 Average loss: 1071.7176\n",
      "PP(train) = 4269.968, PP(valid) = 4355.166\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.8506e-41, 6.4189e-21, 0.0000e+00, 1.2482e-34, 0.0000e+00,\n",
      "         0.0000e+00, 2.1796e-25, 7.1454e-22, 0.0000e+00, 0.0000e+00, 5.4651e-44,\n",
      "         2.1451e-33, 4.6849e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.39437\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1136.9479\n",
      "Test epoch : 32 Average loss: 1071.2914\n",
      "PP(train) = 4252.088, PP(valid) = 4340.814\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.4959e-32, 1.7062e-39, 2.6671e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.3560e-25, 1.6563e-29, 8.3202e-35, 1.4013e-45, 0.0000e+00,\n",
      "         7.9091e-33, 5.2375e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.35559\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1136.4076\n",
      "Test epoch : 33 Average loss: 1070.8718\n",
      "PP(train) = 4234.243, PP(valid) = 4326.407\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, 5.7796e-37, 0.0000e+00,\n",
      "         0.0000e+00, 1.2986e-30, 8.8233e-39, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.4162e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.32063\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1135.8099\n",
      "Test epoch : 34 Average loss: 1070.4494\n",
      "PP(train) = 4216.677, PP(valid) = 4312.274\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.2809e-40, 7.7201e-18, 1.3902e-35, 1.1477e-18, 0.0000e+00,\n",
      "         8.4078e-45, 2.3199e-22, 1.2862e-20, 2.1035e-23, 0.0000e+00, 4.5278e-39,\n",
      "         5.7315e-13, 5.8000e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.28911\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1135.3746\n",
      "Test epoch : 35 Average loss: 1070.0292\n",
      "PP(train) = 4199.457, PP(valid) = 4298.425\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4412e-38, 2.6024e-20, 6.1237e-43, 2.8767e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.5931e-24, 3.2672e-11, 9.8722e-35, 0.0000e+00, 0.0000e+00,\n",
      "         6.0339e-15, 3.9626e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.26068\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1134.7239\n",
      "Test epoch : 36 Average loss: 1069.6165\n",
      "PP(train) = 4181.892, PP(valid) = 4284.249\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.0106e-24, 0.0000e+00, 9.4149e-31, 0.0000e+00,\n",
      "         0.0000e+00, 4.8714e-19, 8.6341e-25, 9.1986e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.2630e-24, 3.8491e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.23506\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1134.1271\n",
      "Test epoch : 37 Average loss: 1069.1984\n",
      "PP(train) = 4164.802, PP(valid) = 4270.424\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.4007e-38, 2.1094e-29, 2.8640e-35, 0.0000e+00,\n",
      "         0.0000e+00, 2.8981e-25, 1.5487e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.7062e-24, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.21196\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1133.4927\n",
      "Test epoch : 38 Average loss: 1068.7818\n",
      "PP(train) = 4148.074, PP(valid) = 4257.002\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3870e-39, 4.9404e-22, 3.1707e-32, 7.0983e-07, 0.0000e+00,\n",
      "         1.4013e-45, 8.0911e-02, 2.5951e-17, 4.3776e-27, 8.4078e-45, 1.2744e-28,\n",
      "         1.5572e-18, 2.2670e-24, 9.1909e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0825, 0.0479, 0.0997, 0.0501, 0.0619, 0.0692, 0.1055, 0.0876, 0.0592,\n",
      "         0.0512, 0.0658, 0.0549, 0.0500, 0.0730, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.19113\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1132.9871\n",
      "Test epoch : 39 Average loss: 1068.3696\n",
      "PP(train) = 4131.225, PP(valid) = 4243.326\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8026e-45, 2.3452e-20, 1.3292e-38, 9.1408e-23, 0.0000e+00,\n",
      "         1.2612e-44, 3.8776e-20, 6.8496e-22, 1.1180e-14, 1.4013e-45, 3.8691e-38,\n",
      "         8.4072e-11, 1.2844e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.17235\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1132.4523\n",
      "Test epoch : 40 Average loss: 1067.9608\n",
      "PP(train) = 4114.644, PP(valid) = 4229.964\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8799e-35, 8.8944e-37, 1.0824e-33, 4.1853e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.1535e-32, 3.3933e-28, 9.5288e-44, 0.0000e+00, 4.4842e-44,\n",
      "         4.0896e-27, 1.6202e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.15542\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1132.0419\n",
      "Test epoch : 41 Average loss: 1067.5514\n",
      "PP(train) = 4098.141, PP(valid) = 4216.621\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1432e-35, 4.6181e-22, 2.6138e-39, 7.4536e-17, 0.0000e+00,\n",
      "         0.0000e+00, 7.8472e-10, 4.8909e-19, 6.9199e-30, 2.1876e-41, 6.7157e-37,\n",
      "         1.8482e-12, 1.0749e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.14015\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1131.3949\n",
      "Test epoch : 42 Average loss: 1067.1460\n",
      "PP(train) = 4081.374, PP(valid) = 4203.007\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.1964e-40, 1.7849e-26, 2.9206e-28, 5.4482e-16, 0.0000e+00,\n",
      "         0.0000e+00, 4.7534e-18, 4.2771e-14, 1.9324e-41, 0.0000e+00, 1.5414e-44,\n",
      "         3.2806e-25, 5.7137e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.12638\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1130.9110\n",
      "Test epoch : 43 Average loss: 1066.7432\n",
      "PP(train) = 4064.648, PP(valid) = 4189.325\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.1850e-31, 3.3217e-38, 4.1201e-23, 0.0000e+00,\n",
      "         0.0000e+00, 2.4287e-26, 1.7753e-20, 4.8169e-30, 0.0000e+00, 2.5574e-42,\n",
      "         1.0262e-26, 9.6764e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.11397\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1130.3256\n",
      "Test epoch : 44 Average loss: 1066.3423\n",
      "PP(train) = 4048.106, PP(valid) = 4175.743\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0510e-43, 1.1900e-15, 1.1486e-31, 4.3268e-25, 0.0000e+00,\n",
      "         0.0000e+00, 6.1111e-08, 2.6081e-12, 1.1650e-20, 5.1460e-39, 7.4053e-41,\n",
      "         1.4694e-17, 3.8775e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.10278\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1129.7937\n",
      "Test epoch : 45 Average loss: 1065.9394\n",
      "PP(train) = 4032.125, PP(valid) = 4162.721\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.8217e-38, 2.0941e-22, 1.3694e-28, 1.1661e-14, 1.6325e-42,\n",
      "         0.0000e+00, 7.6664e-17, 5.0394e-15, 1.9779e-33, 7.4410e-40, 2.5504e-43,\n",
      "         2.2131e-14, 3.3342e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.09269\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1129.2772\n",
      "Test epoch : 46 Average loss: 1065.5378\n",
      "PP(train) = 4016.654, PP(valid) = 4150.164\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.3788e-26, 1.5497e-38, 1.4310e-26, 0.0000e+00,\n",
      "         0.0000e+00, 3.7574e-25, 1.7530e-20, 3.2039e-31, 0.0000e+00, 0.0000e+00,\n",
      "         5.4156e-27, 2.2101e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.08359\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1128.6163\n",
      "Test epoch : 47 Average loss: 1065.1334\n",
      "PP(train) = 4001.384, PP(valid) = 4137.774\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.6634e-31, 6.9904e-17, 7.7928e-34, 1.4461e-11, 4.4182e-39,\n",
      "         0.0000e+00, 1.0787e-10, 4.6142e-13, 4.1005e-32, 4.7507e-38, 1.2647e-25,\n",
      "         1.2835e-11, 6.9543e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.07538\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1128.1322\n",
      "Test epoch : 48 Average loss: 1064.7337\n",
      "PP(train) = 3985.730, PP(valid) = 4124.965\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1210e-44, 3.2260e-25, 2.2648e-29, 2.0353e-26, 0.0000e+00,\n",
      "         0.0000e+00, 1.5208e-18, 1.1377e-21, 3.3111e-33, 0.0000e+00, 3.9937e-43,\n",
      "         7.3367e-31, 2.8828e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.06798\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1127.5945\n",
      "Test epoch : 49 Average loss: 1064.3427\n",
      "PP(train) = 3969.733, PP(valid) = 4111.851\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6949e-40, 5.2188e-31, 2.0354e-38, 2.4003e-33, 0.0000e+00,\n",
      "         0.0000e+00, 1.2656e-23, 1.3927e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.7070e-34, 1.8121e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.06130\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1127.0828\n",
      "Test epoch : 50 Average loss: 1063.9520\n",
      "PP(train) = 3954.019, PP(valid) = 4098.954\n",
      "Writing to ./topicwords/6-topwords_e50.txt\n",
      "Topic 0: 植物 軸線方向 桁 イ 定着構造 継手板 採取 前部 緑化 特徴構成\n",
      "Topic 1: 軸線方向 イ 植物 桁 前部 定着構造 継手板 採取 補強材 定着具\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 形態 手段 説明 課題 発明\n",
      "Topic 3: 特開昭 分離 円周方向 定着板 水位差 自身 シールドトンネル 方法等 製品 前記センサ\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 課題\n",
      "Topic 5: 軸線方向 植物 桁 イ 定着構造 継手板 採取 号公報参照 鉛 定着具\n",
      "Topic 6: 軸線方向 桁 植物 イ 定着構造 継手板 採取 特徴構成 補強材 定着具\n",
      "Topic 7: － Ａ 位置 ２つ 柱 技術分野 形態 手段 説明 課題\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 形態 手段\n",
      "Topic 9: 桁 軸線方向 定着具 植物 イ 定着構造 対角線 継手板 白色 採取\n",
      "Topic 10: 軸線方向 イ 植物 桁 定着構造 特徴構成 継手板 採取 補強材 号公報参照\n",
      "Topic 11: 軸線方向 桁 イ 植物 定着構造 定着具 継手板 採取 大 前部\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 13: 軸線方向 桁 イ 定着構造 植物 継手板 採取 定着具 号公報参照 鉛\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 9.4073 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 搬送対象物 カバー部材本体 ガイド部 搬送装置 搬送システム 搬送面 カバー部材 搬送対象物Ｗ 開口 前記カバー部材本体 搬送ベルト 仕切壁 図 クリーンルーム 前記 気流 室 開き角度 発明 空気清浄度 室圧 実施形態 前記搬送装置 前記カバー部材 下端縁 前記ガイド部 蓋 側 方向 製品 隙間 Ａ 作業効率 請求項 前記開口 当該カバー部材本体 前記実施形態 内部 特許文献 上面 当該搬送装置 当該ガイド部 陽圧 差圧 別 記載 特徴 搬送方向側 断面図 斜視図 前記仕切壁 Ｗ 説明 ベルトコンベヤ 目的 先端 外部 ローラ 製造 形態 作業 筒状 先行技術文献 技術分野 背景技術 加工 ローラーコンベヤ 課題 上述 部屋 当該 部分 下面 均等 効果 図面 右側 矢印 室圧差 白抜き矢印方向 四角筒状 矩形状 製造工場 特開平 号公報 時計回り 回転軸 改良等 医薬品 食品 使用 塵埃 発生 参照 － 概要 包装 梱包 ２つ 他方 室内 コンベヤ 姿勢 風速 手段 複数 ４つ 辺 破線 側壁 範囲 変形 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 10.2378 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : モジュール型顕熱処理装置 冷却コイル 空調機 送風機 サーバー室 モジュール 空調システム 顕熱負荷 負荷 ユニット 顕熱 図 実施形態 冷却負荷 顕熱処理装置 熱負荷処理用 サーバーラック 冷水コイル 処理対象空気 請求項 床チャンバ 冷却能力 チューブピッチ データセンター等 設置 顕熱負荷処理用 負荷変動 コールドエリア 構成単位 可変風量 特許文献 発明 上記 単位風量 単位冷却能力 顕熱処理用 コンピューター関連施設 処理空気 負荷偏在 冷却管 空調負荷 型空調機 前記モジュール型顕熱処理装置 データセンター 空調設備 形態 圧力損失 コイル面積 壁チャンバ 該モジュール型顕熱処理装置 フィンピッチ 膨コイル インバーター内蔵モータ 冷却 熱 顕熱負荷処理能力 冷房専用 設置パターン 所望 コイル 小形 ラック型空調機 潜熱負荷 空調システム設計 床 汎用性 組み合わせ台数 所望台数 ホットエリア 前記送風機 記載 天井チャンバ サーバー 圧換気扇 冷房専用機 小形化 サーバー室内 前記冷却コイル 例 構成 露点温度 局所空調対応 設置位置 システム 台数 冷却空気 コンピュータ関連施設 対象 冷風 ℃程度 組み合わせ ～図 取付フレーム 負荷増等 ローコスト化 一体 特開 号公報 特徴 壁面設置 冷水 ～ 列 最適台数 風量制御 方式 空調機械室 局所負荷増 前記サーバー室 チューブ列数 負荷高密度化 壁面 床面 最適位置 仕様 ｂ ａ 信頼性 冗長性 コスト 省エネルギー化 フィルターボックス 空調方式 所定風量 該サーバー室 コスト増 設置台数 シングルコイル ダブルコイル h程度 説明 上部 コスト化 ユニット当たり 上下方向 通風方向 効果 mm× 床吹空調機 図示例 対応 ファン フィルター 付属部品 自由度 上下 空気 インバーター制御盤 等 可変性 一般 課題 構造 － 同等 基本 通常 架台 m Pa 形式 下向き 設置作業 インバーター本体 床面積 稼働 簡易 冷媒 水温制御 小形軽量化 冷水管 ノイズフィルター等 冷房 程度 コスト高 製作コスト 前記ユニット 所要台数 ノイズフィルター 蒸発温度 小形軽量 ファン動力増 先行技術文献 増改築 水量制御 制御手段 床部 一過性 安定性 事業性 簡易化 温度環境 乾球温度 新築施設 既存施設 技術分野 背景技術 箱形 種 下層 セクション 機内空気圧力損失 各種 設定 込み 同一 縦長 矢印 配置 kw程度 予定位置 近傍位置 横方向 冷媒管 最適運転 上部天井空間 箱形形状 取付工事 説明図 潜熱 条件 当たり 設計 所要静圧 入口水温 具体例 所定風速 所定規格 COLD),（HOT 要求条件 背面側 蒸発器 ファンケーシング 前面側 単純作業 建築的制約条件 形状 流路 事情 ケーシング 模索段階 上記事情 手段 相対湿度 kw 結露防止 複数台 運転 前面 稼働率 建物 周知 上層 大型 重量 分 エネルギー 配線 概要 現時点 実状 遙 目的 図面 後部 前部 破線 最高 レン ドレンパン 断熱 対策 電源 ケーブル 格別 コストダウン インバータ 外形 寸法 容量 正方形 横長 自体 単体 ｃ 床下 個々 交互 流れ 下部 両側 片側 追加 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 10.2662 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 探査工程 探査工程Ｓ 切羽前方 弾性探査法 反射面 探査 切羽 断層破砕帯 トンネル トンネル切羽前方探査方法 弾性波 掘削工程Ｓ 前方 発振手段 工程 弾性波探査法 反射波 発振孔 細孔 実施形態 掘削工程 前記 切羽前方探査方法 掘削孔 探査深度 可能性 受振器 図 探査方法 Ｓ波反射係数 発振 施工 トンネル前方 トンネル施工 実施 探査法 切羽前方ｂ 弾性波探査 特許文献 トンネル軸 探査深度ｂ 探査深度ａ トンネル切羽前方 位置 掘削 トンネル切羽探査方法 地震波 前記実施形態 トンネル軸方向 作業 補助工法 ａ ｂ 発明 切羽崩壊 層反射法 サイクルタイム 計測作業 方向 前記発振手段 受振孔 切羽前方ａ 前記細孔 幅 トンネル断面図 切羽前方地質 地震波データ 発振点 データ 形態 Ｖｐ速度 地表面 トンネル掘削 ポアソン比 変化率 断面図 内壁面 トンネル坑内 Ｓ波反射係数分布図 複数 物性 ｍ 範囲 掘削工法 地山 解析図 地震探査分野 断面掘削工法 影響 反射面分布図 断面形状 切羽表面 孔 他方 推定伝播速度 設定方法 地山状況 等 ピッチ 危険性 左右 前記受振器 手段 直線状 横方向 施工方法 課題 解析方法 突発湧水 配設ピッチ 概要 最小限 目 ～ 解析 速度解析 伝播速度 ＴＳＰ法 層 号公報 実施頻度 坑内 スプリングライン 設置 山岳トンネル 変化 説明 受振点 震源 情報 距離 ダイナマイト 発破 進 ＡＧＦ工法 ＨＳＰ法等 事前 箇所 表面 地盤凍結工法等 ポアソン比変化率 ベンチカット工法 薬液注入工法 地山条件 導坑先進工法等 施工段階 解析手法 ＡＶＯ解析 先行技術文献 技術分野 前記課題 前記他方 背景技術 状態 外周方向 － 種類 規模 図面 横断方向 ＳＬ 付近 下向き 内部 Ａｍｐｌｉｔｕｄｅ Ｏｆｆｓｅｔ マイグレーション処理等 角度等 有無等 線状構造物 地盤調査 数 状況 設置数 ＣＤＰアンサンブル作成 地表踏査 問題点 側方 反対側 安全かつ 事前調査 特開 特開平 設置間隔 箇所数 ＣＤＰ重合 段階 有無 フォアポーリング 入射角 ＮＭＯ補正 振幅値 ＡＶＯ 支保設計 受振レイアウト 構成要素 効率的かつ 計画 設計 予測 区間 回数 目的 特徴 効果 フローチャート 器 ハンマー 配置 ° 到達 通常 Ｖｅｒｓｕｓ Ｖａｒｉａｔｉｏｎ ｗｉｔｈ 確保 逆 最初 採用 性状 前述 趣旨 変更 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 10.1014 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 外周壁 開口部 暑熱緩和構造 保水層 内部空間 面状構造体 暑熱緩和構造Ａ 壁 実施形態 空間 周壁 図 外表面 暑熱緩和効果 多孔質材料 給水管 給水手段 暑熱環境 ｂ 組積造式 端面図 暑熱 前記外周壁 膜状体 暑熱緩和構造Ｂ 発明 音環境 ブロック 千鳥状 周面 遮音効果 気化熱 音 中心線 改善効果 多孔質セラミックス 開口面積 水 保水性 空気 前記保水層 冷放射 中心線Ｐ 導水管 特許文献 市松模様状 ａ 中空構造物 ｃ 立面図 矩形角筒状 横方向 周囲 方向 面積 中心線Ｑ 中間部 複数 冷気 内面 ポーラスコンクリート 前記開口部 騒音源 表面 前記内周壁 居住空間 ヒートアイランド現象 下部 段 保水層表面 矩形状 外周壁自体 給水部 課題 日射 ｍ －Ｚ 隙間 仕切等 ポーラスコンクリート等 壁厚 多孔質セラミックス等 上端面 形態 騒音 Ｐ ｃｍ 上下方向 レンガ 露出面積 Ｘ－Ｘ端面図 Ｙ－Ｙ端面図 効果 制限 参照 － 等 箱抜き 型枠 変形例 Ｚ 透孔 各種条件 配置パターン 中心線Ｑ２ Ａ 都市部 表層部 説明 手段 温度 出し 四つ 下 形状 夏期 効率 状態 程度 自体 騒音低減効果 給水量 中心軸Ｐ 公園等 地面等 前記課題 前記ブロック 強度等 鉄骨等 タイル等 広場等 夜間等 外装材 先行技術文献 技術分野 背景技術 低減量 縦ｎ段横ｍ列 手法 上部 気温 遮音壁 蒸発 外気 比重 コンクリート 分 手間 基礎 上面 一つ ｎ 間隔 ｃｍ 範囲 縁 凹部 寸法 外面 遮音性能 当該外装材 明瞭性 配置 Ｂ 補強材 上下左右 植生域 植生域内 解決法 蒸散作用 隣接地域 日射側 特開 号公報 道路 対処方法 方法 図示 焼成レンガ 自然数 列 雨水タンク 揚水用 湿潤状態 低下度合い 道路脇 ｄＢ程度 周波数帯 数 離隔距離 内外二つ 図示略 冷却効率 Ｑ２ クールスポット 植物 概要 鉄道 風 流れ 周辺 建物 日光 虞 観点 特徴 向上 図面 自立 上方 屋根 庇 種類 規模 鉄筋 直線 ランダム 水源 上水道 中水道 受水槽 ポンプ 既存 外壁 最大 ℃ 街 ～ 年間 コストパフォーマンス 最小限 表面積 吸音材 同軸 単一 内側 関係 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 10.2367 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 胴縁金物 太陽電池パネル 台座金物 ハゼ把持金物 太陽電池パネルＰ ハゼ 取付構造 図 係合溝 底板部 係合部 爪部 底部 側板部 前記胴縁金物 把持金物 ハゼ式折板屋根 上板部 着部 前記台座金物 前記ハゼ把持金物 Ｃ型鋼 支持剛性 実施形態 金物 フランジ部 合 係合 取付金具 ボルト フレームレス － 内側支持部 前記底部 Ｚ方向 前記 爪 前記太陽電池パネル 特許文献 壁部 ネジ部 両側端部 閉断面Ｓ 係合状態 方向 発明 Ａ 平面図 ｂ 一対 Ｘ方向 Ｂ 前記底板部 長手方向 上記取付構造 フレーム付き太陽電池パネル 下端部 線Ｉ－Ｉ 線ＩＩ－ＩＩ 上方 上部 状態 前記ハゼ 特開 号公報 延設方向 前記上部 前記上板部 剛性 左側面図 断面図 Ｙ方向 複数 閉断面 側方 Ｓ ｂ側 係 正面図 金属板 構造 金物等 内部 取り付け 固定 部材 中央部 開口部 上端部 先端部 凸部 Ｉ字 Ｙ 裏面 簡易 凹部 側部 形態 構成 挿通孔 ナット 移動 隙間 フレーム 字 Ｘ 矢印Ｚ 法線方向 先行技術文献 矢印Ｘ Ｌ字 反対側 技術分野 背景技術 Ｌ字状 説明 課題 下方 箱 締結 図示 施工 断面形状 形状 矢印 力骨状 ボルト締結 締結作業 雪止め金物 ひび割れ等 長円形状 作業 可能性 コストアップ 複数個所 プレス加工 内部空間 コスト 概要 強度 地震 変形 揺れ 衝突 利用 工数 増加 目的 手段 効果 図面 頭部 アダプタ コ 部分 帯状 側壁 接着 装着 支障 方位\n",
      "\n",
      "===== # 6, Topic : 14, p : 11.2521 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 柱状構造物 沈下抑止部材 液状化対策構造 液状化 地盤 沈下 構造 前記柱状構造物 前記沈下抑止部材 構造体 地盤構造体 構造物 根入れ部 実施例 部材 液状化層 礫層 透水性 前記地盤構造体 部材等 沈下低減構造 保護部材 図 鍔状 凸部 ポスト液状化状態 地盤改良 周面 液状化被害 側断面図 液状化対策 発明 地盤表面 ハンチ部 前記保護部材 前記根入れ部 コンクリート等 液状化地盤 請求項 液状化そのもの 地盤沈下 構成 不同沈下 袋状 周囲 側 実施 可能性 前記凸部 特許文献 外周面 硬化材 転倒モーメント 状態 上記 転倒 地盤改良体 液状化状態 力 剛性 傾斜 沈下低減 液状化現象 断面 せん断力 地盤沈下量 凹部 水平方向 地震 直下 材料 円柱状 円形断面 矩形断面 沈下量 多角形断面 説明 被害 ジオテキスタイル 水平断面形状 表層 ～ 圧縮側 膨張側 複合材料系 電信柱 効果 荷重 斜め下方 役割 符号 地盤表面ＧＬ ボルト等 先端支持力 金属系 木質系 周辺地盤 東北地方太平洋沖地震 礫材料 特徴 形態 同一 摩擦力 交通標識用 夜間照明用 鉛直方向 面 変形 礫層周囲 礫層径 地盤表面ＧＬ下 建築物 位置 下面 隙間 ポール 一定 拘束作用 拘束圧 中間層 セメント系材料等 コンクリート製 支持力 利用可能性 技術 課題 影響 直径 モルタル 構成材料 太平洋三陸沖 安定性 排水性 排水 砂層等 アスファルトコンクリート コンクリート製電柱 ＧＬ 粘性土等 先行技術文献 技術分野 背景技術 直上 領域 目的 鉄筋コンクリート 鉄骨 木材 中空 実 傾き 図面 可逆 液体状態 円柱 当該方向 上方 二つ どうし せん断変形 東北 作用 過剰間隙水圧比 構成材質 東日本一帯 東日本 埋め立て地 周辺 東京湾沿岸 表層直下 許容範囲 特開 号公報 継続使用 使用 震源 関東 広範囲 大半 千葉 浦安 市 部分 市街地 建物 最小限 所定 他 － 概要 上述 家屋 ビル 態様 開発 手段 一つ 互い 外縁 同軸 ダイレイタンシー 噴砂 破壊 寸法 側溝 強度 ａ 底面 産業\n",
      "\n",
      "===== # 7, Topic : 14, p : 9.9088 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : カーテンウォール 前記カーテンウォール 前記カーテンウォール用型枠 コンクリート コンクリート構造物 構造物 構築方法 図 カーテンウォール側 カーテンウォール用コンクリート カーテンウォール用型枠 目地部 カーテンウォール カーテンウォール支持用 カーテンウォール部 固定筋 前記 型枠 カーテンウォール用配筋 前記カーテンウォール側ファスナー 手順 ファスナー 発明 躯体側ファスナー 用 作業 カーテンウォール側ファスナー 柱 工程 カーテンウォール同士 カーテンウォール形成 前記躯体側ファスナー 梁 目地 前記カーテンウォール 外型枠 特許文献 発泡材 安全性 位置 側面側断面図 前記型枠 断面図 前記構造物 前記コンクリート Ｂ 実施例 目地幅 課題 現場 切断 取付け用 Ａ 造 前記ファスナー 現場打ち テーパ付仕切板 止水用 作業能率 前記目地部 内型枠 ＳＲＣ造 鉄骨鉄筋コンクリート ＲＣ造 所要位置 所要強度 横方向 方法 特開 号公報 アウトフレーム 圧入工法 切断範囲 層間変位 コスト低減 配筋 変成シリコーン 前記発泡材 止水用立上り 前記現場打ち 説明 パネル 楊重機 大型 ファスナー部材 作業性 前記ロッキング機能 コンクリートポンプ車 コンクリート打設時 前記柱 目地棒 当該コンクリート荷重 鉄筋コンクリート 鉄筋コンクリート造 躯体精度 施工性 取付け作業 水返し 先行技術文献 工程図 スリット成形材 技術分野 背景技術 容易性 意匠性 ロッキング機能 利用可能性 外 セット 発現 解体 硬化 挙動 寸法 効果 運搬 下 シール 正面図 壁下耐震スリット 部材引き込み ＲＣ 取付け 等 ＳＲＣ 施工誤差吸収 発砲ポリエチレン等 外足場 水平方向 三角形空白部分 工程手順 ボルト等 搬送車両 重量感 上記課題 揚重 ロックウール 荷重 摩擦力 圧縮変形 錆止め塗装 同士 立上り 相違 工場 壁面 概観 見栄え 雰囲気 概要 搬入 内部 面 材料 デザイン 手段 目的 要旨 該柱 製造 費用 制約 隙間 外観 表現 図面 上部 下部 形態 設 一体 支保工 参照 周囲 縁 否 設荷重 検査 ボンドブレーカー シーリング 段差 内側 クレーン ～ 産業 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 10.2123 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : スラグ混じり土 セメント 破砕 溶出量 フッ素 セメント添加率 粒度 スラグ混じり土Ｍ 回転式破砕混合装置 細粒分質礫質砂 添加量 発明 土 水砕スラグ インパクトチェーン 図 礫分 砂分 細粒分混じり砂質礫 不溶化剤 スラグ 溶出基準 フッ素溶出量 処理方法 請求項 礫質土 前記スラグ混じり土 砂質土 粒度範囲 砂質礫 分類 溶出 前記セメント 円筒ケーシング 粒径 細粒化 路盤材 セメントＣ セメント添加量 不溶化 破砕混合装置 添加 混合 前記回転式破砕混合装置 フッ素混じり土 実施形態 回転軸 粒度別 最大粒径 セメント等 横軸 縦軸 環境基準 スラグ混じり残土 土壌 土質分類 回転数 基準 細粒分 Ｃ 粒径加積曲線 サンプルＡ 本願発明例 前記セメント添加量 前記破砕混合装置 土質 粒度調整 経済性 効率性 レベル 実験 サンプルＣ 特許文献 必要量 粒径以下 混練性 ランニングコスト 打撃力 打撃 効率 使用材料 Ｌ 土壌環境基準値 データ 程度 ～ 破砕効果 不溶化処理 砂状 溶出試験 比較例 説明 Ｂ 特徴 多段 関係 組み合わせ 基 サンプル 機械撹拌混合装置 高圧噴射混合装置 ツイスター混合装置 前記フッ素 製鉄所 スラリー状 前記円筒ケーシング 粉体 再生利用 方法 固化不溶化 不溶化効果 技術 低減 表面積 増加 記載 材齢 課題 水硬性結合材 形態 Ｍ サンプルＢ 各々材齢 配合サンプルＡ 土質分類レベル 先行技術文献 該円筒ケーシング 技術分野 背景技術 水分 状態 通常 － 可能性 混練 最適 条件 図面 サイズ 仕様 パラメータ 材料 鎖 おもり 一緒 重量 ｐＨ グラフ 実験例 バックホウ等 効果 曲線 製鉄所内 ＧＳ－Ｆ 固化強度 上記事情 上記課題 中心部 端部 残土 有害元素 特開 号公報 説明図 ベルトコンベア 先端側 ｐＨ 環境省告示 号 ポゾラン活性 スラグヤード 種 地中 過程 概要 上述 均一 部分 アルカリ性 目的 手段 内部 モータ ブレード 該鎖 棒状 対数 百分率 次 直径 ｒｐｍ 段数 本数 段 ３つ Ｄ ミキサー 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 10.0487 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 廃グリセリン 最終処分場 水 安定化 処分 水処理施設 廃グリセリン混合水 浸出水 却残渣 廃棄物 図 固化 処分場 処理 早期安定化 バイオディーゼル燃料 実施例 混合 早期安定化方法 石灰分 散水水 埋立 浸出水集排水管 方法 廃グリセリン混合液 発明 最終処分 早期 装置 散水 維持管理費 量 投入量 一軸圧縮強度 散水設備 地域住民 廃止 安定化方法 浸出水処理施設 廃グリセリン投入 特許文献 説明図 埋立地 説明 効果 焼却工場 スケルトンバケット 廃棄物処理場 安定化期間 散水装置 コンクリート固化遅延材 水和反応 必要量 不燃破砕ごみ 水和作用 無機化 無害化 混合工程 二酸化炭素 設備 燃料 洗い出し効果 バイオディーゼル燃料製造 特開 号公報 廃止基準 跡地利用 有害物質 土砂混合方法 減容化 排ガス処理装置 発生量 産業廃棄物 目的 課題 次 配管 機能 灰 主成分 埋め立て 遅延 固化防止 温暖化ガス発生抑制 程度 － 実施 段階 濃度 投入 状態 管理者 維持管理コスト 防止方法 製造 防止 埋立地内部 埋立終了 発生源 粉塵発生 埋立層 防止効果 薄厚埋立 先行技術文献 技術分野 背景技術 製造工程 散水範囲 計画 年間 可能性 達成 住民サービス 了解 原因 易分解性有機物 現象 図面 形態 ～ 浸透 みち 反応 数値 バックホー 公知 一緒 透水性 実験 添加量 塩化水素ガス 製造段階 比較図 利用 基準 工程 有機物濃度 反応速度 油糧植物 ＣＯＤ等 ＢＯＤ等 主原料 植物 問題点 新規立地 期間 大量普及 ＢＯＤ 負担 環境負担 硬化状態 前提条件 煙道 範囲 Ｃａ ＣＯＤ ごと 場所ごと 比較 Ｆ－ 重量比 ＭＰａ程度 油脂 ディーゼルエンジン 循環 地上 利点 ネック 観点 いかん 用地 各種 概要 法 閉鎖 一つ 転用 事故 増加 手段 上記 廃液 特徴 規模 他方 影響 新設 低減 参照 理由 塩化ビニール 消石灰 生石灰 降雨 データ 基 水質 掘削 ミキサー トロンメル 他 トラック 荷台 下 行程 通常 被覆 調整 分析 養生 資料 同等 値 上面 雨水 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 10.0699 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ビルトＨ ロールＨ 芯材 図 ソイルセメント連続壁 狭隘部 山留め壁 躯体ライン ドライエリア躯体 前記ビルトＨ 建物 隣地境界線 ソイルセメント 地下躯体 軸混練オーガー機 施工方法 Ｈ形鋼 Ｈ寸法 前記ロールＨ 横断面図 部 治具 Ｈ型鋼 寸法 前記芯材 ｂ オーガー機 ボルト継手 前記狭隘部 該ロールＨ ロールＨ同士 継手位置 ソイルセメント面 発明 土留め鋼材 施工コスト 登録商標 規格品 圧入機 躯体側 間隔 下方 上部 水性能 切梁 ロール材 幅 場所 クリアランス 形状 ウェブ 重心位置 Ｂ寸法 工法 上方 下部 示的 フランジ 鋼材 特許文献 該ソイルセメント連続壁 専用 所定深度 セメントスラリー 縦長形状 効果 Ｈ形鋼等 切欠部 施工範囲 前記オーガー機 縦断面図 平面図 説明図 説明 既存建物 地盤 符号 例 ボルト 前記 費用 施工 前記建物 ＳＭＷ 先端 課題 工程 部位 余裕 ｍｍ×ｔ ｃ 同士 該山留め壁 連結部分 シートパイル 連結 止水性 継手部分 掘削混練 建物位置 周知 参照 関係 ブレード 該芯材 クレーン 孔 目 前記建物側 規格品幅 施工現場 正面図 斜視図 コスト 先行技術文献 技術分野 背景技術 要旨 漏水 存在 作業 支障 図面 形態 ｔ サイズ 理由 所定 添板 規格外 フランジ同士 フランジ幅 許容範囲 強度等 特開 号公報 問題点 請求項 製作費用 強度 Ｕ字 側 移動 検討 剛性 ゼロパイラー 採用 － 概要 手段 記載 変位 組み合わせ 種々 実施 工場 使用 内部\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.5706e-38, 4.7137e-39, 2.5197e-20, 0.0000e+00,\n",
      "         0.0000e+00, 4.5503e-21, 6.3532e-32, 7.5406e-38, 0.0000e+00, 2.4032e-40,\n",
      "         2.4962e-26, 5.3249e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.05529\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1141.9123\n",
      "Test epoch : 1 Average loss: 1157.1804\n",
      "PP(train) = 4089.419, PP(valid) = 4040.308\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.6805e-38, 1.7856e-32, 4.3381e-36, 0.0000e+00,\n",
      "         0.0000e+00, 1.4275e-32, 2.0550e-34, 1.6984e-42, 0.0000e+00, 0.0000e+00,\n",
      "         2.3574e-37, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.04986\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1141.5114\n",
      "Test epoch : 2 Average loss: 1156.7321\n",
      "PP(train) = 4074.642, PP(valid) = 4027.555\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.7841e-32, 2.1303e-13, 1.5480e-22, 4.1029e-13, 0.0000e+00,\n",
      "         0.0000e+00, 9.7562e-14, 6.7505e-24, 1.2910e-31, 7.7205e-39, 1.2612e-44,\n",
      "         6.7691e-17, 2.0965e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.04497\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1141.0504\n",
      "Test epoch : 3 Average loss: 1156.2866\n",
      "PP(train) = 4058.961, PP(valid) = 4014.521\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2990e-35, 6.5982e-13, 2.3243e-22, 1.0539e-11, 0.0000e+00,\n",
      "         1.4013e-44, 4.8616e-14, 4.0142e-16, 4.0854e-32, 1.3765e-40, 7.0191e-32,\n",
      "         1.1601e-12, 1.3160e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.04055\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1140.4966\n",
      "Test epoch : 4 Average loss: 1155.8412\n",
      "PP(train) = 4042.898, PP(valid) = 4001.471\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7613e-26, 5.4546e-30, 3.2197e-27, 5.7697e-17, 0.0000e+00,\n",
      "         0.0000e+00, 1.3294e-22, 6.5428e-18, 1.5246e-42, 7.0065e-45, 9.9378e-39,\n",
      "         3.1052e-19, 1.2376e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.03658\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1139.9611\n",
      "Test epoch : 5 Average loss: 1155.3912\n",
      "PP(train) = 4026.729, PP(valid) = 3988.464\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.9927e-26, 3.3796e-21, 1.1990e-30, 2.2851e-13, 0.0000e+00,\n",
      "         4.4847e-38, 5.2841e-06, 8.1451e-20, 1.5396e-33, 8.9069e-34, 1.8124e-31,\n",
      "         5.7650e-23, 6.7200e-27, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.03299\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1139.4559\n",
      "Test epoch : 6 Average loss: 1154.9415\n",
      "PP(train) = 4010.740, PP(valid) = 3975.700\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.3248e-39, 2.8639e-26, 2.5143e-31, 1.7471e-03, 2.0536e-35,\n",
      "         0.0000e+00, 3.9406e-19, 1.0520e-07, 5.7023e-25, 4.5921e-36, 7.0485e-43,\n",
      "         3.7620e-17, 1.0466e-24, 9.9825e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0601, 0.0669, 0.1076, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02975\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1138.9407\n",
      "Test epoch : 7 Average loss: 1154.4943\n",
      "PP(train) = 3994.680, PP(valid) = 3962.962\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1377e-40, 3.3653e-26, 3.1641e-29, 6.8499e-14, 5.7404e-40,\n",
      "         0.0000e+00, 1.8369e-12, 1.0332e-26, 1.2478e-30, 0.0000e+00, 7.5295e-38,\n",
      "         1.3936e-16, 2.3563e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02683\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1138.2692\n",
      "Test epoch : 8 Average loss: 1154.0471\n",
      "PP(train) = 3978.807, PP(valid) = 3950.337\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.9776e-38, 6.9244e-39, 4.4408e-31, 0.0000e+00,\n",
      "         0.0000e+00, 6.4502e-33, 6.9633e-34, 9.6742e-37, 0.0000e+00, 0.0000e+00,\n",
      "         2.7526e-32, 1.4013e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02420\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1137.7578\n",
      "Test epoch : 9 Average loss: 1153.6014\n",
      "PP(train) = 3963.306, PP(valid) = 3938.089\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4776e-37, 1.0893e-32, 1.2766e-38, 5.8834e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.0526e-25, 4.0406e-27, 1.2877e-41, 0.0000e+00, 1.7937e-43,\n",
      "         4.6268e-40, 5.5406e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.02183\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1137.1489\n",
      "Test epoch : 10 Average loss: 1153.1551\n",
      "PP(train) = 3947.821, PP(valid) = 3925.790\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7309e-35, 2.0231e-35, 9.3157e-37, 4.8063e-23, 8.1275e-44,\n",
      "         0.0000e+00, 1.8711e-09, 1.0859e-19, 5.9579e-35, 0.0000e+00, 5.6672e-36,\n",
      "         6.7957e-25, 9.5088e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01969\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1136.6363\n",
      "Test epoch : 11 Average loss: 1152.7129\n",
      "PP(train) = 3932.070, PP(valid) = 3913.156\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.0939e-27, 6.5693e-40, 1.0954e-18, 0.0000e+00,\n",
      "         1.4013e-45, 1.0535e-12, 1.8338e-19, 0.0000e+00, 0.0000e+00, 3.5312e-37,\n",
      "         1.3294e-24, 1.5512e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01776\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1135.8949\n",
      "Test epoch : 12 Average loss: 1152.2710\n",
      "PP(train) = 3916.654, PP(valid) = 3900.923\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4379e-30, 4.5996e-38, 9.7488e-29, 0.0000e+00,\n",
      "         0.0000e+00, 3.0550e-23, 1.7293e-30, 5.6042e-36, 0.0000e+00, 0.0000e+00,\n",
      "         1.3764e-25, 4.5768e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01601\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1135.5128\n",
      "Test epoch : 13 Average loss: 1151.8357\n",
      "PP(train) = 3901.314, PP(valid) = 3888.696\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8382e-22, 1.5044e-11, 3.5778e-24, 6.9209e-12, 8.9515e-42,\n",
      "         0.0000e+00, 1.1917e-14, 2.3206e-18, 3.2796e-19, 2.0506e-39, 1.4476e-34,\n",
      "         3.6338e-15, 1.0941e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01444\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1135.0261\n",
      "Test epoch : 14 Average loss: 1151.4014\n",
      "PP(train) = 3886.156, PP(valid) = 3876.584\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.0751e-32, 0.0000e+00, 1.1952e-20, 0.0000e+00,\n",
      "         0.0000e+00, 2.4612e-26, 2.6613e-28, 1.4928e-39, 0.0000e+00, 6.8664e-44,\n",
      "         1.3692e-25, 8.0361e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01303\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1134.4250\n",
      "Test epoch : 15 Average loss: 1150.9697\n",
      "PP(train) = 3871.057, PP(valid) = 3864.510\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6816e-42, 1.0767e-31, 4.0885e-30, 2.0143e-30, 0.0000e+00,\n",
      "         0.0000e+00, 6.2008e-22, 7.7471e-25, 4.4271e-29, 0.0000e+00, 0.0000e+00,\n",
      "         2.3633e-22, 3.6320e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01175\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1133.9459\n",
      "Test epoch : 16 Average loss: 1150.5412\n",
      "PP(train) = 3855.932, PP(valid) = 3852.378\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.8473e-44, 3.9492e-28, 2.3476e-39, 2.1249e-13, 0.0000e+00,\n",
      "         0.0000e+00, 5.4955e-10, 4.5715e-20, 1.0200e-29, 0.0000e+00, 4.4977e-31,\n",
      "         1.4728e-13, 3.0676e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.01060\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1133.4027\n",
      "Test epoch : 17 Average loss: 1150.1116\n",
      "PP(train) = 3841.079, PP(valid) = 3840.478\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 1.3737e-22, 2.9550e-34, 1.8347e-22, 0.0000e+00,\n",
      "         0.0000e+00, 6.4967e-24, 1.7896e-24, 1.1895e-30, 0.0000e+00, 0.0000e+00,\n",
      "         9.1038e-23, 1.0094e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00956\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1132.8636\n",
      "Test epoch : 18 Average loss: 1149.6854\n",
      "PP(train) = 3826.545, PP(valid) = 3828.831\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.4393e-41, 1.5864e-27, 9.4406e-34, 1.7019e-15, 0.0000e+00,\n",
      "         0.0000e+00, 7.4951e-18, 5.0368e-22, 8.7611e-37, 5.2784e-41, 4.2422e-40,\n",
      "         6.8904e-26, 5.8168e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00862\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1132.4179\n",
      "Test epoch : 19 Average loss: 1149.2593\n",
      "PP(train) = 3812.154, PP(valid) = 3817.291\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.3871e-39, 1.2045e-30, 2.4331e-33, 9.9767e-18, 0.0000e+00,\n",
      "         0.0000e+00, 5.3132e-15, 6.6962e-22, 6.4283e-29, 8.0081e-41, 1.8174e-30,\n",
      "         9.7418e-10, 4.5052e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00778\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1131.7056\n",
      "Test epoch : 20 Average loss: 1148.8359\n",
      "PP(train) = 3798.016, PP(valid) = 3805.990\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.7184e-31, 3.7617e-29, 1.0329e-23, 0.0000e+00,\n",
      "         0.0000e+00, 1.1446e-11, 1.1498e-19, 2.8026e-45, 8.1294e-38, 1.6395e-42,\n",
      "         3.4700e-21, 2.2727e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00701\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1131.2569\n",
      "Test epoch : 21 Average loss: 1148.4132\n",
      "PP(train) = 3783.844, PP(valid) = 3794.610\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7090e-36, 3.7858e-22, 3.2288e-37, 1.3530e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.6549e-25, 1.7169e-22, 1.7972e-37, 0.0000e+00, 4.0851e-38,\n",
      "         1.0591e-20, 2.7787e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00633\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1130.7532\n",
      "Test epoch : 22 Average loss: 1147.9915\n",
      "PP(train) = 3769.484, PP(valid) = 3782.975\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.7275e-43, 6.8328e-22, 4.2576e-30, 3.0135e-33, 0.0000e+00,\n",
      "         0.0000e+00, 7.9431e-27, 1.4919e-27, 1.5607e-35, 0.0000e+00, 0.0000e+00,\n",
      "         1.1925e-25, 5.5938e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00571\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1130.2353\n",
      "Test epoch : 23 Average loss: 1147.5742\n",
      "PP(train) = 3754.996, PP(valid) = 3771.210\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0274e-33, 2.4205e-19, 9.0802e-25, 9.2652e-12, 0.0000e+00,\n",
      "         2.4803e-43, 7.8955e-08, 4.2554e-24, 3.7013e-24, 1.4013e-45, 1.7038e-37,\n",
      "         5.6251e-24, 2.4477e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00515\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1129.8324\n",
      "Test epoch : 24 Average loss: 1147.1588\n",
      "PP(train) = 3741.228, PP(valid) = 3760.139\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.4645e-31, 4.3440e-44, 6.4177e-26, 0.0000e+00,\n",
      "         0.0000e+00, 2.8815e-34, 3.1392e-27, 3.2865e-40, 0.0000e+00, 1.5414e-44,\n",
      "         1.6050e-32, 5.1848e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00464\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1129.2903\n",
      "Test epoch : 25 Average loss: 1146.7470\n",
      "PP(train) = 3727.449, PP(valid) = 3749.042\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 7.0694e-22, 3.5938e-39, 4.7978e-23, 2.4422e-41,\n",
      "         0.0000e+00, 6.5105e-12, 3.3065e-26, 1.0891e-35, 0.0000e+00, 0.0000e+00,\n",
      "         5.1129e-23, 2.3221e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00419\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1128.7510\n",
      "Test epoch : 26 Average loss: 1146.3333\n",
      "PP(train) = 3713.735, PP(valid) = 3737.911\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6169e-37, 4.5395e-13, 3.5846e-30, 2.9944e-20, 0.0000e+00,\n",
      "         0.0000e+00, 1.0225e-03, 1.3906e-20, 3.8163e-23, 4.2039e-45, 4.8650e-41,\n",
      "         4.0668e-22, 8.8921e-35, 9.9898e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00378\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1128.1973\n",
      "Test epoch : 27 Average loss: 1145.9216\n",
      "PP(train) = 3700.177, PP(valid) = 3726.894\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 1.8410e-32, 1.2655e-28, 1.4749e-17, 0.0000e+00,\n",
      "         0.0000e+00, 1.0850e-25, 9.9217e-26, 4.1803e-39, 1.0300e-42, 1.1817e-38,\n",
      "         7.1566e-24, 8.6375e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00341\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1127.7812\n",
      "Test epoch : 28 Average loss: 1145.5135\n",
      "PP(train) = 3686.532, PP(valid) = 3715.832\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.2550e-42, 3.7327e-24, 9.3567e-36, 2.3696e-10, 1.4013e-45,\n",
      "         0.0000e+00, 1.0655e-15, 2.6299e-13, 3.3808e-25, 0.0000e+00, 1.1977e-39,\n",
      "         1.7431e-17, 1.2829e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00307\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1127.1992\n",
      "Test epoch : 29 Average loss: 1145.1048\n",
      "PP(train) = 3673.487, PP(valid) = 3705.312\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.0522e-42, 4.9987e-23, 1.1848e-12, 4.2425e-22, 0.0000e+00,\n",
      "         2.4903e-39, 4.3760e-16, 6.4372e-12, 2.1848e-30, 0.0000e+00, 1.5377e-34,\n",
      "         2.0977e-15, 1.7916e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00277\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1126.7436\n",
      "Test epoch : 30 Average loss: 1144.6977\n",
      "PP(train) = 3660.359, PP(valid) = 3694.669\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 4.8611e-42, 3.3940e-28, 2.4509e-29, 2.5275e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.9387e-20, 9.4601e-17, 1.0021e-30, 0.0000e+00, 2.5223e-44,\n",
      "         5.9059e-32, 5.6439e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00250\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1126.2616\n",
      "Test epoch : 31 Average loss: 1144.2938\n",
      "PP(train) = 3647.188, PP(valid) = 3683.998\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.0465e-35, 7.1215e-39, 3.5326e-15, 0.0000e+00,\n",
      "         0.0000e+00, 1.9263e-21, 1.6707e-26, 6.3623e-38, 0.0000e+00, 0.0000e+00,\n",
      "         1.3389e-24, 3.0940e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00226\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1125.6290\n",
      "Test epoch : 32 Average loss: 1143.8916\n",
      "PP(train) = 3634.136, PP(valid) = 3673.385\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6536e-34, 6.3279e-26, 5.1631e-36, 1.4979e-20, 0.0000e+00,\n",
      "         0.0000e+00, 9.2359e-26, 9.6360e-19, 6.2673e-25, 0.0000e+00, 0.0000e+00,\n",
      "         8.5715e-32, 5.8014e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00203\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1125.1765\n",
      "Test epoch : 33 Average loss: 1143.4878\n",
      "PP(train) = 3621.149, PP(valid) = 3662.824\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.6052e-45, 9.8118e-21, 1.1203e-26, 1.1508e-19, 0.0000e+00,\n",
      "         4.9466e-43, 9.1712e-18, 3.1046e-21, 2.6580e-34, 6.9356e-41, 9.9588e-36,\n",
      "         4.4143e-19, 1.9310e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00183\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1124.8768\n",
      "Test epoch : 34 Average loss: 1143.0882\n",
      "PP(train) = 3608.301, PP(valid) = 3652.360\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.0406e-32, 2.1450e-20, 4.3910e-28, 1.9389e-11, 0.0000e+00,\n",
      "         2.3675e-41, 3.1538e-07, 5.1947e-12, 4.4520e-21, 9.3200e-42, 1.4379e-26,\n",
      "         1.0661e-11, 1.6019e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00166\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1124.3333\n",
      "Test epoch : 35 Average loss: 1142.6947\n",
      "PP(train) = 3595.543, PP(valid) = 3641.957\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2650e-37, 2.9820e-42, 2.0830e-04, 7.4790e-15, 3.0897e-10, 8.9683e-44,\n",
      "         9.6690e-44, 5.0199e-09, 1.8085e-14, 4.8280e-41, 4.5269e-40, 1.2811e-37,\n",
      "         1.3410e-09, 1.3634e-29, 9.9979e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00149\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1123.8200\n",
      "Test epoch : 36 Average loss: 1142.3022\n",
      "PP(train) = 3582.762, PP(valid) = 3631.545\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2017e-38, 4.2242e-16, 7.6381e-17, 2.8160e-16, 5.1428e-43,\n",
      "         2.0319e-43, 4.6429e-08, 3.0616e-10, 1.2897e-31, 1.9618e-44, 2.5793e-25,\n",
      "         1.1468e-19, 6.9739e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00135\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1123.4263\n",
      "Test epoch : 37 Average loss: 1141.9071\n",
      "PP(train) = 3569.880, PP(valid) = 3620.966\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2489e-35, 3.0184e-35, 8.4874e-29, 3.7685e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.4706e-21, 5.9627e-26, 5.5879e-32, 0.0000e+00, 9.2245e-36,\n",
      "         1.6615e-19, 4.9515e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00121\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1122.8098\n",
      "Test epoch : 38 Average loss: 1141.5170\n",
      "PP(train) = 3557.713, PP(valid) = 3611.062\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8080e-28, 1.0472e-21, 2.9175e-33, 4.4744e-11, 7.5670e-44,\n",
      "         0.0000e+00, 5.1299e-09, 2.0975e-10, 1.4764e-21, 5.1288e-43, 2.8553e-33,\n",
      "         5.3047e-10, 3.1983e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00110\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1122.3770\n",
      "Test epoch : 39 Average loss: 1141.1265\n",
      "PP(train) = 3545.621, PP(valid) = 3601.250\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1561e-38, 2.9077e-23, 3.1060e-22, 1.3480e-21, 0.0000e+00,\n",
      "         7.0065e-45, 4.1638e-08, 1.4519e-18, 6.7196e-33, 0.0000e+00, 5.3203e-36,\n",
      "         8.7020e-24, 1.1033e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00099\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1121.8891\n",
      "Test epoch : 40 Average loss: 1140.7378\n",
      "PP(train) = 3533.375, PP(valid) = 3591.219\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.6179e-31, 2.0141e-23, 6.7663e-23, 2.9562e-22, 0.0000e+00,\n",
      "         0.0000e+00, 3.3549e-08, 4.1096e-16, 9.9189e-40, 0.0000e+00, 5.9981e-41,\n",
      "         4.3853e-27, 3.0829e-44, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00089\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1121.3937\n",
      "Test epoch : 41 Average loss: 1140.3502\n",
      "PP(train) = 3521.076, PP(valid) = 3581.140\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7763e-26, 2.9928e-26, 4.9979e-21, 2.4545e-18, 0.0000e+00,\n",
      "         0.0000e+00, 7.0409e-11, 3.1810e-15, 5.8929e-20, 0.0000e+00, 1.3368e-42,\n",
      "         9.9114e-17, 2.6350e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00080\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1120.9194\n",
      "Test epoch : 42 Average loss: 1139.9676\n",
      "PP(train) = 3508.915, PP(valid) = 3571.212\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3431e-33, 2.7371e-12, 6.6408e-23, 9.5674e-13, 2.2103e-38,\n",
      "         1.9189e-28, 9.9952e-01, 4.7904e-04, 2.0732e-35, 2.2240e-38, 1.7150e-28,\n",
      "         2.6234e-16, 1.7342e-23, 2.0019e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0443, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00073\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1120.5379\n",
      "Test epoch : 43 Average loss: 1139.5860\n",
      "PP(train) = 3496.932, PP(valid) = 3561.362\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7852e-41, 6.3813e-35, 0.0000e+00,\n",
      "         0.0000e+00, 8.2041e-32, 1.5960e-30, 8.4078e-45, 0.0000e+00, 0.0000e+00,\n",
      "         4.1638e-29, 2.0439e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00065\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1119.8580\n",
      "Test epoch : 44 Average loss: 1139.2021\n",
      "PP(train) = 3485.262, PP(valid) = 3551.824\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3561e-42, 1.4925e-32, 9.7424e-40, 5.1067e-21, 1.9158e-38,\n",
      "         0.0000e+00, 9.3267e-07, 6.0317e-23, 4.8154e-34, 0.0000e+00, 7.5810e-43,\n",
      "         6.6730e-42, 2.8830e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00059\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1119.5832\n",
      "Test epoch : 45 Average loss: 1138.8212\n",
      "PP(train) = 3473.374, PP(valid) = 3542.059\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0069e-21, 1.1430e-11, 1.3873e-14, 2.1082e-04, 4.2039e-45,\n",
      "         8.1135e-33, 8.6706e-02, 3.8911e-08, 2.1753e-13, 3.1189e-33, 1.0688e-25,\n",
      "         6.2127e-03, 6.4353e-22, 9.0687e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0823, 0.0484, 0.0992, 0.0501, 0.0619, 0.0694, 0.1049, 0.0872, 0.0593,\n",
      "         0.0514, 0.0661, 0.0549, 0.0500, 0.0732, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00053\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1119.0993\n",
      "Test epoch : 46 Average loss: 1138.4457\n",
      "PP(train) = 3461.393, PP(valid) = 3532.189\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 1.2049e-26, 1.8895e-34, 2.0563e-25, 0.0000e+00,\n",
      "         0.0000e+00, 1.0684e-25, 3.0625e-30, 4.9860e-39, 0.0000e+00, 0.0000e+00,\n",
      "         2.3243e-25, 1.5743e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00048\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1118.6697\n",
      "Test epoch : 47 Average loss: 1138.0690\n",
      "PP(train) = 3449.698, PP(valid) = 3522.584\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.8255e-43, 2.5193e-33, 1.3710e-38, 5.2602e-26, 0.0000e+00,\n",
      "         0.0000e+00, 4.0943e-26, 3.1192e-24, 0.0000e+00, 0.0000e+00, 9.8091e-45,\n",
      "         3.5668e-27, 1.9115e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00043\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1118.0627\n",
      "Test epoch : 48 Average loss: 1137.6890\n",
      "PP(train) = 3438.399, PP(valid) = 3513.352\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9447e-32, 2.2388e-17, 1.2193e-36, 1.4288e-15, 1.4013e-45,\n",
      "         0.0000e+00, 9.9221e-23, 1.7211e-12, 4.2305e-25, 0.0000e+00, 9.8213e-31,\n",
      "         2.1215e-25, 3.6777e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00039\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1117.6770\n",
      "Test epoch : 49 Average loss: 1137.3153\n",
      "PP(train) = 3427.170, PP(valid) = 3504.149\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7656e-43, 9.6343e-27, 4.1303e-27, 1.9957e-19, 0.0000e+00,\n",
      "         0.0000e+00, 2.6579e-11, 6.9905e-16, 3.1566e-26, 5.0513e-41, 5.9292e-37,\n",
      "         8.6015e-21, 4.9633e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00035\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1117.1393\n",
      "Test epoch : 50 Average loss: 1136.9431\n",
      "PP(train) = 3415.792, PP(valid) = 3494.765\n",
      "Writing to ./topicwords/7-topwords_e50.txt\n",
      "Topic 0: 大 定着構造 緑化 ピストンロッド イ 継手板 伸長 接合作業 上面側 建築\n",
      "Topic 1: 定着構造 大 伸長 イ ピストンロッド 継手板 緑化 仕上げ材 浸透 建築\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 3: 特開昭 円周方向 斜面 楕円形 盛土 上層階 水位差 上部工 駆動装置 立坑\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 課題\n",
      "Topic 5: 大 定着構造 緑化 イ ピストンロッド 継手板 伸長 接合作業 表裏 マンホール\n",
      "Topic 6: 大 ピストンロッド 定着構造 緑化 継手板 イ 伸長 接合作業 上面側 表裏\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 手段 形態 説明 課題 発明\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 形態 手段\n",
      "Topic 9: 大 ピストンロッド 表裏 定着構造 継手板 緑化 接合作業 伸長 イ 上面側\n",
      "Topic 10: イ 大 ピストンロッド 定着構造 継手板 緑化 伸長 接合作業 くさび 特徴構成\n",
      "Topic 11: イ 大 定着構造 緑化 継手板 ピストンロッド 伸長 接合作業 仕上げ材 上面側\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 13: イ 定着構造 継手板 大 伸長 緑化 ピストンロッド 接合作業 マンホール 一辺\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 10.4299 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 域遮断周波数 蓄電池補償帯域決定部 蓄電池容量 負荷電力 蓄電池 補償帯域 出力 高域遮断周波数 周波数 値 蓄電池出力 蓄電池出力指令値計算部 電力 蓄電池出力指令値 電力管理システム 制御パラメータ決定部 蓄電池制御部 前記蓄電池 負荷電力取得部 補償周波数帯域 容量 負荷変動補償 決定補償帯域格納部 蓄電池制御 容量Ｗ 気象情報 ｆ 実績データ 蓄電池補償帯域 周波数ｆ 制御対象日 定置用蓄電池部 決定 気象類似日負荷電力データ取得部 図 ｋ 実効蓄電池容量 蓄電池容量Ｗ 制御 出力値 電力管理方法 気象情報取得部 前記制御パラメータ 高域遮断周波数 指令値 放電電力 買電電力 電力量 変動 制御パラメータ 補償帯域決定 前記蓄電池制御部 位相差φ 実績データＤＢ 域遮断周波数ｉ 域周波数 前記蓄電池容量 補償帯域決定機能 前記制御パラメータ決定部 放電量 高域遮断周波 蓄電池出力指令 出力周波数 決定方法 出力リミッタ 前記出力値 蓄電池制御処理 充電量 域側 出力初期値 前記蓄電池補償帯域決定部 システム演算部 実施形態 数式 蓄電池補償帯域決定ステップ 入力 前記制御パラメータ決定ステップ 周波数成分 ピーク電力削減量 負荷電力プロファイル 前記 放電電力量 位相差 下限値 上限値 固定値 指令値計算部 φｆ 高域周波数 入力値 発明 コンピュータシステム 前記負荷電力取得部 負荷 域遮断周波数 データ 蓄電池制御ステップ 電力計等 前記入力値 パラメータ 制御パラメータ決定ステップ 負荷電力取得ステップ 上記補償帯域 出力信号 グラフ 高域 制御周期 φ バンドパスフィルタ 合計値 充電電力 負荷変動 前記蓄電池制御ステップ 周波数 前記負荷電力取得ステップ ピーク電力削減効果 線形補間 ピーク電力 上下限周波数 ｔ マイクログリッド発電 発電電力等 数 位相差特性 半周期分 機能 システム 充電電力量 上記気象情報取得部 － 初期値 ｋＷ ｋＷｈ ブロック図 正弦波 加算器 記録媒体 電力需要 売買電力 リアルタイム制御 プログラム 買電電力 高域側 実数部Ｒ 虚数部Ｉ 振幅 ｒａｄ ゲイン 範囲 電池残量 ～φ 合計 湿度 関係 ゲイン特性 前記位相差 充放電電力量 制御系 リアルタイム 特許文献 リアルタイムコントローラ 充電電池容量 上記 Ｘ 類似 Ｈｚ 基本周波数ｆ ナイキスト周波数ｆ 発電量 前記買電電力 説明 ｃｏｓ 湿度等 当該気象情報 周期 サンプル数Ｎ 構成 天気 所定 推移予想グラフ 算出方法 推移グラフ 気象情報提供サーバ等 半周期 ｘ 離散フーリエ変換 使用範囲 課題 アルゴリズム 温度 ローパスフィルタ ｚ 等 太陽光発電等 相関関係 ～ 斜線部分 ｄＢ 風力発電等 コンピュータシステム内部 形態 ｃｏｓφ 放電期間 図面 ハイパスフィルタ ｙ インターネット 図示半周期分 太陽光発電装置 特性 ピーク 充電期間 コンピュータ 電話回線等 ハードディスク等 インターネット等 設計等 ＣＤ－ＲＯＭ等 降水確率等 周辺機器等 先行技術文献 技術分野 背景技術 －π 通信回線 目的 最大 積分 上限 下限 上述 外部 お互い 気温 時刻 ＳＯＣ 処理 ＲＯＭ サーバ 理想特性 範囲外 フィルタリング処理 ｋｗ 効果 算出 Ｎ ｉ π サンプリング間隔Δｔ 縦軸 横軸 ｘ軸 上記プログラム 光磁気ディスク 揮発性メモリ °（π 記憶装置 通信線 特開 号公報 図示 詳細説明 ハードウエア 参照 概要 事情 手段 データベース 土日 祝日 対応 履歴 マイナス 否 判定 天候 経過 シミュレーション 推定 電源 負 ｓ ｓｅｃ ）≒ イ 通り 特定 波形 ↑－↑ （－ エネルギー ロ ニ 分母 ハ ホ ｍ ＯＳ フレキシブルディスク 搬媒体 ネットワーク クライアント 一定 組み合わせ ＦＰＧＡ Field Programmable Gate Array プログラマブルロジックデバイス 要旨 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 10.9315 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ベントナイト成形体 乾燥 ベントナイト 温度 乾燥方法 乾燥密度 実施形態 処分坑道 成形 廃棄物埋設処分施設 乾燥温度 乾燥開始 押し出し造粒工程 乾燥完了 乾燥工程 状態 状態変化 粒径 水分低下 発明 送風温度 放射性廃棄物 湿潤状態 円筒容器 水分低下率 図 乾燥処理 廃棄体 特許文献 粉体 乾燥経過 密度 水 転動造粒工程 送風式回転乾燥機 動造粒機 一定 略球形 製造方法 所定 ベントナイト原鉱石 ベントナイト破砕材 材 孔径 造粒 動造粒工程 充填率 常温 密度低下 ℃ 乾燥収縮 押出孔 水層 球形 変化 押し出し造粒機 バリア性能 設定値 地下水 送風 ベントナイト成形体Ｏ 段階 空間 含水比 ～ 円柱状 号公報 経過 水分減少量 程度 単一粒径 処分孔 特許 一定温度 水分 地山 自由落下 密度増加 軸線Ｏ 関係 乾燥密度増加 廃棄物埋設処分施設Ａ 水分量 温度管理 ベントナイト粒状体 上記 説明 高温 主要坑道 ディスクダイ 乾燥炉 乾燥対象 充填 方法 形態 等方圧加圧処理 所定量 ℃）、乾燥完了 前記一定 水分低下曲線 地下深部 形状 水分変化 透水性 ベントナイトプレート ベントナイトボール 減少 製造工程 吸着性 信頼性 ガラス固化体 自然環境 ～数 発明者 多段階 空気 放射性物質 製造 軸線 ボーリング孔 回転速度 ｇ ｍ 割合 量 水みち 坑道構築 側部 方向 体積 特許出願 先行技術文献 板状 体積変化 空気量 略環状 空気層 製造コスト 技術分野 背景技術 レベル 参照 外部 本願 欠け － 課題 目的 手段 特性 後述 効率 ドラム 低温 任意 ℃→ 調節 他 地山内 金属板 環境 周辺地 出願人 周面 周方向 所定比率 透水係数 特開 特開平 金属メッシュ ガラス 炭素鋼 オーバーパック 材料 ロックボルト 粒子形状 コスト 上記事情 効果 フロー図 粒子 同形同大 飽和度 外側 内部空間 通過風速 側部外側 破線矢印 風速 ℃）、 ℃、 混合材料 作用効果 ℃、常温 作用 ℃程度 緩み 同等 膨潤性 侵入 膨潤 浸透 ベントナイトペレット 種 工 隙間 質量 施工 膨潤後 ＭＰａ 圧力 装置 概要 上昇 手法 特徴 図面 概念 一般 産地 粉々 落花生 水量 複数 周り Ｌ 径方向 突起 図示 等間隔 一端 他端 開口 ｒｐｍ ｓ 中間 ａ ｂ ｃ 高低 知見 もと 実験 所望 長期 趣旨 範囲 屋内 屋外 振動 場所 漸次 通り 状況 割れ 用途 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 10.9263 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 植栽基盤 通気孔 カバー部材 大気浄化システム 空気 座面 図 前記植栽基盤 植栽用 浄化 内側ケース 通気層 上板部 空気浄化 実施形態 側板部 前記座面 通気性 孔 部分 植物 部材 送風手段 前記カバー部材 通気経路 人 発明 土壌 ファン 上面 大気 浄化効率 浄化能力 ルーバー状 斜視図 開口部 室 側方 側面部 速度 同一面 特許文献 平面形状 ｂ 空気浄化用 土壌層 清浄 上方 口 灌水設備 風 形態 通気用 ベンチ 主室 複数 水 水タンク Ａ 前記通気層 前記通気孔 前記孔 通風用開口 汚染空気 排気ガス等 肌 流速 他 ～図 通気速度 形状 側面 構成 ネット状部材 周辺 sec 例 位置 ポンプ 下方 底面部 前記内側ケース 仕切り壁 等 香り成分 香り 底部 前記灌水設備 ネット状 排水孔 説明 機能 手段 一部分 程度 面積 ｃ 円形 容器状 マット状 スポンジ状 水平状 フェルト状材 上記実施形態 無機系土壌改良材等 平面図 断面図 有機系土壌改良材 自然土壌 III－III断面図 ファン等 セラミック等 コード等 水道等 ローズマリー等 先行技術文献 バーク堆肥等 パーライト等 排出口等 技術分野 背景技術 室内 屋外 自動車 目的 － 課題 コスト 騒音 過程 乾燥 低下 抵抗 周り 空間 上側 長方形 キャスター 上部 バッテリー 下 上端 外部 水分 側面部分 耐水性 繊維強化プラスチック 一定速度 上面部分 幹線道路 特開平 号公報 上記 設置 設置箇所 緑化植物 小型化 小型 概略構成 樹脂製 水性素材 電源 制御装置 商用電源 外周縁 状態 任意 保水状態 生長促進 風量 一定 任意曲線 箇所 フィルタ 脇 規模 概要 大型 重量 移動 通常 近傍 効果 図面 四隅 内部 配管 タイプ 下面 灌水用 駆動 代わり 別 水源 下部 流れ 最高 出力 隙間 材質 強度 湿気 ＦＲＰ 寸法 個数 種類 ラベンダー 周囲 育成 根 目 表面積 先 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 9.7256 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 結束具 鉄筋 結束 線材 鉄筋結束ヘッド 交差部 結束処理制御部 接着剤 実施形態 接着部材 配筋結束制御部 結束線 結束処理 鉄筋結束方法 接着剤吐出部 線材配置部 鉄筋結束装置 結束作業 吐出ヘッド移動部 配筋結束処理 配筋結束装置 壁筋 線材成形部 前記結束具 結束具配置部 結束ヘッド移動装置 中央部 図 先組鉄筋 線材切断部 接着面 交差部検知部 部 両端部 結束材 移動処理 吐出処理 前記交差部 ステップＳ 貫通孔 作業面側 線状 上記鉄筋結束方法 端部 位置 結束具配置手段 作業 鉄筋自動結束装置 上記 処理 切断具 作業面 接着 正面断面図 押圧部材 鉄筋交差部 特許文献 接着材料 配置処理 前記作業面側 逆Ｕ字形状 鉄筋メッシュ用結束機 吐出 点付け接着 面状 側 格子状 配置 前記 ｂ 説明図 面 上記鉄筋結束装置 略逆Ｕ字形状 成形処理 配筋 切断処理 鉄筋同士 判定処理 自動結束装置 前記接着材料 上記実施形態 上面図 説明 結束処理終了信号 ガイドアーム メッシュ材 結束線ガイド通路 装置 形状 隅肉接着 回転ホルダ 複数 ａ 成形 検知信号 発明 上側 鉄 ノズル 回転体 針金等 中央 形態 金属材 Ｕ字形状 前記面状 状態 － 自動 要部 処理手順 把持孔 効果 ｃ 部分 帯状 中心線 柱筋等 移動停止 吐出タイミング 一対 接着形成手段 前記貫通孔 Ｚ字形状 Ｌ字形状 凸形状 課題 指示信号 記載 巻 ２つ ｄ 両側 方向 正面図 構造物 特開平 号公報 中央位置 両端 技術 動作 １つ 構成 符号 位置センサ等 位置センサ 直線状 ロウ付け ブロック図 ～図 信号 材料 四角筒状 先行技術文献 把持体 反対側 片面側 上面 Ｚ形状 Ｌ形状 ワイヤ等 カッタ等 金具等 真空チャック等 技術分野 背景技術 接触位置 動体 手段 上記課題 範囲 所定 置部材 否 ＮＯ ＹＥＳ 終了 他 同一 代わり 個数 針金 突出形状 Π形状 揺動アーム 刃体 把持片 水平方向 樹脂系 ｘ方向 ｙ方向 合成樹脂 コンクリート打設時 液混合タイプ 状態下 自由度 効率 変更例 効率化 金具 中心 柔軟性 鉄筋コンクリート 参照 負荷 増加 軌跡 任意 本体 上下動 縁 プッシュバー 概要 機構 上述 目的 図面 流れ図 乾性 公知 量 低減 力 広がり 両方向 金 幅 材質 強度\n",
      "\n",
      "===== # 5, Topic : 14, p : 10.9066 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 制御用鉄筋 配筋用スペーサ 鉄筋 構造鉄筋 配筋 配筋方法 係止部 本体部 配筋作業 配筋工程 下段鉄筋 配筋用 コンクリート構造物 図 実施形態 鉤型片 鉄筋径 前記配筋用スペーサ 上段鉄筋 前記本体部 スペーサ 鉄筋用スペーサ 係止 前記鉄筋 端部 前記係止部 下段筋 発明 腐食性材料 正面図 上段筋 状態 床面 ひび割れ制御用鉄筋 隙間 前記構造鉄筋 特許文献 前記配筋工程 組立工程 可能性 鉄筋コンクリート構造物 先端同士 工程 制御 先端部 ａ ｂ 該配筋用スペーサ 前記実施形態 上下筋スペーサ 前記 所定 横 コンクリート 上段 縦断図 作業性 位置 一対 ｔ 反対側 組み立て作業 作業 前記鉤型片 上面 置工程 先端 同士 コンクリート部分 腐食性 等 材料 腐食防止 細径 モルタル部 課題 モルタル製 先端部同士 格子状 組立作業 作業員 上方 他方 樹脂製 形態 表層部 下段 型枠 号公報 柱状部材 部分 樹脂 部材 作業員等 施工性 耐久性 説明 目的 表面 置可能 障害 簡易 幅 面 鋼線加工部 側 鉤型片同士 前記課題 先行技術文献 Ｕ字状 ハ字状 断面図 斜視図 技術分野 背景技術 延長面 外周面 が床面 観点 損傷 上端 ブロック 下方 形状 底面 モルタル 間隔 開口側 中心側 下面側 床版 フック状 金属等 ずれ等 梁等 防止 実用新案登録 品質確保 特開 ずれ 錐台 下部分 地盤上等 設置状態 構成要素 上下 複数 － 概要 存在 手段 特徴 効果 図面 立方体 角柱 直方体 円柱 鉤型 外面 側面 二つ 下 両者 弧状 鋭角 参照 外側 空間 方向 ピッチ 同等 支障 自重 軸 ひび 前述 趣旨 範囲 変更 フーチング 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 9.7762 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : コンクリート打設 床スラブ 鉄骨躯体部分 外壁 階層 範囲Ｊ コンクリート床スラブ 上層部分 範囲 位置 複数階層 線Ｗ 鉄骨躯体 コンクリート外壁 躯体部分 取り合い部分 腰壁 複合構造建物 外端部側 図 型枠 下端部 状況 垂壁 施工状況等 部分 上層階側 基準部 Ｉ 建物中央部 基準線Ｋ 外端部 コンクリート 複数 ＩＩ 設対象 施工方法 鉄骨鉄筋コンクリート躯体 外部足場 建物 ｂ 上端部 床等 特徴構成 実施形態 基準線Ｋ側 形態 下層部分 建方 下層階側 コンクリート打設後 所望位置 ～ 鉄骨梁 上記実施形態 設対象階層 鉄骨柱 Ｊ 墨出 発明 下層階 前記鉄骨躯体部分 鉄骨鉄筋コンクリート躯体等 ａ 短縮化 夫 等 ～Ｗ 設起点階 下方側 縦断側面図 特許文献 コンクリート打設用 前記床スラブ 曲面状部分 設定距離Ｐ 設置作業等 平面視 形状 側面視 建物外周部 作業用床 枠組足場 足場板 コンクリート外壁等 工期 工事 養生期間 精度 順打ち ～（ＶＩＩＩ 前記複合構造建物 ｂ等 下方側部分 割出線 説明 形成 鉄骨梁等 割出線Ｗ 要部 設置状況 位置等 平面状部分 長期化 別 次 ＶＩＩＩ 距離 サポート 基準 前記外壁 屋上部分 建物外側 １つ下 支持部材 設置作業 課題 順序 点線 ２つ 上述 下方側部位 角材 上記特許文献 割り出し作業 角材等 施工過程 位置調整 先行技術文献 技術分野 背景技術 周囲 簡易 図面 実線 交互 間隔 ＩＩＩ 変更 ３つ 設置箇所 斜視図 １つ 免震装置 特開 号公報 割り出し 箇所 傾斜姿勢 同一数 数 同一 種 参照 － 概要 記載 虞 実情 手段 制限 効率 先 状態 矢印 下記 順番 準備 ＩＶ Ｖ ＶＩ ＶＩＩ 各部 上部 図示 経過 強度 各種 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 10.2412 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 免震装置 下側柱部 免震下部基礎 上側柱部 免震装置設置工事 コンクリート 免震構造物 上側部位 免震装置設置階 鉄柱部分 スラブ型枠 側 下側部位 コンクリート支持部 下側柱部鉄柱部分配設工事 下側柱部コンクリート打設工事 上側柱部配設工事 設置 免震下部基礎下側部位構築工事 免震上部基礎構築工事 ベース部 コンクリート打設工事 免震下部基礎上側部位構築工事 当該下側柱部 下部柱部 施工方法 コンクリート部分 コンクリート打設 実施形態 工事 免震装置設置面 スラブ構築用 柱部 下部フランジ 型枠 免震上部基礎 前記下側柱部 下側柱部配設工事 部位 前記免震装置 上部フランジ 前記免震下部基礎 前記上側柱部 設置階 免震装置下部基礎 複合構造 上方 特徴構成 中間免震構造物 上記実施形態 下部柱部コンクリート打設工事 図 ｂ 躯体工事 上層階 上端部 工程 配設 上記免震下部基礎下側部位構築工事 上端面 該免震下部基礎 構造 構築 上記下側柱部コンクリート打設工事 上端部側 上記免震装置設置工事 上記下側柱部鉄柱部分配設工事 上記免震下部基礎上側部位構築工事 上部柱部配設工事 ベースプレート コンクリート部位 設置面 図示省略 ｂ側 下端部側 上記上側柱部配設工事 免震装置固定用 前記 形態 コンクリート充填鋼管構造 段階 前記コンクリート支持部 Ｈ形鋼 コンクリート工事 ＣＦＴ構造 形鋼 参照 スラブ構築用コンクリート 床 実施 構造物 コンクリート支持用 ＳＲＣ造 発明 硬化部分 該免震装置 下層階 建物 鉄骨梁 当該スラブ型枠 前記スラブ型枠 状態 鉄骨造 当該鉄柱部分 構築手順 鉄骨鉄筋コンクリート造 設 上記 同時設置 中間階 工期 鉄筋 ａａ 側面型枠 特許文献 複数階 短縮 領域 周囲 下端面 階 ＣＦＴ構造等 施工 工法 ａ Ｓ造 当該工程 上述 下方 別 所定 残部 ａｂ ボルト ボルト挿通孔 工事内容 鉄筋コンクリート造 段階目 クロスＨ形鋼 当該鉄骨梁 施工状況等 具体的手順 説明 課題 技術 上面 省略 ナット 鋼管 同時 支承式 等 デッキプレート等 接合プレート ボルト等 ＲＣ造 当該床 先行技術文献 技術分野 背景技術 順 開始 起点 期間 完了 状況 位置 図面 アイソレータ ｃ 他 複合方式 鉛プラグ等 積層ゴム支承等 積層ゴム支承式 補強等 頭付きスタッド 長期化 省力化 振動周期 振動エネルギー 特開 号公報 断面形状 定着用材 分上方 水平度 収縮モルタル 接合 方式 － 概要 原因 実情 手段 部材 強度 ～ 効率 鋼材 ダンパー 上下 地震 記載 耐力且 剛性 各階 同等 重量 姿勢 調整 隙間 架設 一体 １つ 範囲 内部 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 11.9310 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 補強筋 補強筋素材 ＰＣ鋼線 スラブ筋 図 間隔保持筋 並行部 前記補強筋素材 補強筋素子 結束線 前記補強筋 スラブ 延長部 鉄筋 前記アンボンドＰＣ鋼線 間隔保持 開口部 ＰＣ鋼線緊張端部 アンボンドＰＣ鋼線 割裂防止用補強筋 結束筋 割裂防止用補強筋素材 間隔 所定間隔 前記延長部 補強部分 周辺固定スラブ 前記間隔保持筋 アンボンドＰＣ鋼線緊張端部 緊張端 溶接 固定端 位置保持 補強筋部分 部分拡大説明図 スターラップ 鉛直部 説明 前記結束線 端 補強部位 緊張端定着具 固定端定着具 発明 梁 要部 一対 前記補強筋素子 Ｕ字状 緊張端部 アンボンドＰＣ鋼線端部 アンボンドＰＣ鋼線緊張端部 前記 位置 溶接部 特許文献 夫 説明図 緊張 Ａ 割裂防止用補補強筋素材 形態 Ｄ 縦筋 上端筋 配筋 壁筋 コンクリートスラブ Ｕ字状鉄筋 前記特許文献 現場 実施 状態 梁側 安定化 割裂防止用 緊張端定着具側 技術 ～ Ｂ 寸法 割裂防止 立断面図 外観図 正面図 品質 Ｃ 全長 鉄骨梁 想像線 可能性 前記梁 位置ズレ 並行部同士 主筋 上下 方向 耐力 他方 周辺部分 向い合せ Ｅ 開放部 両端部 ＰＲＣスラブ 下側鉄筋部分 構造 固定力 請求項 固定手段 立体構造 上側鉄筋部分 構成素材 躯体内部 側面図 建物 互い 両側 性能 図上 当該部分 壁 前記主筋 前記上方 前記他方 床スラブ設置 鉄筋工 作業 平面図 下外側面 先行技術文献 横外側面 技術分野 背景技術 コンクリート 課題 手間 記載 周囲 相互 ずれ 一つ一つ 工場 剛性 変形 脱落 利用可能性 上記構成 現場作業 スポット溶接 コンクリート打設時等 簡素化 上記 部位 組み立て作業そのもの 上記目的 Ｄ同士 プレストレス 床 荷重耐力 荷重 特開平 号公報 問題点 目的 手段 上方 所期通り 図面 実施形態 添付図面 上下寸法 外側面 左右方向 組み付け 内部空間 周り － 概要 工程 不測 事態 斯 別 観点 特徴 効果 手法 向上 一体 紙面 ｍｍ 中心 距離 個所 省略 手数 俟 手順 矩形 産業 箇所 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 10.3756 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 箱抜き用装置 コンクリート 箱抜き方法 箱抜き 図 箱抜部材 路盤コンクリート 固定用穴 基端部 実施形態 断面図 先端部 型枠 箱抜き用 水平部材 固定部 基準面 枠 位置出し 棒 指標装置 ステップＳ 設置位置 取付金具 ベース部 位置 鉄筋 アンカーボルト 表面 端部 所謂箱抜き 斜視図 支持体 固定位置 抜き用装置 コンクリート打設時 発明 箱抜き箇所 棒状部 状態 柱部材 開口 穴 組立 仮設 底 変形例 断面形状 面 前記箱抜き用装置 インバートコンクリート 壁 収縮モルタル 前記箱抜部材 設備 平面図 設置 箱抜き設置工程 前記コンクリート 位置出し工程 水平部 固定 水和反応 充填工程 幅方向 ロックボルト 当該コンクリート 奥側 Ａ－Ａ断面図 充填材 ロックボルト用 方法 螺旋溝 円筒形 前記 形状 棒状部材 移動 ｂ 両端部 施工フロー図 拡大斜視図 テーパ形状 四角形 鉄筋組立工程 内壁面 特許文献 方向 変位 技術 箇所 プラスチックコーン 抜部材 部材 コンクリート打設工程 Ｌ字形状 位置出し箇所 形態 軸方向 長手方向 先端 面状 施工開始 説明 課題 型枠等 グラウト材 設置工程 該仮棒 長方形 土木工事 建築工事 水平方向 地墨 インバート部 環状部 水密性 組み立て状態 角度α 直線部分 工程 施工方法 前記充填工程 前記鉄筋組立工程 断面視円形状 設置作業 施工フロー 周面 － ガードレール 標識 浮沈 回転 浸入 周囲 設 測量 夫 棒状 構成 確認 説明図 固定力 鋼材 木材 該箱抜部材 モルタル Ｕ字形状 設時 Ｈ型鋼材 先行技術文献 技術分野 背景技術 空間 所定 経過 窓 出入口 緩衝 効果 図面 参照 円柱状 板状 上部 他 中心 内側 下端 開口側 底側 一対 反対側 段差 面積 地山 作業領域 木材等 特開平 号公報 上記課題 砕石表面 傾斜角 領域 角 トンネル 工 概要 複数 手段 圧後 地盤 凹部 寸法 例示 態様 径 ナット 間隔 下部 点線 Ｃ 他方 内径 一端 番線 針金 解体 強度 上端 量 符号 ・・・インバートコンクリート ・・・アンカーボルト\n",
      "\n",
      "===== # 10, Topic : 14, p : 9.2190 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 棒状繊維 リング リング状繊維 繊維 前記リング状繊維 繊維補強コンクリート 繊維補強材 前記棒状繊維 コンクリート 鋼繊維 リング状 補強用鋼繊維 じん性試験 ＰＰ繊維 請求項 混入量 補強効果 図 前記特許文献 強度 コンクリート補強用鋼繊維 リング形状 特許文献 コンクリート標準示方書 mm 前記繊維補強材 合成樹脂繊維 じん性 直線状鋼繊維 実施例 炭素繊維 合成繊維 オレフィン系樹脂 鋼製 延伸繊維 ガラス繊維 アラミド繊維 バサルト繊維 直線状 ～ 上記請求項 突起部 添加量 補強繊維材 リングサイズφ 前記 発明 真直部 コンクリート供試体 ひび割れ幅 Vol 該補強用鋼繊維 前記リング状 PP繊維 波形状部 前記鋼繊維 形状特性 NEXCO基準 部材幅ｂ 拘束効果 コンクリート補強材 スライス状 外径Ｄo 該コンクリート 長手方向 作業員 表 号公報 圧縮強度試験 前記真直部 形状 繊維端部 前記合成繊維 内径Ｄi 特開 ポリビニルアルコール樹脂 防水シート 厚みｔ ステンレス鋼 セメント系成形体用補強 両端部 強度性能 波形状 記載 部材幅 部材端 特性 ひび割れ初期段階 波形部 線材 部材 繊維投入 繊維同士 繊維表面 有機繊維 試験 下記特許文献 外径 周方向 方向 商標名 断面形状 管材 じん性性能基準 ポリプロピレン樹脂 鉄製リング 強度低下 前記規定 欠点 】( ケース 初期段階 骨材径以上 炭素鋼 合成樹脂 供試体寸法 ひび割れ セメント補強用ポリオレフィン 合成値 該略四角形 所定 － 怪我 JSCE ファイバーボール mm× 相乗効果 相互 骨材径 ＳＳ鋼 ＳＭ鋼 ポリプロピレン コンクリートｍ コンクリート体積 コンクリートリート ベースコンクリート 素材 凹部 剛性 利点 単体 フック形状部 フィン系樹脂 厚み 骨材 φ じん性性能 強度特性 内径寸法 課題 曲線 性能 効果 ポリオレフィン樹脂 中央部分 該繊維 間隔 G 破損 種類 最大 吹付 シムロック コスト 供試体 端部側 該突起部 凸部 コア圧縮強度試験 問題点 破断面 両端部付近 長手方向両端部近傍 実施 例 等量混合 トンネル施工管理要領](コンクリート標準示方書 強度増加 Ｘ断面 ひび割れ抵抗性 施工性確認試験 説明 主成分 一定 組み合わせ 環状 割合 添加 配合 dtex 合成曲線 特徴 内径 断面 公報 合成曲線 先行技術文献 靭性向上効果 抵抗力 混入率 略Ｘ形 斜視図 分散性 該略Ｘ形 技術分野 背景技術 上記課題 上記条件 相乗的効果 ポリビニルアルコール 該 略多角形 お互い 径方向 図面 形態 ｄ 断面積 作業機械 未満 後述 差分 F 表記 概念斜視図 特開平 相互的補完効果 経済的効果 正面図 側面図 特殊界面活性剤 付着力増強処理 寸法 下記 中央寄り 横断面 略四角形 特徴点 周辺部分 模擬トンネル 平均値 フック等 膨張剤 コア抜き 剪断強度 当り 材料費 システム 製造方法 出願人 立米当り 凹及び 凸 ノズルシステム プレーンコンクリート 種々 夫 波高 先端 凹部又 概要 インデント ワーカビリティ JHS 準拠 vol 他方 手段 材質 薄板 詳説 A B 板 鋼材 一般 願人 表面積 密度 mg 嵩 酸 アルカリ 錆 kg 所望 荷重 作用 通常 加工 引き抜き ダイレタンシー 挙動 弱点 発生 程度 工場 機材 通り N 併用 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9892e-41, 4.4228e-32, 4.0360e-40, 1.2117e-30, 0.0000e+00,\n",
      "         1.7264e-42, 8.0344e-24, 4.1532e-22, 6.4007e-38, 0.0000e+00, 1.1559e-41,\n",
      "         7.5967e-15, 9.8091e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00032\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1041.9314\n",
      "Test epoch : 1 Average loss: 929.7918\n",
      "PP(train) = 3935.589, PP(valid) = 3788.867\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.8024e-41, 3.7490e-30, 1.3372e-25, 3.9327e-23, 9.0396e-40,\n",
      "         4.4001e-43, 1.3721e-23, 4.8622e-14, 9.9764e-33, 0.0000e+00, 4.8181e-32,\n",
      "         3.8118e-12, 4.6954e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00029\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1041.4239\n",
      "Test epoch : 2 Average loss: 929.4841\n",
      "PP(train) = 3924.255, PP(valid) = 3779.834\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4013e-45, 1.7698e-21, 5.2700e-25, 1.9406e-23, 6.1657e-44,\n",
      "         2.0630e-41, 3.6380e-18, 7.8626e-16, 2.8107e-35, 1.1925e-42, 1.0537e-40,\n",
      "         2.6865e-24, 6.2912e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00026\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1041.0221\n",
      "Test epoch : 3 Average loss: 929.1868\n",
      "PP(train) = 3912.044, PP(valid) = 3770.614\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9951e-35, 7.4822e-37, 9.7493e-20, 4.9117e-12, 2.7322e-13, 1.0336e-36,\n",
      "         2.8466e-36, 2.5026e-13, 9.9823e-01, 1.1226e-28, 1.6384e-33, 3.6501e-27,\n",
      "         1.1004e-13, 5.1968e-25, 1.7653e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0443, 0.0628, 0.0304, 0.0760, 0.0630, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00023\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1040.6924\n",
      "Test epoch : 4 Average loss: 928.8858\n",
      "PP(train) = 3898.569, PP(valid) = 3760.522\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.4078e-44, 2.7750e-24, 2.4614e-34, 1.3489e-23, 1.4705e-37,\n",
      "         0.0000e+00, 1.2127e-22, 7.4519e-27, 5.9412e-38, 0.0000e+00, 2.3455e-33,\n",
      "         1.1436e-26, 1.0225e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00021\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1040.2175\n",
      "Test epoch : 5 Average loss: 928.5874\n",
      "PP(train) = 3884.433, PP(valid) = 3749.986\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.1657e-44, 2.1168e-28, 1.8608e-36, 1.0792e-15, 3.9797e-43,\n",
      "         1.6792e-39, 1.1612e-17, 1.8725e-19, 3.9626e-31, 4.1338e-42, 7.4633e-42,\n",
      "         1.5524e-20, 1.7942e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00019\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1039.7884\n",
      "Test epoch : 6 Average loss: 928.2911\n",
      "PP(train) = 3870.422, PP(valid) = 3739.658\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.4360e-38, 5.2885e-18, 6.3188e-29, 1.3752e-28, 2.5927e-34,\n",
      "         0.0000e+00, 1.1398e-15, 2.3593e-25, 2.2329e-22, 5.6552e-41, 2.4092e-39,\n",
      "         2.7185e-29, 8.3167e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00017\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1039.4277\n",
      "Test epoch : 7 Average loss: 927.9920\n",
      "PP(train) = 3856.752, PP(valid) = 3729.618\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7980e-42, 4.3197e-34, 7.3084e-29, 9.1478e-20, 4.2263e-14, 1.0213e-36,\n",
      "         5.4455e-37, 6.4231e-17, 2.9077e-14, 5.3627e-21, 8.4078e-45, 3.4301e-37,\n",
      "         1.1842e-19, 8.0234e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00015\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1039.0652\n",
      "Test epoch : 8 Average loss: 927.6932\n",
      "PP(train) = 3843.354, PP(valid) = 3719.796\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1862e-37, 4.0150e-20, 2.3125e-24, 4.5197e-06, 9.0353e-37,\n",
      "         7.7071e-44, 5.7722e-17, 4.2877e-17, 8.2691e-26, 3.3458e-27, 1.8376e-32,\n",
      "         1.7199e-20, 3.0703e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00014\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1038.5602\n",
      "Test epoch : 9 Average loss: 927.3976\n",
      "PP(train) = 3830.047, PP(valid) = 3710.078\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5929e-30, 1.1435e-29, 1.5291e-21, 6.0954e-10, 6.9193e-40,\n",
      "         0.0000e+00, 6.3288e-15, 2.2267e-21, 5.2592e-32, 8.4078e-45, 3.0972e-39,\n",
      "         2.0055e-24, 1.3284e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00013\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1038.0093\n",
      "Test epoch : 10 Average loss: 927.0997\n",
      "PP(train) = 3816.844, PP(valid) = 3700.424\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.4454e-35, 3.3579e-23, 2.1717e-35, 6.2014e-16, 1.1859e-41,\n",
      "         1.2996e-41, 6.1572e-19, 1.0344e-15, 2.2201e-32, 3.7604e-39, 1.9315e-27,\n",
      "         2.1426e-20, 8.8503e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00011\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1037.6292\n",
      "Test epoch : 11 Average loss: 926.8069\n",
      "PP(train) = 3803.562, PP(valid) = 3690.669\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0065e-45, 2.0319e-42, 2.0484e-35, 1.1385e-31, 0.0000e+00,\n",
      "         0.0000e+00, 1.5860e-40, 4.2693e-23, 2.2188e-41, 0.0000e+00, 2.6576e-41,\n",
      "         9.2282e-31, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00010\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1037.1755\n",
      "Test epoch : 12 Average loss: 926.5115\n",
      "PP(train) = 3790.710, PP(valid) = 3681.241\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.6188e-37, 1.0167e-17, 7.7000e-24, 8.7755e-23, 0.0000e+00,\n",
      "         6.6141e-43, 1.1068e-21, 5.0815e-21, 4.2904e-18, 1.5022e-42, 1.3342e-30,\n",
      "         1.7079e-20, 1.8793e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00009\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1036.7533\n",
      "Test epoch : 13 Average loss: 926.2178\n",
      "PP(train) = 3778.061, PP(valid) = 3671.914\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7252e-40, 1.5199e-28, 5.7733e-43, 8.6748e-23, 0.0000e+00,\n",
      "         8.4078e-45, 4.0335e-14, 1.4974e-21, 2.0879e-43, 1.4013e-45, 0.0000e+00,\n",
      "         1.9906e-31, 2.9130e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00008\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1036.2821\n",
      "Test epoch : 14 Average loss: 925.9245\n",
      "PP(train) = 3765.432, PP(valid) = 3662.657\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 3.4663e-20, 2.4264e-31, 1.1888e-19, 7.7071e-44,\n",
      "         4.2039e-45, 4.7179e-22, 5.5090e-16, 1.0291e-22, 0.0000e+00, 7.2559e-42,\n",
      "         2.0113e-21, 3.9164e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00008\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1035.8071\n",
      "Test epoch : 15 Average loss: 925.6335\n",
      "PP(train) = 3752.777, PP(valid) = 3653.287\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.2009e-35, 1.4682e-33, 7.6339e-37, 2.8542e-25, 4.7632e-41,\n",
      "         9.8091e-45, 1.1157e-14, 1.1771e-22, 1.0681e-29, 0.0000e+00, 8.6372e-41,\n",
      "         4.5846e-26, 2.2378e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00007\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1035.4461\n",
      "Test epoch : 16 Average loss: 925.3482\n",
      "PP(train) = 3739.879, PP(valid) = 3643.701\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2521e-39, 8.3381e-18, 9.8339e-23, 7.8018e-30, 1.3333e-16, 4.6383e-43,\n",
      "         1.3499e-34, 5.7112e-23, 1.4306e-07, 2.9302e-15, 1.5013e-37, 5.8642e-32,\n",
      "         1.4103e-16, 2.3035e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00006\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1035.1437\n",
      "Test epoch : 17 Average loss: 925.0660\n",
      "PP(train) = 3727.168, PP(valid) = 3634.199\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.6291e-41, 2.9050e-32, 2.3247e-14, 3.7318e-25, 1.9150e-09, 2.0522e-28,\n",
      "         2.6927e-40, 1.4230e-05, 4.9876e-12, 3.4145e-27, 4.3100e-40, 9.6511e-28,\n",
      "         6.2119e-19, 9.9886e-21, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00006\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1034.6375\n",
      "Test epoch : 18 Average loss: 924.7853\n",
      "PP(train) = 3714.812, PP(valid) = 3625.040\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7733e-30, 4.5902e-18, 1.9734e-23, 2.1105e-15, 4.0397e-39,\n",
      "         2.7185e-43, 6.8328e-06, 4.2086e-15, 1.6341e-29, 2.8713e-42, 5.6377e-24,\n",
      "         1.3539e-18, 1.5287e-23, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00005\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1034.2071\n",
      "Test epoch : 19 Average loss: 924.4982\n",
      "PP(train) = 3702.913, PP(valid) = 3616.269\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3053e-37, 1.3047e-24, 7.5125e-24, 3.5262e-12, 1.8606e-36,\n",
      "         0.0000e+00, 2.0331e-14, 4.5648e-33, 1.4578e-28, 7.5957e-33, 1.2892e-43,\n",
      "         2.3588e-24, 1.1387e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1033.8089\n",
      "Test epoch : 20 Average loss: 924.2128\n",
      "PP(train) = 3690.986, PP(valid) = 3607.372\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.8858e-28, 1.2969e-28, 2.2186e-19, 2.7381e-28, 0.0000e+00,\n",
      "         1.2250e-28, 1.1078e-17, 1.8015e-09, 3.0441e-27, 1.2212e-35, 1.1551e-27,\n",
      "         2.6174e-25, 8.4808e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1033.4492\n",
      "Test epoch : 21 Average loss: 923.9335\n",
      "PP(train) = 3678.823, PP(valid) = 3598.284\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.6690e-43, 6.3278e-31, 1.7718e-22, 6.7068e-25, 2.9377e-20, 3.3481e-38,\n",
      "         3.2230e-44, 1.2831e-25, 1.0251e-13, 7.8380e-21, 3.5453e-42, 1.9388e-33,\n",
      "         2.2882e-13, 2.7672e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00004\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1033.0264\n",
      "Test epoch : 22 Average loss: 923.6542\n",
      "PP(train) = 3666.753, PP(valid) = 3589.178\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.4460e-44, 3.9259e-29, 2.5697e-33, 3.5369e-31, 0.0000e+00,\n",
      "         0.0000e+00, 5.3923e-29, 7.3787e-18, 1.3730e-36, 0.0000e+00, 0.0000e+00,\n",
      "         4.7954e-28, 1.5701e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.999, l1 strength = 0.00003\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1032.5360\n",
      "Test epoch : 23 Average loss: 923.3761\n",
      "PP(train) = 3654.921, PP(valid) = 3580.337\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6044e-38, 3.2006e-38, 5.5760e-23, 6.0582e-20, 3.1194e-06, 2.8306e-43,\n",
      "         2.6070e-36, 2.7871e-19, 6.2949e-11, 2.0808e-32, 1.9618e-44, 7.9889e-28,\n",
      "         3.9552e-10, 1.1569e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00003\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1032.1629\n",
      "Test epoch : 24 Average loss: 923.0962\n",
      "PP(train) = 3643.381, PP(valid) = 3571.695\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 2.2103e-34, 5.2749e-19, 9.2388e-28, 1.9231e-25, 1.9980e-35,\n",
      "         1.1092e-40, 3.1539e-08, 6.0021e-17, 5.9420e-25, 2.5010e-40, 9.7380e-34,\n",
      "         1.2399e-19, 3.1438e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00003\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1031.7523\n",
      "Test epoch : 25 Average loss: 922.8180\n",
      "PP(train) = 3631.936, PP(valid) = 3563.156\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2509e-39, 6.2259e-35, 1.0905e-18, 1.6930e-30, 1.0000e+00, 7.9061e-28,\n",
      "         1.2618e-28, 3.9244e-12, 7.7060e-19, 6.8859e-21, 1.8913e-40, 2.6097e-28,\n",
      "         4.2495e-21, 1.9486e-16, 1.5507e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1031.4475\n",
      "Test epoch : 26 Average loss: 922.5453\n",
      "PP(train) = 3619.986, PP(valid) = 3554.088\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0065e-45, 1.9315e-31, 1.9483e-24, 9.7223e-15, 0.0000e+00,\n",
      "         3.2048e-37, 7.5434e-22, 1.2069e-11, 2.5447e-31, 7.3916e-21, 5.7224e-40,\n",
      "         3.1519e-19, 3.8226e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1030.9662\n",
      "Test epoch : 27 Average loss: 922.2758\n",
      "PP(train) = 3608.284, PP(valid) = 3545.249\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1420e-38, 1.6782e-28, 2.8908e-34, 1.1889e-25, 7.9033e-43,\n",
      "         0.0000e+00, 3.9281e-11, 1.1491e-16, 1.7519e-28, 6.4460e-44, 2.6402e-22,\n",
      "         1.3944e-16, 8.3083e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1030.5502\n",
      "Test epoch : 28 Average loss: 922.0011\n",
      "PP(train) = 3597.125, PP(valid) = 3536.901\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3028e-39, 3.7145e-33, 1.5389e-39, 1.2391e-25, 0.0000e+00,\n",
      "         6.2918e-43, 5.8260e-18, 6.6495e-18, 3.1093e-32, 0.0000e+00, 9.5798e-41,\n",
      "         8.5375e-34, 3.5159e-41, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1030.0837\n",
      "Test epoch : 29 Average loss: 921.7259\n",
      "PP(train) = 3586.188, PP(valid) = 3528.728\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4837e-41, 1.5171e-23, 3.9778e-32, 1.5291e-26, 0.0000e+00,\n",
      "         1.1210e-44, 9.0632e-16, 2.5725e-30, 1.0995e-33, 0.0000e+00, 4.2577e-39,\n",
      "         3.1969e-36, 2.8708e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00002\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1029.7048\n",
      "Test epoch : 30 Average loss: 921.4543\n",
      "PP(train) = 3575.073, PP(valid) = 3520.311\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5988e-41, 1.3823e-21, 3.6209e-19, 9.9156e-13, 2.0267e-34,\n",
      "         0.0000e+00, 7.9148e-23, 1.2833e-17, 1.7721e-19, 1.1279e-38, 9.6184e-34,\n",
      "         4.3247e-24, 4.1030e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1029.3659\n",
      "Test epoch : 31 Average loss: 921.1839\n",
      "PP(train) = 3563.975, PP(valid) = 3511.914\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2017e-27, 3.7461e-17, 1.7941e-23, 9.9959e-01, 2.6139e-33,\n",
      "         8.8133e-26, 5.1815e-15, 9.2255e-09, 2.7113e-19, 5.6906e-39, 1.0630e-25,\n",
      "         1.8463e-04, 2.3095e-25, 2.2062e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1028.9877\n",
      "Test epoch : 32 Average loss: 920.9174\n",
      "PP(train) = 3552.858, PP(valid) = 3503.491\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.9177e-42, 8.5083e-38, 2.4409e-19, 5.6910e-20, 1.5320e-12, 3.6851e-35,\n",
      "         6.7350e-40, 1.4914e-22, 2.4404e-08, 1.8282e-30, 1.6602e-37, 3.0375e-32,\n",
      "         9.9118e-10, 7.0542e-15, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1028.6048\n",
      "Test epoch : 33 Average loss: 920.6554\n",
      "PP(train) = 3541.812, PP(valid) = 3495.105\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5394e-33, 7.6681e-29, 1.3799e-14, 7.4388e-19, 1.0763e-19, 3.2381e-40,\n",
      "         2.1145e-36, 7.3598e-10, 1.6943e-17, 1.4018e-16, 1.9842e-42, 2.3649e-23,\n",
      "         2.6673e-15, 1.8243e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1028.2759\n",
      "Test epoch : 34 Average loss: 920.3937\n",
      "PP(train) = 3530.784, PP(valid) = 3486.752\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2738e-42, 6.9899e-39, 8.9185e-25, 7.3727e-22, 4.7953e-16, 6.2278e-29,\n",
      "         1.1066e-37, 1.6998e-14, 3.8269e-10, 4.4388e-22, 8.4498e-43, 9.7053e-28,\n",
      "         5.0626e-17, 3.2229e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1027.8872\n",
      "Test epoch : 35 Average loss: 920.1308\n",
      "PP(train) = 3519.871, PP(valid) = 3478.402\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9045e-44, 1.1943e-34, 3.3208e-15, 9.4981e-35, 5.6830e-21, 6.4674e-35,\n",
      "         1.2934e-40, 4.0212e-22, 3.0358e-14, 1.9541e-30, 3.7703e-40, 2.5937e-33,\n",
      "         2.9280e-14, 7.0506e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1027.4861\n",
      "Test epoch : 36 Average loss: 919.8657\n",
      "PP(train) = 3509.413, PP(valid) = 3470.443\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9071e-39, 1.8543e-35, 1.6917e-30, 4.5374e-24, 2.9021e-16, 5.4088e-29,\n",
      "         3.9584e-41, 9.9991e-01, 5.8249e-11, 1.6776e-17, 2.9938e-33, 1.0654e-34,\n",
      "         1.6601e-16, 3.8835e-13, 8.8947e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1027.1371\n",
      "Test epoch : 37 Average loss: 919.5995\n",
      "PP(train) = 3499.137, PP(valid) = 3462.698\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6621e-36, 4.8061e-25, 8.8855e-20, 5.2868e-12, 2.5171e-34,\n",
      "         0.0000e+00, 2.3175e-22, 5.6858e-10, 9.1217e-20, 8.5122e-41, 1.0662e-28,\n",
      "         3.9953e-07, 4.0747e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1026.6009\n",
      "Test epoch : 38 Average loss: 919.3345\n",
      "PP(train) = 3488.801, PP(valid) = 3454.847\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.3970e-40, 3.7031e-29, 8.6180e-23, 3.0342e-11, 1.6672e-34,\n",
      "         6.4889e-33, 1.5470e-22, 2.5924e-12, 9.8353e-22, 8.1861e-36, 4.1652e-27,\n",
      "         5.3968e-15, 2.4610e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1026.3690\n",
      "Test epoch : 39 Average loss: 919.0751\n",
      "PP(train) = 3478.293, PP(valid) = 3446.843\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2113e-34, 1.3200e-11, 1.2725e-13, 1.1206e-21, 2.0706e-38,\n",
      "         1.4013e-45, 7.9079e-26, 9.1631e-17, 6.4971e-35, 9.8091e-45, 5.5004e-39,\n",
      "         7.2803e-19, 1.7275e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1025.8950\n",
      "Test epoch : 40 Average loss: 918.8180\n",
      "PP(train) = 3467.883, PP(valid) = 3438.884\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4060e-42, 2.2647e-38, 2.1524e-17, 2.1457e-31, 2.1040e-23, 1.2587e-39,\n",
      "         2.4991e-33, 1.5628e-16, 1.6215e-09, 2.2033e-11, 1.7473e-34, 5.6803e-35,\n",
      "         4.1223e-08, 1.4562e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00001\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1025.5702\n",
      "Test epoch : 41 Average loss: 918.5581\n",
      "PP(train) = 3457.632, PP(valid) = 3431.042\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0930e-42, 7.9971e-28, 1.2576e-14, 1.0096e-21, 3.3511e-15, 5.7505e-40,\n",
      "         1.1438e-38, 1.2036e-18, 4.2224e-09, 2.3898e-29, 6.3364e-30, 2.3020e-34,\n",
      "         4.5647e-11, 1.2732e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1025.2033\n",
      "Test epoch : 42 Average loss: 918.3032\n",
      "PP(train) = 3447.484, PP(valid) = 3423.355\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0787e-38, 2.5787e-12, 1.0889e-21, 2.2718e-13, 3.3679e-33,\n",
      "         3.5873e-42, 3.2165e-10, 5.2431e-09, 9.8481e-18, 1.4013e-44, 8.1973e-36,\n",
      "         3.9969e-08, 3.9682e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1024.7328\n",
      "Test epoch : 43 Average loss: 918.0472\n",
      "PP(train) = 3437.304, PP(valid) = 3415.528\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.3387e-38, 3.0025e-26, 9.1454e-41, 2.3817e-21, 0.0000e+00,\n",
      "         2.8026e-44, 3.6047e-25, 3.4873e-18, 3.2140e-27, 7.5328e-41, 0.0000e+00,\n",
      "         7.9193e-22, 1.0576e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1024.4031\n",
      "Test epoch : 44 Average loss: 917.7923\n",
      "PP(train) = 3427.258, PP(valid) = 3407.853\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7930e-41, 4.1124e-18, 3.2221e-20, 1.1264e-17, 3.3975e-35,\n",
      "         1.3048e-40, 5.2794e-10, 2.4087e-08, 1.2418e-33, 1.7376e-42, 6.6922e-40,\n",
      "         2.1349e-16, 2.5559e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1024.0390\n",
      "Test epoch : 45 Average loss: 917.5399\n",
      "PP(train) = 3417.271, PP(valid) = 3400.191\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9703e-40, 1.7497e-39, 7.8808e-21, 3.0422e-17, 1.2711e-17, 2.9944e-30,\n",
      "         1.0426e-31, 8.9956e-08, 9.9003e-05, 3.1651e-21, 2.6208e-30, 4.8109e-22,\n",
      "         1.7144e-05, 2.2333e-17, 9.9988e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1023.7344\n",
      "Test epoch : 46 Average loss: 917.2886\n",
      "PP(train) = 3407.281, PP(valid) = 3392.512\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8026e-45, 1.7391e-34, 5.9839e-20, 1.5669e-25, 1.6452e-12, 6.6387e-37,\n",
      "         1.3722e-25, 1.0046e-07, 4.7621e-21, 1.0091e-20, 1.0981e-40, 6.1380e-28,\n",
      "         5.8327e-16, 7.9695e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1023.4287\n",
      "Test epoch : 47 Average loss: 917.0410\n",
      "PP(train) = 3397.410, PP(valid) = 3384.972\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2579e-41, 4.8004e-27, 1.9101e-24, 2.0091e-19, 1.2612e-44,\n",
      "         3.7415e-43, 6.3146e-18, 9.0619e-16, 2.0700e-35, 3.3080e-36, 2.1876e-40,\n",
      "         2.9742e-22, 5.9966e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1022.9427\n",
      "Test epoch : 48 Average loss: 916.7907\n",
      "PP(train) = 3387.688, PP(valid) = 3377.475\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 2.4442e-36, 1.8376e-20, 4.3157e-18, 4.7513e-21, 3.4919e-38,\n",
      "         5.2581e-40, 1.0015e-19, 2.3106e-02, 2.3516e-22, 5.2005e-40, 2.3277e-33,\n",
      "         4.1456e-17, 6.1730e-18, 9.7689e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0812, 0.0449, 0.1025, 0.0506, 0.0593, 0.0672, 0.1064, 0.0925, 0.0594,\n",
      "         0.0527, 0.0625, 0.0559, 0.0511, 0.0720, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1022.6478\n",
      "Test epoch : 49 Average loss: 916.5396\n",
      "PP(train) = 3378.229, PP(valid) = 3370.261\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7555e-37, 2.7725e-14, 1.0452e-24, 6.4228e-33, 0.0000e+00,\n",
      "         3.7835e-44, 3.1937e-19, 1.8160e-18, 1.1909e-30, 1.4013e-45, 6.1056e-33,\n",
      "         4.7109e-24, 3.0106e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1022.2208\n",
      "Test epoch : 50 Average loss: 916.2892\n",
      "PP(train) = 3368.847, PP(valid) = 3363.060\n",
      "Writing to ./topicwords/8-topwords_e50.txt\n",
      "Topic 0: ピストンロッド 定着構造 仕上げ材 Ｆ１ 要領 並行 表裏 送風 ｘ 粉砕\n",
      "Topic 1: 仕上げ材 ピストンロッド 定着構造 表裏 要領 並行 蒸気 斜め方向 Ｆ１ 粉砕\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 手段 形態 説明 発明 課題\n",
      "Topic 3: 物質 河川 日 特開昭 浄化方法 固形分 圧縮強度 駆動装置 ＦＲＰ 定着板\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 手段 形態 説明 課題\n",
      "Topic 5: ピストンロッド 定着構造 仕上げ材 表裏 要領 Ｆ１ 斜め方向 並行 継手板 送風\n",
      "Topic 6: ピストンロッド 定着構造 仕上げ材 並行 Ｆ１ 表裏 斜め方向 要領 白色 送風\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 手段 形態 説明 発明 課題\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 手段 形態\n",
      "Topic 9: ピストンロッド 定着構造 表裏 仕上げ材 斜め方向 並行 要領 Ｆ１ 蒸気 送風\n",
      "Topic 10: ピストンロッド 定着構造 仕上げ材 並行 Ｆ１ 要領 蒸気 送風 粉砕 モニタリング\n",
      "Topic 11: ピストンロッド 仕上げ材 定着構造 並行 要領 Ｆ１ 表裏 継手板 対角線 送風\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 手段 形態 説明 発明 課題\n",
      "Topic 13: ピストンロッド 定着構造 仕上げ材 並行 Ｆ１ 対角線 白色 粉砕 蒸気 送風\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 9.6188 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 外側足場 内側足場 足場 躯体 躯体Ｋ 昇降装置 型枠足場 構造物 内部空間 外側 内側 型枠 図 クライミング足場 コンクリート 外側型枠 連結部材 クライミングジャッキ クライミングロッド 発明 スリップフォーム工法 クライミング 躯 クライミング操作 施工法 枠 Ｋ 接部材 外側用 水平力 内側型枠 ｂ 車輪 内側用 特開平 号公報 鉛直荷重 状態 側面図 躯体表面 外側足場共通 外側部材 工法 発明クライミング足場 連結ビーム コストアップ 内側部材 装置 側 ｃ 外足場 内外側型枠 左側部分 右側部分 外側面 コンクリート壁 部材 内側面 外側共 説明 ー 相互 鉄筋コンクリート構造物 橋脚 共通 所定 位置 センターコア構造 施工 施工コスト 進退装置 橋脚Ｋ 課題 台数 力受け 操作 接 ブラケット 内外面 内外 コアー等 煙突等 円形等 前記問題点 技術分野 煙突 技術 所定位置 前記 断面形状 形状 建物 鉄筋 分割 手段 実施 形態 先端 外面 内面 態様 交互 四角形 効果 数 図面 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 10.1556 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : パイロット弁 開閉弁 パイロット流路 止め弁 作動油圧 作動室 作用室 弁座 導入流路 作動油 主弁体 主流路 弁体 弁本体 接続流路 止め弁体 実施形態 閉弁用流路 前記作動室 前記作用室 弁 用油圧ダンパ 前記パイロット流路 孔 連通流路 小径孔 図 流路 前記開閉弁 作用力 貫通孔 作用 環状溝 油圧回路図 前記閉弁用流路 摺動孔 底孔 圧力 作動 形電磁弁 応答性 前記導入流路 側 作用方向 前記パイロット弁 コイルばね 路 環状室 連通 接続孔 着座面 栓部材 開弁方向 径孔 励磁信号 方向 前記 開弁 制御回路 応答 シート部材 該パイロット弁 閉弁状態 圧力センサ 作動油圧力 付勢力 状態 前記主弁体 部材 リリーフ弁 発明 閉弁方向 該導入流路 実施 形態 低圧側 作動油圧力 他方 前記作動室側 拡大断面図 規制部材 ケース部材 振動外力 パイロット形 シリンダ 開弁状態 ばね付勢力 アキュムレータ スプール 挿入孔 技術 押さえ部材 摺動 ストロークセンサ 課題 ｂ 軸方向 頭部側 該作用室 振動 外力 対象物 開位置 説明 変位 流出 構成 一端 受圧面 断面図 貫通溝 円錐形状 有底孔 離座方向 着座方向 出力 背部 圧力均衡等 シリンダ本体 開状態 上昇等 構造物等 背部側 外周面 吐出 吸入 一対 向き 逆 ピストン 温度上昇 分 瞬時 上昇 閉作動 手段 両側 図面 閉位置 ソレノイド アーマチュア 高圧 効果 破損等 周囲温度 技術分野 基台 減衰 周囲 減衰特性 構造 外周 地震 風 気温 環境 低下 次 遮断 特徴 該弁座 流入 番号 中心 螺 圧入 移動 ２つ 径方向 鉄心 他 所定 値 閉弁 かかわり 例 本題 要旨 範囲 態様 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 9.7861 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 硬化材 先端部材 硬化材混入 硬化材混入工法 硬化材混入層 注入管 硬化材混入装置 四角柱状硬化材混入層 硬化材注入 四角柱状硬化材混入工法 噴射孔 円柱状硬化材注入工法 図 回動 四角柱状硬化材混入層Ｖ 下側 噴射 発明 円柱状硬化材注入層 流体 土砂 硬化材注入層 先端 孔 地盤 縦断面図 硬化材注入工法 制御板 行い硬化材混入 ノズル 行い硬化材注入 高圧 円柱状硬化材混入工法 圧縮空気 四角柱状硬化材注入工法 高圧水 先端両側 工法 垂下状 効率 壁体 部分 流量 状態 貯留槽 上側 側面 圧力 例 揺動 連壁 油圧シリンダ 円柱状混入工法 一定 地中 壁 噴出 下端 上方 入管 影響 前記先端部材 左右 前記注入管 手段 変動 水平状 斜視図 施工 長方形状 斜め状 上記 両側 土 軟弱地盤 対象地盤 切削跡 実施例 床版 セメントミルク 切削 水平壁体 止水壁 壁厚 地中壁 垂直壁体 説明 下方 アスファルト ｃｍ 程度 複数個 効果 距離 噴射効率 ＣＣＰ工法 ＪＳＧ工法 ＣＪＧ工法 上下側 方向 噴射順序 ノズル孔 噴出孔 移動方向 廃泥 一定方向 制御盤 場所 課題 図面 ～ 所要 ステップ Ｖ 方法 チェーンソー ワイヤー 数 筒体 上下面 接線方向 左右方向 軸方向 廃泥液 地下駐車場 実施 前記 地下室 地下ダム 水平状態 図示例 外周面 周面 移動速度 移動区間 切断跡 施工効率 一定流量 技術分野 構築物 基礎 地山 技術 円弧部分 基礎工事 隣接地域 基端 ボーリングマシン 到達距離 平面視 箇所 遠隔操作 切断 特定箇所 流動速 付近土砂 連結部 ユニバーサルジョイント 支保 目的 内外 中心 放射状 住宅 態様 アースオーガー 深度 軟岩 終了 単独 角度 エスカレータ 片側 ソイルカッター アースカッター 等 地上 位置 機構 関係 適用 加工 間隔 増加 複数 面積 差 排出 形成 上述 開口 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 9.9735 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 掘削土砂 掘削土砂搬出施設 スラリー処理施設 泥量 泥管 土砂 排泥管 排泥量 掘削 掘削土砂量 スラリー 泥ポンプ 排出管 分岐排出管 排泥ポンプ 泥水 量 搬送掘削土砂量 搬出量 スラリー管理 切替装置 掘削土砂管理システム 搬出 泥水量 前記 搬送土砂量制御手段 処理 スラリー量 スラリー管理システム 計量掘削土砂量 掘削土砂管理方法 スラリー返送量制御装置 処理施設 搬送土砂量制御装置 泥水供給量制御装置 残存処理能力 運行管理装置 排泥量ＱＤ スラリー返送量制御手段 処理量 運転制御施設 泥水供給量制御手段 前記搬送掘削土砂量 掘削土砂管理 前記計量掘削土砂量 運搬装置 圧送ポンプ 切羽Ｈ 掘削土砂処理施設 処理能力 運行管理 土砂ホッパ 切替制御装置 泥水式シールド工法 スラリー管理方法 請求項 切羽 走行位置 加泥材 スラリー返送手段 排出量 前記スラリー 施設 制御 当該運搬装置 発進立坑Ｋ 泥水供給手段 実施形態 排泥 スラリー処理施設スラリー処理施設 装置 前記掘削土砂 掘削土砂搬送施設 供給量 前記掘削土砂搬出施設 前記運搬装置 返送 返送量 掘削土砂搬出施設等 前記泥水 スラリー返送量 位置検出手段 シールド工事 発明 当該掘削土砂 ＱＤ 管理システム 圧力 複数 ダンプトラック 搬送量 進機 泥土式シールド工法 搬送手段 当該計量掘削土砂量 進機Ｓ 泥水供給量 処理設備 位置 前記切羽 制御方法 前記スラリー処理施設 計量手段 運行 スラリー処理システム 走行位置情報 前記泥水供給量制御装置 泥量ＱＳ 用地 ＱＳ ロードセル 供給 前記排泥ポンプ 泥量ＱＳ スラリー返送量制御装置地上部 地上部 走行速度等 前記排出管 混合土砂 前記圧送ポンプ 泥量ＱＤ 排泥量ＱＤ トンネルＴ 前記スラリー管理システム 地山 各種使用機材 前記運行管理装置 進 該主排泥管 加泥材供給装置 圧力管理 前記主排泥管 公共用地等 タンク 管理方法 前記スラリー返送手段 発進立坑 当該運行管理装置 切羽圧力 泥水圧 分流器 回転数 式 前記複数 工法 排泥流速等 搬入等 当該土砂ホッパ 前記泥水供給手段 実施 一定量 図 進長 複数箇所 数 前記切替装置 運行位置 加泥材等 記載 水位データ シールドトンネル 当該 水位レベル 前記搬送手段 比重管理 粘性管理 ストローク数 切替装置切羽Ｈ 前記ダンプトラック 当該泥水 位置表示装置 レベル 方法 Ｋ 連絡用シャフトＫ 前記計量手段 含砂量管理等 ＧＰＳ衛星 形態 接続 スラリーポンプ等 手段 トンネル上部 合流器 圧送 対象地盤Ｇ 泥水槽 説明 制限 等 流量 検出 条件 運転 進機Ｋ 通信装置等 長距離シールド シールド径 進速度 規模 走行 前記ロードセル 泥土圧シールド工法 進能力 当該複数 搬送距離 雨水管路等 当該実施形態 前記レベル 交通公害 調整槽 分岐部 メンテナンス費用 構成要素 下流側 概念図 ポンプ圧送等 工事 公共用地 走行データ 当該合流器 工期 特徴 他 合計 所 遮断 リアルタイム 連絡用シャフトＴ 運行指令 トンネルＧ 管長さや管径等 当該ダンプトラック 礫等 摩耗等 低下等 騒音等 ローヘッドスクリーン等 増車等 水位 Ｓ 物流条件等 分岐配管 硬質地盤 データ 目的地 走行ルート 工事費 対象地盤H ＧＰＳ 各種設備 管内圧力 圧力容器 ＧＰＳ信号 都市部 先端部 前面 砂 課題 高速化施工 多量 設備 搬入 長期化 増大化 短縮 衛星 図面 同一 符号 近傍 貯溜量 ）～（ 圧 車 効果 沿道条件 入力データ 施工現場周辺 混合物 産業廃棄物 交通渋滞 受信信号 送信信号 塑性流動性 機械費 技術分野 技術 バインダー分 礫 コンクリート状 増大 作業効率 問題点 目的 該総泥水量 配管 データ通信機能 データ通信 混雑個所 通信 設計変更 山留め 部分 確保 増加 セグメント 種々 原因 一つ プラント 低減 材料 Global Positioning System 前者 後者 公知 各々 基 内部 後記 圧送量 停止 増減 最適 ＰＨＳ 趣旨 範囲 設置 公園 下水道\n",
      "\n",
      "===== # 5, Topic : 14, p : 9.3816 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 外側足場 内側足場 躯体 足場 昇降装置 内部空間 外側 内側 構造物 躯体Ｋ 力 構造 水平力 クライミング装置 型枠足場 フレーム 図 外側型枠 水平方向 クライミング操作 型枠 方向 スリップフォーム工法 閉鎖型足場 発明 方向性 施工法 垂直荷重 コンクリート 内側型枠 断面四角形 コンクリート壁 閉鎖型外側足場 特開平 号公報 車輪 枠 閉鎖型内側足場 力受け クライミングジャッキ 溝断面 断面円形 躯体表面 正面図 剛性フレーム 外側共 外側面 接部材昇降装置 中空構造物 説明 ー 数 図示 例 ブラケット 煙突 レール 表面 躯体側 進退装置 センターコア構造 断面四角形状 クライミングロッド クライミング過程 工法 接部材 橋脚 鉄筋 課題 該躯体 ｂ 状態 所定 位置 チェーンブロック等 コアー等 煙突等 レール等 コンンクリート壁 平面図 側面図 油圧ジャッキ 前記問題点 技術分野 技術 複数分割 分割 側面 ワイヤーロープ 引き上げ方式 コンンクリート 建物 台数 手段 実施 形態 形状 要所 形式 押し上げ つて 効果 図面 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 9.8287 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : シールドジャッキ装置 ピストンロッド 抵抗ジャッキ装置 シールド機本体 進機 セグメント端部 端部 ジャッキ ピストンロッド先端 調整バルブ装置 抵抗値 開閉弁 セグメント 方向 所定値 セグメント端面 ガーダー部 開閉力 図 フランジ部 ジャッキ本体 力 抵抗値Ｆ 進方向 シリンダー 抵抗ジャッキ本体 油圧室 油圧力 実施形態 ｂ 油圧回路 端面 曲線施工 値 固定金具 弁 説明 スプレッダー 構成説明図 テール部 変更方向 先端 当該シールド機本体 抵抗 発明 伸長 前記ピストンロッド 所定 圧油供給手段 シールドジャッキ装置等 手動ハンドル 圧油供給装置 推進力 推進 スプリング ジャッキ等 弁座 付勢力 圧力 ｃ 方向側 ピストンロッド伸長 固定 カッターヘッド カッターヘッド駆動装置 先端側 凹状段部 排土装置 周方向 フード部 主要部 先端部 段部 推進用 圧油供給源 方向変更 前記抵抗値 圧油 部分 シールドスキンプレート 進方向変更 構成要素 推進方向 所定方向 球面継手 作動 相互固定 伸長方向 進 構成 シールドスキンプレート内側 切羽側 環状 ｄ 推進操作 推進量 方向変更手段 装置群 開閉力可変弁 軸方向 止水機能 当該フランジ部 テールシール 前記シリンダー 空間 複数 推力 ボルト ハウジング 板 ｅ 状態 方向制御 直進方向 掘削方向 調整 工作業 油圧力 相互 曲率 凹状段部形成側 実施 形態 側 引き抜き力 方法 特徴 位置 課題 スチール製セグメント 推進状態 ハンドル軸 手段 ネジ穴 伸長動作開始点 具体的構成 チェーンブロック 具体的構成要素 チェーンブロック等 ３つ 機器 場所 後端 エレクター 内側 箇所 直し 他方 伸長移動 支点 時点 停止 ～ 番号 ２つ 作用 動作 表面 断面図 貫通ネジ穴 切羽部分 係止 配置状態 技術 機能 周 作業 鋼殻部分 ねじ穴 開放状態 技術分野 問題点 配置例 保護下 掘削 均等荷重 配列位置 相関関係 直進 圧縮度 技術的課題 外部 内部 全面 後部 一般 等間隔 適性 形状 構造 使用 ルート 他 目的 １つ 一端 他端 意味 逆 内方 凹状 厚み 態様 ６つ 最大 次 繰り返し 経過 効果 図面 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 9.8121 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 法枠ブロック 底面壁 面壁 連結用底面壁部 法面 側方 係合凹部 法枠ブロック同士 図 係合穴 前記底面壁 実施形態 盛土 側端部 控え壁 前記係 法枠本体 アンカーピン 発明 側方方向 法面勾配 目用 面 当該法枠ブロック 前記連結用底面壁部 法枠ブロック裏面 端部 請求項 連結 盛土層 前記アンカーピン 係 矩形状底面壁 端面 凹部 上下方向 前記 上下 目 後方 安定性 盛土補強土工法 矩形状 該前面壁 突出片 上面 地盤 ｂ 係合 側 記法枠ブロック 法面全面 ｃ 階段状 係合固定 前記側端部 ジオテキスタイル 土砂 空孔 ａ 該底面壁 前端部 ブロック部材 自立安定性 斜視図 補強材 壁 網目状 前記空孔 転圧 前記控え壁 型枠 圧 位置 該連結用底面壁部 方向 傾斜面 側端 当該法枠本体 ｃ－ｃ線 アンカー 下端面 内側面 前記補強材 直近後方 安定化 施工 上記発明 ｅ－ｅ線矢視 IV－IV線矢視 基礎地盤Ｇ 直方体状 格子状 裁頭円錐状 記法枠本体 平面部 勾配 幅 断面図 軽量化 － 前方 ブロック分 穴 バットレス部 水平方向 上方 作業能率 前記階段状 縦断面図 盛土層間 両側端部 列 フック 字状 側面図 平面矩形状 上端面 支持面 控え壁上面 一体化 先端上面 領域 実施 形態 略矩形状 位置調整 水抜き用 前記突出片 説明 状態 同一 下面 特徴 他方 階段状部分 タイヤローラ タイヤローラー 該アンカーピン 幅方向 千鳥状 前記先端 プレキャストコンクリート製 相対移動 中央部 盛土補強工法 裁頭円柱状 係止具 根張り 基礎地盤 コ字状 後方直近 挿入係合し 幅方向中央 最深部 複数 効果 一対 課題 重心 任意 記載 直上 互い 植林 配列 ２つ １つ 前記ジオテキスタイル 孔 地盤低下 ～（ｃ 先端 同士 該係合凹部 上記 相対位置 垂直後方 直近領域 コスト削減 多段 作業 張力 開口 植生 目的 該控 上層 下層 緩み 直下 中央 前述 手順 造成 転圧作業 転圧付与 符号 平面半円形状 傾斜部分 横幅 半円柱状 正面視 材料コスト削減 傾斜角度 一体 水平 部材 鋼製 号公報参照 技術分野 技術 工法 特開平 こぼれだし 景観向上 傾倒モーメント 添付図面 盛り立て 角度 煉瓦積み 景観 図面 側部 面積 虞 メッシュ 手段 自重 ～ 奥行 程度 間隔 同形 工場 トラック 実線 紙面 配置 腐食 素材 耐食性 ステンレス 位置決め 図示 敷設 所期 植物 構成 相違 前面 薄肉 水捌け 小径 両者 隙間 要旨 範囲 変形 搬送 設置 広範囲\n",
      "\n",
      "===== # 8, Topic : 14, p : 9.5577 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 連層耐震壁 並列連層耐震壁 前記連層耐震壁 前記並列連層耐震壁 建物 扁平梁 桁行方向 層数 梁間方向 連結梁 桁行方向Ｙ 前記建物 梁成 梁 層 前記扁平梁 請求項 梁幅 前記連結梁 方向 鉄筋コンクリート 梁間方向Ｘ 構造 ラーメン架構 前記連層耐震壁相互 前記 図 耐震性 連層耐震壁相互 ダンパ 上記並列連層耐震壁 地震応答変形 相互 階高 地震力 変形 地震エネルギー 前記ラーメン架構 記載 面外方向 前記ダンパ 実施 形態 下層階 耐震効果 鉛直方向 エネルギー吸収能力 階高寸法 耐震性能 耐震要素 発明 構面内 基準階 制震ダンパ 幅内部 断面寸法 鉄筋コンクリート造建物 鉄筋コンクリート造 制震効果 梁間方向Ｙ 平面形状 Ｄ 効果 鉄骨鉄筋コンクリート造 柱 建 スリット入り鋼板 Ａ－Ａ断面図 内部 断面形状 ｂ 上記 建程度 建～ 平面図 板状建物 一体 変形状態 剛性 該ダンパ 定着 相対変位 模式図 水平変位δ 階段室等 鉄骨鉄筋コンクリート建物 空間内部 程度 参照 特徴 断面図 基準階平面図 鉄骨鉄筋コンクリート ダンパＸ 定着部 ダンパ 複数箇所 エレベータシャフト 説明 上層 課題 任意 増加 耐力 損傷 スラブ 主筋 せん断変形 鉄骨 強度 図面 例 下面 面 両側 Ｔ コ Ｈ ユニットバス等 細部構造等 オイルダンパ等 水平剛性 上記事情 技術分野 技術 回転角 バリアフリー対応 単独 構面 上限 目的 手段 他 レベル Ｌ ａ 趣旨 範囲 種々 改良 設計 変更 各種 符号\n",
      "\n",
      "===== # 9, Topic : 12, p : 7.7205 %\n",
      "Topic words : 施工, 内側, Ａ, 図面, 技術分野, 手段, 形態, 説明, 発明, 課題\n",
      "Input : 収容壁部 図 テナントエリア 冷暖房機器 天井面 設備機器類 ブレース 請求項 開口部 設置スペース 字状 建物 テナントビル 実施形態 外周部 発明 スパン ユニット分 フレーム 照明器具 配管ダクト類 コスト 前記収容壁部 形態 上記実施形態 外周側 設備機器設置スペース 梁 ユニットごと 設置工事 ユニット テナント変更等 平面図 天井伏図 耐震性 架構 窓 レースウエイ 前記天井面 ラーメン架構 天井 コアエリア 内部空間 追加変更工事 ハ バルコニー 下階 上記 図示例 電灯設備工事 ～図 柱 ペリメータ負荷処理用 インテリア負荷処理用 配線用 有効面積比 位置 位置変更 構造的安定性 通線工事 テナント 配線 テナント変更 合成床版 桁行方向 有効天井高 側 空間 立面図 スラブ 実施 改変工事等 断面図 工事 梁伏図 間仕切り壁 腰壁 廊下 断面 盤 躯体コスト 口 冷暖房負荷 デッドスペース 動線計画 建設コスト テナントビル等 天井高 部材断面 内部 通風 Ｈ形鋼 ユニット分つまり 保守点検 基本 所要 側部 床 外壁 出入口 位置変更等 冷暖房 ペリメータ負荷 インテリア負荷 スペース スパンごとつまりテナントエリア 直射日光 屋外ユニット等 屋外ユニット 日射負荷 電気配線用 電気配線類 説明 各階 下面 採光 屋内ユニット 分電盤 削減 スパン分 階高 鉄骨ラーメン構造 構造設計 設計的変更 住宅用アルミサッシ等 端子盤等 建具工事費 梁下 採算性 耐久性 快適性 利便性 融通性 可変性 鉄骨製作コスト ケーブル等 際等 ルーバ等 予備スペース 例 外周フレーム コスト削減 Ｘ状 基準階 前記ブレース 性能 機能 課題 外部 用途 符号 境界 着脱 増減 動力盤 サッシ 庇 通常 山形 支障 向上 柱空間 点検扉 幅方向中央 立体駐車場棟 通風口 排気口 斜めブレース 技術分野 技術 ボルト締結 スラブ厚 ＯＡフロア 作業員 軽量パネル パイプシャフト 外気 熱交換 上部壁面 給気 新鮮外気 汎用品 効果 遮蔽効果 要請 方策 実状 手段 事務所 建て 付属 溶接 併用 おき 取り付け 外観 内観 自体 ＥＰＳ 一体 ウオールスルータイプ 吹出口 下部 吸込口 入口 換気 制御 独立 使い勝手 外側 夏期 低減 一般 Ｋ 所望 規模 犠牲 格別 図面\n",
      "\n",
      "===== # 10, Topic : 14, p : 8.1740 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 風力発電装置 図 回転軸 屋根面 回転羽根 風車 面 請求項 発電機 角部 発電 実施 形態 発電効率 防風カバー 風 平面図 発明 発電エネルギー 回転 壁面 建物 ｂ 設置面 回転効率 屋根 頂部 面側 一方向 軸受け板 方向 延長線 断面図 他方 ～ 風向き パドルタイプ 特徴 回転方向 風力 孔 サボニウスタイプ 効果 防波堤 回転エネルギー 時計回り 右回り 前記風車 該延長線 一直線状 一般住宅 角部付近 説明 課題 該風車 箇所 屋上 バネ 上部 下部 鍔 外壁部 正面図 上記 手段 図面 構成 位置 上側 前記 斜面 状態 軸受け板間 ｂ側 下側 前記長孔 電気エネルギー 技術分野 技術 高層建物 構造物 半円形 目的 台数 一定 同士 年間 内側 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7115e-38, 7.7552e-41, 3.5936e-13, 3.8869e-19, 3.6687e-09, 5.9541e-37,\n",
      "         1.1032e-36, 4.7767e-21, 3.0659e-14, 3.7761e-06, 2.1691e-29, 4.0813e-31,\n",
      "         3.3335e-17, 3.3324e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1096.8116\n",
      "Test epoch : 1 Average loss: 1071.6260\n",
      "PP(train) = 3269.252, PP(valid) = 3349.028\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.6700e-37, 1.0615e-14, 1.0840e-38, 8.2431e-10, 1.3488e-34,\n",
      "         3.6154e-42, 1.0858e-19, 1.4251e-10, 1.9330e-37, 5.6052e-45, 4.8995e-35,\n",
      "         1.3859e-17, 8.3994e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1096.3402\n",
      "Test epoch : 2 Average loss: 1071.3373\n",
      "PP(train) = 3258.408, PP(valid) = 3340.069\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3465e-37, 4.6734e-19, 3.0527e-29, 1.1002e-09, 2.8026e-45,\n",
      "         2.2871e-40, 7.3800e-16, 2.5347e-14, 2.0298e-19, 1.3490e-40, 2.3384e-37,\n",
      "         1.8381e-17, 6.0733e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1096.0253\n",
      "Test epoch : 3 Average loss: 1071.0305\n",
      "PP(train) = 3247.343, PP(valid) = 3331.425\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 5.0381e-14, 1.5852e-27, 1.0389e-17, 0.0000e+00,\n",
      "         0.0000e+00, 3.7859e-19, 4.8533e-22, 5.6365e-29, 1.0566e-42, 6.4649e-41,\n",
      "         4.0905e-19, 1.0308e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1095.4563\n",
      "Test epoch : 4 Average loss: 1070.7107\n",
      "PP(train) = 3236.969, PP(valid) = 3323.733\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8855e-44, 8.0381e-23, 1.0342e-15, 7.8451e-22, 8.9920e-08, 0.0000e+00,\n",
      "         1.4713e-36, 1.5614e-01, 3.9824e-20, 4.7688e-19, 0.0000e+00, 4.4269e-31,\n",
      "         3.2479e-18, 3.7482e-35, 8.4386e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0831, 0.0515, 0.0955, 0.0499, 0.0636, 0.0713, 0.1034, 0.0832, 0.0591,\n",
      "         0.0502, 0.0696, 0.0543, 0.0491, 0.0750, 0.0412]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1095.0672\n",
      "Test epoch : 5 Average loss: 1070.3919\n",
      "PP(train) = 3226.826, PP(valid) = 3316.431\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.9236e-44, 1.1134e-28, 1.0379e-32, 1.9058e-22, 0.0000e+00,\n",
      "         1.4013e-45, 8.2463e-01, 1.0985e-10, 1.6690e-23, 3.0746e-40, 1.5047e-30,\n",
      "         5.7969e-32, 2.1844e-32, 1.7537e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0844, 0.0921, 0.0618, 0.0459, 0.0765, 0.0885, 0.0828, 0.0505, 0.0549,\n",
      "         0.0399, 0.1080, 0.0464, 0.0393, 0.0907, 0.0382]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1094.5570\n",
      "Test epoch : 6 Average loss: 1070.0761\n",
      "PP(train) = 3216.241, PP(valid) = 3308.868\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1446e-28, 5.2265e-23, 2.0327e-21, 1.7605e-16, 0.0000e+00,\n",
      "         0.0000e+00, 1.0864e-13, 2.6562e-17, 7.9442e-32, 2.8026e-45, 4.7042e-42,\n",
      "         2.5144e-22, 1.1820e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1094.0456\n",
      "Test epoch : 7 Average loss: 1069.7617\n",
      "PP(train) = 3205.310, PP(valid) = 3300.948\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5414e-44, 2.0385e-33, 9.3467e-43, 1.0454e-23, 3.7500e-41,\n",
      "         1.4013e-44, 4.0864e-22, 1.7387e-23, 2.1917e-40, 1.4013e-45, 2.2150e-41,\n",
      "         2.2971e-25, 6.4385e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1093.7158\n",
      "Test epoch : 8 Average loss: 1069.4465\n",
      "PP(train) = 3194.221, PP(valid) = 3292.871\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.3071e-43, 2.0261e-23, 3.7057e-33, 1.6506e-32, 0.0000e+00,\n",
      "         0.0000e+00, 8.1090e-29, 1.6556e-29, 5.3235e-34, 1.4013e-45, 0.0000e+00,\n",
      "         2.0801e-32, 9.2434e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1093.2005\n",
      "Test epoch : 9 Average loss: 1069.1328\n",
      "PP(train) = 3183.264, PP(valid) = 3284.904\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7662e-26, 2.9618e-21, 3.8476e-25, 5.3780e-18, 0.0000e+00,\n",
      "         1.0720e-42, 1.2540e-11, 2.3965e-10, 1.7602e-24, 1.6363e-35, 2.0204e-34,\n",
      "         9.9213e-01, 1.1009e-26, 7.8686e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0485, 0.0742, 0.0764, 0.0605, 0.0414, 0.0646, 0.0542, 0.0790, 0.0745,\n",
      "         0.0934, 0.0483, 0.0530, 0.0585, 0.0644, 0.1091]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1092.7891\n",
      "Test epoch : 10 Average loss: 1068.8221\n",
      "PP(train) = 3172.517, PP(valid) = 3277.131\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4618e-24, 3.0366e-20, 5.7936e-25, 4.4916e-07, 0.0000e+00,\n",
      "         2.6344e-43, 9.9997e-01, 7.2089e-15, 3.8139e-26, 3.6504e-42, 1.6764e-34,\n",
      "         1.1787e-13, 3.3894e-28, 2.4740e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1092.2576\n",
      "Test epoch : 11 Average loss: 1068.5154\n",
      "PP(train) = 3161.955, PP(valid) = 3269.564\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6511e-27, 2.1195e-22, 1.3258e-34, 7.2536e-27, 0.0000e+00,\n",
      "         0.0000e+00, 4.6344e-17, 2.8484e-17, 1.6078e-33, 9.7432e-42, 1.4812e-42,\n",
      "         6.9339e-18, 8.1679e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1091.8012\n",
      "Test epoch : 12 Average loss: 1068.2104\n",
      "PP(train) = 3151.451, PP(valid) = 3262.036\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.7581e-43, 1.6559e-31, 8.3197e-10, 1.9957e-24, 2.0011e-05, 4.6431e-32,\n",
      "         1.5668e-34, 1.1592e-04, 2.0882e-07, 5.4029e-25, 9.1164e-38, 6.4284e-34,\n",
      "         1.8425e-12, 6.3532e-30, 9.9986e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1091.4561\n",
      "Test epoch : 13 Average loss: 1067.9066\n",
      "PP(train) = 3140.926, PP(valid) = 3254.405\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4344e-38, 6.6961e-22, 2.8511e-31, 9.3064e-17, 0.0000e+00,\n",
      "         5.7453e-44, 1.4454e-06, 1.2435e-13, 4.2697e-34, 8.9627e-42, 1.5311e-37,\n",
      "         9.3128e-31, 1.6914e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1090.8880\n",
      "Test epoch : 14 Average loss: 1067.6024\n",
      "PP(train) = 3130.510, PP(valid) = 3246.813\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.4809e-40, 6.6785e-22, 1.2604e-30, 3.5485e-16, 0.0000e+00,\n",
      "         0.0000e+00, 2.3968e-17, 1.2781e-17, 5.4313e-28, 0.0000e+00, 6.4460e-44,\n",
      "         2.4544e-17, 4.7536e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1090.4278\n",
      "Test epoch : 15 Average loss: 1067.3017\n",
      "PP(train) = 3120.414, PP(valid) = 3239.554\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.6940e-38, 1.3929e-25, 2.6808e-25, 4.1040e-20, 1.4013e-45,\n",
      "         0.0000e+00, 3.3878e-26, 8.7219e-19, 9.1904e-32, 0.0000e+00, 1.4248e-35,\n",
      "         1.3301e-31, 1.8155e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1090.1499\n",
      "Test epoch : 16 Average loss: 1067.0018\n",
      "PP(train) = 3110.476, PP(valid) = 3232.373\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2757e-42, 1.4303e-16, 6.2189e-18, 2.3061e-19, 7.1631e-39,\n",
      "         0.0000e+00, 4.6200e-15, 1.1681e-11, 4.7607e-28, 8.6233e-34, 5.9220e-37,\n",
      "         9.1043e-33, 1.0814e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1089.6040\n",
      "Test epoch : 17 Average loss: 1066.7062\n",
      "PP(train) = 3100.395, PP(valid) = 3225.023\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6778e-39, 8.8260e-21, 5.0102e-26, 1.2078e-16, 0.0000e+00,\n",
      "         5.6052e-45, 3.8232e-14, 1.2969e-11, 3.8455e-22, 0.0000e+00, 3.7887e-40,\n",
      "         2.8744e-13, 2.8440e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1089.0828\n",
      "Test epoch : 18 Average loss: 1066.4116\n",
      "PP(train) = 3090.417, PP(valid) = 3217.806\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.8932e-42, 1.0790e-22, 4.9967e-33, 7.9797e-34, 0.0000e+00,\n",
      "         0.0000e+00, 6.4982e-21, 8.9186e-10, 1.1656e-37, 3.0829e-44, 2.2424e-39,\n",
      "         2.1227e-24, 3.8725e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1088.7160\n",
      "Test epoch : 19 Average loss: 1066.1175\n",
      "PP(train) = 3080.568, PP(valid) = 3210.636\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3392e-28, 1.0836e-21, 2.0994e-22, 1.2235e-14, 1.4013e-45,\n",
      "         5.0209e-42, 1.0000e+00, 2.1323e-22, 1.8983e-25, 6.7857e-39, 9.9973e-38,\n",
      "         1.8058e-19, 2.3553e-34, 2.5880e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1088.3692\n",
      "Test epoch : 20 Average loss: 1065.8281\n",
      "PP(train) = 3070.754, PP(valid) = 3203.513\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2595e-37, 1.3104e-26, 6.9855e-12, 1.5154e-19, 1.5037e-07, 3.2940e-31,\n",
      "         1.2371e-35, 3.9203e-02, 9.6079e-01, 2.3375e-17, 1.6440e-39, 3.7405e-24,\n",
      "         5.7837e-20, 6.0263e-28, 2.4250e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0572, 0.0688, 0.0448, 0.0622, 0.0317, 0.0768, 0.0636, 0.0896, 0.0570,\n",
      "         0.0678, 0.0861, 0.0643, 0.0506, 0.1301, 0.0493]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1087.9855\n",
      "Test epoch : 21 Average loss: 1065.5403\n",
      "PP(train) = 3060.974, PP(valid) = 3196.357\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1366e-22, 5.0098e-22, 2.1907e-17, 6.2753e-19, 8.1359e-39,\n",
      "         1.8511e-42, 3.5516e-01, 1.4471e-15, 1.7488e-29, 1.3312e-43, 3.0109e-34,\n",
      "         1.4287e-29, 2.1673e-21, 6.4484e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0843, 0.0618, 0.0847, 0.0491, 0.0678, 0.0768, 0.0977, 0.0724, 0.0584,\n",
      "         0.0473, 0.0801, 0.0523, 0.0464, 0.0802, 0.0407]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1087.5599\n",
      "Test epoch : 22 Average loss: 1065.2526\n",
      "PP(train) = 3051.475, PP(valid) = 3189.405\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.3029e-26, 1.6315e-30, 2.3867e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.2012e-11, 2.7078e-23, 5.6472e-43, 0.0000e+00, 1.7941e-38,\n",
      "         8.7457e-31, 1.9903e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1087.0049\n",
      "Test epoch : 23 Average loss: 1064.9612\n",
      "PP(train) = 3042.162, PP(valid) = 3182.666\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1749e-39, 1.2084e-34, 2.3066e-18, 2.0619e-23, 9.9999e-01, 2.8026e-45,\n",
      "         8.2328e-41, 1.7325e-07, 3.4774e-18, 2.4806e-23, 6.6744e-37, 4.2039e-45,\n",
      "         3.3845e-06, 1.1341e-23, 6.4232e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1086.6427\n",
      "Test epoch : 24 Average loss: 1064.6759\n",
      "PP(train) = 3032.979, PP(valid) = 3176.023\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1829e-35, 5.8875e-12, 1.9704e-25, 2.5563e-09, 0.0000e+00,\n",
      "         4.9520e-40, 1.0000e+00, 8.7780e-13, 3.1426e-20, 1.9080e-32, 7.9329e-30,\n",
      "         1.2516e-17, 1.4969e-23, 3.3664e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1086.1824\n",
      "Test epoch : 25 Average loss: 1064.3930\n",
      "PP(train) = 3023.616, PP(valid) = 3169.218\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2805e-38, 8.9844e-21, 1.1559e-23, 1.6556e-13, 5.8855e-44,\n",
      "         1.7579e-35, 1.0263e-11, 4.3560e-14, 6.5663e-23, 2.1683e-33, 8.2432e-32,\n",
      "         3.3038e-19, 2.4710e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1085.8449\n",
      "Test epoch : 26 Average loss: 1064.1114\n",
      "PP(train) = 3014.256, PP(valid) = 3162.323\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.5670e-44, 0.0000e+00, 6.0942e-25, 1.3189e-30, 7.6360e-28, 4.2039e-45,\n",
      "         0.0000e+00, 3.7589e-15, 1.9958e-13, 8.6681e-27, 0.0000e+00, 2.8026e-45,\n",
      "         5.4756e-27, 4.3449e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1085.4603\n",
      "Test epoch : 27 Average loss: 1063.8343\n",
      "PP(train) = 3004.885, PP(valid) = 3155.451\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.4921e-34, 6.4121e-17, 1.0878e-12, 1.1067e-08, 4.4055e-38,\n",
      "         0.0000e+00, 2.8637e-09, 4.1685e-14, 1.0429e-28, 0.0000e+00, 6.8221e-34,\n",
      "         3.7380e-17, 2.6762e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1084.9213\n",
      "Test epoch : 28 Average loss: 1063.5555\n",
      "PP(train) = 2995.821, PP(valid) = 3148.792\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7347e-41, 6.2358e-43, 7.4954e-33, 3.4307e-28, 1.5200e-21, 0.0000e+00,\n",
      "         0.0000e+00, 1.2942e-13, 2.3078e-09, 3.5912e-36, 1.4555e-35, 2.2976e-34,\n",
      "         1.0070e-15, 3.9246e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1084.5848\n",
      "Test epoch : 29 Average loss: 1063.2765\n",
      "PP(train) = 2986.960, PP(valid) = 3142.310\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8352e-33, 9.6315e-20, 1.8489e-34, 5.7164e-16, 0.0000e+00,\n",
      "         0.0000e+00, 2.8267e-09, 1.1488e-14, 6.6428e-38, 0.0000e+00, 1.9919e-22,\n",
      "         4.1600e-17, 5.4867e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1084.1160\n",
      "Test epoch : 30 Average loss: 1063.0014\n",
      "PP(train) = 2978.307, PP(valid) = 3136.050\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3829e-39, 1.2981e-19, 1.1390e-15, 7.9607e-21, 4.6299e-05, 6.4460e-44,\n",
      "         2.8026e-44, 4.8031e-04, 8.0786e-16, 1.3219e-32, 1.2752e-42, 6.5434e-23,\n",
      "         5.4757e-08, 1.1206e-14, 9.9947e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1083.7248\n",
      "Test epoch : 31 Average loss: 1062.7300\n",
      "PP(train) = 2969.482, PP(valid) = 3129.641\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2967e-42, 5.8214e-34, 1.7233e-26, 2.8352e-23, 5.1180e-13, 9.8000e-36,\n",
      "         2.7982e-39, 6.6746e-13, 2.1449e-12, 3.5566e-30, 7.0065e-44, 4.7947e-39,\n",
      "         7.0296e-06, 7.2516e-31, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1083.3965\n",
      "Test epoch : 32 Average loss: 1062.4578\n",
      "PP(train) = 2960.547, PP(valid) = 3123.079\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9403e-32, 6.7735e-32, 1.1601e-05, 1.8575e-17, 6.7168e-10, 1.7512e-26,\n",
      "         1.2612e-44, 1.1043e-08, 8.6580e-09, 3.7750e-24, 3.1341e-40, 1.9312e-27,\n",
      "         1.4159e-18, 1.3090e-29, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1083.0182\n",
      "Test epoch : 33 Average loss: 1062.1886\n",
      "PP(train) = 2951.733, PP(valid) = 3116.551\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 9.3584e-25, 1.5655e-13, 7.4494e-06, 0.0000e+00,\n",
      "         0.0000e+00, 1.7439e-16, 9.9492e-12, 4.9201e-20, 6.0583e-34, 2.6813e-31,\n",
      "         2.3581e-24, 3.1068e-20, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1082.5135\n",
      "Test epoch : 34 Average loss: 1061.9198\n",
      "PP(train) = 2942.961, PP(valid) = 3110.081\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3717e-41, 3.7825e-30, 1.0210e-41, 1.2097e-11, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-17, 1.0815e-12, 2.0281e-41, 0.0000e+00, 2.2421e-44,\n",
      "         4.5761e-17, 2.3127e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1082.0898\n",
      "Test epoch : 35 Average loss: 1061.6509\n",
      "PP(train) = 2934.387, PP(valid) = 3103.773\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1074e-41, 2.5831e-28, 1.7798e-25, 2.2872e-26, 0.0000e+00,\n",
      "         1.4013e-45, 5.9929e-13, 1.5052e-17, 8.8455e-27, 1.9618e-44, 4.8301e-34,\n",
      "         5.1202e-22, 4.3135e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1081.7618\n",
      "Test epoch : 36 Average loss: 1061.3858\n",
      "PP(train) = 2926.043, PP(valid) = 3097.682\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2820e-34, 6.4310e-29, 5.8779e-05, 1.5735e-17, 1.0049e-07, 5.6642e-31,\n",
      "         1.2746e-31, 8.2769e-01, 1.5036e-01, 8.8306e-20, 1.0065e-40, 1.3049e-34,\n",
      "         1.7601e-05, 8.8872e-15, 2.1875e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0796, 0.0982, 0.0542, 0.0474, 0.0691, 0.0902, 0.0762, 0.0503, 0.0545,\n",
      "         0.0415, 0.1133, 0.0474, 0.0392, 0.0996, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1081.4352\n",
      "Test epoch : 37 Average loss: 1061.1215\n",
      "PP(train) = 2917.632, PP(valid) = 3091.519\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7628e-42, 6.3384e-41, 1.1359e-13, 1.2715e-38, 1.4775e-15, 0.0000e+00,\n",
      "         2.3262e-43, 1.8848e-06, 2.2903e-03, 1.1929e-28, 9.6230e-34, 4.8054e-36,\n",
      "         2.0757e-17, 2.6436e-31, 9.9771e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1074, 0.0924, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0710, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1080.9606\n",
      "Test epoch : 38 Average loss: 1060.8610\n",
      "PP(train) = 2909.305, PP(valid) = 3085.446\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 4.3454e-35, 7.5305e-02, 1.0720e-11, 8.4081e-10, 3.9123e-37,\n",
      "         4.4512e-34, 1.3081e-08, 2.7533e-06, 3.4388e-16, 6.3043e-37, 2.9322e-17,\n",
      "         3.2197e-09, 3.1860e-17, 9.2469e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0470, 0.0983, 0.0518, 0.0619, 0.0656, 0.1052, 0.0914, 0.0618,\n",
      "         0.0530, 0.0623, 0.0552, 0.0513, 0.0723, 0.0411]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1080.5823\n",
      "Test epoch : 39 Average loss: 1060.6007\n",
      "PP(train) = 2900.819, PP(valid) = 3079.161\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-44, 1.1743e-32, 9.2089e-18, 1.9572e-17, 9.3409e-11, 5.5424e-38,\n",
      "         6.3092e-37, 1.0690e-07, 3.4545e-07, 1.8184e-24, 1.3211e-36, 1.6559e-27,\n",
      "         2.4663e-12, 8.6325e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1080.2665\n",
      "Test epoch : 40 Average loss: 1060.3424\n",
      "PP(train) = 2892.526, PP(valid) = 3073.053\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.0024e-31, 1.0445e-09, 3.0137e-22, 1.9882e-17, 0.0000e+00,\n",
      "         3.1950e-43, 5.7094e-17, 7.1428e-04, 2.0602e-17, 0.0000e+00, 6.4669e-32,\n",
      "         4.1460e-30, 7.5916e-34, 9.9929e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1079.7620\n",
      "Test epoch : 41 Average loss: 1060.0845\n",
      "PP(train) = 2884.500, PP(valid) = 3067.157\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 2.2501e-30, 9.2049e-20, 2.6048e-26, 2.1690e-13, 7.0065e-45,\n",
      "         6.3851e-39, 1.0023e-06, 1.0705e-10, 6.4071e-29, 6.0141e-33, 5.5811e-31,\n",
      "         6.3072e-17, 3.3110e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1079.4118\n",
      "Test epoch : 42 Average loss: 1059.8273\n",
      "PP(train) = 2876.590, PP(valid) = 3061.406\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3037e-40, 1.0396e-19, 4.6038e-30, 1.2265e-23, 1.5372e-42,\n",
      "         0.0000e+00, 6.2639e-22, 8.1685e-10, 2.9025e-20, 4.6597e-40, 2.6348e-32,\n",
      "         2.1249e-25, 7.3306e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1079.1042\n",
      "Test epoch : 43 Average loss: 1059.5714\n",
      "PP(train) = 2868.579, PP(valid) = 3055.505\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5764e-41, 3.6718e-23, 1.3838e-06, 5.4060e-14, 1.7468e-08, 0.0000e+00,\n",
      "         2.0636e-24, 4.3680e-07, 1.5456e-17, 2.0870e-30, 1.2649e-32, 4.1130e-30,\n",
      "         1.3132e-06, 1.3492e-10, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1078.7449\n",
      "Test epoch : 44 Average loss: 1059.3164\n",
      "PP(train) = 2860.527, PP(valid) = 3049.505\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6742e-37, 4.5288e-31, 4.6820e-39, 4.5088e-26, 0.0000e+00,\n",
      "         3.0254e-42, 6.9556e-19, 2.9538e-20, 6.9795e-34, 3.8554e-40, 1.5569e-38,\n",
      "         1.2010e-20, 3.2686e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1078.3659\n",
      "Test epoch : 45 Average loss: 1059.0659\n",
      "PP(train) = 2852.446, PP(valid) = 3043.494\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4086e-40, 8.7775e-28, 1.0851e-23, 2.9616e-19, 0.0000e+00,\n",
      "         0.0000e+00, 2.1175e-27, 5.6386e-13, 1.4055e-39, 1.2884e-41, 1.9946e-41,\n",
      "         7.6604e-27, 1.4386e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1077.9201\n",
      "Test epoch : 46 Average loss: 1058.8130\n",
      "PP(train) = 2844.649, PP(valid) = 3037.761\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7946e-22, 1.8683e-19, 1.5888e-26, 2.3953e-11, 0.0000e+00,\n",
      "         0.0000e+00, 1.7231e-01, 1.6182e-15, 9.2235e-21, 2.2466e-41, 2.0268e-23,\n",
      "         1.3451e-10, 1.5686e-27, 8.2769e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0832, 0.0523, 0.0946, 0.0498, 0.0639, 0.0717, 0.1030, 0.0823, 0.0591,\n",
      "         0.0500, 0.0704, 0.0541, 0.0489, 0.0755, 0.0412]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1077.4738\n",
      "Test epoch : 47 Average loss: 1058.5622\n",
      "PP(train) = 2837.020, PP(valid) = 3032.201\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0464e-24, 3.3282e-11, 3.1829e-15, 8.4006e-04, 5.2611e-28,\n",
      "         4.4299e-34, 5.5285e-19, 9.0910e-01, 1.8182e-27, 2.4424e-32, 3.8907e-26,\n",
      "         3.0669e-15, 3.4795e-28, 9.0062e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0584, 0.0653, 0.0481, 0.0620, 0.0326, 0.0756, 0.0665, 0.0926, 0.0576,\n",
      "         0.0680, 0.0828, 0.0646, 0.0514, 0.1250, 0.0493]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1077.2355\n",
      "Test epoch : 48 Average loss: 1058.3126\n",
      "PP(train) = 2829.397, PP(valid) = 3026.567\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.2843e-35, 2.2046e-32, 2.1076e-11, 2.0156e-26, 7.7615e-08, 1.1953e-40,\n",
      "         0.0000e+00, 1.1865e-18, 1.5633e-12, 6.5885e-30, 1.4244e-32, 1.1105e-30,\n",
      "         9.9239e-18, 1.5408e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1076.8586\n",
      "Test epoch : 49 Average loss: 1058.0668\n",
      "PP(train) = 2821.725, PP(valid) = 3020.916\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8026e-45, 6.9962e-35, 2.4829e-16, 5.2005e-15, 1.5695e-22, 0.0000e+00,\n",
      "         0.0000e+00, 9.9994e-01, 5.5539e-05, 8.5356e-23, 0.0000e+00, 1.6389e-35,\n",
      "         3.2177e-20, 1.5403e-23, 4.4986e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1076.3508\n",
      "Test epoch : 50 Average loss: 1057.8205\n",
      "PP(train) = 2814.074, PP(valid) = 3015.251\n",
      "Writing to ./topicwords/9-topwords_e50.txt\n",
      "Topic 0: ピストンロッド 定着構造 セグメントリング 人間 蒸気 送風 継手板 白色 日常 撮像画像\n",
      "Topic 1: ピストンロッド 定着構造 人間 蒸気 白色 セグメントリング 継手板 床パネル 撮像画像 送風\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 3: 河川 物質 侵入 分離 自身 日 特徴構成 実施態様 セグメントリング 切断\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 課題\n",
      "Topic 5: ピストンロッド 定着構造 セグメントリング 継手板 蒸気 送風 白色 床パネル 日常 人間\n",
      "Topic 6: ピストンロッド 定着構造 セグメントリング 人間 蒸気 白色 送風 継手板 モニタリング 日常\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 手段 形態 説明 発明 課題\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 形態 手段\n",
      "Topic 9: ピストンロッド 定着構造 人間 セグメントリング 蒸気 白色 送風 継手板 撮像画像 モニタリング\n",
      "Topic 10: ピストンロッド 定着構造 蒸気 人間 送風 セグメントリング 継手板 白色 モニタリング 撮像画像\n",
      "Topic 11: ピストンロッド セグメントリング 定着構造 人間 継手板 蒸気 白色 送風 撮像画像 床パネル\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 13: ピストンロッド 定着構造 人間 セグメントリング 蒸気 白色 送風 対角線 床パネル モニタリング\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 9.1706 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : メッシュ状 対象土壌 メッシュ部 対象土壌Ｇ 円筒状メッシュ体 平板状メッシュ体 対象土壌Ｇ１ 管 図 細分化処理手段 該円筒状メッシュ体 該対象土壌 装置 前記細分化処理手段 先端部 口 周面 支持軸 調節板 メッシュ単位 対象土壌Ｇ１どうし 作業 円盤状 該平板状メッシュ体 実施形態 対象土壌どうし 支持アーム 所定 メッシュ体 作業船 円柱状 上方位置 発明 先端部周辺 多孔ボール 管先端 位置 浚渫 円筒軸心 該細分化処理手段 方法 ポンプ 排出管 特許文献 先端 水平移動 構造 すき間 水流 接地圧 線材 スラリー状 ブロック状 円形状 球体状 椀状 力 側面 固定ネジ Ｇ１ 中心 側面図 正面図 上下移動 内部 水底 間隔 状態 下方側 多孔板 傾斜角度 水平方向 四角形状等 吸引圧力 平面 結びつき 捨土場所 Ｇ 移動 平面方向 形状 掘削手段 説明 エキスパンドメタル板 浚渫対象 説明図 接地面積 棒材 特性等 手段 効率 ～ スクレーパ 円筒軸方向外側 ダム湖 特開平 号公報 水流流速 接地面側 形態 水平移動装置 水上 近傍 圧状態 該吸込 表面 上下方向 上下移動装置 周辺 接地側 移動台船上 平面図 所定位置 円周方向 所定角度 転動駆動装置 作業ヤード 作業スペース 転動 参照 カバー 範囲 汚濁 拡散 － 課題 特徴 広範囲 陸上 放射状 フランジ 互い 周辺水域 別 枠材 種々 個々 開口 ブレード等 他方側 板材等 土砂等 駆動機構 対向角度 側面視 技術分野 背景技術 浚渫用 所定ピッチ 浚渫設備 断面くさび形状 外側 障害物 スペース 目的 上記目的 内部構造 構成要素 上記 円環状 引揚げ せん断応力 中空 内側 提案 凹凸 開示 該埋入 効果 最良 鎖線 単独 複数 ２つ 独立 柱状 量 異物 フィルタ 長期 一定 部分 外形 上述 先 次 最小限 泥土 土地 大型 図面 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 8.2016 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : テーパ部 溶接 溶接部 溶接状態 径スタッド スタッド 前記テーパ部 軸径 溶接機 溶接側端部 小径部 面積比Ａａ 径 溶接側 アークスタッド溶接 アーク 側 外周縁部 軸部 外径 コンタクト方式アークスタッド溶接用 溶融金属 アーク電圧 コンタクト方式 スタッド溶接 Ｌ 円柱部 図 断面積Ａａ 先端部側 基端部側 Ａｂ 前記面積比Ａａ 断面積Ａｂ コンタクト方式アークスタッド溶接 テーパ 作用効果 ≦（Ａａ Ｖ程度 実験 発明 溶融 溶接断面 ≦ ≦Ｌ≦ Ａｂ 先端 電圧 状態 試験 程度 外周部 条件 スタッド断面 溶接実験 前記円柱部 異形棒鋼 溶接対象 溶接ガン 溶接機内 溶接性 溶接完了 説明 ～ 既存 断面 中央部 周縁部 要部 Ｖ 形態 厚み 範囲 内部 溶接余盛 エネルギ密度 径部 前記 上記溶接実験 アークシールド アークエネルギ 収縮孔 正面図 ギャップ方式 母材側 特許文献 傾斜角度 安定化 実施例 パイロットアーク アーク電流 マクロ断面 偏り 均一 使用 割れ 印 初期 前述 節 メインアーク アーク柱 アーク発生 横軸 縦軸 具体的形態 課題 効果 断面欠損 溶融金属層 前記課題 前記各種 前記条件 母材 収縮割れ等 ガスシールド効果 冷間圧造用炭素鋼線材 容量 原因 影響 特段 適合 不適合 中心 逆 高温割れ等 一般構造用圧延鋼材 技術分野 背景技術 丸鋼 マクロ試験 拡大説明図 鉄筋コンクリート棒鋼 先端形状 パイロット電流 電流 角度 電源設備 設備費用 試験体 耐熱磁器製 実用新案登録 同心円状 凸状 技術的状況 号公報 電磁界 変圧器 作業者 状況 内部応力 発生 盛 ブローホール 頭付 チャック 差込み ルール 突起 負荷 実状 品質 一定 方法 観点 開示 使い勝手 目的 手段 種々 後述 最良 ＳＳ ＳＷＲＣＨ ＳＤ 素材 Ｄａ Ｄｂ 個々 否 合否 良否 関連 形成 最大限 凝固 周 大気 スパッタ 欠点 欠陥 差異 確保 破断 上述 適用 図示 図面 図表 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 9.5353 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 開口 鉄筋コンクリート梁 補強構造 コンクリート梁 開口補強構造 梁主筋 試験体ＮＯ 試験体 開口補強部材 図 前記コンクリート梁 コンクリート 実施形態 フラットバー 前記開口 外周面 せん断補強筋 梁 鉄筋コンクリート 梁せい Ａ 前記開口補強部材 Ｂ 鋼管 補強 軸方向 コンクリート部材 梁構造 鉄骨鉄筋コンクリート 断面 発明 強度 応力 長手方向鉛直断面図 幅方向両端 鋼板 幅方向 断面図 円筒状 外周 製造方法 梁部材 リング状 内側 前記鋼板 鉄骨鉄筋コンクリート梁 開口近傍 両端 性能確認 実物大 複数 荷重 鉄筋コンクリート構造 変形角 変形性状 構成 手筋コンクリート梁 方法 変形 ひび割れ状況 上記 周辺 解析 せん断補強筋降伏応力 梁主筋降伏応力 径 変位 Ａ－Ａ Ｂ－Ｂ 変形関係 開口補強用鋼管 溶接部 上下 － －Ｄ 溶接手法 ＵＴ検査 非破壊検査 最大せん断力 構造 コンクリート強度 端面同士 最大せん断耐力 面内方向 内部応力 特許文献 特徴 内側部分 部分 長手方向 開口周辺 鉄筋コンクリート造 せん断荷重 鉄板 夫 一対 形態 上下方向 側面方向 型枠 コスト せん断力 接合部 コ 字 ｒａｄ 周囲 降伏応力 先端同士 自由度 使用量 ｋＮ程度 せん断強度等 断面形状 配筋 説明 手間 設計 流れ 低下 ＭＰａ 相当 ～ＮＯ グラフ 破壊 集中 圧縮応力 荷重－変位曲線 溶接 ジョイント部材 応力分布 応力集中 配筋作業 内部荷重 ～図 鋼種ＳＳ 鋼種ＳＤ 設備配管 圧縮荷重 両端付近 設備配管等 一般 課題 鋼材 円筒形 合計 値 効果 目視 図面 通常 変化 異型ＰＣ鋼棒 中央 破壊状況 程度 技術分野 背景技術 設備機器 有限要素法 特開平 号公報 特願 号参照 オフィスビル 経済性 発明者 コスト高 付近 数値解析 縮小モデル 内法スパン 開口径 正負交互 建物 開示 施工 障害 鉄筋 原因 手段 最良 帯状 理由 一体 亀裂 記載 揚 装置 現場 外側 一端 位置 Ｃ 工程 ２つ ３つ 継手 Ｎ ｍｍ 荷 ｋN 同等 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 10.2634 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 浮遊物質造粒促進板 浮遊物質 回収容器 浮遊物質ＳＳ 凝集浮遊物質 貯水槽 濁水 濁水処理槽 バブル発生装置 凝集浮遊物質Ｃ 容器本体 バブル 貫通穴 濁水処理装置 前記浮遊物質 前記浮遊物質造粒促進板 セメント微粒子 濁水処理 バブルＢ 物質 処理 図 プラグ 汚濁物質 粒径 ｂ 前記貯水槽 ガイドパイプ 前記バブル発生装置 実施 処理濁水 係止片 差し込み穴 濁水Ｗ 装置 支持ワイヤ スラリー槽 回収装置 係止溝 前記回収容器 形態 中和槽 スラリー 回収 処理槽 水槽 中和処理 発明 前記バブル 原水槽 胴部 プラグ部材 凝集反応槽 工事 支持片 トンネル工事 沈降 前記プラグ部材 処理水槽 鍔部 セメント粒子 フィルタープレス 沈降速度 底部 シックナー 発生等 供給管 ワイヤ支持片 回収容器内部 特許文献 下部 河川等 ＳＳ 程度 上下方向 マイクロバブル発生装置 浮上力 連続処理 浮上 水和反応 断面図 汚水処理装置 部分 アルミニウム等 直径 ｃ ｅ ごと貯水槽 取り付け穴 回収部 高分子凝集剤 ろ布 自然沈降力 放流処理水槽 浮上沈降作用 説明 炭酸ガス 微細気泡 上方 複数 下方 ｄ ｆ バキューム清掃 貯水槽内下部 有機物等 セメント 湧水 ハツリ清掃 清掃費用 前記 アクリル樹脂製 特徴 上部 ポリプロピレン 間隔 ポリ塩化アルミニウム 水質活性浄化剤 凝集物 所定期間経過 平面図 アクリル製 ワイヤ 前記ガイドパイプ トンネル工事等 トンネル掘削工事 トンネル掘削工事等 汚水 上記 構成 状態 沈降力 等 上昇速度 上水 排水ポンプ 水 羽根 等分 取っ手 シックナー下部 清掃 バキューム装置 Ｂ トンネル掘削工事用 Ｃ 脱水ケーキ 所定 有機物 汚れ成分 特開 号公報 目詰まり コスト高 強アルカリ 材料 金属材料 内部底部 アクリル等 バキューム等 電解装置 水中 課題 自重 ａ ｍｉｎ 樹脂 洗い水等 斜視図 構造 概略断面図 構造物基礎工事用等 セメント分 ダム建設工事 軸方向中央部 Ｗ 部分断面図 原水槽下流 セメント系注入材 先端部 軸方向 径 上端部 池等 コンクリート等 バルブＢ 排水管 分解斜視図 一定期間経過 供給 ｂ内部 ブロック図 ｂ下部 動作説明図 工程 注入 上層 下層 気泡 参照 － 配管 運転 労力 薬剤 塩酸 硫酸 位置 別 中和ユニット 効果 小径 図面 圧 他 壁面 鋼 表面 塗装 フック １つ 残り 一体 外周 ｂ外周 内径 加圧溶解方式 排出管 加圧ポンプ 技術分野 背景技術 基本技術 外周面 土粒子密度 ろ液 キャビテーション方式 旋回方式 ベンチュリー方式 水質 作用 実施態様 両側長辺側 上記実施 肉厚ｔ 利用可能性 支保部材 周面 ごと取り外し 概略構成 排出 液 栄養塩類 分離除去 コンクリート 補助工法 アジテーター車 交換頻度 マイクロ ミリオーダー 宙吊り状態 相互干渉 乱流 土砂 妨げ 場所 Suspended Solid 水域 水面 開示 対象 吹付 メンテナンス 遅延 量 目的 手段 性質 時点 径以下 空間 プロセス 多量 最良 図示 空気 タイプ 稼動 左右 障害 中間 各部 一端 他端 周辺 支保用 効率 役目 元 ｃｍ 水温 ℃ ｈ 偏流 数値 目安 ～ 均一 重量 Ｔ 任意 形状 弾性 連結 両端 人力 機械 産業 例 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 9.2416 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 濁水 加圧浮上槽 油分 濁水処理方法 濁水処理 セメント微粒子 油分付着ＳＳ 濁水処理装置 筒状案内部材 前記加圧浮上槽 濁水供給管 加圧空気 バイオレメディエーション処理 ＳＳ 処理濁水 濁水供給装置 汚泥 処理水槽 油分供給装置 バイオレメディエーション処理フィールド 油分供給管 加圧浮上効果 加圧空気発生装置 気泡 加圧浮上処理 処理水排出管 前記濁水 汚泥回収装置 加圧浮上法 図 工程 供給 加圧ポンプ 発明 処理槽 工事 加圧浮上技術 河川等 前記油分 中和槽 装置 微粒分 中和処理 特許文献 スラリー 下部 底部 他端部 セメント分 連続処理 処理対象 形態 湧水 気性菌 スラリー槽 フィルタープレス 実施 水 圧縮加圧空気 シックナー 物質 加圧し 微生物 加圧タンク セメント 前記気泡 一端部 原水槽 分解 程度 固化 排出 液 無機 効果 トンネル掘削工事 トンネル工事 工程図 トンネル掘削工事等 トンネル工事等 液分離 油分濃度 トンネル掘削工事用 炭酸ガス 号公報 ろ布 コスト高 ミクロン単位 環境負荷 バイオレメディエーション技術 凝集反応槽 洗い水等 説明 課題 成分 特徴 ＳＳ濃度 セメント粒子 水槽 構造物基礎工事用等 ダム建設工事 微生物等 清掃 フィールド セメント系注入材 ガソリン等 閉塞等 バキュームポンプ等 概略構成図 概略構成 地下水等 汚泥量 技術分野 背景技術 基本技術 概略ブロック図 注入 上層 上水 下層 場所 空気 参照 － 目的 軽油 灯油 図面 符号 気性 シックナー下部 バキューム清掃 ハツリ清掃 清掃費用 固化反応 土着菌 量 油 高分子凝集剤 内部下部中央 流量制御弁 圧力調整弁 円錐形 円筒形 角錐形 液面 ポリ塩化アルミニウム 宙ずり状態 利用可能性 特開平 特開 支保部材 動植物油 鉱物油 微生物分解 一定量 脱水ケーキ 汚染物質 補助工法 アジテーター車 目詰まり 交換頻度 強アルカリ 添加 レジン 添加割合 Ｌ Ｌ程度 タンク ガソリン マイクロバブル 専用菌株 攪拌道具 栄養塩 土砂 妨げ 水質 所定 Suspended Solid 働き 土壌 浄化 修復 開示 吹付 コンクリート 上記 配管 メンテナンス 運転 遅延 労力 薬剤 塩酸 硫酸 手段 逆 アスファルテン 重油 パラフィン 支障 不適 目安 つまり 最良 矩形 大型 上部 代わり 縄 ロープ レーキ 手道具 図示 適量 コンプレッサー 別 直径 浄水 Ａ 攪拌機 酸素 水分 燐 窒素 カリウム 産業\n",
      "\n",
      "===== # 6, Topic : 14, p : 9.5702 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 湾曲せん断補強筋 鉄筋コンクリート梁 開口 補強筋 補強構造 梁主筋 開口補強構造 せん断補強筋 湾曲部 コンクリート梁 試験体 試験体ＮＯ 図 前記コンクリート梁 実施形態 前記湾曲せん断補強筋 コンクリート 前記開口補強部材 前記湾曲部 Ａ 梁せい 開口補強部材 鋼管 前記開口 Ｂ 屈曲部 拘束筋 鉄筋コンクリート 応力 外周面 梁構造 断面図 長手方向鉛直断面図 複数 開口補強用鋼管 発明 強度 端部 開口補強用 せん断補強筋同士 鉄骨鉄筋コンクリート せん断補強筋降伏応力 開口部周辺 Ｂ－Ｂ コンクリート部材 湾曲鉄筋 リング状 梁部材 －Ｄ 最大せん断力 開口近傍 せん断荷重 方向 鉄骨鉄筋コンクリート梁 実物大 上下 最大せん断耐力 内側 変形角 変形性状 略鉛直方向 せん断力 鉄筋コンクリート構造 鉄筋 構成 周辺 ＮＯ 幅方向 － ひび割れ状況 接線方向 軸方向 梁主筋降伏応力 径 製造方法 性能確認 Ａ－Ａ 降伏応力 前記複数 解析 荷重 円筒状 変位 変形関係 長手方向両側 前記鋼管 主筋 形態 外周 変形 ＵＴ検査 非破壊検査 開口周辺 構造 内部応力 前記 せん断強度等 コンクリート強度 特許文献 上記 開口周囲 鋼種ＳＤ 配筋 両端部 鉄板 一対 鉄筋コンクリート造 溶接部 複数組 方法 コスト 部分 周囲 中間部 特徴 力 ＭＰａ 相当 ｒａｄ 幅方向両端 実施形態>　 断面形状 配筋作業 自由度 使用量 両側 ｋＮ程度 Ｃ－Ｃ 説明 手間 設計 夫 流れ 低下 グラフ 荷重－変位曲線 ～ＮＯ 圧縮応力 破壊 集中 応力分布 応力集中 鋼種ＳＳ ～図 両端 内部荷重 設備配管 圧縮荷重 直線状 フック状 設備配管等 一般 課題 鋼材 溶接 合計 値 効果 図面 変化 一体 角度 中央 異型ＰＣ鋼棒 両端付近 破壊状況 円筒形 略 程度 技術分野 背景技術 設備機器 溶接手法 溶接箇所 有限要素法 特開平 号公報 特願 号参照 オフィスビル 経済性 発明者 コスト高 内側部分 付近 同士 重ね継手 継手 数値解析 縮小モデル 内法スパン 開口径 正負交互 建物 開示 施工 障害 原因 手段 目視 最良 通常 理由 上部 下部 記載 現場 一端 左端 他端 右端 伝達 】　< 工程 先端 間隔 Ｎ ｍｍ 荷 ｋN 同等 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 9.5230 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 制震部材 境界梁 制震建物 鋼製ウェブ 鋼製フランジ 梁 鋼製端板 前記制震部材 建物 制震性能 上記制震部材 図 鉄筋コンクリート造 前記鋼製端板 鉄骨鉄筋コンクリート造境界梁 耐震壁 水平力 ＲＣ造 梁主筋 梁鉄筋 ｂ 境界梁部分 柱 耐震要素 中間部 制震 ＳＲＣ造境界梁 架構 ＲＣ梁ａ 履歴特性 実施例 請求項 鉄筋コンクリート ｃ 制震架構 発明 ＳＲＣ造 梁型枠 ＲＣ部材 枠体 前記梁主筋 特許文献 鉄骨 地震 部材断面 前記鋼製ウェブ 変形 立面図 力 板 プレキャストコンクリート製 構成 側面鋼板 上下 前記プレキャストコンクリート製 応答特性 鉄骨鉄筋コンクリート造 性能 中央 Ｈ形鋼ｄ 鋼板 プレキャストコンクリート 溶接接合 前記ＲＣ梁ａ 剛性 応力 耐力 ＳＲＣ造建物 鋼製筒体状 柱等 特徴 舌片 参照 複数 両側 弾性域 コンクリート 耐震建物 Ｘ型配筋ｂ 両端部 鉄筋 ｄ 該梁主筋 確保 一対 該上下 間隔 Ａ 耐震性能 技術分野 該制震部材 部材剛性 前記枠体 地震等 対角線方向 正面方向 枚数 モーメント 水平鋼板 建物中央部 中間 耐震壁ｃ 断面 ～（Ｃ 説明 早期 手段 他 課題 強度 孔 架構全体 上記特許文献 接合 接合部分 矩形 スパン 略水平方向 側面視 側面 圧縮力 下層部 Ａ－Ａ線矢視断面図 住宅部分 性能選択 減衰性能 断面矩形 図示例 背景技術 共有コア部分 効果 内側 － 役割 目的 上昇 調整 設計 形状 正方形 条件 溶接 現場 先端 ナット 記載 斜視図 両端 集合住宅 エネルギー吸収能力 技術的思想 スパン化 塑性率 応力集中 建築計画 平行状態 特開平 号公報 ダンパー 鋼材ダンパー エネルギー 経済性 中心 方法 空間 同等 取合い 膜 周囲 外面 開示 他方 伝達 次 最良 形態 ～ 最上階 コアウォール 一定 横 後述 最適 最後 分 要旨 業者 応用 図面 荷重 関係 グラフ 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 8.3183 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 梁 鋼管柱 梁主筋 異形鉄筋 鉄骨梁 接合構造 図 柱 鉄骨鉄筋コンクリート梁 コンクリート 鋼管 鉄骨コンクリート梁 鉄筋 角部 接合 梁鉄筋 熱影響 最大変位比 断面図 スターラップ ｔ 鉄筋コンクリート 関係 ｌ 水平部 Ｂ 熱影響部 せん断補強筋 発明 鉄筋接合比 先端部 軸鉄筋 Ｈ形鋼 梁せいＤ 軸芯 鉄骨柱 鉄筋距離比 補強プレート 軸線方向 リブ 溶接部 板厚中心 本願発明 ≦ 上下 グラフ図 軸力 接合位置 板厚ｔ 該鉄骨鉄筋コンクリート梁 ダイヤフラム 鉄骨鉄筋コンクリート ｄ 距離ｌ 距離ｄ 補強 該鉄骨梁 耐力 上記 靭性 溶接 平面図 φ ｔ≦ Ｄ≦ コンクリート部 正面図 地震 鋼材比 ≦（ｌ ≦（ｄ フランジ 節 面外変形 鉄骨鉄筋コンクリート造 － 軸 特開平 号公報 径φ 幅Ｂ 位置 Ａ－Ａ線断面図 説明 課題 応力 δ Ｄ 鉄骨量 断面欠損 水平方向 先端 孔 充填 両側 かつ 正規 形態 図面 反対 角度 破壊 原因 該梁主筋 配筋 せん断強度 垂直方向 技術分野 背景技術 平面方形 上下側 節側 耐火被覆 特許文献 前記上側 フランジ表面 強度 上側 表面 剛性 左右 開示 構成 障害 目的 手段 特徴 効果 建物 コストダウン 設置 最良 実施 内部 外周 欠点 靱性 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 9.5105 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 進ヘッド 進ロッド 薬液注入ヘッド 薬液注入孔 注入 圧力水噴射ノズル 圧力水 進 薬液注入管 薬液 薬液注入 特許文献 地盤改良用 自在ボーリング装置 進機 閉塞手段 図 請求項 地盤改良工法 薬液注入用 地盤改良 前記圧力水噴射ノズル 地盤 部分断面図 噴射 前進後退 進ヘッド内部空間 発明 注入孔 位置 ヘッド 注入用 前記薬液注入孔 実施例 薬液注入孔付近 孔 ドリルヘッド パイロット孔 末端部分 樹脂ボルト 記載 マグネットシート ボーリングロッド先端 工程 空気管 結剤 ボーリング 改良 自在ボーリング 押圧部材 圧力水経路 前進用モータ ボーリングロッド 工法 ロッド 構造物 斜め方向 号公報 水平方向 特開 進ヘッド前進用 内部 進用 注入段階 断面図 前記 後退位置 先端 瞬結剤 薬剤注入孔 前進フレーム ヘッド方向 目的位置 進段階 先端部 側面 進ヘッド付近 手段 性 技術 パッカ ｂ 押し工法 進ヘッド側面 圧力水ホース 装置 回転用モータ 掘削ヘッド 斜め向き 進ロッド付近 構造物Ｓ 水平ボーリング 所定位置 反対側 進装置 管 圧力空気 自在ボーリング工法 作業 高圧水 ホース 充填剤 概念図 中空 － 後方 状態 液混合形 末端 硬化剤 掘削用 進作業 ノズル方向 付近 実施 噴出孔 地表 立坑 地上 所定 管内 送水用 後退開始位置 カバーリング 内部空間 位置センサ 回転 パイロット孔形成 位置センサ等 下部地盤 空気 部分Ｚ 前進機構 構造物直下 説明 混合 軸 栓 内面 例 開口 管路 管体 シート状 斜め ジョイント ボルト 発明実施例 外観図 充填材 充填グラウト工法 外観斜視図 受信機 ジョイント構造 正面図 特開昭 樹脂製 進行方向 直線方向 下部等 鋼管等 無線送信機 Ａ液 Ｂ液 掘削 大型タンク等 外側 チェーン 引き抜き 向き 砂等 バックリーマ 課題 最低 挿入 接続等 切替 目的 特徴 効果 図面 公知 ウインチ等 隙間 周囲 ナット 内径 他 屈曲部 所定距離 要部 円板状 掘削工程終了 発進側 円筒状 カップ状 下部 経路 鋼管製 技術分野 背景技術 作業ヤード Ｏリング 外径 系外 指向性アンテナ 施工距離 開閉機構 後部側面 液状化 本明細書 電解トランスジューサ 単位長 吐出口 止弁 排土 形成 問題点 複数系統 施工 珪酸ナトリウム 複数個所 首下 シール部材 系統 磁性ゴム 個所 米国 任意 形状 ベース クランプ チャック 傾度 検出 脇 破線 対象 エリア 領域 開示 費用 日数 多量 拡径 切り替え 配管 利点 ねじれ 現地 経済 換装 拡径工 最良 形態 ａ 直前 効率 水ガラス 種 触媒 一般 周 保護 外周 先頭 障害 中間 最初 工事 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 10.0937 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 亜硫酸カルシウム クロム 処理材 量 セメント系材料 溶出量 量比率ｙ クロム含有量ｘ 配合量 改質セメント系材料 セメント ｙ≧ 還元剤 ｙ 排煙脱硫装置 溶出 地盤改良材 ｘ↑-↑ 式 化学両論量 生石灰混合物 発明 クロム含有量 特許文献 水和物 臨界量 含有量 材 Ｈ 亜硫酸カルシウム配合量ｎ 材料 ｎ 生石灰 Ａ 比率 ｘ 添加量 実施例 粉砕物 Ｂ 含有量ｘ 化学量論量ｎ 号試験 クロム量 Ｏ 号公報 セメント成分 混合物 還元反応 コンクリート粉砕物 クロム溶出量 硫酸カルシウム 地盤改良 無害化 地盤材料 Ｌ 配合倍率 還元 下記 含有量レベル コンクリート クロムｎモル 集材 吸収剤 方法 ｋｇ ＯＨ 物質 溶出試験 亜硫酸カルシウム含有物質 コンクリート構造物 セメント等 重金属低減材 対象 クロム量測定 使用量 石膏量 コンクリート塊 粒径 地盤 カルシウムフェライト 亜硫酸カルシウム粉体 水酸化クロム ｎモル 鉄 ～ ｍｇ ＣａＳＯ 範囲 環境 モルタル 環境省告示 環境庁告示 Ｏ換算 反応 特許 溶出防止 水和物添加量 亜硫酸カルシウム源 地盤改良材等 還元機能 還元作用 クロム含有物質 ソイルセメント 特開 骨材 発明者 セメント混練時 アルミナセメント セメントそのもの 銅 上記 石膏 混合粉体 カルシウムフェライト等 質量 形態 Ｃｒ 試料 表 ｍｍｏｌ 関係 セメント固化体 原料セメント粉体 信頼性 地盤材料等 式ｙ≧ μｍ 粉体 用途 手法 ＳＯ タイプ 効果 前記 種類 水和セメント分 細骨材 骨材込み 環境基準 供試材 モルタル構造物 土質材料 クロム自体 クロム濃度 装置 水 各種構造物 粉粒状 配合組成 錫 － 製法 X 例 図 粉粒状 溶出抑制効果 プレミックス粉体 酸性 添加 検討 種々 現場 ＣａＯ 還元不足 還元力 生石灰成分 無害化効果 防腐剤 殺菌剤 後述実施例 無害 火力発電所等 Ｈ↑+（アルカリ性 固定化 生石灰ＣａＯ ↑-＋Ｈ コンクリート残渣 覆土等 銅等 エトリンガイト等 実験例 説明 盛土 カルシウムアルミネート 開示 課題 薬剤 金属 土壌 観点 手段 硬化 ii アルカリ性 ＣｒＯ 調査 挙動 前述 段階 １つ 混練物 反比例 曲線 余裕 係数 信頼性向上 水準 モルタル成分 火力発電設備 ｍｇ 硫酸 技術分野 背景技術 金属元素 遷移金属 アルカリ性領域 不溶化効果 高温酸化雰囲気 質量割合 質量比 焼成工程 鉄塩 錆 錆流れ 使用 目的 上記目的 v ～（v ↑-＋ ＯＨ↑- 浄水器 発酵工業 酸性域 本明細書 後述 実験 種々検討 原料 解体現場 液 比 ろ液 管理指標 塊状 亜鉛 鉛 微量 粘土 カルシウムアルミノフェライト 放出 バナジウム 記載 コスト 恐れ 併用 塩類 さび 原因 上述 性質 i 土 iii iv 副 微粉 当たり 失敗 主成分 最良 一般 水中 イオン ⇔ ↑+＋ 食品 Ｃａ 不溶性 通常 事情 逆 状態 規定 個々 不純物 集塵機 下流 プロセス スラッジ 程度 分析 アンダー 他 近辺 市販 値 別 代わり サンプル 図面 グラフ\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.0816e-37, 3.3235e-15, 7.1617e-30, 2.2365e-06, 1.7269e-35,\n",
      "         1.2051e-43, 1.2899e-01, 1.7877e-21, 5.2367e-26, 0.0000e+00, 3.9538e-34,\n",
      "         8.1804e-22, 2.5977e-21, 8.7101e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0829, 0.0502, 0.0970, 0.0500, 0.0630, 0.0705, 0.1042, 0.0848, 0.0592,\n",
      "         0.0506, 0.0682, 0.0545, 0.0494, 0.0743, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1037.9773\n",
      "Test epoch : 1 Average loss: 1101.1107\n",
      "PP(train) = 2857.518, PP(valid) = 2911.525\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2501e-33, 8.7375e-19, 1.4222e-19, 3.6700e-13, 0.0000e+00,\n",
      "         2.8023e-35, 1.3076e-05, 3.3359e-23, 2.9233e-23, 1.8240e-29, 2.2244e-27,\n",
      "         1.7817e-14, 9.6437e-31, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1037.6835\n",
      "Test epoch : 2 Average loss: 1100.8443\n",
      "PP(train) = 2850.885, PP(valid) = 2906.452\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2039e-45, 5.8809e-24, 1.4197e-07, 4.0225e-17, 1.2128e-04, 1.4090e-41,\n",
      "         1.6073e-27, 9.9985e-01, 2.4336e-12, 5.5817e-19, 7.0183e-29, 2.4279e-31,\n",
      "         3.6498e-10, 3.3891e-29, 2.7841e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1037.3804\n",
      "Test epoch : 3 Average loss: 1100.5697\n",
      "PP(train) = 2843.156, PP(valid) = 2900.773\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.7853e-33, 4.4720e-32, 4.1965e-28, 7.6877e-22, 0.0000e+00,\n",
      "         1.8266e-40, 5.8252e-14, 4.5032e-18, 2.0319e-40, 3.7025e-41, 1.3388e-37,\n",
      "         1.5350e-25, 1.2899e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1036.9288\n",
      "Test epoch : 4 Average loss: 1100.2906\n",
      "PP(train) = 2834.740, PP(valid) = 2894.728\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9811e-35, 9.8563e-13, 2.7408e-36, 8.5698e-17, 3.6013e-40,\n",
      "         0.0000e+00, 1.0000e+00, 7.5473e-17, 1.2474e-31, 1.0631e-39, 9.0975e-33,\n",
      "         1.0983e-22, 7.1764e-27, 4.1692e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1036.5941\n",
      "Test epoch : 5 Average loss: 1100.0117\n",
      "PP(train) = 2826.113, PP(valid) = 2888.588\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8701e-35, 1.1571e-27, 1.9608e-24, 3.6877e-16, 0.0000e+00,\n",
      "         0.0000e+00, 2.0694e-14, 2.5707e-16, 8.8818e-29, 4.4392e-39, 0.0000e+00,\n",
      "         2.4893e-20, 3.6662e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1036.2183\n",
      "Test epoch : 6 Average loss: 1099.7295\n",
      "PP(train) = 2817.736, PP(valid) = 2882.721\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6804e-39, 2.3299e-20, 2.0988e-04, 3.6491e-26, 1.5277e-08, 4.0743e-36,\n",
      "         1.6816e-35, 5.9402e-13, 1.5857e-22, 2.3557e-24, 3.2838e-38, 9.2545e-34,\n",
      "         9.1325e-17, 1.9022e-28, 9.9979e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1035.9692\n",
      "Test epoch : 7 Average loss: 1099.4484\n",
      "PP(train) = 2809.493, PP(valid) = 2876.998\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6034e-24, 2.6066e-15, 1.9144e-17, 3.2901e-09, 4.0638e-44,\n",
      "         4.8695e-42, 9.9329e-11, 5.3987e-19, 1.5127e-22, 4.9481e-41, 2.9076e-30,\n",
      "         8.2681e-17, 2.5705e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1035.4819\n",
      "Test epoch : 8 Average loss: 1099.1700\n",
      "PP(train) = 2801.195, PP(valid) = 2871.242\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.7960e-28, 1.9783e-22, 6.1298e-34, 1.0431e-05, 2.6905e-43,\n",
      "         3.9236e-44, 9.8533e-01, 5.8434e-12, 7.5464e-31, 3.9362e-42, 2.4093e-30,\n",
      "         9.0402e-26, 8.1574e-30, 1.4662e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1044, 0.0549, 0.0444, 0.0789, 0.0920, 0.0774, 0.0442, 0.0532,\n",
      "         0.0372, 0.1184, 0.0440, 0.0368, 0.0937, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1034.9742\n",
      "Test epoch : 9 Average loss: 1098.8883\n",
      "PP(train) = 2792.876, PP(valid) = 2865.428\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1210e-44, 7.3674e-24, 1.7764e-31, 5.3265e-18, 0.0000e+00,\n",
      "         1.4083e-42, 2.0856e-18, 1.4076e-17, 3.0421e-35, 2.7285e-36, 7.6709e-40,\n",
      "         2.2491e-21, 1.3568e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1034.6424\n",
      "Test epoch : 10 Average loss: 1098.6118\n",
      "PP(train) = 2784.629, PP(valid) = 2859.682\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.2626e-43, 4.6590e-30, 7.9686e-18, 1.6077e-26, 2.5521e-13, 1.4321e-42,\n",
      "         4.3440e-44, 4.1984e-08, 1.0312e-21, 4.2145e-16, 2.5566e-32, 6.3197e-33,\n",
      "         1.0399e-20, 3.1867e-15, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1034.4085\n",
      "Test epoch : 11 Average loss: 1098.3352\n",
      "PP(train) = 2776.442, PP(valid) = 2853.951\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4944e-33, 5.9967e-22, 1.8102e-19, 1.2436e-10, 9.3994e-29,\n",
      "         0.0000e+00, 1.0599e-16, 1.7888e-15, 2.0911e-24, 3.1832e-41, 1.2567e-40,\n",
      "         7.8957e-11, 1.0828e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1033.9165\n",
      "Test epoch : 12 Average loss: 1098.0598\n",
      "PP(train) = 2768.305, PP(valid) = 2848.237\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1903e-29, 4.6841e-15, 2.5874e-24, 7.4756e-01, 4.7644e-44,\n",
      "         6.2196e-34, 6.8548e-06, 3.1067e-12, 1.2177e-20, 2.1762e-37, 4.8167e-36,\n",
      "         9.5741e-22, 1.2785e-27, 2.5243e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0528, 0.0476, 0.0538, 0.0414, 0.0865, 0.0669, 0.1084, 0.1056, 0.0490,\n",
      "         0.0822, 0.0668, 0.0598, 0.0476, 0.0651, 0.0666]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1033.5020\n",
      "Test epoch : 13 Average loss: 1097.7867\n",
      "PP(train) = 2760.233, PP(valid) = 2842.553\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.6849e-40, 2.7959e-19, 3.2032e-34, 2.5068e-11, 1.3495e-28,\n",
      "         0.0000e+00, 2.0806e-08, 4.3876e-26, 3.4741e-30, 2.2128e-39, 4.7843e-41,\n",
      "         5.6093e-21, 2.5991e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1033.2026\n",
      "Test epoch : 14 Average loss: 1097.5148\n",
      "PP(train) = 2752.283, PP(valid) = 2836.978\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7644e-44, 2.8635e-32, 2.0103e-06, 7.7301e-28, 1.5447e-08, 1.2892e-43,\n",
      "         8.7161e-35, 1.4830e-08, 5.9051e-14, 2.1157e-21, 3.0788e-30, 9.6536e-34,\n",
      "         6.1855e-14, 4.2536e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1032.7906\n",
      "Test epoch : 15 Average loss: 1097.2448\n",
      "PP(train) = 2744.394, PP(valid) = 2831.448\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4185e-36, 1.3133e-15, 3.4971e-15, 1.6477e-18, 4.2691e-08, 8.1836e-43,\n",
      "         2.5115e-22, 3.3424e-04, 3.8363e-02, 6.7696e-22, 6.9826e-32, 3.4666e-31,\n",
      "         1.3330e-08, 4.8473e-19, 9.6130e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0808, 0.0452, 0.1013, 0.0508, 0.0587, 0.0675, 0.1057, 0.0926, 0.0594,\n",
      "         0.0530, 0.0629, 0.0561, 0.0512, 0.0728, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1032.5151\n",
      "Test epoch : 16 Average loss: 1096.9755\n",
      "PP(train) = 2736.605, PP(valid) = 2826.006\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2981e-43, 1.2177e-34, 4.3001e-22, 8.7469e-22, 9.3293e-13, 3.6818e-41,\n",
      "         5.9212e-41, 6.1231e-15, 2.4204e-22, 1.6867e-25, 0.0000e+00, 5.6655e-36,\n",
      "         1.0799e-26, 2.9005e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1032.1240\n",
      "Test epoch : 17 Average loss: 1096.7092\n",
      "PP(train) = 2728.765, PP(valid) = 2820.483\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.0240e-39, 1.1459e-07, 4.2449e-23, 3.1769e-31, 1.7725e-41,\n",
      "         1.5724e-41, 6.3663e-08, 4.0284e-10, 1.5799e-28, 1.8661e-38, 2.7915e-34,\n",
      "         3.2849e-23, 7.0652e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1031.6907\n",
      "Test epoch : 18 Average loss: 1096.4434\n",
      "PP(train) = 2720.961, PP(valid) = 2814.968\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.0103e-43, 5.0279e-28, 1.1952e-12, 6.8739e-22, 4.6542e-10, 0.0000e+00,\n",
      "         0.0000e+00, 9.6789e-14, 2.7498e-17, 6.2689e-28, 8.5858e-37, 3.1581e-33,\n",
      "         1.5676e-13, 4.3359e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1031.3230\n",
      "Test epoch : 19 Average loss: 1096.1789\n",
      "PP(train) = 2713.380, PP(valid) = 2809.632\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1124e-24, 1.9440e-16, 4.5678e-09, 2.0809e-20, 5.4532e-01, 4.6085e-26,\n",
      "         5.2698e-34, 9.6979e-04, 1.9420e-03, 2.7388e-25, 4.5421e-28, 2.0184e-22,\n",
      "         1.7376e-16, 3.1148e-06, 4.5177e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0599, 0.0472, 0.0647, 0.0440, 0.0790, 0.0676, 0.1090, 0.1027, 0.0520,\n",
      "         0.0734, 0.0661, 0.0591, 0.0489, 0.0673, 0.0591]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1031.1159\n",
      "Test epoch : 20 Average loss: 1095.9163\n",
      "PP(train) = 2705.920, PP(valid) = 2804.378\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5470e-30, 1.5262e-26, 4.0073e-38, 4.8902e-29, 0.0000e+00,\n",
      "         0.0000e+00, 8.6879e-19, 1.1166e-22, 1.0854e-33, 0.0000e+00, 8.2677e-43,\n",
      "         2.9737e-24, 4.7396e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1030.4884\n",
      "Test epoch : 21 Average loss: 1095.6555\n",
      "PP(train) = 2698.549, PP(valid) = 2799.235\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 1.4740e-30, 2.8394e-12, 5.7310e-26, 5.9218e-01, 3.1604e-39,\n",
      "         2.1015e-35, 4.0482e-01, 2.9445e-03, 2.0165e-18, 1.5855e-32, 4.2158e-26,\n",
      "         3.0286e-06, 2.1027e-27, 4.4300e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0597, 0.0684, 0.0484, 0.0420, 0.0916, 0.0781, 0.0966, 0.0776, 0.0499,\n",
      "         0.0667, 0.0879, 0.0548, 0.0432, 0.0762, 0.0590]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1030.2101\n",
      "Test epoch : 22 Average loss: 1095.3942\n",
      "PP(train) = 2691.166, PP(valid) = 2794.041\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.5716e-39, 6.9238e-14, 2.3461e-35, 1.3534e-26, 0.0000e+00,\n",
      "         0.0000e+00, 5.5124e-17, 8.7130e-13, 9.3945e-34, 2.6421e-38, 6.6842e-43,\n",
      "         6.5634e-28, 1.1591e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1029.8089\n",
      "Test epoch : 23 Average loss: 1095.1363\n",
      "PP(train) = 2683.630, PP(valid) = 2788.670\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7739e-39, 2.0434e-10, 1.6931e-19, 4.9708e-08, 1.5316e-42,\n",
      "         8.5671e-37, 4.5859e-09, 1.7717e-06, 1.2420e-22, 6.7630e-36, 4.6309e-41,\n",
      "         6.8991e-12, 1.9012e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1029.4294\n",
      "Test epoch : 24 Average loss: 1094.8814\n",
      "PP(train) = 2676.254, PP(valid) = 2783.450\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5138e-34, 1.0330e-30, 1.4980e-26, 6.4883e-23, 2.5784e-43,\n",
      "         2.8026e-45, 1.5944e-22, 2.3450e-18, 2.6477e-38, 5.1367e-40, 7.4845e-36,\n",
      "         2.5895e-27, 8.6018e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1029.2063\n",
      "Test epoch : 25 Average loss: 1094.6268\n",
      "PP(train) = 2669.127, PP(valid) = 2778.458\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.2321e-23, 1.0959e-23, 7.5113e-24, 5.5648e-01, 5.8855e-44,\n",
      "         1.7810e-39, 3.5359e-16, 6.5553e-10, 4.1297e-27, 4.8304e-32, 1.6370e-23,\n",
      "         4.4563e-18, 9.1477e-39, 4.4352e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0596, 0.0472, 0.0642, 0.0439, 0.0795, 0.0675, 0.1091, 0.1029, 0.0518,\n",
      "         0.0739, 0.0661, 0.0591, 0.0488, 0.0671, 0.0595]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1028.7914\n",
      "Test epoch : 26 Average loss: 1094.3735\n",
      "PP(train) = 2661.935, PP(valid) = 2773.351\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1464e-42, 1.2823e-27, 1.4870e-20, 4.7950e-26, 1.1885e-05, 2.2079e-34,\n",
      "         4.4802e-31, 7.0021e-16, 2.7541e-12, 4.3963e-23, 6.3957e-20, 1.1081e-24,\n",
      "         2.7449e-09, 3.3382e-21, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1028.5747\n",
      "Test epoch : 27 Average loss: 1094.1217\n",
      "PP(train) = 2654.729, PP(valid) = 2768.196\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.0395e-27, 4.6414e-16, 2.8081e-22, 1.2978e-27, 0.0000e+00,\n",
      "         0.0000e+00, 1.9970e-15, 2.7623e-23, 7.1776e-36, 0.0000e+00, 0.0000e+00,\n",
      "         2.2856e-30, 3.6854e-43, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1027.9754\n",
      "Test epoch : 28 Average loss: 1093.8682\n",
      "PP(train) = 2647.563, PP(valid) = 2763.093\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0884e-27, 4.3317e-33, 1.5375e-30, 4.2883e-19, 7.0065e-45,\n",
      "         2.1203e-40, 5.4907e-13, 1.3593e-28, 1.8855e-28, 3.0254e-31, 0.0000e+00,\n",
      "         3.6076e-21, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1027.7966\n",
      "Test epoch : 29 Average loss: 1093.6167\n",
      "PP(train) = 2640.698, PP(valid) = 2758.237\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8640e-38, 6.1470e-27, 4.1173e-18, 5.4975e-13, 9.8293e-02, 6.6655e-35,\n",
      "         1.8276e-26, 1.5574e-02, 4.5382e-09, 2.7250e-08, 3.6775e-32, 7.6292e-22,\n",
      "         1.4283e-18, 2.4494e-19, 8.8613e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0777, 0.0457, 0.0953, 0.0492, 0.0637, 0.0677, 0.1078, 0.0935, 0.0581,\n",
      "         0.0556, 0.0636, 0.0563, 0.0506, 0.0708, 0.0443]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1027.5297\n",
      "Test epoch : 30 Average loss: 1093.3702\n",
      "PP(train) = 2633.918, PP(valid) = 2753.480\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.2841e-28, 2.9387e-09, 5.6297e-19, 6.5353e-08, 3.3495e-41,\n",
      "         4.5992e-34, 1.3241e-08, 1.2100e-06, 3.6368e-30, 1.4557e-31, 3.7880e-35,\n",
      "         6.3631e-21, 5.2434e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.998, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1027.0177\n",
      "Test epoch : 31 Average loss: 1093.1241\n",
      "PP(train) = 2626.909, PP(valid) = 2748.460\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.4074e-30, 4.8938e-17, 2.2356e-31, 9.2057e-21, 0.0000e+00,\n",
      "         2.4803e-43, 1.3407e-19, 4.5167e-11, 2.4367e-26, 6.3242e-32, 1.9899e-20,\n",
      "         5.8610e-22, 5.0070e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1026.7844\n",
      "Test epoch : 32 Average loss: 1092.8819\n",
      "PP(train) = 2619.870, PP(valid) = 2743.427\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8213e-33, 3.2564e-19, 2.2421e-23, 2.0589e-25, 1.2352e-20, 6.3457e-36,\n",
      "         2.7231e-33, 9.9933e-01, 1.2451e-13, 1.4334e-14, 2.8026e-45, 4.3536e-29,\n",
      "         1.6203e-15, 3.7793e-28, 6.6521e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1026.4883\n",
      "Test epoch : 33 Average loss: 1092.6396\n",
      "PP(train) = 2612.975, PP(valid) = 2738.509\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.4619e-28, 1.3248e-11, 1.6033e-24, 9.7286e-09, 2.2321e-37,\n",
      "         4.2039e-45, 3.6528e-09, 1.7274e-16, 7.3178e-29, 4.0020e-29, 6.5323e-25,\n",
      "         1.6418e-19, 3.8480e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1026.0647\n",
      "Test epoch : 34 Average loss: 1092.3961\n",
      "PP(train) = 2606.311, PP(valid) = 2733.777\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.0256e-44, 6.6068e-10, 2.2055e-31, 4.3022e-17, 0.0000e+00,\n",
      "         0.0000e+00, 5.4324e-10, 1.6956e-20, 4.9634e-26, 1.2450e-38, 1.7305e-30,\n",
      "         1.4152e-27, 3.4329e-36, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1025.5991\n",
      "Test epoch : 35 Average loss: 1092.1523\n",
      "PP(train) = 2599.792, PP(valid) = 2729.158\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7492e-43, 1.0392e-26, 3.0510e-23, 1.6370e-20, 3.0757e-04, 2.3808e-40,\n",
      "         8.6745e-31, 7.7743e-05, 2.8482e-10, 1.3941e-24, 2.2491e-27, 1.9889e-31,\n",
      "         2.5052e-20, 3.2442e-17, 9.9961e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1025.4152\n",
      "Test epoch : 36 Average loss: 1091.9123\n",
      "PP(train) = 2593.229, PP(valid) = 2724.521\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8236e-40, 3.4152e-22, 1.5042e-27, 1.4205e-08, 0.0000e+00,\n",
      "         4.7891e-35, 3.7160e-12, 1.9001e-06, 6.9521e-22, 3.5629e-41, 6.8634e-35,\n",
      "         5.4719e-21, 9.5697e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1024.9661\n",
      "Test epoch : 37 Average loss: 1091.6719\n",
      "PP(train) = 2586.706, PP(valid) = 2719.874\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7381e-37, 7.2174e-30, 4.0350e-23, 1.7021e-24, 9.7191e-01, 1.1613e-37,\n",
      "         8.7727e-29, 1.1454e-02, 4.5005e-09, 4.5978e-20, 5.1128e-25, 3.6928e-22,\n",
      "         4.8405e-09, 6.5998e-06, 1.6634e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0456, 0.0484, 0.0430, 0.0384, 0.0953, 0.0661, 0.1065, 0.1073, 0.0454,\n",
      "         0.0923, 0.0677, 0.0600, 0.0457, 0.0627, 0.0755]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1024.7991\n",
      "Test epoch : 38 Average loss: 1091.4372\n",
      "PP(train) = 2580.078, PP(valid) = 2715.130\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5656e-41, 6.7279e-18, 2.8761e-22, 3.0401e-17, 8.4178e-39,\n",
      "         1.8173e-35, 1.6834e-13, 3.9195e-31, 4.8308e-32, 0.0000e+00, 0.0000e+00,\n",
      "         9.8602e-23, 8.4507e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1024.3418\n",
      "Test epoch : 39 Average loss: 1091.2017\n",
      "PP(train) = 2573.406, PP(valid) = 2710.347\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.7260e-27, 2.8903e-16, 6.2011e-18, 2.7467e-11, 5.7137e-39,\n",
      "         2.0198e-38, 8.0821e-01, 7.3750e-19, 6.0807e-15, 3.0326e-26, 9.1560e-29,\n",
      "         1.5819e-12, 6.1692e-28, 1.9179e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0844, 0.0909, 0.0626, 0.0461, 0.0763, 0.0881, 0.0833, 0.0512, 0.0551,\n",
      "         0.0402, 0.1070, 0.0466, 0.0396, 0.0904, 0.0383]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1024.0825\n",
      "Test epoch : 40 Average loss: 1090.9675\n",
      "PP(train) = 2566.850, PP(valid) = 2705.628\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.4720e-33, 6.2165e-16, 3.4713e-32, 6.9712e-21, 0.0000e+00,\n",
      "         3.1669e-43, 2.5465e-19, 8.4640e-20, 3.1773e-37, 0.0000e+00, 6.9133e-34,\n",
      "         1.1812e-13, 1.9625e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1023.7148\n",
      "Test epoch : 41 Average loss: 1090.7334\n",
      "PP(train) = 2560.553, PP(valid) = 2701.149\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 9.6109e-30, 4.2316e-15, 2.9076e-24, 3.0320e-12, 5.2170e-42,\n",
      "         2.8026e-45, 6.9303e-19, 8.0302e-21, 2.2226e-27, 3.9447e-39, 1.0178e-36,\n",
      "         8.6358e-23, 7.0552e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1023.3754\n",
      "Test epoch : 42 Average loss: 1090.5021\n",
      "PP(train) = 2554.426, PP(valid) = 2696.851\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1512e-23, 9.8152e-22, 1.1776e-16, 1.8007e-05, 0.0000e+00,\n",
      "         4.4295e-42, 2.0631e-12, 5.9673e-08, 3.3757e-23, 1.5414e-44, 6.9122e-38,\n",
      "         4.0678e-06, 3.4707e-33, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1022.9804\n",
      "Test epoch : 43 Average loss: 1090.2714\n",
      "PP(train) = 2548.114, PP(valid) = 2692.346\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0829e-44, 1.9328e-35, 1.2705e-19, 1.1958e-32, 2.1841e-27, 0.0000e+00,\n",
      "         0.0000e+00, 1.1613e-15, 1.1632e-15, 1.7283e-30, 3.0913e-42, 1.0089e-43,\n",
      "         3.6560e-24, 3.8461e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1022.7482\n",
      "Test epoch : 44 Average loss: 1090.0431\n",
      "PP(train) = 2541.795, PP(valid) = 2687.800\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7208e-30, 1.0643e-27, 5.7252e-31, 1.0033e-16, 5.4651e-44,\n",
      "         0.0000e+00, 9.9994e-01, 4.9192e-10, 8.4078e-45, 0.0000e+00, 3.4173e-35,\n",
      "         1.6159e-16, 2.6645e-28, 6.1134e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1022.2957\n",
      "Test epoch : 45 Average loss: 1089.8162\n",
      "PP(train) = 2535.567, PP(valid) = 2683.344\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 1.3377e-30, 2.8557e-13, 1.6353e-18, 1.8341e-06, 0.0000e+00,\n",
      "         6.1889e-37, 5.7813e-10, 5.6521e-06, 3.1399e-27, 6.6706e-35, 1.4847e-31,\n",
      "         8.5502e-20, 8.0849e-27, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1022.0632\n",
      "Test epoch : 46 Average loss: 1089.5906\n",
      "PP(train) = 2529.489, PP(valid) = 2679.055\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.1957e-31, 2.5581e-19, 2.3491e-30, 6.0972e-19, 0.0000e+00,\n",
      "         0.0000e+00, 4.0216e-17, 8.5762e-30, 1.8777e-43, 3.1321e-40, 0.0000e+00,\n",
      "         6.5778e-38, 4.2187e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1021.7668\n",
      "Test epoch : 47 Average loss: 1089.3646\n",
      "PP(train) = 2523.441, PP(valid) = 2674.740\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.7603e-40, 4.5285e-21, 3.3306e-28, 9.9482e-21, 0.0000e+00,\n",
      "         0.0000e+00, 3.0418e-13, 4.8480e-27, 2.3928e-31, 3.6656e-37, 6.5637e-42,\n",
      "         4.3928e-20, 6.7963e-42, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1021.3928\n",
      "Test epoch : 48 Average loss: 1089.1411\n",
      "PP(train) = 2517.409, PP(valid) = 2670.450\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5471e-40, 3.6875e-35, 1.4115e-18, 1.3264e-21, 7.7267e-02, 2.3956e-34,\n",
      "         1.0628e-31, 4.7071e-12, 1.2296e-16, 3.1996e-31, 2.6747e-28, 5.7445e-25,\n",
      "         1.8983e-08, 2.7826e-25, 9.2273e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0785, 0.0449, 0.0978, 0.0494, 0.0627, 0.0672, 0.1081, 0.0940, 0.0584,\n",
      "         0.0550, 0.0627, 0.0562, 0.0509, 0.0705, 0.0437]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1021.2980\n",
      "Test epoch : 49 Average loss: 1088.9185\n",
      "PP(train) = 2511.328, PP(valid) = 2666.105\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.3215e-37, 1.2244e-23, 1.0669e-36, 1.6827e-21, 3.1556e-39,\n",
      "         0.0000e+00, 1.5030e-05, 3.5823e-11, 2.2794e-25, 0.0000e+00, 5.7827e-35,\n",
      "         5.4955e-26, 5.2309e-33, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1020.8060\n",
      "Test epoch : 50 Average loss: 1088.6970\n",
      "PP(train) = 2505.335, PP(valid) = 2661.822\n",
      "Writing to ./topicwords/10-topwords_e50.txt\n",
      "Topic 0: ピストンロッド 定着構造 セグメントリング 蒸気 ガイドレール 降雨 発破 粉砕 外柱 撹拌\n",
      "Topic 1: ピストンロッド 定着構造 蒸気 セグメントリング 電磁弁 粉砕 降雨 外柱 発破 裏側\n",
      "Topic 2: 参照 配置 位置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 河川 物質 特開昭 侵入 フィルター 固定手段 つぎ パネル セグメントリング 号\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: 定着構造 ピストンロッド セグメントリング 蒸気 緑化 発破 電磁弁 外柱 粉砕 ガイドレール\n",
      "Topic 6: ピストンロッド 定着構造 セグメントリング 蒸気 ガイドレール 緑化 発破 粉砕 白色 外柱\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 課題\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 貫通孔 技術分野 形態 手段\n",
      "Topic 9: 定着構造 蒸気 ピストンロッド セグメントリング 降雨 天井部 ガイドレール 緑化 内型枠 夜間\n",
      "Topic 10: ピストンロッド 定着構造 蒸気 セグメントリング 電磁弁 粉砕 ガイドレール 発破 外柱 ＲＣ構造\n",
      "Topic 11: 定着構造 セグメントリング ピストンロッド 蒸気 発破 ガイドレール 外柱 粉砕 緑化 ＲＣ構造\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: 定着構造 ピストンロッド 蒸気 セグメントリング 発破 粉砕 白色 電磁弁 裏側 外柱\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 9.5735 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 水平搬送台車 搬送物 台車本体 台車 搬送 自己位置 位置関係 昇降機構 状態 シザースリンク機構 下部フレーム 台座部 伸縮機構 図 支持機構 操舵輪 フォークリフト 位置 水平搬送 上部フレーム シザース跳ね上げピン ピストンロッド フォーク 位置関係認識手段 ピン係合部 資材搬送 駆動操舵輪 連結フレーム 制御手段 資材パレット 変換機構 荷取り 搬送動作 荷 建設現場 発明 自己位置推定手段 搬送状態 中間部材 駆動輪 動作 眼カメラセンサー 接続部材 台車軌道 台車重量 フォークリフト台車 方向 リンク部材 安定性 特許文献 位置情報 搬送経路 レーザーセンサー シザースリンク機構展開 後端下部 機構 中央側 資材 立壁部 中央部 下側 シザース状 搬送作業 上下方向 シリンダー 自動搬送システム 通信手段 搬送システム Ｂ 準備状態 終了状態 作業 床面 上端 垂直搬送 搬送モード 手段 前端下部 Ａ 搬送対象物 台車等 図面データ 号公報 施工現場 台車重心 係合 概略斜視図 形態 センサー 磁気テープ 左右方向 上下 フォーク格納 シザースリンク機構格納状態 シザース跳ね上げピン格納状態 効果 外部 実施 相対位置 シザース跳ね上げピン展開状態 逆Ｕ字状 荷上げ 上昇状態 テープ 建築物 技術 移動 略中間位置 カメラセンサー 下部フレーム下降状態 持状態 後端 動力式台車 シザースリンク機構格納完了 特徴 Ｃ 動き 溝 下端 側部 図面 情報 側 自動化 省人化 付け作業 上昇動作 床耐荷重 床補強 ～図 側左右 － 上記 無線 有線 下面 上方 レール 特開平 特開 後端上部 展開 昇降駆動源 伸縮駆動源 前端上部 維持管理 自律走行 逆 下降状態 Ｘ字状 フォーク伸張状態 説明 資機材 人工 通常 課題 床 フォーク作業 重心 変更 部分 前側 積載物 左右方向外側 四隅側 前端面 略Ｌ字状断面 壁状 固定状態 中央上端 フォーク伸張 上端面 下降 図面情報 先行技術文献 尺箱状 展開変形 磁気テープ等 建設業 上昇 前端 前側左右 適応性 安全性 利用可能性 作業員 技術分野 背景技術 が床面 内側面 レーザー光 現状 記載 重量 地面 設備 手間 目的 増大 左側 右側 マーカー 角度 荷重 カウンターウェイト 車輪 スライダー 一体 凹部 周辺 既知 フォーク部分 後端どうし 軽量化 無軌道化 域情報 施工場所 ～ カウンターウエイト方式 労働者不足 工事用エレベータ 端太角 回動支点 シザースリフト方式 相対距離 側面視 構造体 側面 モード 積載 距離 計画変更 工数 人力 開発 概要 実線 鎖線 検出 電動 ヤード 両側 転倒 他方 門 ２つ 上面 片方 格好 前方 形状 フロー 手順 進路 所定 建物 向上 採用 対応 構築 産業 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 8.3546 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 混合材 活性度指数 セメント用混合材 混合セメント 活性度推定方法 セメント 活性度推定装置 積算発熱量 材齢 活性度 高炉スラグ微粉 水粉体比 反応 促進温度 推定手段 反応活性剤 活性度試験 水和反応 塩基度 発明 図 製造方法 セメント混合用 方法 水 試料 混合材単体 粉体 ℃ 測定手段 フライアッシュ 一定温度 所定 関係 温度 ステップ 装置 促進試験 回帰式 基準セメント 伝導熱量計 推定方法 上記 養生温度 促進 コンクリート用高炉スラグ微粉 混合セメントペースト 混合材料 化学組成 ＪＩＳ ～ 効果 試験体 コンクリート用フライアッシュ 精度 水和発熱量 基準用普通ポルトランドセメント 強度発現性 特許文献 表 Ｍ値 ステップＳ 水セメント比 圧縮試験 形態 水和発熱速度 試験粉末 下 特徴 実施 活性化 Ａ 強度 養生 式 量 高炉スラグ微粉 水酸化カルシウム粉体 Ｏ 可能性 材料 規格値 圧縮強度 促進方法 ℃養生 次式 使用材料 参考文献 説明 Ｃ ＣａＯ ＭｇＯ ＳｉＯ 手段 フロー 測定 寄与率 置換率 試験モルタル ポゾラン反応性 Ａｓ 特開 号公報 確認実験 モルタル 温度ごと 試験終了 程度 人工 Ｒ 課題 目的 種類 指標 算定方法 刺激剤 相関 定温測定 ＪＩＳ規格 値 普通ポルトランドセメント 上記ＪＩＳ規格 先行技術文献 ℃、刺激剤 利用可能性 圧縮 関数式 技術分野 背景技術 基準モルタル 型枠 水酸化カルシウム 株式会社東京理工製 Ｎ ｍｍ 水中 Ａｌ － 図面 既往 Ｂａｓｉｃｉｔｙ 記号 ℃定温 通り 材料費 内容 データベース メモリ 化学成分 鉱物組成 コンクリート論文集 ～Ｓ 円柱型枠 和光純薬工業株式会社製 実験 概略フロー図 演算処理部 上記情報 標準砂 元素分析 検量線 ＯＨ ＢＦＳ 作用効果 大塚拓 検討 試薬特級 恒温漕中 事前検討 相関関係 情報 基 込み 研磨 水準 多額 費用 参照 数値 作製 価格 概要 質量 Ｃａ ℃） 度合 ｘ ｙ 研究 例 日本 海外 国 産地 下記 No pp )【 φ 手法 割 保管 理由 ＢＦＳ 工程 幅 削減 コンピュータ 産業 品質\n",
      "\n",
      "===== # 3, Topic : 14, p : 9.2029 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 給気経路部 排気経路部 水素供給エリアＨＡ 境界部材 水素供給エリア 水素貯蔵エリアＨＡ 水素 水素ステーション 実施形態 下部経路部 居住エリア 居住エリアＲＡ 地下室 経路部 中央経路部 上部経路部 境界 貯蔵エリア 給気口 中空Ｓ 前記貯蔵エリア 水素貯蔵エリア 上端部 前記境界部材 開口部Ｕ 開口Ｓ 前記水素供給エリア 給気ファン 給気スペース 図 排気口 Ｓ ＨＡ 上記水素ステーション 貯蔵タンク 部材 特許文献 中空 給気 排気用 排気 水素ガス 上記実施形態 排気管 給気用設備 水素検知センサ 前記排気経路部 給気用 地下 給排気 上方 水素供給ホース 縦給気管 発明 前記居住エリア 建物 開口部 構成 前記境界 ｂ 垂直断面 開口 燃料 垂直部 燃料電池車 ディスペンサ ｃ 前記貯蔵タンク 燃料電池車両 端部 水平部 水素濃度 下方 ａ 地上 形状 Ｕ 排気用設備 位置 水平面 上部 形態 斜視図 曲面形状 階段形状 円弧形状 イワタニ水素ステーション芝公園 説明 課題 換気 水素吸蔵合金 水素防爆仕様 側方 車両 領域 漏洩 角度 断面 構成部材 冷却液供給装置 空間 上方空間 連通路 断面図 ＲＡ 記載 距離 側壁 壁 配置 気体 燃料電池車Ｃ 模式断面図 燃料タンク 模式図 側 先行技術文献 ファン 上記課題 技術分野 背景技術 キャノピー 所定 周囲 庇 効果 説明図 ～図 地面 環状形状 左右 底面 分 空気 地上階等 複数階 反対側 ＳＯｘ等 自動車等 ショールーム等 換気設備 冷却機 概略構成 大型車両 地球温暖化 給水素スタンド 建築設計プロジェクト http://www design/#!/featured hydrogen>【発明 圧縮機 大気汚染 有害物質 特開平 号公報 敷地面積 有効活用 変更例 蓄圧器ユニット 矩形状 密度差 通風力 面積 曲率 Ｃ ＣＯ２ 原因 ＣＯ ＮＯｘ 参照 側部 － ARCHITORIUM OBAYASHI DESIGN PROJECTS 大林組 ｏｎｌｉｎｅ 平成 検索 インターネット URL obayashi co jp 概要 通常 ガソリンスタンド 土地 目的 手段 図面 一体 方向 ３つ 鈍角 作用 １つ 態様 交互 構造 前者 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 9.4989 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 支保工ユニット 鋼製梁 山留め支保工 合成切梁 山留め支保工ユニット 貫通孔 実施形態 切梁 固定孔 図 山留め壁 結合部 支保工 上記実施形態 支持部 説明図 棚杭 合成梁 前記鋼製梁 構築方法 上記山留め支保工ユニット 前記支保工ユニット 収縮モルタル スタッド ＲＣ切梁 上部フランジ 結合 鉄筋コンクリート造 山留め壁間 事前形成ブロック 掘削領域 Ｈ形鋼 上記山留め支保工 ａ ｂ 特許文献 ｃ 前記貫通孔 鉄筋コンクリート造部材 上記課題 前記固定孔 説明 領域 凸部 接続部 支保工ユニット同士 発明 プレロードジャッキ 鉄筋コンクリート 複数 結合部材 形状 固定 設置 ブラケット 方法 技術 コンクリート杭 課題 腹起し 解体 形態 ナット 前記結合部 前記山留め壁間 下部フランジ 結合方法 プレキャストコンクリート部材 鉄骨柱 上面側 下面側 下側 鉄筋コンクリート部 ＲＣ造 形成ブロック ＲＣ部分 コンクリート造部材 ｄ 側面 内径 号公報 現場打ち 上面 下面 鉄骨 火打ち梁等 部材 特徴 効果 ２つ 凹部 荷重 鉄筋 中間部 外周部 先端部 設 柱 文献 ブロック ソイルセメント柱列壁 繊維強化コンクリート 矢板壁等 事前 斜視図 ～図 先行技術文献 技術分野 背景技術 解体方法 コンクリート打設 廃棄コスト 掘削工事 参照 目的 記載 － 振動 騒音 スクラップ 空間 図面 完成 主筋 配置 位置 隙間 地盤 前面 本数 等 繊維強化プラスチック 施工コスト コスト抑制 せん断補強筋 方向 廃棄物 プレキャストコンクリート 技術的思想 配筋 ばら筋 コンクリートガラ等 略直方体形状 地下躯体 特開平 特開 構造物 直交方向 上方向 工事 作業性 長大スパン 座屈 リース品 先端 ねじ山 充填材 施工 プレロード 周囲 平面 縦横 低減 向上 概要 撤去 工期 手段 記事 挿入 注入 厚み 余裕 ウェブ 他 長上 建物 橋渡し 幅 逆 発生 横滑り 両者 数 ＦＲＣ ＦＲＰ 外力 耐力 材料 例 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 9.7681 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 側 溝 通し材 端部 上端面 柱 梁 端面 定着部 端部側 側縁 定着部挿入溝部 鉄筋部 側縁側 せん断補強筋 材 端面側 両端側 鉄筋挿入溝部 ｂ側 木口面 鉄筋 Ａ 中央部 一端部側 一端側 当該通し材 上方 溝幅 軸部 充填材 通し材設置ステップ 当該鉄筋部 他端側 下側 図 接合方法 構造 機械式継手 接合構造 当該溝 溝形成ステップ コンクリート 連結材 鉄筋部挿入溝部 木質系材料 方法 両端 Ｂ 特許文献 中央側 当該柱 梁設置ステップ 延長端側 側面 図外 当該定着部挿入溝部 鉄筋同士 上方側 鉄筋コンクリート製 他方 定着部挿入溝部部 鉄筋コンクリート構造 コンクリート打設ステップ 溝底 柱主筋 断面四角形状 当該梁 両端部 定着部挿入溝部 製造コスト ねじ部 端面同士 補強筋 発明 柱連結部構造 技術 実施形態 定着 先端側 後端側 側面側 横側 左右 定着性能 連結ボルト 当該せん断補強筋 同士 上面 上記 穴底側 円板状 集成材梁 アンカー部材 ｂ 接合 鉄筋コンクリート柱 円板体 棒材 位置 コ字状 例 径 当該接合方法 鉄筋等 当該柱主筋 木製集成材 コンクリート打設 板状 現場 中空穴 当該方法 － 周壁 隙間 構成 上側 ４つ ｃ ねじ孔 一体性 上下方向 上下 エポキシ系 連結部材 連結用ボルト 連結ボルト等 説明 下方 Ａ－Ａ断面図 中心 平面図 プレキャスト鉄筋コンクリート製 先行技術文献 口構造 形態 断面矩形形状 十字状 技術分野 背景技術 参照 所定 課題 等 左 右 一定 間隔 寸法 辺 日本建築学会大会学術講演梗概集 接着剤等 方向 延長 該円板体 規定本数 規定位置 特開 号公報 中心線 施工現場 長方形環状 研究 九州 ｐ 概要 手段 特徴 図面 ＣＬＴ Ｃｒｏｓｓ Ｌａｍｉｎａｔｅｄ Ｔｉｍｂｅｒ 下面 位置決め 上部 複数 直前 手順 交互 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 9.5745 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 屋根架構 支持架台 免震支承 図 前記屋根架構 ジャッキ 支柱 屋根 架構 梁行方向 材 鉄骨材 前記支持架台 スパン構造物 中空部 梁行方 正面図 架台 桁行方向 中央部 荷重 工程 拡大断面図 前記ジャッキ 下部構造体 下 緩衝材 出力 発明 変形例 隙間 実施形態 グラウト ロッド 張出屋根架構 張出屋根 位置 設置 水平方向 外周部 下側 引き揚げ フランジ 曲率 両側 センターホールジャッキ アーチ状 外周部分 トラス構造 ｂ 上記実施形態 リフトアップ 前記支柱 特許文献 上側 周柱 部分 温度変化 外側 状態 上端 下端 シムプレート アーチ形状 梁行方向中央部 設構台 下面 断面 梁行方向中央側 側部 設置位置 除荷 III－III 形態 平面図 梁行方向外側 リフトアップ工法 内側 複数 上部 形状 拡大図 方法 上方 施工方法 上面 内面 矢印方向 横方向 概略拡大断面図 上下 油圧 上下長 変更点 楔形 センターホールジャッキ等 角部 外周部近傍 説明 設 － 課題 斜め 地震 間隔 角 例 上昇 応力 ゴム弾性材等 鉄骨鉄筋コンクリート造 先行技術文献 等価物 技術分野 背景技術 鉄筋コンクリート造 ドーム状 参照 ゆえ 地盤 揺れ 効果 図面 施工 図示例 正方形 長方形 列方向 部位 鋼板 完成 台座 値 変形量 止め 代わり 伸長 発生 ドーム形状 ボルト等 誤差等 風等 上記事情 力 変更 地組工程 鉛直軸力 仮設用タワー 構築方法 施工コスト 引き揚げ工程 ゴム層 仮設構台 油圧装置 油圧供給 衝撃力 特開平 号公報 応力状態 内部空間 対向フランジ 上昇停止 記載 地面 概要 均衡 目的 手段 特徴 種々 限定 範囲 面 基礎 列 直方体 立方体 ４つ 交互 振動 領域 頭部 自重 直上 張力 一定 程度 充填 間中 設計 補強 損傷 工期 短縮 抑制 理解 趣旨 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 10.8929 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 繊維シート 補強パネル 板材 コンクリート構造物 開口部 コンクリート面Ｃ 補強方法 接着剤 接合シート 作業 接合部 実施形態 図 補強パネル同士 接合部材 アンカー孔 表面 面 既設コンクリート構造物 前記 コンクリート面 繊維シート同士 シール材 同士 コンクリート表面 繊維強化セメント板 充填材 特許文献 注入孔 前記繊維シート 接合作業 前記実施形態 シート材 固定作業 接着作業 空気抜き孔 側方 固定方法 繊維シート側 発明 上層板材 アンカー 端面 部 連結作業 前記開口部 コンクリート構造物側 連続性 コンクリート面Ｃ側 接着剤等 前記補強パネル ｂ 前記繊維シート同士 接着面側 下層板材 複数 ａ 注入材 端部 一体性 コンクリート部材 形態 前記コンクリート 作業状況 状態 角部 裏面側 セメント系材料 炭素繊維 矩形状 フレキシブルボード 充填 形状 ｃ 端面同士 アラミド繊維 ガラス繊維 Ｃ 斜視図 材料 注入作業 作業性 部分 位置 接着 密着性 楕円形 方法 ビニロン繊維等 表面側 板材同士 断面図 課題 構造性能レベル 補強効果 数 外形 板状部材 下層板材同士 枠状板材 炭素繊維量 号公報 注入 枠状部材 枠状 中央部 説明 隙間 特許 参照 当該下層板材 直径 接着工程 小判形等 形状等 小判形 多角形 先行技術文献 前記課題 注入パイプ 注入圧 取扱い性 分解斜視図 技術分野 背景技術 直径等 グラウト等 充填箇所 作用応力等 施工 特徴 一体 一対 辺 配置 強度 範囲 凹凸 円形 樹脂 連結 平面図 多角形状 角 矩形 工期短縮化 軽量化 特開 平滑度 効果 ～（ｃ 進行具合 湿潤状態 構成要素 地組 － 概要 手間 観点 手段 図面 他 ３つ 等間隔 鉄板 Ｎ ｍｍ 同等 劣化 周縁 同質 材質 ポリマーセメントモルタル リスク 最小限 スペーサー スペース 変形 前述 趣旨 変更 １つ 含浸 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 9.3954 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 振動試験 振動台テーブル 振動試験装置 変位 断層変位 比例制御電磁弁 積載テーブル 油圧シリンダ 振動台 振動試験方法 制御装置 変位付与手段 油圧制御手段 制御 蓄圧手段 試験体 油圧源 制御信号 弁開度制御信号 作動油 予備試験 手段 動作 油圧 Ａ～ 振動 蓄圧ボンベ 油圧ポンプ 水平方向 試験体Ｃ 相対変位 発明 変位制御信号 開閉パターン 地震動 方法 左右方向 動作連動手段 振動台テーブル変位 特開 特許文献 フィードバック制御 δ 外部 接続配管 図 Ｄ テーブル 機 号公報 目標 熊本地震 形態 接続 断層近傍 観測地震波 実施 ロッド つまり弁開度制御信号 信号 地震動変位 ～ 目標変位 伸縮変位 効果 地震 装置 鉛直軸方向 リニアガイド 開閉 可動変位 永久変位 変位センサ クローズループ制御 オープンループ制御 特徴 準備 支持架台 耐圧ホース 管路 断層 構造物 伸縮動作 波形 成分 サーボ弁 － 所望 方向 軸方向 成分波 目標地震動 水平軸方向 課題 上面 構成 建物 構造 慣性質量 入力地震波 地震波形 説明 既存 ピストン 動き 所定 分割 断層運動 左右 空圧シリンダ 可能性 右方向 周期構造物 先行技術文献 Ｃ 免震構造物 入力波 利用可能性 技術分野 背景技術 近傍 程度 通常 重ね 開発 上記 目的 図面 一端 軸受 耐震性評価 特開平 リニアモータ リニアアクチュエータ 固有周期 側断面図 右端部 左端部 サーボバルブ 耐震性能 擁壁 研究課題 最大 再現実験 実験 原点位置 上面視 右端 移動式 位置 通信回線 流量 最大流量 地表 被害 Ｆｌｉｎｇ Ｓｔｅｐ 影響 衝突 前提 参照 概要 凹状 下方 四角形 右側 スライダー 左側 内部 気体 場所 無線 有線 情報 範囲 作用 機能 産業 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 9.1349 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 継手 鋼製セグメント 部材 セグメント部材 リング継手 セグメント 雌継手 セグメント組み立て リング継手構造 雌部材 前記雄継手 鋼製セグメント主桁Ｓ 面 雄側継手 雌側継手 前記 トンネル軸方向 前記雌継手 雄側ボルト 端面板 特許文献 前記雄部材 雌側ボルト 桁 連結方法 ナット 反対側 発明 雄側端部 接合面 雄側 割り筒 蓋状部材 変形 軸方向 凸部 部 継手板 コンクリート系セグメント 鋼製セグメント主桁 筒 継手軸部 前記連結面 ボルト継手 トンネル壁体 雌ネジ 開口部 後端部 図 周面 雌継手孔部 前記雄継手軸部 貫通孔 継手接合方向 ボルト 板部材 実施形態 接合状態 雌部材用 外周面 孔Ｓ 該板部材変形部 前記雄継手頭部 継手頭部 板部材変形部 ナット筒 弾性体 雄側端部近傍 ケース 端部 構造 雄側端部内周面 ＲＣセグメント 前記板部材変形部 詰めコンクリート鋼製セグメント等 方向 ネジ 特開 号公報 増し締め 回転治具 板部材固定部 プレート トンネル 連結 孔 桁材 半径方向内方 リング ネジ棒 リング状 ケーシング 面外方向 ワンパス シールド掘削機 形態 継手箇所数 ワンパス型継手 溶接 凹部 組み立て 治具用凸部 内側 接合 中心 円錐棒 傾斜面 シールドトンネル用 前記ナット シールドトンネル 切断線 筒体 土水圧 荷重履歴 鋼殻 シール材 該雌継手孔部 各種部材同士 ボルト結合 治具用凹部 説明 ２つ 他方 － 図面 ａ ｂ 所定 図示 Ｓ 挿入方向 力 トルクレンチ 組み立て精度 末広状 受け筒 先行技術文献 止水シール材 実施 合構造 技術分野 背景技術 該ナット 溶接Ｓ 構成 適用 程度 進 ジャッキ 段階 課題 螺 特徴 締め 記載 参考 位置 押圧力 四角形 形状 目開き等 封入等 トルクレンチ等 変更等 外周 結合装置 製作精度 挿入 反発力 スキンプレート 縦リブ 挿通可能 トルク値 実績 背面 挿通後 他 同軸 内部 進入 寸法 内径 各々 複数 形式 理由 規模 傾向 目違い 通例 特性 概要 目的 手段 効果 挿通時 施工 突出 互い 合し 仮 要旨 範囲 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 9.5777 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 箱型部材 せん断伝達構造 地中連続壁 コンクリート 鉄筋籠 図 安定液 地下コンクリート構造物 液圧 部材 コ字部 コンクリート構造物 伝達 蓋部 せん断力 請求項 定着部 せん断伝達構造構築 発明 前記箱型部材 コンクリート等 該箱型部材 方法 アンカ部 地中連続壁頂部 箱抜き 状況図 掘削溝 形状 側圧 建設廃棄物 側面図 漏水防止部材 特許文献 せん断伝達 コンクリート打設時 せん断伝達部材 箱 内面 せん断伝達力構造 作用力 泥水等 せん断伝達構造構築部 側壁 挿通孔 型枠部材 完了 当該箱型部材 箱抜き部材 掘削完了状況 せん断伝達構造Ｇ 所定 ｂ 前記地中連続壁 側 状況 地下コンクリート構造物構築完了状況 桁材 発泡スチロール等 取り付け状況図 地中連続壁内面側 内部 外面 該地中連続壁 側壁頂部 地下コンクリート構造物構築完了状況図 箱抜き部材内部 型枠形状 地中連続壁コンクリート打設状況図 補強 形状保持材 箱抜き形状 内面側 当該箱抜き部材 地中連続壁内部地盤 先行エレメント 異形鉄筋 ａ 行エレメント 構造概要図 底版 作業 寸法 治具 形状寸法 箱抜き寸法 正面図 俯瞰図 緩衝材 鉄筋籠内面側 前記 記載 側部 開口側 地盤Ｇ 鉄筋籠建て込み完了状況図 該箱型部材内部 周回溝 一体部材 フレーム部材等 作用力等 等 該安定液 前記挿通孔 開口部 課題 発泡スチロール 特徴 凹部 ワイヤ 側圧作用状況図 縦筋 せん断キー 抵抗力 処分作業 荷重 号公報 形態 実施形態 箇所数 外荷重 深度 該凹型部 端部 土留め壁 せん断補強筋 ＬＮＧ地下貯蔵タンク等 該鉄筋籠 蓋部取り外し ＬＮＧ地下貯蔵タンク 前記特許文献 説明 水位 凹型部 流入 構成 図面 設高 地下水 鋼製板内面側 設計 骨材 カットアンカ等 引き抜き抵抗力 側圧等 先行技術文献 突起状せん断キー 概念図 平面形状 漏水防止用 クレーン等 布等 変更等 側面 境界 境界面 鋼製板 面 骨材径 地盤Ｗ タンク躯体 略境界面 当該方法 技術分野 背景技術 安全側 － 搬出 概要 位置 ｃ 鋼板 配置 上方 横筋 揚 設状況 図示 躯体重量 取り付け深度 土圧及び地下水Ｗ ジベル筋 周面 外周面 当該設計 実施 取り出し作業 接続方法 固定方法 特開平 特開 略外面 設置深度 浮き上がり 外周 施工性 原則溶接 溶接 一体 発泡プラスチック 阻害でき 従前 自重 壁面 コスト 工程 周囲 凹型 空間 撤去 四面 目的 手段 上述 空気抜き 効果 Ｄ 強度 接合 耐力 隅 空気 下方 置 種々 一端 他端 機械 水圧 挿入 以浅 差分 配慮 他 要因 交互 付近 一般 沈下 下端 程度 素材 要旨 範囲 符号 装置\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.5854e-38, 1.4743e-08, 3.1290e-15, 3.9255e-07, 2.6853e-38,\n",
      "         2.2281e-43, 7.9801e-03, 2.0094e-13, 1.9090e-24, 3.9526e-27, 5.7263e-26,\n",
      "         2.9583e-10, 2.3523e-14, 9.9202e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0447, 0.1039, 0.0502, 0.0603, 0.0671, 0.1073, 0.0919, 0.0593,\n",
      "         0.0522, 0.0623, 0.0555, 0.0509, 0.0711, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1043.1414\n",
      "Test epoch : 1 Average loss: 1049.8517\n",
      "PP(train) = 2616.723, PP(valid) = 2714.737\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 6.6972e-34, 7.0065e-45, 3.8452e-39, 0.0000e+00,\n",
      "         0.0000e+00, 8.1095e-23, 4.1301e-30, 4.5597e-32, 0.0000e+00, 0.0000e+00,\n",
      "         8.2738e-32, 4.2039e-45, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1042.9299\n",
      "Test epoch : 2 Average loss: 1049.6457\n",
      "PP(train) = 2611.004, PP(valid) = 2710.538\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0575e-30, 4.5220e-16, 3.1928e-14, 5.8792e-12, 3.8816e-42,\n",
      "         3.4370e-39, 3.0399e-11, 2.5721e-15, 3.7878e-32, 1.9358e-38, 1.7669e-39,\n",
      "         1.0865e-08, 1.5289e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1042.5667\n",
      "Test epoch : 3 Average loss: 1049.4369\n",
      "PP(train) = 2604.876, PP(valid) = 2706.365\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.6414e-34, 2.5666e-35, 4.4672e-16, 1.4202e-11, 2.2218e-09, 7.8473e-44,\n",
      "         2.6407e-38, 9.9798e-07, 5.8390e-06, 6.6562e-16, 7.7985e-32, 2.0865e-15,\n",
      "         3.9207e-03, 3.9485e-10, 9.9607e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0816, 0.0445, 0.1043, 0.0503, 0.0600, 0.0669, 0.1073, 0.0923, 0.0594,\n",
      "         0.0524, 0.0619, 0.0556, 0.0511, 0.0708, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1042.3385\n",
      "Test epoch : 4 Average loss: 1049.2238\n",
      "PP(train) = 2598.387, PP(valid) = 2702.022\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.3329e-40, 5.4383e-22, 1.2972e-07, 2.2369e-20, 8.9505e-07, 8.8114e-34,\n",
      "         1.3627e-35, 6.5746e-06, 4.2132e-08, 1.1048e-21, 2.0220e-34, 4.7474e-29,\n",
      "         4.2604e-01, 1.1321e-24, 5.7395e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0668, 0.0566, 0.0934, 0.0556, 0.0524, 0.0674, 0.0820, 0.0884, 0.0669,\n",
      "         0.0686, 0.0570, 0.0557, 0.0553, 0.0696, 0.0642]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1041.9947\n",
      "Test epoch : 5 Average loss: 1049.0099\n",
      "PP(train) = 2591.664, PP(valid) = 2697.544\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.5244e-39, 6.8922e-26, 3.6610e-32, 1.9580e-23, 0.0000e+00,\n",
      "         8.0154e-43, 1.9702e-26, 2.3146e-21, 2.7304e-26, 0.0000e+00, 0.0000e+00,\n",
      "         7.7887e-25, 1.6347e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1041.5930\n",
      "Test epoch : 6 Average loss: 1048.7969\n",
      "PP(train) = 2584.970, PP(valid) = 2693.177\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7222e-42, 2.8065e-29, 1.4669e-07, 4.6845e-13, 1.7421e-08, 7.0127e-37,\n",
      "         1.0002e-37, 2.3733e-02, 1.0113e-05, 5.4407e-12, 1.2720e-28, 1.4841e-25,\n",
      "         6.7002e-01, 3.0158e-13, 3.0624e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0588, 0.0655, 0.0851, 0.0581, 0.0481, 0.0674, 0.0687, 0.0835, 0.0706,\n",
      "         0.0785, 0.0544, 0.0547, 0.0568, 0.0684, 0.0813]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1041.2882\n",
      "Test epoch : 7 Average loss: 1048.5837\n",
      "PP(train) = 2578.412, PP(valid) = 2688.920\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 4.3647e-24, 8.9526e-38, 1.1040e-17, 0.0000e+00,\n",
      "         0.0000e+00, 3.4129e-09, 2.1944e-18, 1.7098e-26, 3.0702e-42, 2.1290e-30,\n",
      "         6.9286e-23, 4.1906e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1040.8479\n",
      "Test epoch : 8 Average loss: 1048.3708\n",
      "PP(train) = 2571.926, PP(valid) = 2684.733\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4735e-32, 6.6929e-11, 1.1491e-20, 6.8642e-15, 1.5632e-35,\n",
      "         0.0000e+00, 2.1290e-08, 2.6530e-20, 1.0566e-30, 1.1190e-40, 5.8460e-39,\n",
      "         2.6151e-19, 2.2318e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1040.5642\n",
      "Test epoch : 9 Average loss: 1048.1618\n",
      "PP(train) = 2565.458, PP(valid) = 2680.612\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8297e-30, 1.8040e-22, 1.6002e-34, 1.4447e-14, 2.8867e-42,\n",
      "         7.0065e-45, 6.0495e-17, 7.4222e-13, 3.0224e-30, 9.5330e-42, 0.0000e+00,\n",
      "         1.5758e-23, 3.4481e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1040.2464\n",
      "Test epoch : 10 Average loss: 1047.9508\n",
      "PP(train) = 2558.887, PP(valid) = 2676.323\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2948e-41, 1.1899e-23, 1.5787e-23, 2.0012e-11, 6.7599e-40,\n",
      "         0.0000e+00, 2.1284e-20, 1.0547e-19, 1.5260e-32, 1.3056e-38, 5.5108e-39,\n",
      "         1.6260e-10, 1.7646e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1039.8642\n",
      "Test epoch : 11 Average loss: 1047.7436\n",
      "PP(train) = 2552.354, PP(valid) = 2672.041\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7263e-33, 1.4420e-30, 6.3849e-33, 6.4523e-17, 6.0536e-43,\n",
      "         0.0000e+00, 3.9223e-11, 7.6147e-16, 5.2276e-34, 2.7634e-40, 2.0959e-39,\n",
      "         3.3706e-21, 3.3984e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1039.5141\n",
      "Test epoch : 12 Average loss: 1047.5384\n",
      "PP(train) = 2545.916, PP(valid) = 2667.889\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.2102e-34, 8.1066e-19, 3.6564e-09, 2.7742e-23, 1.4347e-14, 5.6274e-34,\n",
      "         4.9114e-36, 9.8708e-01, 1.9753e-07, 2.2718e-19, 1.2602e-28, 1.3610e-26,\n",
      "         1.2467e-02, 1.9302e-09, 4.5792e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0830, 0.1053, 0.0547, 0.0445, 0.0786, 0.0920, 0.0767, 0.0440, 0.0534,\n",
      "         0.0375, 0.1182, 0.0440, 0.0368, 0.0937, 0.0375]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1039.3203\n",
      "Test epoch : 13 Average loss: 1047.3315\n",
      "PP(train) = 2539.635, PP(valid) = 2663.814\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.4064e-27, 6.4465e-23, 1.4251e-25, 1.4371e-17, 9.9268e-42,\n",
      "         1.1210e-44, 9.9594e-01, 6.0298e-17, 6.2019e-33, 3.2907e-37, 1.2781e-26,\n",
      "         4.9925e-23, 1.2264e-32, 4.0556e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1053, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0370, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1038.8674\n",
      "Test epoch : 14 Average loss: 1047.1270\n",
      "PP(train) = 2533.383, PP(valid) = 2659.755\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 5.3658e-40, 1.9936e-23, 9.0503e-35, 4.2222e-09, 6.6088e-38,\n",
      "         1.0229e-43, 8.8695e-18, 1.8925e-18, 1.2151e-34, 4.4149e-39, 5.4944e-37,\n",
      "         9.2126e-19, 1.3526e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1038.6012\n",
      "Test epoch : 15 Average loss: 1046.9246\n",
      "PP(train) = 2527.175, PP(valid) = 2655.736\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8284e-37, 1.5721e-23, 3.0243e-25, 3.1148e-24, 0.0000e+00,\n",
      "         0.0000e+00, 1.8558e-10, 8.3195e-28, 1.0069e-32, 0.0000e+00, 4.3098e-27,\n",
      "         2.5622e-26, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1038.2437\n",
      "Test epoch : 16 Average loss: 1046.7206\n",
      "PP(train) = 2521.007, PP(valid) = 2651.670\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.2325e-38, 5.2500e-09, 9.7412e-21, 1.1801e-15, 0.0000e+00,\n",
      "         2.3849e-38, 1.8563e-13, 3.3231e-08, 3.7606e-28, 8.4078e-45, 1.8622e-33,\n",
      "         1.3925e-15, 2.3357e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1037.8607\n",
      "Test epoch : 17 Average loss: 1046.5189\n",
      "PP(train) = 2514.853, PP(valid) = 2647.630\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 0.0000e+00, 2.2426e-18, 5.3194e-30, 1.0807e-20, 0.0000e+00,\n",
      "         0.0000e+00, 5.5634e-15, 1.6654e-14, 1.6696e-28, 1.3649e-38, 1.9927e-35,\n",
      "         6.5403e-16, 2.1586e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1037.5226\n",
      "Test epoch : 18 Average loss: 1046.3197\n",
      "PP(train) = 2508.703, PP(valid) = 2643.587\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0065e-45, 1.7132e-25, 1.2396e-19, 5.1985e-35, 6.7222e-01, 1.4731e-39,\n",
      "         1.8030e-37, 1.9169e-13, 8.2357e-09, 1.1405e-20, 7.3687e-26, 7.7533e-32,\n",
      "         6.7890e-16, 1.0513e-31, 3.2778e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0554, 0.0475, 0.0577, 0.0424, 0.0837, 0.0672, 0.1087, 0.1046, 0.0501,\n",
      "         0.0789, 0.0665, 0.0596, 0.0481, 0.0659, 0.0637]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1037.3534\n",
      "Test epoch : 19 Average loss: 1046.1208\n",
      "PP(train) = 2502.629, PP(valid) = 2639.584\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3440e-43, 3.3187e-36, 4.2963e-12, 2.3422e-18, 9.5285e-21, 4.0077e-43,\n",
      "         7.5914e-38, 7.0213e-02, 5.1996e-04, 1.9956e-21, 4.6759e-21, 6.7065e-27,\n",
      "         3.8170e-03, 1.3835e-24, 9.2545e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0823, 0.0476, 0.1002, 0.0501, 0.0616, 0.0689, 0.1055, 0.0882, 0.0593,\n",
      "         0.0515, 0.0653, 0.0550, 0.0502, 0.0728, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1036.9628\n",
      "Test epoch : 20 Average loss: 1045.9240\n",
      "PP(train) = 2496.755, PP(valid) = 2635.751\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 4.5156e-34, 4.4643e-14, 2.6783e-30, 4.1314e-23, 0.0000e+00,\n",
      "         3.2099e-35, 5.6778e-07, 2.5667e-16, 3.0552e-32, 2.6499e-42, 7.4576e-27,\n",
      "         6.4815e-19, 1.1246e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1036.6195\n",
      "Test epoch : 21 Average loss: 1045.7284\n",
      "PP(train) = 2490.882, PP(valid) = 2631.931\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5350e-39, 7.7453e-33, 4.7310e-16, 4.1670e-29, 1.3748e-10, 7.4042e-41,\n",
      "         4.7924e-43, 7.7472e-14, 1.3139e-21, 6.8741e-19, 1.0185e-34, 9.4647e-35,\n",
      "         1.3797e-14, 8.9885e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1036.4143\n",
      "Test epoch : 22 Average loss: 1045.5310\n",
      "PP(train) = 2484.942, PP(valid) = 2627.986\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.4384e-37, 2.2635e-26, 8.6063e-29, 1.0387e-16, 0.0000e+00,\n",
      "         0.0000e+00, 7.9841e-17, 4.7389e-21, 9.2550e-35, 0.0000e+00, 6.6071e-42,\n",
      "         7.3579e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1035.9024\n",
      "Test epoch : 23 Average loss: 1045.3375\n",
      "PP(train) = 2479.160, PP(valid) = 2624.223\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.3496e-39, 1.3655e-17, 2.6401e-24, 1.5320e-24, 1.1787e-38,\n",
      "         9.4365e-38, 1.0638e-11, 5.9428e-12, 6.6076e-33, 1.5964e-30, 2.8125e-36,\n",
      "         1.1299e-09, 2.8846e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1035.7767\n",
      "Test epoch : 24 Average loss: 1045.1450\n",
      "PP(train) = 2473.377, PP(valid) = 2620.420\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.5009e-38, 1.0814e-09, 1.7420e-10, 6.4064e-03, 0.0000e+00,\n",
      "         2.8306e-43, 7.3672e-08, 9.7849e-23, 2.8639e-23, 5.9548e-39, 2.2333e-33,\n",
      "         2.8746e-25, 3.6765e-27, 9.9359e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0444, 0.1038, 0.0501, 0.0603, 0.0669, 0.1076, 0.0925, 0.0593,\n",
      "         0.0525, 0.0620, 0.0556, 0.0510, 0.0708, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1035.3300\n",
      "Test epoch : 25 Average loss: 1044.9542\n",
      "PP(train) = 2467.602, PP(valid) = 2616.614\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0795e-40, 4.0752e-30, 5.8183e-17, 3.2632e-25, 2.8917e-13, 3.4784e-32,\n",
      "         1.4425e-41, 1.1603e-13, 7.6618e-12, 1.6866e-24, 1.6227e-42, 1.3647e-19,\n",
      "         5.7760e-11, 1.4788e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1035.1845\n",
      "Test epoch : 26 Average loss: 1044.7646\n",
      "PP(train) = 2461.917, PP(valid) = 2612.865\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2039e-45, 2.0622e-22, 4.4539e-13, 3.2846e-17, 1.5673e-03, 7.7956e-32,\n",
      "         7.2631e-38, 4.3258e-10, 5.0276e-04, 2.1804e-20, 6.8236e-39, 5.3084e-15,\n",
      "         2.3060e-09, 7.1581e-21, 9.9793e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0601, 0.0669, 0.1075, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0709, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1034.7818\n",
      "Test epoch : 27 Average loss: 1044.5761\n",
      "PP(train) = 2456.312, PP(valid) = 2609.185\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.1620e-31, 4.4802e-18, 7.2733e-19, 5.2633e-27, 9.9072e-43,\n",
      "         0.0000e+00, 2.6712e-01, 3.9413e-04, 3.0883e-34, 3.3117e-35, 5.2041e-28,\n",
      "         1.6065e-15, 2.1691e-25, 7.3248e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0838, 0.0571, 0.0894, 0.0495, 0.0660, 0.0744, 0.1003, 0.0771, 0.0588,\n",
      "         0.0486, 0.0753, 0.0532, 0.0476, 0.0780, 0.0410]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1034.4632\n",
      "Test epoch : 28 Average loss: 1044.3867\n",
      "PP(train) = 2450.768, PP(valid) = 2605.495\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.3728e-41, 9.4981e-28, 3.9890e-27, 1.1237e-36, 4.2936e-06, 2.1869e-38,\n",
      "         0.0000e+00, 2.5616e-06, 2.8455e-11, 5.7369e-29, 1.3841e-40, 1.6425e-35,\n",
      "         1.1133e-17, 3.0827e-26, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1034.2170\n",
      "Test epoch : 29 Average loss: 1044.1990\n",
      "PP(train) = 2445.268, PP(valid) = 2601.877\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3320e-29, 3.5044e-21, 1.0548e-35, 1.0164e-08, 7.0065e-45,\n",
      "         0.0000e+00, 7.0352e-18, 3.4541e-16, 2.5232e-37, 1.1094e-37, 7.2402e-36,\n",
      "         1.8339e-17, 1.1203e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1033.9093\n",
      "Test epoch : 30 Average loss: 1044.0148\n",
      "PP(train) = 2439.805, PP(valid) = 2598.312\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5414e-44, 1.3470e-29, 1.2276e-19, 1.7915e-20, 1.6108e-16, 5.2128e-43,\n",
      "         1.0687e-34, 5.8623e-18, 2.1170e-22, 1.4675e-26, 2.1319e-29, 1.5804e-38,\n",
      "         6.2347e-20, 1.4137e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1033.6942\n",
      "Test epoch : 31 Average loss: 1043.8311\n",
      "PP(train) = 2434.351, PP(valid) = 2594.726\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8338e-35, 3.3705e-23, 1.8946e-21, 9.0564e-19, 4.8209e-18, 1.0047e-31,\n",
      "         1.2230e-32, 8.6747e-05, 1.2219e-17, 3.9855e-19, 4.6033e-28, 4.3857e-25,\n",
      "         1.1964e-20, 3.0027e-27, 9.9991e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1033.5050\n",
      "Test epoch : 32 Average loss: 1043.6488\n",
      "PP(train) = 2428.821, PP(valid) = 2591.046\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0555e-31, 2.0223e-36, 1.1930e-01, 9.2601e-18, 5.3090e-11, 1.2999e-30,\n",
      "         1.1655e-29, 3.1321e-07, 1.3633e-09, 2.2025e-26, 2.7050e-37, 1.5433e-21,\n",
      "         7.6688e-01, 9.7653e-32, 1.1382e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0551, 0.0731, 0.0753, 0.0616, 0.0478, 0.0638, 0.0618, 0.0814, 0.0762,\n",
      "         0.0845, 0.0521, 0.0535, 0.0577, 0.0686, 0.0875]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1033.1309\n",
      "Test epoch : 33 Average loss: 1043.4690\n",
      "PP(train) = 2423.244, PP(valid) = 2587.304\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.6387e-18, 1.6347e-17, 4.8982e-17, 1.5436e-11, 0.0000e+00,\n",
      "         2.5736e-40, 3.2271e-02, 4.9951e-16, 5.2348e-20, 1.1998e-32, 2.4681e-28,\n",
      "         1.4413e-19, 1.3241e-25, 9.6773e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0821, 0.0458, 0.1025, 0.0502, 0.0608, 0.0678, 0.1067, 0.0904, 0.0593,\n",
      "         0.0519, 0.0635, 0.0553, 0.0506, 0.0717, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1032.7476\n",
      "Test epoch : 34 Average loss: 1043.2887\n",
      "PP(train) = 2417.916, PP(valid) = 2583.800\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 3.5685e-39, 1.2369e-15, 1.4710e-35, 8.1065e-25, 7.8473e-44,\n",
      "         2.8950e-36, 1.3046e-03, 1.7821e-13, 8.6844e-17, 4.8440e-37, 3.2550e-27,\n",
      "         4.4189e-17, 1.2998e-29, 9.9870e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1032.4189\n",
      "Test epoch : 35 Average loss: 1043.1068\n",
      "PP(train) = 2412.785, PP(valid) = 2580.462\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 7.5214e-32, 1.2647e-26, 1.3100e-31, 1.5463e-27, 0.0000e+00,\n",
      "         0.0000e+00, 6.1512e-12, 1.5297e-15, 2.3594e-39, 2.0573e-38, 2.1228e-38,\n",
      "         6.0531e-22, 8.2966e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1032.0768\n",
      "Test epoch : 36 Average loss: 1042.9271\n",
      "PP(train) = 2407.737, PP(valid) = 2577.207\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4987e-41, 4.1295e-24, 1.6979e-16, 1.0519e-17, 4.3066e-18, 1.8446e-36,\n",
      "         1.5779e-38, 1.0000e+00, 2.7168e-08, 2.4778e-26, 3.3011e-34, 1.2574e-35,\n",
      "         1.5590e-16, 1.3798e-24, 2.6936e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1031.9255\n",
      "Test epoch : 37 Average loss: 1042.7481\n",
      "PP(train) = 2402.549, PP(valid) = 2573.765\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.8558e-21, 1.8074e-19, 4.3938e-31, 3.4694e-15, 5.8564e-38,\n",
      "         2.6092e-42, 4.0685e-16, 2.3210e-11, 3.3337e-25, 4.5891e-25, 1.8562e-24,\n",
      "         2.6988e-07, 1.3529e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1031.5855\n",
      "Test epoch : 38 Average loss: 1042.5739\n",
      "PP(train) = 2397.252, PP(valid) = 2570.261\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0065e-45, 1.2742e-33, 2.7339e-16, 2.6172e-24, 9.6360e-17, 1.3124e-38,\n",
      "         1.3649e-42, 9.9341e-09, 7.2077e-16, 3.7146e-22, 9.5378e-39, 3.1898e-25,\n",
      "         5.8183e-06, 6.7538e-27, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1031.2521\n",
      "Test epoch : 39 Average loss: 1042.3975\n",
      "PP(train) = 2392.028, PP(valid) = 2566.753\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4627e-41, 1.5022e-31, 2.0880e-08, 2.0493e-17, 9.5115e-18, 2.3271e-39,\n",
      "         2.0045e-39, 5.0388e-07, 5.1938e-14, 1.5621e-20, 5.1984e-36, 7.5997e-30,\n",
      "         9.3108e-17, 1.1162e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1031.0591\n",
      "Test epoch : 40 Average loss: 1042.2228\n",
      "PP(train) = 2386.901, PP(valid) = 2563.349\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0497e-41, 1.0872e-30, 8.2215e-16, 1.4204e-26, 4.5945e-22, 0.0000e+00,\n",
      "         1.1210e-44, 5.3621e-17, 5.2789e-10, 1.9451e-34, 3.1861e-39, 2.3910e-30,\n",
      "         6.1212e-26, 1.2174e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1030.7508\n",
      "Test epoch : 41 Average loss: 1042.0479\n",
      "PP(train) = 2381.926, PP(valid) = 2560.076\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6052e-45, 1.2307e-36, 3.4646e-12, 1.9093e-25, 5.5592e-08, 6.3733e-34,\n",
      "         1.7583e-36, 8.8988e-10, 8.3019e-15, 1.0148e-32, 1.4044e-37, 1.1868e-26,\n",
      "         7.8934e-19, 1.6930e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1030.4665\n",
      "Test epoch : 42 Average loss: 1041.8754\n",
      "PP(train) = 2376.968, PP(valid) = 2556.825\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.8136e-36, 2.4551e-16, 1.1258e-10, 1.6401e-22, 6.6541e-05, 8.4416e-37,\n",
      "         2.2953e-39, 6.3446e-10, 9.9811e-13, 7.8744e-20, 1.3095e-26, 8.4221e-18,\n",
      "         9.9987e-01, 1.3818e-13, 6.6159e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1030.2410\n",
      "Test epoch : 43 Average loss: 1041.7034\n",
      "PP(train) = 2371.952, PP(valid) = 2553.477\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.8026e-45, 5.4610e-24, 9.0448e-17, 3.3142e-23, 0.0000e+00,\n",
      "         8.1275e-44, 1.0144e-11, 5.5453e-13, 1.8972e-27, 4.5462e-38, 3.3442e-31,\n",
      "         1.7936e-25, 1.1603e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1029.8219\n",
      "Test epoch : 44 Average loss: 1041.5343\n",
      "PP(train) = 2366.975, PP(valid) = 2550.218\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.7038e-40, 4.9136e-18, 2.3150e-38, 5.8459e-24, 5.6052e-44,\n",
      "         1.2191e-43, 1.2949e-14, 2.7217e-24, 1.2581e-30, 1.0187e-42, 7.9912e-37,\n",
      "         8.5169e-12, 5.5179e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1029.6123\n",
      "Test epoch : 45 Average loss: 1041.3643\n",
      "PP(train) = 2362.121, PP(valid) = 2547.012\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0065e-45, 1.3132e-17, 1.1694e-15, 1.6523e-19, 4.1673e-05, 3.0540e-36,\n",
      "         9.9902e-35, 2.7560e-13, 9.4696e-06, 1.1096e-30, 3.4179e-35, 1.6005e-22,\n",
      "         3.5184e-12, 6.6562e-18, 9.9995e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1029.3822\n",
      "Test epoch : 46 Average loss: 1041.1969\n",
      "PP(train) = 2357.279, PP(valid) = 2543.835\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7404e-38, 1.9280e-20, 1.4765e-05, 4.2894e-20, 7.5003e-11, 1.1566e-40,\n",
      "         9.6371e-41, 4.1376e-12, 3.1308e-11, 1.1295e-24, 2.0160e-22, 5.2080e-32,\n",
      "         2.2813e-19, 3.8006e-30, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1029.0790\n",
      "Test epoch : 47 Average loss: 1041.0290\n",
      "PP(train) = 2352.432, PP(valid) = 2540.607\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.6880e-34, 1.2796e-23, 2.7210e-28, 3.1530e-08, 0.0000e+00,\n",
      "         1.1064e-32, 4.9613e-20, 6.6179e-13, 3.4851e-18, 6.6184e-38, 1.6181e-24,\n",
      "         1.3185e-31, 2.4847e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1028.8581\n",
      "Test epoch : 48 Average loss: 1040.8639\n",
      "PP(train) = 2347.620, PP(valid) = 2537.438\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.7800e-24, 7.5727e-18, 1.5126e-16, 1.1672e-08, 1.7136e-32,\n",
      "         5.1253e-31, 5.3687e-12, 6.6084e-12, 1.6586e-11, 9.4120e-31, 2.6249e-36,\n",
      "         2.6224e-09, 2.8346e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1028.5428\n",
      "Test epoch : 49 Average loss: 1040.6980\n",
      "PP(train) = 2342.856, PP(valid) = 2534.275\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9292e-35, 6.4890e-20, 1.0270e-31, 8.6857e-23, 2.4999e-42,\n",
      "         1.4588e-42, 1.8876e-16, 4.0558e-12, 2.6830e-32, 0.0000e+00, 1.2823e-40,\n",
      "         9.0587e-18, 9.2692e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1028.2609\n",
      "Test epoch : 50 Average loss: 1040.5322\n",
      "PP(train) = 2338.174, PP(valid) = 2531.161\n",
      "Writing to ./topicwords/11-topwords_e50.txt\n",
      "Topic 0: セグメントリング ピストンロッド 定着構造 撹拌 シールド工法 内型枠 つぎ 降雨 公報 製作コスト\n",
      "Topic 1: ピストンロッド セグメントリング 定着構造 つぎ 撹拌 内型枠 製作コスト 固定手段 床パネル 発破\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 河川 元 衝撃 進機 固定手段 挙動 つぎ 混合 セグメントリング 添加\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: セグメントリング ピストンロッド 定着構造 撹拌 内型枠 継手板 ピット つぎ マンホール 難点\n",
      "Topic 6: セグメントリング ピストンロッド 定着構造 シールド工法 つぎ 撹拌 内型枠 製作コスト カッタヘッド 発破\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 図\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: セグメントリング つぎ シールド工法 ピストンロッド 定着構造 内型枠 公報 ｓｅｃ せん断変形 降雨\n",
      "Topic 10: ピストンロッド セグメントリング 定着構造 シールド工法 撹拌 製作コスト 内型枠 カッタヘッド ピット 日常\n",
      "Topic 11: セグメントリング ピストンロッド 定着構造 撹拌 シールド工法 内型枠 継手板 発破 カッタヘッド 製作コスト\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: セグメントリング ピストンロッド 定着構造 つぎ 公報 発破 シールド工法 製作コスト ｓｅｃ ピット\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 8.1047 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 光ファイバセンサ 計測器 前記光ファイバセンサ 構造物 コンクリート コンクリート打設予定領域 敷設方法 延伸サイクル 敷設箇所 コンクリート打設 図 計測 移動型枠 光ファイバセンサ破断 計測対象量 次節延伸ブロック 前記構造物 コンクリート打設後 破断 型枠脱 敷設 請求項 斜張橋 始端 コンクリート打設中 床版 上床版 施工 前記計測器 発明 手順 前記別 次節延伸サイクル コンクリート打設サイクル 方法 仮設材 終端 リール 切断端 敷設手順図 前記リール 光ファイバセンサ両端 温度 － 前記切断端側 コンクリート打設用型枠 桁 外側 前記コンクリート コンクリート打設前 コンクリート打設作業 敷設対象 分 延伸ブロック 温度等 特許文献 実施例 変化 長大コンクリート構造物 前記切断端 予備長 成端処理 側 特徴 ａ 破断部 別 残長分 延伸部分 始端側 ｄ 張り出し施工法 敷設対象位置 光切り替えスイッチ等 ｂ ｃ 前記仮設材 箇所 計測チャンネル数 説明 塔 課題 上記 新設 敷設済み 位置 長保管用リール 技術分野 背景技術 先行技術文献 断線復旧方法 例 長大橋モニタリングシステム 外部 全長 プレストレストコンクリート 社団法人プレストレストコンクリート技術協会 目的 効果 図面 内部 影響 残り 既設 チャンネル 接続 終端側 下面側 適用例 岩城英朗 配筋完了 利用可能性 センサ両端 線状 稲田裕 原敏裕 ～（ｄ 尺 橋梁 トンネル 参照 通常 開発 号 概要 手段 次 裏側 ～ 一つ 前半 形態 前節 段階 最終 原因 開口 複数 最小限 産業 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 9.8134 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 免震装置 免震装置台座 台座 プレート 免震装置台座施工方法 コンクリート 工程 免震構造物 型枠 施工工程 袋ナット モルタル 図 前記免震装置 施工方法 連結部材 中央部 台座用型枠組立工程 前記プレート 連結部材設置工程 発明 リング状 プレート位置決め工程 台免震装置台座 前記連結部材 請求項 形態 前記型枠 実施 基礎部 モルタル仕上げ工程 コンクリート打設工程 構造物 レベル調整装置 位置決め コンクリート充填孔 施工内容 ボルト プレート取り外し工程 前記台座 特許文献 コンクリート型枠 上面 工程別 表面 凹部 施工コスト 設置精度 頂面 底面 気泡 面 硬化 環状 施工段階 設置 空隙 前記コンクリート 設置方法 プレート取外し工程 前記袋ナット コンクリート基礎 貫通孔 周囲 Ａ 記載 利用 充填 鋼板製 コンクリート打設時 コンクリート打設用 コンクリート製 軸力 鋼板 一定間隔 前記モルタル 延長部材 説明 特徴 内部 プレート表面 該コンクリート 接合用 アンカーナット 環状部 先行技術文献 技術分野 背景技術 下面 Ｂ Ｃ 課題 表面積 凹凸 状態 上方 位置 ボルト取付け孔 引用文献 木板製 構成 コスト低減 上記構成 基本構成 流動性 特開 号公報 上記 フロー図 低減 品質向上 現場 先 手順 空気 伝達 発生 － 概要 目的 手段 平面 効果 図面 例 場所 完成 コンンクリート Ｄ 分 コテ 支障 最後 Ｅ 強度 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 9.6590 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : フラットスラブ構造 フラットスラブ 柱 ラーメン構造 Ｘ軸方向 Ｙ軸方向 水平力 門形ラーメン 支板 図 構造物 方向 実施 スラブ 曲げモーメント 請求項 線Ｋ Ｘ方向 多角形 前記ラーメン構造 床版 発明 前記柱 形態 剛性 モーメント 構成 建物 モーメント値Ｍ 線 特性 中心 外周 実施例 特許文献 基本構成 外形寸法ｄ Ｌ字形 鉄筋コンクリート製 力Ｐ モーメント図 ラーメン構造部分 前記支板 モーメント特性 記載 他 値Ｍ 断面形状 地震 頭部 水平荷重 寸法 円形 外壁 立面図 水平力Ｐ 応用例 上下階 説明 耐力壁 開口面積 平面視 外形寸法 接合 剛接合 前記床版 柱形状 柱ｄ 梁 Ａ Ｂ 外形寸法Ｄ 鉄筋コンクリート Ｌ 平面形状 外周部 先行技術文献 技術分野 背景技術 上部 課題 手段 特徴 １つ 下部 強度 開口部 内部 正方形 建物外周部 階高 上記構成 施工性 柔軟性 施工方法 幅Ｄ 特開平 号公報 上記 千鳥配置 空間 長所 壁面 窓 天井 プラン 増加 － 概要 目的 効果 図面 Ｈ 等 使用 任意 分 範囲 同士 不足 犠牲 両側 複数 四角形 Ｃ 六角形 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 8.3979 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 回転磁石部 磁石 先端ビット 固定磁石部 偏位方向検出装置 孔管 先端部 回転角度計測器 孔 孔機 先端 孔管内 図 偏位方向 孔方法 ジャイロスコープ 回転角度 孔方向 孔軌跡 孔管先端部 検出 テーパー部 実施形態 方向 発明 回転 角度 装置 信号ケーブル 概略図 Ｎ Ｓ 基準位置 状態 角度調整装置 基端部 回転軸 検知装置 接続切断装置 推進駆動装置 複数 特許文献 拡大図 信号 部分 方法 吸引力 反発力 Ｎ極 Ｓ極 向き 雄 雌 孔ロッド先端部 円形状 技術 先端部付近 工程 基台 ディスプレイ装置等 位置 ジャイロスコープ先端 進方向 ボーリング工法 軌跡 孔ロッド 孔方向決定部材 挿入角度 説明 説明図 ～図 係合状態 中心軸方向 所定位置 ディスプレイ装置 異物 精度 構成 リール パルスエンコーダ 建造物 付属機器 動作原理 係合 形態 参照 コンピュータ 薬液注入作業孔 地上 特徴 特許 係合部分 先行技術文献 係 収納位置 初期位置 技術分野 背景技術 鉄粉 計画線 鉄粉等 下方 工法 課題 目的 配置 作用 図面 接続 地盤 ケーシング 進行 先細状 筒状 予定計画線 パルス 地上側 当該ジャイロスコープ ロータリーエンコーダ 真上側 真下側 液状化現象 号公報 特徴点 挿入 引き込み 土圧 外径 製造コスト 予定 地震 斜め 種々 地点 概要 事情 手段 効果 万が一 不都合 公知 後方 一体 内径 単位 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 9.0204 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 鉄筋 外周部 尺鉄筋 基礎配筋方法 配筋 フーチング 下端筋 鉄筋量 独立基礎 鉄筋メーカ 格子状 上端筋 はかま筋 図 四角形状 拡径頭部付き鉄筋 配筋方法 ベース部 外周部近傍 格子状配筋 上部柱位置Ｐ 尺物 配筋密度 異形鉄筋 作業 発明 上部柱位置 経済性 端部 工事現場 規格品 中央側 モーメントＭ 施工性 拡径頭部 上部構造 配筋作業 請求項 内部 フーチング端部 交互 間引き方 支持力Ｆ 形 現場 端 作業能率 コストダウン Ａ 地盤 荷重Ｆ 鉄筋径 特許文献 能率 実施形態 Ｆ 上記 記載 必要量 Ｐ 縦断側面図 主筋 梁 平面図 同一 スクラップ 通り 刻み 減少 一定長 直線状 改良地盤 四角形 支持力 説明 Ｄ 一端 特徴 市販 既製 効果 構造物 Ｍ モーメント 先行技術文献 技術分野 背景技術 利用可能性 両面 次 現状 通常 歩留まり 課題 目的 特注 むら 状態 偏り 強度 不都合 例 形態 構成 図示 コンクリート 左 建設 荷重 施工管理 技術的手段 一定間隔 平面視 事務所ビル等 太径 マス目 問題点 接地面積 梁主筋 特開 号公報 手段 指定寸法 目 汎用材料 重ね継手 指定 商業施設 種 Ｂ 全域 精度 手間 残り 処理 費用 両端 種類 － 概要 意味 ケース 図面 厚 低減 均一 切断 右 産業 数 規模 工場 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 9.2071 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 反転防止用押え板 発進口 コーナー部材 矩形推進函体 推進函体 推進方向 外側面 角部 止水構造 立坑内空側 固定用鋼材 幅方向内周側 周面 開口縁部 止水性能 内側面 図 推進工法用エントランス エントランスパッキン 外周面 推進方向前方 推進工法 開口部 出隅部 前記発進口 外側 立坑内空側表面 枠体 隅部 平面方向 先端部 実施形態 側 前記推進函体 位置 曲面状 推進 地山側 前記反転防止用押え板 平面部 函体 水 反転 立坑 前記角部 曲面部 立坑内空 前記コーナー部材 内側 方向 発進口周辺 地下水圧 中心側 水平中間部 垂直中間部 部材 止水構造Ｗ 正面図 形状 発明 周方向 幅方向外周側 中間部 進機 構造 曲線状 直線状 推進方向前方側 左右方向 上下方向 止水 推進方向後方側 通過位置 先端 鋼材 ｂ 部分 断面矩形 周縁部 平面形状 形態 外方側 傾斜部分 矩形 金属部材 工法 前記エントランスパッキン 断面形状 断面方向 推進ジャッキ 斜め内側 特許文献 リップ部 隙間 周面形状 平面 シール手段 トンネル函体 舌片部 基端部 構成 コーナー部材４つ シール部材 断面図 横方向 軸方向 配置方向 状態 密着性 ａ 固定方法 トンネル 土水圧 開口周縁部 表面 相対位置関係 枠状 前記実施 前記 一定 刃口 固定 弾性部材 外周形状 略矩形枠状 ４つ 閉塞部材 課題 曲面 応力 鉄骨 取付状態 押しジャッキ 板ばね 平面上状 貫通孔 幅 直角三角形 ワイヤーロープ 位置関係 断面 固定状態 先端部分 説明 複数 力 四隅 止水性 同士 断面多角形状 前記外側面 筒状 プレート状 井桁状 拡大正面図 前記構成 固定ピン 固定強度 水圧下 左右 正面視 ゴムブロック 平面同士 リップシール等 特許 上下左右 先行技術文献 ゴムブロック等 斜視図 技術分野 背景技術 弾性ゴム 地盤 符号 手段 特徴 圧力 効果 図面 推力 壁面 上下 間隔 通常 突出部分 斜辺部分 ブラケット 蛇行 趣旨 Ｗ 方法 金属製 所定ピッチ 号公報 スライド量 距離 離間距離 所定 側面 設計変更 側壁 参照 環状 破線 概要 深度 条件 掘削 虞 セグメント 観点 例 工 坑口 地中 外形 両端 近傍 両側 内部 材質 ２つ インフレートシート 周知 一端 長方形 程度 溶接 逆 低下 手法 夫 出入り ズレ 延 皺 破損 移動 発生 範囲\n",
      "\n",
      "===== # 7, Topic : 14, p : 10.1764 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : セグメント組立装置 掘削断面形状 図 掘削機本体 セグメントピース把持機構 エレクター装置本体 昇降装置 セグメントピース セグメント トンネル掘削機 組立 昇降台 シールド掘削機 セグメントピースＳ 横板部 掘削断面 レール セグメントＳ 掘削機本体軸心 断面Ｔ字状 ｂ ガイドローラ 昇降ジャッキ リングガーダー部 断面視 前記エレクター装置本体 掘削 特許文献 形状保持装置 面 縦板部 スラスト用ガイドローラ 面側 トンネル ピンローラ列 左右 後側昇降台部 リニアガイド 該昇降装置 走行油圧モータ ガイドレール Ｓ セグメント把持機構 背面図 楕円形 側断面図 実施例 前記レール リング状 機構 シールドジャッキ ローラ列 作用状態 走行用ガイドローラ 周 － リング状部 矩形 形状 支持板 発明 シールド掘削機等トンネル掘削機 前側昇降台部 前記セグメント組立装置 スプロケット Ａ－Ａ線断面図 Ｂ－Ｂ線断面図 Ｃ－Ｃ線断面図 ～Ｓ 前記 テール部 ローラ 前記昇降装置 円弧状部 伸縮作動 左右面 接線力 矩形ガイドレール ガイドポスト スイング機構 駆動装置 スイング 矩形状 駆動力 オイルタンク 複合円形 パワーユニット 保持 要部側面図 泥水式シールド掘削機 機械式シールド掘削機 基台 サポートジャッキ セグメントリング 側面図 掘削機本体内周 等 手掘り式シールド掘削機 反対側 回転軸 ピンローラ 複数 泥土圧式シールド掘削機 円形 方向 複合動作 掘削容積 掘削土砂 前記セグメントＳ スイングジャッキ 上下面 式 正面図 号公報 荷重 ton程度 各種 汎用性 問題点 搬送荷重 力 旋回 頂部等 スライドジャッキ リング式 走行フレーム シールド軸心方向 前記筺体部 前記テール部 回転 円弧 筺体部 参照 該レール 長手方向 円形突部 頂部 矩形等 既設セグメントＳ 特許 方向中間部 コーナー部 一般部 隅部 中央部 支持 特徴 増大 ブラケット 両者 搬送 駆動機構 基端部 走行モータ 中空軸式等各種構造 走行スプロケット－チェーンラック方式等 モータ駆動 前記ピン支持ブラケット 変更等各種変更 上下 ギアトレイン 周面 前面側 ｂ等 矩形等種々 ～特許文献 ラックアンドピニオン式 ピン支持ブラケット 伸縮ジャッキ 手掘り式 油圧シリンダ 説明 大型 組み合わせ 形 前進 進 ベルトコンベヤ 保持フレーム 接線方向 支持ブラケット ラック グリッパー 円型等種々 チェーンラック 機器搬出入等 複数リング 上記実施例 トンネル施工 止めジャッキ 伸縮 前記グリッパー 先行技術文献 最高位状態 最低位状態 走行路 オイル 法線方向 正面視 作動油 技術分野 背景技術 相似形 ラックアンドピニオン方式 モノラック 課題 ピニオン フレーム 限界 目的 環状 全長 図面 テールスキンプレート ボルト 上面 所定 範囲 金具 姿勢 補助タンク 個数変更 左右両側 ギア 種々 有効空間 作業空間 円型 特開平 旋回フレームエレクタ 構造 土砂 重量 重量バランス スキンプレート モノレール 片道 車線 幅 要請 広幅 部分 効率 概要 エレクターフレーム 手段 任意 ガイドローラ 効果 形態 設定 単一 一端 出入り 角度 前方 地盤 外部 繰り返し 構成 モーメント 位置決め 左 右 側部 要旨 トンネルボーリングマシーン ＴＢＭ 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 9.7675 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 構造体 免震構造 構造 塔状構造体 免震構造物 点 塔状構造物 材 上下方向 支持点 構造物 震動 振動絶縁体 水平方向 上下震動 下側支持点Ｐ 上側支持点Ｐ 支持点Ｐ 震動制御 上側支持点 下側支持点 図 ダンパー 構成 免震効果 形態 実施 延長方向 発明 上下 方向 Ｐ 免震 棟頂部 上端縁部 下端縁部 制震効果 斜め上下方向 一体構造 特許文献 低層部 平面視 軸方向 向き 効果 空気ばね 位置 Ａ 複数 筒状 正方形状 水平震動 本数 Ｂ 上端 下端 状態 問題点 立断面図 説明 動き ばね部材 平面図 概要構成 構成要素 Ｖ字形状 水平断面 外周縁部 壁体 上下二つ 吸収エネルギー 方向中間 技術 手段 サイズ コスト 低減 地震 数量 ｂ 水平断面形状 コア部 目的 中間 水平荷重 水平成分 水平変位 開口部 角部 平面視中央 先行技術文献 技術分野 背景技術 材どうし 幅 課題 個々 水平面 性能 変更 利点 内部 所定 上述 同一 符号 ４つ 趣旨 範囲 抑制効果 材自体 複数層 層どうし 柱材 階段室等 斜め下向き 斜め上向き 概要 吸収 部材 建物等 ＲＣコア 軸剛性 配置等 複数本 許容荷重 安全性 弾性限度 特開 号公報 面積当り 上記目的 地盤Ｇ 自立状態 剛性 鉄筋コンクリート造 正方形 共用スペース 建物 開口 中央 鉄骨梁 コンクリートスラブ 前記ダンパー 自体 空胴 下向き 上記 固定位置 形状 参照 複数個 － 現状 空気圧 制限 特徴 図面 ａ エレベータ 一般 周囲 用 空間 各層 一端 他端 部分 間隔 周知\n",
      "\n",
      "===== # 9, Topic : 7, p : 8.5759 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 管 外管 間隙水圧計測用パイプ 岩盤計測用管体構造 パッカー用パイプ パッカー ボーリング孔 内管 岩盤計測システム 間隙水圧 圧力センサー 前記外管 水圧 孔底 パイプ 図 岩盤 前記内管 地震 前記間隙水圧計測用パイプ 実施形態 外周面 用作業空間 空間 前記パッカー 透水孔 隙間空間 断面図 地震計 前記パッカー用パイプ 内部充填空間 内部空間 間隙水圧計測 該パッカー 圧力センサー接続口 周面 外側空間 水圧センサー 前記ボーリング孔 前記外管周囲 間隙水圧データ 計測システム 地盤振動 発明 鉛直断面図 該内部空間 設置 該パイプ 該内部充填空間 計測値 請求項 下端 材軸 複数 位置 形態 － 特許文献 下方 制御部 地震データ 前記圧力センサー接続口 周囲 孔壁 方向 鉛直方向 作業 システム 貯蔵処分施設 地下水 中間位置 該水圧センサー 説明 上方 パイプ類 水平断面図 施設 実施 保守点検 断面中心 Ａ－Ａ線 Ｂ－Ｂ線 Ｃ－Ｃ線 Ｄ－Ｄ線 Ｅ－Ｅ線 Ｆ－Ｆ線 地上 内側 環状 流体 挿通 状態 技術 コスト面 中空空間 メンテナンス 透水性 メンテナンス作業 材軸直交方向 利用形態 基準軸 概略ブロック図 先行技術文献 技術分野 背景技術 水平方向 設置位置 健全性 特許 課題 コスト 記載 上端 b 同一 符号 ～ 下 目 上述 a)、(b)）、 深度地下 背面位置 円周方向 c)）、 d)）、 e)）、 f)）、 下端位置 放射性廃棄物 水 構成 構築コスト 必須構成 設置角度 配置構成 放射能レベル 上下段 特開平 目的 上記目的 図面 a c d e f 変形例 添付図面 部品等 横坑 下段 螺旋状 一つ 強度 評価 所望 上下 多段 単一 号 概要 該計測 負担 手段 事情 該空間 単体 両方 機能 種類 該外管 立坑 程度 全長 互い 干渉 内面 該内管 作用\n",
      "\n",
      "===== # 10, Topic : 14, p : 10.0438 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : セグメント組立装置 エレクター装置本体 掘削機本体 図 掘削断面形状 安全装置 セグメントピース把持機構 昇降装置 トンネル掘削機 セグメントピース セグメント シールド掘削機 装置 組立 昇降台 横板部 リングガーダー部 セグメントピースＳ 掘削断面 ｂ レール セグメントＳ 断面Ｔ字状 掘削機本体軸心 昇降ジャッキ 形状保持装置 掘削 断面視 縦板部 背面図 特許文献 スラスト用ガイドローラ 左右 前記エレクター装置本体 面側 トンネル ピンローラ列 ガイドローラ 実施例 前記 後側昇降台部 ストッパー 面 走行油圧モータ リニアガイド 周 当該エレクター装置本体 作用状態 Ｓ 側断面図 ガイドレール 楕円形 筒部 セグメント把持機構 リング状 該昇降装置 シールドジャッキ 底部 ローラ列 リング状部 走行用ガイドローラ 信用性 前記セグメント組立装置 シールド掘削機等トンネル掘削機 前側昇降台部 － 駆動装置 矩形 前記掘削機本体 形状 支持板 発明 テール部 Ａ－Ａ線断面図 Ｂ－Ｂ線断面図 Ｃ－Ｃ線断面図 スプロケット 前記昇降装置 筒状 走行状態 シリンダ部 駆動装置等 ～Ｓ 機構 伸縮作動 落下防止用 方向 ローラ 駆動力 外筒部 前記レール 左右面 要部側面図 接線力 要部拡大図 矩形ガイドレール 前記安全装置 ガイドポスト スイング機構 掘削機本体内周 泥水式シールド掘削機 機械式シールド掘削機 オイルタンク 矩形状 汎用性 等 説明 側面図 走行フレーム 保持 基台 サポートジャッキ 参照 セグメントリング 複合円形 手掘り式シールド掘削機 落下 動体 マイクロコンピュータ等 作動 泥土圧式シールド掘削機 複数 ピンローラ 円形 筺体部 回転軸 掘削容積 掘削土砂 構造説明図 周方向 式 頂部等 前記セグメントＳ 正面図 スイングジャッキ 上下面 制御手段 油圧駆動式 前記テール部 号公報 荷重 ton程度 各種 問題点 搬送荷重 パワーユニット 円弧状部 力 頂部 リング式 前記筺体部 スライドジャッキ 前記ストッパー 方向中間部 長手方向 シールド軸心方向 回転 コーナー部 一般部 辺部 最前部 隅部 中央部 矩形等 限界 構造 伸縮 円形突部 該レール 駆動機構 既設セグメントＳ 特許 モータ駆動 ピン 支持 基端部 動作数 油圧シリンダ 周面 中空軸式等各種構造 特徴 増大 ブラケット ボルト 両者 ケーシング 搬送 走行スプロケット－チェーンラック方式等 ｂ等 前記ピン支持ブラケット 矩形等種々 旋回フレームエレクタ 補強リブ 上下 ギアトレイン 突出 ラックアンドピニオン式 駆動手段 鍔付き筒状 ～特許文献 前面側 反対側 ピン支持ブラケット 手掘り式 変更等各種変更 伸縮ジャッキ 安全性向上 保持フレーム 接線方向 大型 側部 形 前進 進 ベルトコンベヤ 支持ブラケット ラック 円弧 グリッパー 円型等種々 最高位状態 最低位状態 機器搬出入等 上記実施例 チェーンラック 信頼性 複数リング トンネル施工 止めジャッキ 前記グリッパー スイング 先行技術文献 法線方向 走行路 作動油 作動タイミング 複合動作 フレーム 手段 オイル 正面視 鎖線状態参照 図示例 技術分野 背景技術 突出限界 伸限 相似形 ラックアンドピニオン方式 モノラック 課題 ピニオン 目的 環状 全長 図面 テールスキンプレート 上面 組み合わせ 金具 所定 孔 姿勢 突出限界位置 伸限位置 補助タンク 左右両側 ギア 種々 頂 位置 旋回 有効空間 作業空間 円型 特開平 給排 土砂 重量 重量バランス スキンプレート モノレール 片道 車線 幅 要請 広幅 部分 効率 概要 エレクターフレーム 任意 ガイドローラ 所要 内方 効果 形態 設定 単一 一端 出入り 複数個 角度 圧油 出没 前方 地盤 外部 繰り返し 構成 モーメント 位置決め 左 右 要旨 範囲 トンネルボーリングマシーン ＴＢＭ 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4725e-35, 1.2471e-16, 3.6266e-16, 1.7910e-17, 8.6381e-04, 4.6622e-34,\n",
      "         2.6440e-31, 9.9907e-01, 4.3254e-05, 3.0820e-25, 4.2140e-30, 2.2312e-34,\n",
      "         2.8567e-09, 2.5707e-27, 2.3141e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0543, 0.0442, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1038.7056\n",
      "Test epoch : 1 Average loss: 903.3229\n",
      "PP(train) = 2609.597, PP(valid) = 2538.833\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2228e-35, 8.2873e-28, 6.5857e-21, 2.0964e-19, 2.1427e-13, 5.8574e-43,\n",
      "         1.5254e-32, 1.0702e-02, 9.8930e-01, 1.3944e-23, 2.7009e-31, 3.1315e-24,\n",
      "         5.4300e-07, 2.5659e-22, 3.1696e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0564, 0.0678, 0.0444, 0.0627, 0.0307, 0.0762, 0.0631, 0.0913, 0.0570,\n",
      "         0.0689, 0.0850, 0.0649, 0.0509, 0.1310, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1038.5038\n",
      "Test epoch : 2 Average loss: 903.1589\n",
      "PP(train) = 2604.090, PP(valid) = 2534.966\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.1042e-34, 5.5642e-18, 1.2035e-22, 1.6568e-26, 4.3936e-09, 1.2478e-27,\n",
      "         5.6052e-45, 3.2616e-13, 7.4483e-14, 1.0292e-14, 3.8411e-34, 5.9800e-29,\n",
      "         3.0199e-12, 6.4951e-10, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1038.3116\n",
      "Test epoch : 3 Average loss: 902.9852\n",
      "PP(train) = 2598.222, PP(valid) = 2531.159\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1449e-41, 6.2119e-16, 3.8902e-13, 4.0994e-20, 1.0369e-14, 9.5378e-33,\n",
      "         2.0919e-31, 9.3259e-13, 1.4461e-05, 1.3202e-29, 8.5239e-33, 4.9788e-20,\n",
      "         7.5545e-18, 4.4454e-22, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1037.9997\n",
      "Test epoch : 4 Average loss: 902.8059\n",
      "PP(train) = 2592.008, PP(valid) = 2527.239\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0833e-41, 1.0859e-24, 2.7586e-07, 2.3590e-31, 6.3265e-12, 0.0000e+00,\n",
      "         1.8623e-42, 8.3610e-14, 1.2282e-19, 1.1368e-26, 8.0929e-37, 9.8372e-36,\n",
      "         1.7443e-19, 3.3879e-37, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1037.5486\n",
      "Test epoch : 5 Average loss: 902.6240\n",
      "PP(train) = 2585.775, PP(valid) = 2523.423\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0741e-38, 2.2319e-23, 8.8055e-11, 3.2787e-25, 1.5513e-15, 2.9407e-30,\n",
      "         1.5001e-28, 1.3359e-08, 1.2581e-01, 8.9258e-24, 1.4303e-24, 2.7974e-30,\n",
      "         1.0489e-20, 1.8798e-22, 8.7419e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0787, 0.0472, 0.0946, 0.0521, 0.0557, 0.0686, 0.1015, 0.0932, 0.0596,\n",
      "         0.0547, 0.0651, 0.0572, 0.0515, 0.0773, 0.0428]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1037.3043\n",
      "Test epoch : 6 Average loss: 902.4427\n",
      "PP(train) = 2579.377, PP(valid) = 2519.479\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1701e-28, 2.8110e-18, 5.9119e-35, 1.5239e-17, 1.1901e-37,\n",
      "         2.8026e-45, 1.0000e+00, 1.8239e-18, 3.8830e-29, 6.7262e-44, 5.2607e-30,\n",
      "         3.4084e-24, 8.5128e-37, 1.8810e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1036.9062\n",
      "Test epoch : 7 Average loss: 902.2612\n",
      "PP(train) = 2572.807, PP(valid) = 2515.422\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7094e-35, 6.5980e-39, 3.3639e-09, 1.0501e-32, 1.4118e-12, 3.5053e-33,\n",
      "         3.3564e-34, 4.9766e-13, 8.1686e-05, 3.0734e-33, 9.4218e-25, 7.2569e-26,\n",
      "         1.7058e-10, 8.5544e-26, 9.9992e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1036.6428\n",
      "Test epoch : 8 Average loss: 902.0808\n",
      "PP(train) = 2566.352, PP(valid) = 2511.434\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.2371e-35, 3.5242e-19, 8.9741e-39, 1.2056e-26, 1.1423e-40,\n",
      "         2.7760e-42, 6.6900e-17, 2.1892e-26, 1.7212e-41, 0.0000e+00, 1.2864e-33,\n",
      "         8.6025e-19, 1.9692e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1036.2088\n",
      "Test epoch : 9 Average loss: 901.8994\n",
      "PP(train) = 2560.054, PP(valid) = 2507.555\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7648e-42, 9.3930e-31, 1.3826e-29, 8.5877e-31, 2.7991e-15, 2.3292e-37,\n",
      "         1.0723e-36, 1.0000e+00, 2.5968e-12, 2.1451e-27, 1.9242e-32, 6.6541e-34,\n",
      "         8.8355e-12, 9.5151e-23, 1.7533e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1035.9348\n",
      "Test epoch : 10 Average loss: 901.7199\n",
      "PP(train) = 2553.845, PP(valid) = 2503.770\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4950e-39, 4.2559e-28, 1.2695e-26, 1.9986e-30, 1.4024e-12, 2.7185e-43,\n",
      "         1.1554e-37, 6.5459e-13, 1.9943e-12, 1.2304e-24, 7.1925e-38, 4.9476e-40,\n",
      "         2.8267e-08, 5.4453e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1035.5862\n",
      "Test epoch : 11 Average loss: 901.5427\n",
      "PP(train) = 2547.595, PP(valid) = 2499.911\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0623e-40, 3.2608e-35, 2.8922e-23, 3.2405e-22, 2.5382e-14, 5.6914e-35,\n",
      "         0.0000e+00, 3.0492e-11, 3.9134e-15, 5.6358e-33, 8.9683e-44, 2.5924e-37,\n",
      "         1.5566e-21, 3.6336e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1035.2380\n",
      "Test epoch : 12 Average loss: 901.3676\n",
      "PP(train) = 2541.316, PP(valid) = 2496.029\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0816e-35, 2.3620e-29, 4.6435e-07, 5.2395e-19, 2.3721e-13, 3.6747e-31,\n",
      "         0.0000e+00, 6.2653e-07, 1.6357e-11, 8.0215e-19, 9.6594e-25, 3.8083e-25,\n",
      "         2.8246e-21, 8.4303e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1034.9929\n",
      "Test epoch : 13 Average loss: 901.1936\n",
      "PP(train) = 2535.183, PP(valid) = 2492.238\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7143e-35, 2.5238e-29, 2.0122e-21, 1.5019e-19, 1.2721e-09, 1.6349e-33,\n",
      "         0.0000e+00, 3.9892e-01, 3.9467e-10, 3.5955e-20, 9.5758e-37, 1.0452e-35,\n",
      "         1.8400e-01, 6.4912e-23, 4.1708e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0775, 0.0715, 0.0787, 0.0512, 0.0649, 0.0784, 0.0859, 0.0690, 0.0614,\n",
      "         0.0526, 0.0797, 0.0519, 0.0475, 0.0808, 0.0491]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1034.6282\n",
      "Test epoch : 14 Average loss: 901.0198\n",
      "PP(train) = 2529.161, PP(valid) = 2488.537\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 8.4254e-29, 9.6176e-20, 9.5595e-22, 2.5783e-18, 2.1840e-32,\n",
      "         3.8975e-31, 5.2396e-10, 7.9969e-17, 9.6943e-22, 1.8676e-35, 2.3471e-38,\n",
      "         9.4382e-25, 3.8744e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1034.3672\n",
      "Test epoch : 15 Average loss: 900.8447\n",
      "PP(train) = 2523.172, PP(valid) = 2484.805\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5763e-38, 1.4957e-21, 9.6374e-11, 7.6381e-11, 1.4916e-11, 9.8505e-31,\n",
      "         2.0767e-37, 7.1615e-05, 1.4274e-09, 2.3883e-09, 5.2390e-22, 3.8141e-18,\n",
      "         1.7355e-13, 2.9126e-22, 9.9993e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1034.0765\n",
      "Test epoch : 16 Average loss: 900.6735\n",
      "PP(train) = 2517.150, PP(valid) = 2481.049\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.3883e-32, 7.4493e-18, 1.2284e-17, 6.5802e-05, 3.5125e-34,\n",
      "         2.7015e-36, 2.6028e-05, 8.8245e-07, 6.6989e-17, 5.6164e-37, 1.5149e-30,\n",
      "         2.0361e-09, 4.1379e-21, 9.9991e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1033.6361\n",
      "Test epoch : 17 Average loss: 900.5038\n",
      "PP(train) = 2511.272, PP(valid) = 2477.398\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6781e-36, 1.8116e-20, 1.0977e-15, 2.1720e-30, 8.7167e-01, 1.3932e-39,\n",
      "         3.6803e-29, 8.6248e-02, 6.8613e-10, 1.7278e-15, 8.6739e-35, 1.0800e-24,\n",
      "         3.2917e-18, 1.6273e-23, 4.2082e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0491, 0.0518, 0.0454, 0.0396, 0.0938, 0.0687, 0.1052, 0.1010, 0.0468,\n",
      "         0.0858, 0.0713, 0.0591, 0.0456, 0.0656, 0.0712]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1033.4225\n",
      "Test epoch : 18 Average loss: 900.3372\n",
      "PP(train) = 2505.507, PP(valid) = 2473.875\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0433e-38, 4.7190e-23, 3.7255e-09, 7.3630e-23, 3.8291e-11, 8.0038e-32,\n",
      "         2.3032e-36, 1.8555e-03, 3.0756e-10, 1.5033e-29, 1.8082e-23, 2.6576e-24,\n",
      "         4.4332e-24, 6.7676e-20, 9.9814e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1042, 0.0502, 0.0601, 0.0670, 0.1075, 0.0922, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1033.1000\n",
      "Test epoch : 19 Average loss: 900.1700\n",
      "PP(train) = 2499.791, PP(valid) = 2470.367\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7714e-30, 3.0925e-26, 1.3127e-09, 9.3455e-21, 4.5863e-12, 3.0370e-23,\n",
      "         2.0213e-36, 9.9570e-01, 7.9216e-09, 9.6148e-18, 8.4347e-20, 6.7515e-34,\n",
      "         1.8873e-17, 1.2043e-26, 4.3023e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1052, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0371, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1032.8811\n",
      "Test epoch : 20 Average loss: 900.0038\n",
      "PP(train) = 2493.932, PP(valid) = 2466.629\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.7511e-33, 6.4285e-08, 1.0296e-05, 2.7658e-14, 3.5055e-16, 9.4129e-38,\n",
      "         1.5938e-37, 1.1522e-03, 7.6526e-09, 2.1959e-08, 1.6008e-29, 1.9522e-24,\n",
      "         4.0430e-16, 1.6634e-21, 9.9884e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1032.5996\n",
      "Test epoch : 21 Average loss: 899.8391\n",
      "PP(train) = 2488.215, PP(valid) = 2463.025\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8124e-37, 6.6454e-17, 2.6364e-12, 3.2350e-14, 5.6614e-02, 1.1857e-34,\n",
      "         1.1750e-34, 1.4295e-08, 2.7276e-12, 1.0184e-12, 1.6748e-30, 2.1898e-19,\n",
      "         9.2073e-01, 2.3542e-07, 2.2656e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0489, 0.0722, 0.0746, 0.0591, 0.0439, 0.0651, 0.0573, 0.0811, 0.0726,\n",
      "         0.0932, 0.0497, 0.0537, 0.0579, 0.0647, 0.1060]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1032.2180\n",
      "Test epoch : 22 Average loss: 899.6738\n",
      "PP(train) = 2482.745, PP(valid) = 2459.670\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.6379e-38, 3.9002e-06, 6.4388e-28, 2.1471e-08, 1.9415e-33,\n",
      "         0.0000e+00, 4.4186e-12, 1.4557e-18, 3.4147e-18, 7.1166e-39, 5.5087e-23,\n",
      "         7.1005e-27, 4.5331e-10, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1031.8335\n",
      "Test epoch : 23 Average loss: 899.5099\n",
      "PP(train) = 2477.306, PP(valid) = 2456.282\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5848e-32, 7.5510e-34, 2.0974e-05, 5.0821e-26, 4.5069e-09, 8.3996e-37,\n",
      "         2.7674e-26, 3.4637e-11, 5.1245e-17, 6.7470e-25, 1.0942e-23, 7.6435e-21,\n",
      "         4.7705e-14, 1.5699e-22, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1031.6785\n",
      "Test epoch : 24 Average loss: 899.3482\n",
      "PP(train) = 2471.781, PP(valid) = 2452.839\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.3998e-34, 5.4908e-27, 5.2243e-08, 4.7539e-10, 1.1941e-08, 1.3834e-20,\n",
      "         1.6756e-24, 2.6945e-06, 9.9949e-01, 7.0426e-21, 3.8241e-39, 5.0253e-26,\n",
      "         2.4707e-07, 2.6144e-16, 5.0392e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1031.3434\n",
      "Test epoch : 25 Average loss: 899.1877\n",
      "PP(train) = 2466.197, PP(valid) = 2449.325\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2654e-42, 8.7825e-36, 1.9296e-22, 4.2126e-26, 9.5973e-08, 0.0000e+00,\n",
      "         1.7256e-33, 1.5799e-09, 1.3097e-18, 3.8437e-32, 3.1904e-40, 2.7376e-30,\n",
      "         3.5068e-18, 2.7005e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1031.0061\n",
      "Test epoch : 26 Average loss: 899.0291\n",
      "PP(train) = 2460.716, PP(valid) = 2445.899\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1616e-26, 1.1930e-13, 1.4776e-14, 3.7836e-15, 3.5032e-43,\n",
      "         1.1322e-42, 1.6477e-11, 5.1877e-13, 1.1884e-29, 3.4090e-30, 7.7636e-19,\n",
      "         2.1083e-13, 1.1425e-13, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1030.7098\n",
      "Test epoch : 27 Average loss: 898.8688\n",
      "PP(train) = 2455.480, PP(valid) = 2442.617\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3902e-37, 5.9069e-15, 3.1338e-04, 2.0611e-11, 6.4513e-13, 6.3492e-37,\n",
      "         7.0065e-45, 1.2240e-19, 1.1125e-06, 6.8474e-18, 2.2197e-29, 1.2060e-20,\n",
      "         1.8338e-05, 3.0113e-23, 9.9967e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1030.4743\n",
      "Test epoch : 28 Average loss: 898.7107\n",
      "PP(train) = 2450.227, PP(valid) = 2439.356\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0233e-37, 6.6083e-25, 8.0896e-12, 4.7792e-18, 9.9791e-01, 1.6857e-32,\n",
      "         3.0056e-32, 7.7781e-07, 2.0896e-03, 1.1374e-21, 1.0574e-32, 2.8488e-32,\n",
      "         1.9851e-11, 7.4761e-19, 9.1528e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0957, 0.0657, 0.1065, 0.1084, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0622, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1030.1855\n",
      "Test epoch : 29 Average loss: 898.5552\n",
      "PP(train) = 2444.885, PP(valid) = 2436.012\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.1380e-30, 6.9298e-14, 3.2947e-21, 1.6319e-10, 1.1216e-41,\n",
      "         5.7235e-41, 8.8336e-01, 1.1421e-19, 1.6330e-26, 1.1702e-41, 8.5836e-37,\n",
      "         2.5325e-19, 2.8669e-20, 1.1664e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0841, 0.0965, 0.0593, 0.0454, 0.0774, 0.0898, 0.0808, 0.0481, 0.0543,\n",
      "         0.0389, 0.1118, 0.0455, 0.0384, 0.0919, 0.0378]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1029.8110\n",
      "Test epoch : 30 Average loss: 898.4008\n",
      "PP(train) = 2439.626, PP(valid) = 2432.717\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1533e-36, 1.6233e-26, 3.6832e-02, 2.7019e-08, 1.5773e-14, 1.0318e-38,\n",
      "         1.6137e-35, 2.4811e-02, 4.0819e-14, 1.1352e-24, 2.0715e-29, 3.6080e-20,\n",
      "         4.1956e-05, 2.0190e-21, 9.3832e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0820, 0.0467, 0.1000, 0.0510, 0.0616, 0.0670, 0.1058, 0.0904, 0.0605,\n",
      "         0.0523, 0.0633, 0.0552, 0.0509, 0.0722, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1029.5635\n",
      "Test epoch : 31 Average loss: 898.2460\n",
      "PP(train) = 2434.512, PP(valid) = 2429.506\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 5.1243e-34, 6.7377e-28, 4.2449e-18, 5.5434e-22, 3.6966e-42,\n",
      "         4.6262e-33, 2.5652e-08, 1.6528e-15, 5.3658e-28, 3.3564e-28, 2.2209e-33,\n",
      "         1.1907e-09, 2.4532e-29, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1029.3288\n",
      "Test epoch : 32 Average loss: 898.0917\n",
      "PP(train) = 2429.421, PP(valid) = 2426.312\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2779e-29, 1.0554e-32, 3.3828e-06, 3.0998e-20, 1.5999e-20, 5.0457e-27,\n",
      "         6.1651e-36, 1.5016e-07, 8.0505e-01, 1.0132e-16, 6.6727e-37, 7.8418e-20,\n",
      "         4.0660e-14, 4.6369e-18, 1.9495e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0612, 0.0629, 0.0530, 0.0610, 0.0352, 0.0751, 0.0708, 0.0932, 0.0582,\n",
      "         0.0665, 0.0807, 0.0640, 0.0517, 0.1180, 0.0486]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1029.0607\n",
      "Test epoch : 33 Average loss: 897.9397\n",
      "PP(train) = 2424.282, PP(valid) = 2423.066\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.9645e-43, 1.6360e-38, 5.4186e-14, 2.3018e-20, 1.7797e-13, 4.2940e-33,\n",
      "         5.3855e-36, 4.4676e-17, 2.6117e-09, 1.4408e-16, 7.6902e-33, 9.5187e-31,\n",
      "         3.9892e-17, 6.0837e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1028.7636\n",
      "Test epoch : 34 Average loss: 897.7882\n",
      "PP(train) = 2419.137, PP(valid) = 2419.780\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.1013e-25, 8.0006e-23, 4.0678e-20, 2.5551e-19, 0.0000e+00,\n",
      "         2.8026e-45, 4.6467e-23, 1.1752e-23, 5.0796e-34, 1.9558e-40, 4.0982e-37,\n",
      "         9.8348e-21, 1.5645e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1028.4381\n",
      "Test epoch : 35 Average loss: 897.6386\n",
      "PP(train) = 2414.167, PP(valid) = 2416.680\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4951e-34, 4.2604e-22, 5.7094e-20, 1.0454e-14, 4.9046e-10, 1.1617e-35,\n",
      "         7.2615e-30, 1.3902e-06, 8.2700e-12, 6.0214e-20, 6.7605e-39, 1.6446e-38,\n",
      "         6.0571e-12, 5.2814e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1028.2295\n",
      "Test epoch : 36 Average loss: 897.4894\n",
      "PP(train) = 2409.307, PP(valid) = 2413.675\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2755e-38, 3.0682e-29, 3.3095e-08, 1.5167e-22, 3.9412e-12, 1.1210e-44,\n",
      "         1.5272e-36, 2.3846e-18, 8.6488e-21, 4.5870e-20, 0.0000e+00, 3.3189e-29,\n",
      "         4.9360e-15, 6.6922e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1027.9316\n",
      "Test epoch : 37 Average loss: 897.3430\n",
      "PP(train) = 2404.416, PP(valid) = 2410.632\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7823e-29, 2.4248e-15, 2.4972e-09, 3.0777e-16, 3.3442e-09, 9.8709e-34,\n",
      "         8.9532e-34, 9.8004e-01, 1.1634e-15, 9.2959e-26, 6.5494e-35, 1.2246e-22,\n",
      "         8.1909e-05, 2.7243e-27, 1.9880e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1040, 0.0552, 0.0444, 0.0788, 0.0919, 0.0775, 0.0444, 0.0533,\n",
      "         0.0373, 0.1180, 0.0441, 0.0369, 0.0936, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1027.6731\n",
      "Test epoch : 38 Average loss: 897.1957\n",
      "PP(train) = 2399.417, PP(valid) = 2407.450\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5590e-32, 3.1759e-25, 4.5227e-08, 2.2079e-24, 1.9113e-04, 5.8629e-33,\n",
      "         1.4831e-33, 1.1358e-10, 4.9459e-09, 2.6229e-22, 1.1242e-21, 5.4873e-23,\n",
      "         1.2768e-07, 1.8990e-21, 9.9981e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1027.4766\n",
      "Test epoch : 39 Average loss: 897.0484\n",
      "PP(train) = 2394.514, PP(valid) = 2404.307\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1765e-35, 2.0007e-25, 2.3267e-25, 2.5038e-17, 5.2722e-17, 8.2160e-38,\n",
      "         1.7304e-38, 5.9326e-14, 5.9828e-05, 2.6922e-28, 2.8551e-41, 1.1722e-21,\n",
      "         6.7130e-10, 4.7323e-29, 9.9994e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1027.1867\n",
      "Test epoch : 40 Average loss: 896.9036\n",
      "PP(train) = 2389.733, PP(valid) = 2401.310\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4969e-36, 1.4784e-26, 3.3742e-18, 3.0901e-10, 3.9030e-16, 1.3792e-32,\n",
      "         3.6003e-38, 4.3452e-09, 2.4231e-15, 7.9499e-29, 9.8824e-29, 3.7370e-27,\n",
      "         6.1990e-08, 4.6787e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1026.9003\n",
      "Test epoch : 41 Average loss: 896.7601\n",
      "PP(train) = 2385.054, PP(valid) = 2398.428\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.5955e-41, 3.5010e-23, 1.5228e-10, 1.0392e-26, 4.9330e-18, 4.0012e-36,\n",
      "         1.0425e-33, 2.8713e-14, 1.6675e-13, 2.5284e-09, 3.7959e-36, 1.7395e-24,\n",
      "         3.1892e-22, 3.6074e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1026.6396\n",
      "Test epoch : 42 Average loss: 896.6149\n",
      "PP(train) = 2380.361, PP(valid) = 2395.445\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9214e-40, 9.8719e-31, 2.0984e-25, 2.0909e-19, 1.6338e-13, 2.3632e-40,\n",
      "         0.0000e+00, 8.5307e-08, 3.9163e-05, 3.9299e-30, 6.5989e-35, 1.7975e-15,\n",
      "         1.2226e-11, 5.2689e-21, 9.9996e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1026.3011\n",
      "Test epoch : 43 Average loss: 896.4724\n",
      "PP(train) = 2375.612, PP(valid) = 2392.457\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9236e-44, 1.7974e-26, 1.5363e-04, 1.4486e-14, 2.8523e-07, 1.3468e-32,\n",
      "         4.1502e-36, 3.7631e-11, 1.8812e-03, 6.9877e-26, 1.0824e-27, 3.4383e-27,\n",
      "         4.5839e-19, 1.3366e-22, 9.9796e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1074, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1026.0886\n",
      "Test epoch : 44 Average loss: 896.3297\n",
      "PP(train) = 2370.987, PP(valid) = 2389.520\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.8000e-41, 3.6042e-36, 2.0373e-20, 3.4421e-29, 1.4235e-09, 0.0000e+00,\n",
      "         1.9870e-42, 2.2505e-18, 2.7886e-18, 1.2991e-23, 5.8154e-43, 5.6860e-33,\n",
      "         8.9547e-25, 3.3695e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1025.7628\n",
      "Test epoch : 45 Average loss: 896.1895\n",
      "PP(train) = 2366.343, PP(valid) = 2386.592\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5298e-38, 1.7398e-26, 1.4423e-18, 1.0799e-23, 5.2445e-29, 0.0000e+00,\n",
      "         1.4013e-45, 3.7924e-12, 1.1075e-31, 2.2142e-31, 5.6052e-45, 4.7924e-43,\n",
      "         3.9872e-32, 5.5370e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1025.6324\n",
      "Test epoch : 46 Average loss: 896.0488\n",
      "PP(train) = 2361.813, PP(valid) = 2383.740\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5086e-34, 5.1209e-19, 5.7879e-14, 9.2934e-04, 3.0858e-13, 2.1139e-33,\n",
      "         3.8522e-32, 2.3607e-11, 2.6471e-02, 7.0823e-27, 1.7292e-42, 3.0517e-23,\n",
      "         7.8406e-01, 4.9171e-17, 1.8854e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0544, 0.0684, 0.0809, 0.0595, 0.0447, 0.0664, 0.0627, 0.0829, 0.0721,\n",
      "         0.0848, 0.0522, 0.0546, 0.0577, 0.0678, 0.0910]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1025.3532\n",
      "Test epoch : 47 Average loss: 895.9106\n",
      "PP(train) = 2357.262, PP(valid) = 2380.879\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8169e-41, 5.9484e-21, 6.8676e-06, 8.5480e-16, 5.0554e-16, 1.6202e-29,\n",
      "         2.2143e-25, 2.4357e-03, 4.3184e-16, 5.6560e-21, 4.3374e-35, 9.1155e-25,\n",
      "         9.9755e-01, 1.0555e-14, 6.1147e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0483, 0.0745, 0.0761, 0.0605, 0.0414, 0.0647, 0.0539, 0.0788, 0.0746,\n",
      "         0.0936, 0.0483, 0.0529, 0.0584, 0.0644, 0.1096]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1025.0909\n",
      "Test epoch : 48 Average loss: 895.7729\n",
      "PP(train) = 2352.613, PP(valid) = 2377.928\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.0236e-33, 1.0338e-27, 1.2181e-25, 8.3740e-22, 0.0000e+00,\n",
      "         2.4548e-41, 3.2064e-15, 8.8027e-19, 6.3577e-37, 1.8217e-44, 6.3458e-41,\n",
      "         1.0554e-09, 5.1183e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1024.7393\n",
      "Test epoch : 49 Average loss: 895.6357\n",
      "PP(train) = 2348.175, PP(valid) = 2375.141\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5015e-32, 3.7694e-31, 1.5457e-21, 1.7732e-11, 9.9996e-01, 6.8539e-38,\n",
      "         1.4013e-45, 6.9012e-12, 3.4046e-11, 1.2948e-26, 2.6352e-32, 3.0388e-33,\n",
      "         1.9642e-19, 1.5715e-17, 4.2526e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1024.5811\n",
      "Test epoch : 50 Average loss: 895.4983\n",
      "PP(train) = 2343.862, PP(valid) = 2372.450\n",
      "Writing to ./topicwords/12-topwords_e50.txt\n",
      "Topic 0: ピストンロッド 発破 ピット 水溶性 撹拌 緊張 定着構造 撮像画像 マンホール 設置個所\n",
      "Topic 1: ピストンロッド 発破 コンクリート打設後 マンホール 定着構造 電磁弁 ピット 緊張 バリエーション 添加\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 挙動 添加 現象 新設 角度θ 建設現場 ばね 落下 分解斜視図 緊張\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: ピストンロッド 発破 マンホール ピット 緊張 定着構造 電磁弁 設置個所 水溶性 撮像画像\n",
      "Topic 6: ピストンロッド 緊張 発破 マンホール ピット 設置個所 定着構造 撹拌 撮像画像 コンクリート打設後\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 図\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: ピストンロッド コンクリート打設後 水溶性 発破 緊張 挙動 ピット 内型枠 バリエーション 角度θ\n",
      "Topic 10: ピストンロッド 発破 水溶性 ピット 減衰力 設置個所 定着構造 電磁弁 撮像画像 マンホール\n",
      "Topic 11: ピストンロッド 緊張 発破 水溶性 定着構造 ピット マンホール 減衰力 撮像画像 撹拌\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: ピストンロッド 発破 緊張 マンホール ピット 水溶性 制御信号 バリエーション 設置個所 鋼\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 7, p : 8.0478 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : けい酸カルシウム板 けい酸カルシウム基材 基材 トンネル内装用パネル 視線誘導機能 ケイ酸カルシウム材 表面材 硬質けい酸カルシウム板 パネル ～ けい酸カルシウム材 前記ケイ酸カルシウム材 多孔質無機質板 吸水寸法変化率 硬質けい酸カルシウム系材料 けい酸カルシウム板タイプ トンネル内装材 補強用繊維 耐火材 金属板 繊維 表面 耐火性 ケイ酸カルシウム水和物 発明 μｍ 見掛け密度 針状ワラストナイト そり 前記ケイ酸カルシウム水和物 視線誘導層 トンネル内壁 耐久性 質量 含有量 ｍｍ ケイ酸質原料 トンネル内装用 Ｘ線回折強度 塗膜 接着剤 そり量 範囲 視線誘導内装板 ゾノトライト 変化 含水率 特許文献 変化率試験 ｃｍ 量 寸法 寸法変化率 ｍ 加熱残存収縮率 セルロースパルプ 内装材 長期耐久性 自動車用道路等 危険性 トンネル用パネル マトリックス μｍ程度 汚染洗浄性 強度 原料スラリー 無機塗装 面 ℃ 多孔質 琺瑯 吸水 特許請求 トンネル 前記ゾノトライト 前記トバモライト 繊維原料 トンネル内壁面 トバモライト 程度 石灰質原料 特徴 水 前記石灰質原料 前記繊維 金属板等 火災等 請求項 加熱用ランプ 補強繊維 建築用 未満 片面 ＪＩＳ Ａ 表 発明品 比較品 耐火材料等 エポキシ系接着剤 無機系塗料 耐火防護機能 耐久性並び 無水無機繊維含有断熱性耐火物 μｍ～ 製造方法 方法 本明細書 裏面側 方向 前記原料スラリー 散水量パネル 密着性 持続性 耐火物 不燃性 エポキシ樹脂系接着剤等 凸状 前記加熱養生 材料 等 重量 あたり パネル同士 軽量骨材 機長繊維 晶質シリカ 幅 セルロースパルプ等 利用可能性 塗装等 耐火性能 上記そり量 前記加熱脱水成形 水ガラス系塗料 特許 熱伝導率 無水無機 加熱温度 前記オートクレーブ養生 前記ケーキ 含有合計 塗膜自体 平衡含水率 上述 比 前者 後者 Ｏ 課題 現象 両面 効果 コーティング 種類 目視 亀裂 室温 操作 フクレ 平衡含水率状態 養生 無水 塗料 項 技術分野 背景技術 可燃物 琺瑯引き鉄板 ＭＰａ～ 幅方向 横方向 性能 ケーキ ＭＰａ 温度 アルミナセメント 特開 号公報 号 接合部 シーラー 環境条件 シーラー塗布 コスト高 熱 実施例 供試体 電気炉 横 合計 コンクリート壁 散水 説明 工程 比率 圧力 ℃、 Ｎａ Ｋ ～＜ 残部 微粉 － 開示 水分 水蒸気 分布 均一 原因 隙間 一般 通常 目的 手段 該基材 特定 最良 形態 鉱物 発生 凹状 影響 ａ 特性 ｂ 水中 外観 表側 分 昇 測定 様子 産業\n",
      "\n",
      "===== # 2, Topic : 7, p : 8.2668 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 浚渫用ポンプ 砂 浚渫方法 サンドポケット 浚渫 砂部 浚渫装置 図 吸引口 通過量 遮蔽物 漂砂 排砂管 噴流パイプ ジェットノズル 遮蔽域 高圧水噴射パイプ 浚渫ポンプ 崩壊用ジェットノズル 高圧水 平面図 形態 設置間隔 実施 構造物 堆積砂 箇所 噴射 海岸 給水管 先端部 エジェクタノズル 遠心ポンプ 土砂水 断面図 設置位置 効率 液状化範囲 卓越方向 沿岸漂砂 上記 設置 侵食部 スカート部 部分 吸引部 下側 桟橋 方向 該堆砂部 発明 構成 前記吸引口 斜視図 法肩 該浚渫用ポンプ 波 容量 吸引口内 圧送ポンプ 前記エジェクタノズル 吸引パイプ のど部 拡散部 折曲部 中心点 海岸線 突堤 該噴流パイプ 千鳥配置 側 供給エネルギー 概念図 横移動用 沖合 環状 幅 量 堆積 範囲 上手側 液状化 上側 近傍 岩礁 樋 スラリーピット 噴流 特開 号公報 所定距離 連結材 海底地盤 安息角 堆積量 渫用ポンプ 安定化 説明 直下 港口 周囲 下方 符号 幅広 設計 位置 先端 基端部 浚浚渫装置 該給水管 間隔ごと 採取可能量 桟橋構造 平面円形 下手側 － 課題 効果 図面 防波堤 区域 複数 内側 上下方向 通常 ° 状態 ～ Ｕ字状 上記エジェクタノズル 中心軸 海底 Ｕ字形 技術分野 背景技術 離岸堤 Ｌ字形 垂直状 斜め 特許文献 固定式 底面積 保護機能 経済設計 周面 斜め上向き 港湾 漁港 一定 開示 漂砂量 目的 手段 他 特徴 多方向 最良 形成 背後 現象 捨石 ブロック 反対 既存 先方 付近 波浪 内また 二つ 真下 同心円 外側 圧 周り 上方 直径 下向き 横向き\n",
      "\n",
      "===== # 3, Topic : 12, p : 8.0521 %\n",
      "Topic words : 施工, 内側, Ａ, 図面, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 杭 地下外壁 地下 親杭 耐圧版 既存 建物 山留め先建て親杭工法 前記親杭 杭横矢板 前記地下外壁 支持杭 横矢板 親杭横矢板 既存建物 地下連続壁 地下躯体 支持凹部 図 前記支持凹部 地下空間 前記既存 工法 支持アングル 地下階 発明 特許文献 前記耐圧版 前記親杭横矢板 地下水位 Ｃ型鋼 親杭横矢板工法 止水壁 杭横矢板工法 建物用 土砂 外壁 部分 地中連続壁 鋼チャンネル 既存耐圧版 前記既存建物 前記地下空間 建築機械搬入用 地下部分 支持用 連続壁 既存建物用 前記既存建物用 ～地下 説明 該親杭横矢板 内側 搬入 ＳＭＷ工法 前記支持アングル ボルト 空間 課題 基部 アンカーボルト 当該地下階 コスト 山留め壁 支持材 説明図 Ｈ型鋼 重機搬入 壁体 号公報 騒音公害 汚泥処理 重機搬入用 作業空間 新規建物 工期 低減 凹部 位置 構真柱 解体作業機械 モルタル Ｈ鋼 前記基部 Ｌ型チャンネル等 搬入路 墨出し作業 重機 土留め壁 精度確保用 地中梁 墨出し － 工事 敷地 程度 クレーン ナット 掘削作業能率 切り梁等 上階 ボルト締結 大型重機 上層階 建入れ精度 拡大断面図 ロックオーガー等 モルタル等 騒音 技術分野 背景技術 産業廃棄物 不要部分 特開平 特開 解体撤去 上記課題 立設 所要間隔 端部 水平方向 ～ 床スラブ 腹起し 掘削 参照 開示 使用 振動 周囲 手段 目的 要旨 下部 上部 外側 効果 手間 心材 近隣 最良 形態 地盤 関係 縦横 寸法 上面 位置決め 側面 各々 予定 一角 スロープ 孔 下層 頭部 コンクリート 跡 短縮 図面 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 8.6335 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 流動化処理土 解体ガラ骨材 建設発生土 流動化処理工法 混入率 固化材 圧縮強度 泥水 土 水 流動性 強度 骨材 一軸圧縮 湿潤状態 状態 関係 容易化 請求項 発明 前記解体ガラ骨材 施工性 セメント比 前記建設発生土 施工 乾燥状態 前記 前記流動化処理土 産業廃棄物 粒径 コンクリートガラ 特許文献 フロー値 前記特許文献 コンクリート解体ガラ骨材 解体 処理技術 現場 図 グラフ 工法 調整泥水 泥水比重 締め固め コンクリート 粒土分布 調合強度 記載 体積収縮 － 混練 材料分離 所定 スランプ 解体コンクリート 間隙比 設 削減 建設廃棄物 フロー 前記作用 敷地外発生 品質 圧送 目的 粉砕 混入量 腐植土 セメント比－強度関係 運搬 比重 ひび割れ 作業 要旨 材料 泥状土 建設現場 固化 号公報 安定液 再生微粉 水セメント比 単位セメント量－強度関係 強度管理 下記特許文献 利用 設時 程度 理由 吸水 混入攪拌 泥性 コンクリート構造物 対象泥水 透水性 水質量 吸水量 沈下量 スラリー状物 絶乾状態 前記目的 砂分含有量 説明 技術分野 背景技術 一定 復旧 土砂 均一 コンクリート廃材 Ｗ Ｂ 課題 先行技術 低減 設後 形態 図面 水分 手 mm ）～（ 所期 現場敷地内また 関係式 調整泥土 再生 等 地下水 特殊アジテータ車 テストピース寸法 特開平 特開 コンクリートガラ等 砂利等 下記表 有機物質 粘着力 所定品質 泥土 循環路 採取管 条件 表面 表面水量 ポンプ圧送 粉砕機 上記条件 試験 スランプ試験 表 部分 海 畑 多量 製造 断面 硬性 締 浸食 密度 種類 配合 地震 特徴 加熱 質量 内容 開示 例 不都合 手段 効果 最良 実施 含水量 バケット mm× ｉ ii 割合 ａ ｂ ｃ 根拠 ワーカビリチー 指標 最適 性状\n",
      "\n",
      "===== # 5, Topic : 14, p : 8.8688 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 耐火材 内装材 間隔保持金具 図 係止金具 火炎巻込防止材 内装仕上げ材 セグメント トンネル 耐火材層 耐火内装構造 差込片 上記内装材 内面 上記 間隔 ｃ 上記耐火材取付構造 工内面 工 トンネル内面 ボルト 耐火性能 付き トンネル軸方向 内装板 接着材等 実施形態 トンネル周方向 周方向 内装材付き耐火材 内装効果 方向 内面側 対向片 上記間隔保持金具 上記耐火材 等 上記内装材付き耐火材 トンネル内空断面 内装材付き 前記内装材 内面図 前記 ｅ 上記内装仕上げ材 構造材 アルミメッキほうろう鋼板 下側 耐火材取付構造 インサートナット 上下両辺部 取付孔 ｂ トンネル軸線方向 トンネルＴ ｄ 発明 取付基部 耐火材同士 拡大図 押え片 構成 状態 上記セグメント セグメント側 金具 上下方向 工用セグメント トンネル空間側 表面 ａ ｃ－ｃ断面図 ～（ｃ 工用コンクリート 耐火性 裏面側 背面側 工場等 継手位置 前記図 幅方向 前記間隔保持金具 上側 セグメント内面 道路トンネル 工コンクリート 固定用ボルト 所定 作業 火炎 横断面図 上記特許文献 セグメント内面側 下 内装機能 ～図 斜視図 セグメント内周 断面略コ字形 該間隔保持金具 下辺部 取付ボルト 上記凹部 方向両側 上記図 トンネル両側 インバートコンクリート 基部 隙間Ｓ 凹部 隙間 セグメント内周面 セグメント等 カーブ等 金物等 反対側 また下側 施工性 トンネル内面全面 上下 上記取付孔 施工 側 ｆ 位置 ほうろう層 拡大断面図 コンクリート等 円弧状 スリット状 セラミック系耐火板 上記アルミメッキほうろう鋼板 前記セグメント 底部内面 長手方向両側 長手方向両端部 ほうろう鋼板 施工要領 上記実施形態 説明 アンカー 左右方向 直角方向 進方向 長手方向中間部 鉄道トンネル 各種トンネル トンネル内方 トンネル掘削 火災 手段 効果 圧縮状態 前記押え片 両端部 拡大縦断面図 ダブルナット 長手方向全長 縁部 特許文献 形態 通常図 セラミック等 孔状 下辺側 展開図 側面図 正面図 説明図 連結ボルト等 上記ボルト止め作業 略帯板状 横断平面図 変位 目的 陸 － 両者 上記インサートナット 動き アルミニウム等 実施例 材質 形状 固定手段等 端部 開口部 図中２ｄ ステンレス等 形状等 材質等 地震等 アンカー等 鋼板 上下両側縁 接合方法等 珪酸カルシウム板 コンクリート製 コンクリート躯体 方法 前記凹部 下記特許文献 美観維持等 仕切板 固定方法 施工状態 アルミニウムめっき層 前記インサートナット 接着 変状 長方形状 段差状 弾力性 耐震性 信頼性 耐久性 清掃回復性 利用可能性 珪酸カルシウム 工法 手法 課題 使用 背面 矢印 手 質量 ｍ Ｔ Ｓ 施工現場 継ぎ目位置 カーブ 要領 技術分野 背景技術 調整作業 セラミックブランケット 設置作業 一体形成品 別体 図示例 運転者 側壁部分 ピッチ 美観 特開平 号公報 問題点 安全確保 左上部 粘着テープ 配列ピッチ 一体 図示 ロックウール 通常 種 車 視界 常 遊び 破損 位置決め 荷重 設本数 負担 開示 軽量 特徴 損傷 最良 板材 鉄 横長 上方 下方 ３つ 帯状 設作業 下向き 上向き １つ 先 凹部内 支障 硬質 外面 両面 工程 個々 熱 利点 代わり 設時 設後 産業 図面 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 8.4429 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 導水部 導水壁 壁 通水口 周期波高低減構造物 波 周期波 前記導水部 波効果 前記導水壁 前記通水口 構造物 海洋構造物 奥側 海側 波高低減構造物 図 波高 反射率 導入口 構造体幅 側 構造体幅ｔ 海洋構造物海側面 カーテン壁部 請求項 通水口幅 周期波域 構造 波工 奥 開口部 通水口側 通水口側部 幅 側部 透水部 スリット 周期 現地スケール モデル スリット状 幅ｔ 入射波 端部 壁前面 下側 縦向きスリット状 奥側端 縦向き 遊水部 前記海洋構造物海側面 該導水部 発明 海側面 前面側 海側面部 スリット壁 壁部 周期域 横向きスリット状 構造物側壁 周期波高低減構造物モデル 上側部 波ブロック 重複波 円弧状 該カーテン壁部 ～ 長周期波高低減構造物 透水孔 ｔ エネルギー 船舶 実験 波高計 配置 陸側 前記導入口 通常波域 反射波 水 奥側中央部分 奥側斜め 面部 ａ スリットケーソン 荷役作業等 形状 隔壁 導水部内部 奥側端部 コンクリート構造体 特許文献 構造物側壁内面 奥側中央部 岸壁 通常波 波性能 堤防等 損失 上述 記載 特徴 前面 構造物側壁内面部 スリット状導入口 エネルギー損失 位置 構成 同上 荷役作業 縦断面図 該重複波 カーテン直立壁 係留系 水位変動 奥行き方向構造体幅ｔ 実施例 部分破断斜視図 海側部 波材層 前部 他方 ～幅ｔ 水位 横断面図 雑石層 比較用モデル 開口率 上部工 周期波動 船舶接岸岸壁 節部 奥行き 内部 効果 開口率等 係留索 水位差 波動 例 関係 グラフ 左右対称 ～（ｃ 周期帯 カーテン直立 等 謂両面スリットケーソン 特開 号公報 斜め 断面図 中空状 断面水槽 断面ハ字状 所謂スリットケーソン 開口 説明 複数 流速 距離 港湾 渦 符号 隙間 表 部分 形態 前面形状 中空函状 実験モデル 実験水槽模型 船舶等 雑石 性能実験 波動エネルギー 面部形状 海岸等 位置等 砕石等 防波堤等 破断強度 実験装置 実験条件 実験方法 模型形状 ～数 堤防 参照 波長 － 課題 通常 大型 規模 技術 目的 一対 双方 上下 ｂ 横向き ｃ 裏込材 模型縮尺 ｓ～ ａ)～（ｃ 実施形態 固有振動数 技術分野 背景技術 左右非対称 水平速度 条件 合成繊維 実現性 並列配置 防波堤 相似則 ｓ 後方 空間 減衰 腹 最大 開示 支障 被害 ＤＷＴ 共振 船体 程度 提供 手段 所期 既存 設置 影響 動揺 最良 両部 状況 該隙間 互い 一体 元 フルード 通り ケース 試験 タイプ 傾向 図面 概略\n",
      "\n",
      "===== # 7, Topic : 14, p : 8.7834 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 込部材 型枠 カム部 横孔 図 固定 露出部 固定装置 抜挿軸 部材 コンクリート製品 横 弾性部材 係止部 基準面 込部 半円形 支持板 コンクリート 軸孔 位置決め固定 溝 横寄せ 位置決め縁 操作ハンドル 規制ピン 制限杆 底部 位置決め 装置 発明 部分 孔 円形 込部材部 設置孔 半円部 垂直中心線 垂直中心 コンクリート型枠 込部材固定装置 取付け部 要部 中心 垂直中心Ｓ 上記型枠 前記型枠 軸線方向 振動 ｂ 型枠台 下部 側面図 成形型枠 上記 説明 切欠 同上装置 縦断正面図 押圧部 状態 周面 上記抜挿軸 通り露出部 上記横孔 説明図 課題 構成 右側 込金物 台部 バラツキ 符号 受部 時計方向 金物支持部 垂直中心線Ｓ 対象形 合孔 特許文献 溝外 外 通りコンクリート製品 中央部 製品 上記装置 通り対象形 横寄せ作用 寸法 力 押圧 図面 半円形切欠 側面 左側 操作 縦断側面図 通り 円弧形等 弾性 方向 通り角度θ 前記抜挿軸 定規面 角度θ 係合位置 押圧力 回転 効果 抵抗 字形 後端 前側 ～ 他方 上部 間隔 一対 操作レバー 上記構成 通り単独 前記問題点 Ｕ字形等 前記課題 技術分野 背景技術 鋼材等 利用可能性 成形用 下側 実施形態 実施例 反対側 円形切欠 位置 請求項 形態 右側上面 左側下部 回動 締め固め 合し 一端 参照 － 号 加工 溶接 開示 手段 下記 特徴 上側 解除 脱 弾力 関係 程度 戻り 移動 最良 前端 後部 前方 範囲 方形 緩み 左右 所定 下降 下げ 産業\n",
      "\n",
      "===== # 8, Topic : 7, p : 8.7611 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 梁 梁主筋 柱鉄筋 柱型枠 順梁 柱 柱主筋 フープ筋 パネルゾーン 図 接合方法 端部 梁床コンクリート 下階 柱コンクリート 断面図 上階 割りフープ筋 ハーフプレキャストコンクリート造 前記順梁 梁工法 柱フープ 閉鎖 発明 コンクリート 割フープ筋 強度 上端 上側 下側 フルプレキャストコンクリート造 梁端部 前記柱型枠 隙間 強度鉄筋 該柱型枠 接合部 特開 号公報 パネルゾーンジョイント 説明 上下 課題 上記 普通鉄筋 先端部 － 上部 残り 番線 床板 技術分野 背景技術 在来工法 鉄筋コンクリート構造物 特許文献 継手位置 溶接作業 短縮化 現場施工 間隔ごと 上方 開示 本数 目的 手段 該順梁 特徴 効果 口 品質 雨天 工程 タイル 剥落 最良 形態 状態 図面 符号\n",
      "\n",
      "===== # 9, Topic : 7, p : 8.5758 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 床スラブ 通気性床部材 上流側 通気性カーペット 空調空気 下流側 実施例 空間 図 空調対象室 空調システム 空調機 段部 端側 床 空調機械室 均一化 発明 空調 ｕ 模式的縦断面図 中流側 風速 口 ｄ 特許文献 風速分布 上記床スラブ 模式的平面図 上方 給気チャンバー 静圧分布 圧力損失 搬送動力 構成 符号 側 風量調整シート 下方 上記空間 天井側スラブ 空調対象 流れ 風量 風切り音 空調用空気 Ａ－Ａ線断面模式図 均一 － 構成要素 空調対象室内側 円滑化 課題 傾斜面 鎖線 ハッチング 範囲 通気孔 天井側 左下がり カーペット 利用可能性 横幅 本願発明 床下区画 Ｂ－Ｂ線断面模式図 上記天井側 説明 調整 騒音 ｍ ｂ 右端側 Ｂ－Ｂ線断面図 利用可能性大 右下がり 上側 段 敷設枚数 特開平 号公報 吹出口近辺 符号ｕ 上記特許文献 カーペット上方 一つ 面積 天井板 適所 夫 境界 外側 方向 ダクト 同一 複数 ｃ 廊下 産業 低減 技術分野 背景技術 開口率 制限等 適数 高低差 各種 提案 ～ 部分 下部 開示 記載 一定 上面 距離 程度 傾向 間口 路 目的 手段 効果 最良 形態 中間 左側 右側 乱れ 図面\n",
      "\n",
      "===== # 10, Topic : 14, p : 9.7561 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 図 地形図 処理 説明図 地形図作成方法 画像 数値地形モデル デジタルオルソフォト 地形図作成システム 航空写真 地形図データ 数値地形モデル作成部Ｂ 部 地形 実施形態 演算処理部 ～図 写真 位置精度 表示部 数値地形データ 図化機 オルソ画像 デジタルオルソフォト作成処理 前記デジタルオルソフォト 航空写真撮影処理 公共測量 デジタルオルソフォト作成部Ａ 発明 精度 数値写真 空中三角測量処理 修正処理 データ 道路台帳図 地形データ 地形モデル 前記数値地形モデル 修正 撮影 例 スキャニング処理 モニタ 検査 前記 地形図台帳 数値地形モデル作成部 数値地形モデル作成処理 ずれ 前記演算処理部 地形図等 前記図化機 ブロック図 数値 作成精度 公共測量作業規定 接合部 記憶部 数値図化 説明 ＴＩＮモデル 前記公共測量作業規定 前記航空写真 水平位置精度 前記表示部 ゆがみ等 前記画像 画像データ 数値地形モデルデータ 対象 システム 撮影縮尺 メッシュ間隔 要求精度 写真図縮尺 ｂ 表 入力部 出力部 投影画像 構成 図右側 左側図 右側図 位置精度検証 部分 単点 ・・デジタルオルソフォト作成部 航空写真撮影 ・・・デジタルオルソフォト作成処理 ライン ａ 当該対象個所 位置 モザイク画像 チェック用 撮影位置 縦方向 既存 成果品 形態 拡大図 航空写真画像データ 対象個所 対象位置 システム構成 地形形状 ・・・ブレイクライン追加図化処理 前記空中三角測量処理 位置ずれ メッシュ ブレイクライン図化データ 特徴 色調 写真撮影個所 ＭＤ作成 成果品作成 作業 破線部 デジタル処理 メッシュデータ 空中三角測量 処理例 図面 コスト 段階 複数 画像修正 抽出処理 航空撮影対象個所 デジタルステレオ図化機 演算処理装置 デジタルオルソ ～ 右側 バラつき 撮影対象個所 構成部分 道路台帳図道路線データ 修正例 差 モニタ画面 濃度 最終調整処理 処理プログラム 収集処理 作業方法 航空写真用スキャナ 概略構成 色調補正処理 ズレ 精度管理 作成範囲全域 デジタル 破線 実施 特許文献 色調補正 モザイクソフトウェア 検査精度 左側破線 画像濃度 画像メモリ 技術 左側 前記チェック用 変換ソフトウェア 前記モニタ 平面位置 標高点精度 地上分解能 道路 オルソフォト画像生成方法 オルソ ＴＩＮデータ 精度管理表 公共機関 前記特許文献 検出 品質 個所 ～最終調整 前記国土地理院 課題 収集 投影 最終調整 画面 等 Ｂ 検査用 ゆがみ 間隔 各種データ 成果品完成 国土地理院 モニタ画面全面 ソフトウェア Ａ 計算機 手法 光軸方向 鉛直方向 投影図上 基準 等間隔 コンター 中心投影 範囲 コンターデータ 家屋 橋梁 歪み ソフト 横方向 グリッド間隔 マップデジタル 地図情報レベル 具体例 平地部分 山際部分 当該地域 変換 検証 技術分野 背景技術 標準値 自動抽出 検査漏れ 最小値 最大値 参照地図 外部評定要素 比高差 ラスター重ね状態 利用可能性 撮像装置 オクルージョン領域 領域 使用機器 特開 － 号公報 標準 上記課題 明細書 オルソフォト 対空標識 計算 ＭＤ 標高 平地 山間地 格子状 地域 状態 既存図面 階調 対応関係 －タ カメラ 物体 種々 用途 国 自治体 原則 開示 相当 項目 記載 提供 目的 手段 バラツキ 効果 所定 最良 先 座標 場所 マーク ロールフィルム 航空機 姿勢 等高線 アナログ Triangulated Irregular Network 所要 一連 任意 デジタルオルソデータファイル ｃ 影響 白丸 逆 有無 緑色 カラー 利点 汎用 専用 オペレータ 目視 ハードウェア 起伏 屋根 橋 帯 天候 雨天 晴天 相違 輝度 閾値 丸内 所 ｍ 要件 ＣＰＵ キーボード マウス 図形 ＲＯＭ ＲＡＭ 通り 産業 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 5.4474e-37, 2.0416e-23, 2.7469e-24, 8.6961e-31, 6.1951e-42,\n",
      "         0.0000e+00, 9.5982e-27, 1.2757e-14, 3.6960e-28, 6.0355e-37, 1.3096e-33,\n",
      "         1.2919e-19, 3.7516e-33, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 921.7447\n",
      "Test epoch : 1 Average loss: 999.8067\n",
      "PP(train) = 2680.779, PP(valid) = 2564.299\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1735e-27, 1.1627e-24, 1.5367e-14, 9.1538e-10, 8.6324e-09, 3.5258e-39,\n",
      "         4.6200e-34, 1.9053e-30, 4.4622e-07, 4.9199e-21, 3.7635e-26, 4.5112e-22,\n",
      "         1.1037e-03, 1.6403e-19, 9.9890e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0594,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 921.5637\n",
      "Test epoch : 2 Average loss: 999.6828\n",
      "PP(train) = 2676.392, PP(valid) = 2562.483\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0676e-31, 2.3777e-22, 6.1781e-07, 8.2294e-18, 1.9399e-15, 3.9191e-31,\n",
      "         2.7923e-31, 7.7361e-12, 7.0908e-01, 6.5046e-23, 3.2197e-21, 1.0787e-27,\n",
      "         6.8532e-13, 1.5979e-20, 2.9092e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0637, 0.0607, 0.0578, 0.0599, 0.0377, 0.0745, 0.0748, 0.0937, 0.0587,\n",
      "         0.0650, 0.0786, 0.0633, 0.0519, 0.1117, 0.0480]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.997, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 921.3455\n",
      "Test epoch : 3 Average loss: 999.5544\n",
      "PP(train) = 2670.413, PP(valid) = 2559.805\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9618e-39, 1.6183e-33, 8.8999e-14, 1.7177e-27, 7.7365e-22, 1.0676e-28,\n",
      "         4.3565e-31, 4.4995e-15, 2.1447e-18, 8.1803e-26, 1.4013e-45, 5.3186e-23,\n",
      "         2.4339e-24, 4.3974e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 921.0401\n",
      "Test epoch : 4 Average loss: 999.4252\n",
      "PP(train) = 2663.757, PP(valid) = 2556.825\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1017e-36, 1.4548e-32, 6.7283e-13, 8.2974e-15, 2.8530e-10, 5.7299e-30,\n",
      "         5.3429e-30, 6.4531e-10, 8.5896e-23, 1.3232e-25, 5.1558e-34, 3.0766e-26,\n",
      "         6.5888e-09, 6.2585e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 920.8081\n",
      "Test epoch : 5 Average loss: 999.2956\n",
      "PP(train) = 2657.255, PP(valid) = 2554.066\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.2304e-30, 9.1676e-32, 4.6270e-21, 7.1702e-19, 6.7388e-01, 1.2115e-39,\n",
      "         4.6762e-23, 9.0545e-18, 1.0941e-04, 1.6082e-18, 6.4671e-28, 5.4136e-23,\n",
      "         2.6805e-15, 1.2261e-18, 3.2601e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0554, 0.0475, 0.0576, 0.0424, 0.0838, 0.0672, 0.1087, 0.1046, 0.0501,\n",
      "         0.0789, 0.0665, 0.0596, 0.0481, 0.0659, 0.0638]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 920.4898\n",
      "Test epoch : 6 Average loss: 999.1630\n",
      "PP(train) = 2651.057, PP(valid) = 2551.642\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2609e-27, 2.5056e-31, 5.7891e-20, 8.4311e-22, 4.7870e-29, 1.5250e-24,\n",
      "         1.0632e-40, 3.3183e-22, 6.4752e-17, 2.0869e-27, 4.7647e-30, 7.3770e-24,\n",
      "         3.5404e-19, 3.3859e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 920.3209\n",
      "Test epoch : 7 Average loss: 999.0315\n",
      "PP(train) = 2644.806, PP(valid) = 2549.187\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1084e-44, 0.0000e+00, 5.9672e-20, 5.6175e-21, 1.6992e-15, 1.7705e-30,\n",
      "         2.9680e-42, 2.1152e-22, 1.1042e-03, 5.6381e-16, 2.7097e-28, 1.5387e-28,\n",
      "         5.6035e-19, 2.5586e-22, 9.9890e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 919.8758\n",
      "Test epoch : 8 Average loss: 998.9014\n",
      "PP(train) = 2638.396, PP(valid) = 2546.570\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7327e-37, 8.9627e-24, 1.1488e-12, 8.4249e-25, 2.2705e-09, 3.4245e-19,\n",
      "         1.8672e-26, 2.0135e-10, 1.0000e+00, 2.0832e-29, 1.0746e-28, 3.7396e-27,\n",
      "         6.5195e-14, 3.0199e-19, 1.8973e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 919.6526\n",
      "Test epoch : 9 Average loss: 998.7709\n",
      "PP(train) = 2631.895, PP(valid) = 2543.792\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.9492e-44, 1.8676e-35, 1.0331e-19, 7.2939e-30, 2.7093e-18, 1.5817e-38,\n",
      "         1.1403e-33, 1.0142e-13, 1.6188e-10, 4.4035e-26, 1.7759e-35, 2.3022e-26,\n",
      "         8.3193e-17, 2.2399e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 919.3549\n",
      "Test epoch : 10 Average loss: 998.6435\n",
      "PP(train) = 2625.593, PP(valid) = 2541.228\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5951e-22, 1.0639e-20, 1.1006e-05, 5.0953e-15, 3.1969e-02, 1.8244e-14,\n",
      "         2.4950e-30, 7.6923e-09, 4.6750e-02, 2.5128e-09, 2.7259e-29, 1.1574e-15,\n",
      "         9.1956e-08, 1.1973e-10, 9.2127e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0793, 0.0456, 0.0980, 0.0506, 0.0595, 0.0677, 0.1055, 0.0934, 0.0591,\n",
      "         0.0543, 0.0634, 0.0565, 0.0512, 0.0731, 0.0429]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 919.1260\n",
      "Test epoch : 11 Average loss: 998.5184\n",
      "PP(train) = 2619.514, PP(valid) = 2538.819\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5414e-44, 3.6981e-38, 1.9407e-12, 2.5726e-29, 3.6220e-16, 8.0407e-19,\n",
      "         4.9029e-34, 2.6294e-19, 1.9553e-21, 4.2690e-21, 4.0597e-30, 3.1960e-30,\n",
      "         8.7671e-07, 4.1229e-27, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 918.7921\n",
      "Test epoch : 12 Average loss: 998.3913\n",
      "PP(train) = 2613.446, PP(valid) = 2536.355\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2348e-33, 8.7745e-39, 7.6091e-12, 1.3255e-03, 9.8774e-01, 1.3573e-36,\n",
      "         1.7139e-30, 1.0934e-02, 6.0322e-14, 1.2383e-15, 3.4037e-27, 8.0809e-19,\n",
      "         5.9106e-12, 4.3056e-33, 8.4497e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0484, 0.0424, 0.0383, 0.0958, 0.0660, 0.1063, 0.1074, 0.0452,\n",
      "         0.0932, 0.0677, 0.0600, 0.0455, 0.0624, 0.0762]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 918.5235\n",
      "Test epoch : 13 Average loss: 998.2652\n",
      "PP(train) = 2607.376, PP(valid) = 2533.842\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2948e-42, 5.1930e-31, 2.5491e-26, 1.1151e-15, 1.7606e-12, 9.8679e-25,\n",
      "         2.2079e-29, 2.6498e-22, 2.3362e-15, 5.1220e-35, 3.2331e-36, 5.2305e-34,\n",
      "         4.5421e-23, 2.0238e-13, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 918.2437\n",
      "Test epoch : 14 Average loss: 998.1430\n",
      "PP(train) = 2601.347, PP(valid) = 2531.377\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.9753e-39, 4.9331e-27, 1.7807e-15, 1.5584e-10, 4.0811e-09, 1.6168e-37,\n",
      "         5.4364e-31, 2.6755e-16, 2.4092e-14, 1.2585e-09, 1.0457e-30, 4.9830e-26,\n",
      "         1.4924e-10, 2.0895e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 917.9396\n",
      "Test epoch : 15 Average loss: 998.0189\n",
      "PP(train) = 2595.395, PP(valid) = 2528.921\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 1.6494e-35, 4.3269e-10, 1.2791e-19, 5.2162e-14, 5.8709e-34,\n",
      "         4.7720e-27, 2.2832e-11, 5.4819e-14, 1.7954e-12, 3.8425e-32, 2.0454e-25,\n",
      "         1.8528e-12, 1.8853e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 917.6872\n",
      "Test epoch : 16 Average loss: 997.8963\n",
      "PP(train) = 2589.615, PP(valid) = 2526.594\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.9880e-38, 4.2413e-33, 1.8858e-17, 2.8375e-24, 1.0000e+00, 1.7253e-19,\n",
      "         3.7876e-36, 5.0705e-13, 1.5217e-12, 3.2970e-26, 1.4947e-32, 5.1296e-23,\n",
      "         1.6842e-11, 3.1788e-36, 1.0753e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 917.4606\n",
      "Test epoch : 17 Average loss: 997.7763\n",
      "PP(train) = 2583.789, PP(valid) = 2524.209\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2927e-27, 2.9611e-18, 6.0343e-16, 9.9042e-13, 6.3925e-01, 5.0486e-35,\n",
      "         3.4655e-32, 4.4734e-12, 1.3972e-07, 8.9266e-25, 4.0234e-28, 2.9841e-16,\n",
      "         2.0031e-01, 3.4902e-08, 1.6044e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0511, 0.0528, 0.0561, 0.0447, 0.0769, 0.0671, 0.0952, 0.1013, 0.0532,\n",
      "         0.0874, 0.0634, 0.0591, 0.0499, 0.0653, 0.0764]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 917.2100\n",
      "Test epoch : 18 Average loss: 997.6568\n",
      "PP(train) = 2577.951, PP(valid) = 2521.743\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 1.7687e-37, 5.8172e-28, 1.1682e-32, 5.3414e-16, 1.8301e-36,\n",
      "         8.6217e-25, 1.8290e-21, 1.5898e-19, 9.9260e-32, 1.7876e-32, 2.9154e-26,\n",
      "         2.0484e-21, 2.5240e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 916.9728\n",
      "Test epoch : 19 Average loss: 997.5358\n",
      "PP(train) = 2572.229, PP(valid) = 2519.322\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7526e-35, 3.2022e-28, 2.6345e-10, 7.6984e-14, 1.1234e-11, 2.0696e-19,\n",
      "         3.0078e-20, 8.8263e-08, 1.6916e-01, 3.4724e-13, 1.1968e-22, 6.6337e-18,\n",
      "         4.3229e-14, 2.2559e-20, 8.3084e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0777, 0.0482, 0.0914, 0.0528, 0.0542, 0.0692, 0.0994, 0.0934, 0.0597,\n",
      "         0.0555, 0.0661, 0.0578, 0.0517, 0.0796, 0.0433]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 916.7271\n",
      "Test epoch : 20 Average loss: 997.4176\n",
      "PP(train) = 2566.685, PP(valid) = 2517.055\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.4661e-32, 8.4143e-25, 1.5149e-16, 3.0121e-18, 1.2947e-11, 5.3611e-16,\n",
      "         8.8776e-25, 9.5345e-11, 9.9705e-01, 3.2297e-22, 3.1778e-18, 1.1823e-24,\n",
      "         2.6237e-03, 9.5537e-10, 3.2429e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0444, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0845, 0.0651, 0.0511, 0.1311, 0.0499]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 916.5314\n",
      "Test epoch : 21 Average loss: 997.3025\n",
      "PP(train) = 2561.077, PP(valid) = 2514.752\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2815e-36, 5.6256e-28, 1.0766e-04, 4.0696e-13, 7.1717e-07, 3.0316e-21,\n",
      "         1.2335e-38, 7.4887e-10, 5.3836e-07, 7.3479e-19, 1.0275e-20, 1.0252e-21,\n",
      "         2.1274e-12, 4.7737e-23, 9.9989e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 916.1503\n",
      "Test epoch : 22 Average loss: 997.1856\n",
      "PP(train) = 2555.531, PP(valid) = 2512.442\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9741e-39, 2.2872e-24, 5.2429e-24, 4.6815e-19, 7.7285e-13, 3.9674e-32,\n",
      "         7.7859e-40, 4.1446e-13, 2.2217e-13, 1.5227e-20, 2.0079e-26, 7.4988e-18,\n",
      "         3.9387e-12, 2.7740e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 915.9659\n",
      "Test epoch : 23 Average loss: 997.0702\n",
      "PP(train) = 2550.077, PP(valid) = 2510.199\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5461e-38, 1.9332e-38, 1.3105e-05, 6.9301e-13, 5.1991e-09, 1.2421e-19,\n",
      "         4.2722e-30, 1.3197e-17, 5.9220e-06, 7.2787e-21, 1.3125e-28, 1.5822e-17,\n",
      "         1.6015e-18, 1.7726e-30, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 915.6688\n",
      "Test epoch : 24 Average loss: 996.9558\n",
      "PP(train) = 2544.681, PP(valid) = 2507.994\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1160e-43, 1.6960e-32, 2.0625e-07, 5.7054e-26, 2.3261e-14, 2.1313e-23,\n",
      "         5.9179e-24, 1.6923e-22, 1.0264e-15, 2.0135e-27, 3.9247e-37, 6.8308e-31,\n",
      "         8.3388e-17, 1.6791e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 915.4494\n",
      "Test epoch : 25 Average loss: 996.8438\n",
      "PP(train) = 2539.241, PP(valid) = 2505.716\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3770e-41, 1.2202e-26, 1.6148e-12, 5.3866e-20, 2.1766e-07, 4.5174e-29,\n",
      "         4.2039e-45, 2.0922e-11, 8.8486e-15, 1.6131e-25, 7.1361e-28, 1.3496e-28,\n",
      "         2.0708e-18, 3.0443e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 915.1532\n",
      "Test epoch : 26 Average loss: 996.7330\n",
      "PP(train) = 2533.997, PP(valid) = 2503.594\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4013e-45, 3.5473e-23, 2.6932e-26, 2.3423e-29, 2.9698e-20, 5.5489e-30,\n",
      "         1.3203e-38, 1.8151e-29, 4.2522e-30, 5.3420e-26, 9.2065e-43, 4.2534e-34,\n",
      "         3.2650e-30, 1.4911e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 915.0032\n",
      "Test epoch : 27 Average loss: 996.6221\n",
      "PP(train) = 2528.768, PP(valid) = 2501.447\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3159e-40, 1.2883e-25, 2.5608e-10, 5.7518e-18, 3.7366e-07, 1.1193e-18,\n",
      "         1.2602e-31, 8.1264e-07, 7.2979e-01, 8.0509e-17, 5.6079e-21, 6.6858e-13,\n",
      "         1.1095e-10, 2.6859e-01, 1.6154e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0521, 0.0571, 0.0517, 0.0714, 0.0396, 0.0808, 0.0595, 0.0839, 0.0731,\n",
      "         0.0655, 0.0806, 0.0615, 0.0546, 0.1215, 0.0470]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 914.7371\n",
      "Test epoch : 28 Average loss: 996.5122\n",
      "PP(train) = 2523.513, PP(valid) = 2499.259\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3987e-32, 9.3257e-29, 1.0038e-09, 2.1749e-16, 3.1705e-12, 3.2589e-21,\n",
      "         3.6760e-23, 2.9306e-15, 7.2136e-10, 6.7607e-24, 9.1101e-25, 5.8571e-15,\n",
      "         4.0265e-18, 8.2671e-17, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 914.5840\n",
      "Test epoch : 29 Average loss: 996.4035\n",
      "PP(train) = 2518.306, PP(valid) = 2497.079\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.4339e-23, 1.9374e-22, 1.5090e-02, 3.9567e-18, 2.8483e-16, 1.1533e-25,\n",
      "         2.8987e-26, 8.3201e-03, 9.7630e-01, 2.0823e-11, 1.5494e-20, 9.4381e-10,\n",
      "         4.5841e-09, 2.2828e-30, 2.8598e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0567, 0.0680, 0.0444, 0.0629, 0.0312, 0.0757, 0.0633, 0.0912, 0.0575,\n",
      "         0.0689, 0.0846, 0.0647, 0.0510, 0.1304, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 914.2914\n",
      "Test epoch : 30 Average loss: 996.2967\n",
      "PP(train) = 2513.160, PP(valid) = 2494.970\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.9126e-34, 1.5027e-27, 4.6764e-09, 2.1030e-15, 4.5346e-13, 5.7448e-31,\n",
      "         1.8032e-22, 1.1692e-11, 3.5634e-07, 1.9181e-34, 2.3490e-36, 6.0423e-27,\n",
      "         1.9450e-07, 2.0138e-17, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 913.9911\n",
      "Test epoch : 31 Average loss: 996.1895\n",
      "PP(train) = 2508.135, PP(valid) = 2492.952\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 6.6553e-29, 1.9656e-35, 3.3789e-33, 1.0000e+00, 3.7028e-32,\n",
      "         1.1325e-31, 3.1365e-26, 3.5388e-22, 2.4617e-26, 1.6831e-35, 7.8173e-32,\n",
      "         4.0008e-38, 6.4460e-39, 2.2392e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 913.7871\n",
      "Test epoch : 32 Average loss: 996.0804\n",
      "PP(train) = 2503.086, PP(valid) = 2490.829\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8211e-30, 7.7503e-28, 5.0731e-12, 2.5868e-20, 2.4090e-06, 9.4285e-30,\n",
      "         2.3058e-25, 1.1979e-06, 4.2062e-02, 2.0068e-19, 1.9317e-17, 3.4013e-14,\n",
      "         5.2722e-12, 2.5265e-13, 9.5793e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0808, 0.0453, 0.1010, 0.0509, 0.0586, 0.0675, 0.1055, 0.0926, 0.0594,\n",
      "         0.0531, 0.0630, 0.0561, 0.0512, 0.0730, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 913.5213\n",
      "Test epoch : 33 Average loss: 995.9743\n",
      "PP(train) = 2497.998, PP(valid) = 2488.662\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5489e-33, 4.2593e-16, 1.6745e-12, 3.9462e-24, 1.6713e-11, 4.9267e-15,\n",
      "         4.0240e-30, 6.6771e-09, 7.6070e-05, 4.3405e-17, 1.4003e-21, 3.8197e-24,\n",
      "         1.3465e-06, 5.3251e-24, 9.9992e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 913.3603\n",
      "Test epoch : 34 Average loss: 995.8697\n",
      "PP(train) = 2493.098, PP(valid) = 2486.668\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8664e-44, 6.4787e-21, 1.1772e-15, 5.0297e-17, 3.4719e-04, 2.5037e-36,\n",
      "         6.9295e-32, 2.1493e-16, 6.0749e-14, 2.2255e-14, 4.1400e-29, 1.5629e-20,\n",
      "         1.2789e-05, 1.2841e-18, 9.9964e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 913.0075\n",
      "Test epoch : 35 Average loss: 995.7646\n",
      "PP(train) = 2488.257, PP(valid) = 2484.726\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.6004e-35, 2.9425e-20, 6.6238e-09, 6.3381e-13, 3.6479e-02, 1.5029e-32,\n",
      "         1.0464e-28, 4.2328e-15, 1.6616e-02, 4.6881e-15, 1.7525e-27, 1.9403e-27,\n",
      "         1.6583e-06, 4.6472e-23, 9.4690e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0798, 0.0450, 0.0999, 0.0501, 0.0607, 0.0673, 0.1070, 0.0933, 0.0589,\n",
      "         0.0539, 0.0627, 0.0561, 0.0510, 0.0715, 0.0427]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 912.7734\n",
      "Test epoch : 36 Average loss: 995.6610\n",
      "PP(train) = 2483.417, PP(valid) = 2482.760\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.4031e-26, 1.1795e-21, 2.4728e-06, 5.3350e-17, 9.9288e-01, 1.7036e-19,\n",
      "         1.6459e-29, 4.7058e-13, 1.4382e-08, 6.4747e-15, 1.4653e-23, 2.3962e-16,\n",
      "         9.7647e-09, 3.4844e-27, 7.1147e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0449, 0.0479, 0.0424, 0.0382, 0.0957, 0.0657, 0.1067, 0.1083, 0.0451,\n",
      "         0.0936, 0.0672, 0.0601, 0.0456, 0.0622, 0.0764]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 912.6792\n",
      "Test epoch : 37 Average loss: 995.5597\n",
      "PP(train) = 2478.523, PP(valid) = 2480.738\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8281e-30, 8.7803e-41, 8.8092e-19, 5.9845e-22, 7.4250e-14, 1.6018e-27,\n",
      "         2.8138e-28, 5.3076e-10, 1.4468e-02, 3.1989e-21, 7.9966e-27, 1.8808e-29,\n",
      "         4.6061e-26, 1.5040e-31, 9.8553e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.0447, 0.1032, 0.0504, 0.0596, 0.0671, 0.1068, 0.0924, 0.0594,\n",
      "         0.0526, 0.0623, 0.0558, 0.0511, 0.0716, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 912.3644\n",
      "Test epoch : 38 Average loss: 995.4593\n",
      "PP(train) = 2473.656, PP(valid) = 2478.660\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7312e-39, 9.8837e-34, 7.1826e-16, 8.8764e-27, 1.0747e-13, 1.1713e-35,\n",
      "         1.1935e-22, 3.2962e-25, 9.5350e-27, 4.5083e-27, 2.4959e-31, 3.0321e-35,\n",
      "         1.3865e-19, 6.7338e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 912.1472\n",
      "Test epoch : 39 Average loss: 995.3606\n",
      "PP(train) = 2468.948, PP(valid) = 2476.757\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7275e-38, 3.2485e-25, 6.9353e-22, 1.7435e-23, 5.6589e-21, 7.9006e-32,\n",
      "         3.8615e-29, 7.6883e-14, 2.3417e-08, 6.3373e-25, 3.9876e-23, 5.4319e-15,\n",
      "         1.3604e-08, 9.4704e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 911.9523\n",
      "Test epoch : 40 Average loss: 995.2588\n",
      "PP(train) = 2464.277, PP(valid) = 2474.853\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.3207e-23, 2.3875e-20, 5.7649e-08, 3.2276e-08, 2.5276e-04, 8.3863e-17,\n",
      "         1.4059e-18, 5.6264e-07, 2.1806e-07, 3.1076e-12, 2.1250e-18, 2.4201e-16,\n",
      "         9.9417e-01, 8.2020e-16, 5.5811e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0484, 0.0742, 0.0763, 0.0605, 0.0414, 0.0646, 0.0541, 0.0790, 0.0745,\n",
      "         0.0935, 0.0483, 0.0529, 0.0585, 0.0644, 0.1094]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 911.7971\n",
      "Test epoch : 41 Average loss: 995.1608\n",
      "PP(train) = 2459.568, PP(valid) = 2472.912\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3737e-22, 8.0789e-21, 3.7852e-12, 1.7348e-20, 8.1052e-07, 1.5693e-18,\n",
      "         6.3482e-31, 1.7773e-05, 2.2638e-05, 1.1734e-20, 6.5823e-25, 3.5497e-13,\n",
      "         3.5650e-10, 1.3760e-24, 9.9996e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 911.5804\n",
      "Test epoch : 42 Average loss: 995.0639\n",
      "PP(train) = 2454.908, PP(valid) = 2470.980\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6707e-42, 1.8357e-38, 2.6819e-14, 6.8738e-22, 1.8772e-20, 7.2552e-30,\n",
      "         8.8282e-44, 7.0302e-13, 2.4958e-22, 2.8411e-24, 9.9424e-35, 1.7820e-31,\n",
      "         8.4245e-15, 5.1690e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 911.2285\n",
      "Test epoch : 43 Average loss: 994.9643\n",
      "PP(train) = 2450.341, PP(valid) = 2469.128\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0577e-40, 4.6413e-28, 2.2775e-14, 8.8821e-27, 7.9277e-20, 1.8343e-39,\n",
      "         5.1107e-26, 7.0558e-13, 5.6316e-11, 1.2381e-24, 4.2136e-29, 3.9789e-34,\n",
      "         4.4847e-23, 8.9587e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 911.0081\n",
      "Test epoch : 44 Average loss: 994.8674\n",
      "PP(train) = 2445.861, PP(valid) = 2467.387\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5164e-35, 3.6429e-28, 9.0598e-24, 1.2149e-21, 5.5321e-15, 3.5251e-36,\n",
      "         1.2871e-32, 1.2499e-16, 5.8972e-10, 1.1371e-23, 4.0672e-36, 6.4011e-26,\n",
      "         2.3595e-06, 1.5954e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 910.8092\n",
      "Test epoch : 45 Average loss: 994.7724\n",
      "PP(train) = 2441.375, PP(valid) = 2465.569\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6279e-35, 3.0285e-23, 9.7883e-09, 2.5148e-11, 5.0901e-04, 1.0875e-24,\n",
      "         2.2975e-30, 2.5356e-03, 9.7104e-04, 3.4524e-24, 4.7114e-16, 2.5174e-23,\n",
      "         6.7836e-04, 2.1196e-09, 9.9531e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0445, 0.1041, 0.0502, 0.0601, 0.0670, 0.1074, 0.0922, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0710, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 910.5598\n",
      "Test epoch : 46 Average loss: 994.6796\n",
      "PP(train) = 2436.788, PP(valid) = 2463.679\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6052e-45, 4.3505e-38, 3.3141e-17, 1.9565e-18, 1.9760e-19, 1.3347e-35,\n",
      "         4.1653e-39, 4.7885e-27, 1.0245e-15, 9.4085e-37, 1.1930e-38, 3.4014e-37,\n",
      "         6.2588e-24, 1.9079e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 910.3384\n",
      "Test epoch : 47 Average loss: 994.5839\n",
      "PP(train) = 2432.311, PP(valid) = 2461.846\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.1054e-36, 3.1875e-24, 5.8775e-24, 2.4877e-19, 2.8843e-35,\n",
      "         2.8026e-44, 1.6724e-21, 3.2392e-13, 3.4910e-28, 7.8639e-26, 7.6273e-26,\n",
      "         3.8384e-21, 1.3226e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 910.1002\n",
      "Test epoch : 48 Average loss: 994.4887\n",
      "PP(train) = 2427.924, PP(valid) = 2460.041\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.0326e-40, 4.5275e-34, 1.1760e-11, 1.2518e-14, 3.7454e-18, 3.7940e-25,\n",
      "         1.2559e-27, 1.2584e-13, 1.2097e-02, 7.3320e-21, 8.2988e-30, 8.5097e-23,\n",
      "         1.0499e-15, 4.2832e-20, 9.8790e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0446, 0.1034, 0.0504, 0.0597, 0.0671, 0.1070, 0.0924, 0.0594,\n",
      "         0.0525, 0.0623, 0.0557, 0.0511, 0.0715, 0.0416]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 909.9516\n",
      "Test epoch : 49 Average loss: 994.3960\n",
      "PP(train) = 2423.609, PP(valid) = 2458.297\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.0110e-32, 1.8172e-28, 7.3960e-10, 1.6558e-05, 4.6540e-14, 8.9020e-26,\n",
      "         1.6881e-24, 1.3449e-15, 7.6316e-01, 5.7563e-15, 2.4960e-16, 1.5138e-20,\n",
      "         6.2718e-08, 9.5922e-19, 2.3682e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0623, 0.0620, 0.0551, 0.0605, 0.0363, 0.0749, 0.0725, 0.0934, 0.0584,\n",
      "         0.0658, 0.0798, 0.0637, 0.0518, 0.1152, 0.0484]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 909.7773\n",
      "Test epoch : 50 Average loss: 994.3073\n",
      "PP(train) = 2419.198, PP(valid) = 2456.520\n",
      "Writing to ./topicwords/13-topwords_e50.txt\n",
      "Topic 0: 緊張 定着構造 群 水溶性 撮像画像 撹拌 角度θ 活性 人間 制御信号\n",
      "Topic 1: 緊張 つぎ 人間 群 要領 定着構造 位置ずれ コンクリート打設後 制御信号 セパレータ\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 技術的範囲 変化 フック 当該 ばね 質量 板状 突起 各々 前記実施形態\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: 緊張 セグメントリング 定着構造 人間 位置ずれ 群 水溶性 撮像画像 制御信号 要領\n",
      "Topic 6: 緊張 セグメントリング 人間 つぎ 定着構造 群 要領 制御信号 撮像画像 マンホール\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 図\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: 緊張 群 人間 つぎ 水溶性 角度θ 継ぎ目 鋼 コンクリート打設後 伸び\n",
      "Topic 10: 緊張 減衰力 水溶性 人間 制御信号 定着構造 撮像画像 鋼 プレストレストコンクリート 活性\n",
      "Topic 11: 緊張 人間 水溶性 定着構造 減衰力 撮像画像 係止 群 制御信号 要領\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: 緊張 つぎ 人間 制御信号 継ぎ目 位置ずれ 鋼 係止 角度θ 水溶性\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 8.2751 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 切羽作業領域 作業 切羽面 量 気風管 切羽作業 分岐吹出口 換気装置 吹出口 風量調整ダンパ 空気 トンネル 分岐路 側 分岐管 吹出空気 風量Ｑ 粉塵 分岐吹出空気 Ａ 排気風管 発生量 風量Ｑ２ 換気 粉塵濃度 坑道作業領域 送風機 Ｂ 切羽作業種別センサ 実施形態 風量 換気量 図 側断面図 風量調整機構 局所送風機 制御装置 Ｂ側 Ａ側 発破作業 位置 距離 前記切羽面 換気作業 開度 排気式 ダンパ本体 ずり出し作業 気式 粉塵発生量 流路 粉塵量 ビニール管 風量調整 掘削作業 作業領域 切羽作業種別 作業員 前記分岐吹出口 坑口 方向 距離センサ 調整 前記トンネル 切羽面近傍位置 発明 領域 前記吹出口 坑道 風量調整ダンパ等 排気管 開口面積 集塵機 切羽作業種別情報 前記風量調整機構 通風路 効率 ダンパ 切羽面近傍 作業者 作業効果 上方位置 断面方向 清浄 内部 吹付 排気空気 種別 各種風管 発生 蛇腹式 外部 入口 近傍 粉塵発生 調整割合 排出空気 開口部 坑口側 一端側 他端側 反対側 発破 課題 上記 種類 実施 形態 制御装置等 前記 説明 他 ダンパモータ 気流 多量 変形例 音波式距離センサ レーザ式距離センサ パイプ管 開 コンクリート吹付 Ｑ II線断面図 電気式 やや側方 空気流路切換手段 換気効率 表 低減 １つ 距離情報 割合 下方位置 制御部 トンネル工事 山岳トンネル 位置関係 斜め上方 構成図 ダンパ本体 ａ ｂ 短尺 ｍｉｎ 流れ 態様 図面 符号 ～ 範囲 上部 先端 三つ又 部 比率 公知 掘削 掘削工事 斜め下方 斜め 測定例 長手方向 排出口 種類等 保形部材 技術分野 技術 上記課題 使用状況 構成 バグフィルタ 樹脂製 効果 II 恐れ 一般 後方 傾向 逆 手段 特徴 増加 整然 外 要素 Ｌ 中央 全域 一定 ツール 形式 手動 作用 外気 和 原因 同等 多め 寄り 土砂 搬送 機能 乾式 湿式 程度 図示 ２つ ３つ 複数 鋼管\n",
      "\n",
      "===== # 2, Topic : 8, p : 8.0191 %\n",
      "Topic words : 効果, 砂, 側方, Ａ, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 空気 環境制御ユニット クリーンルーム 環境制御システム クリーン度 製造装置 ケミカル除去フィルタ 温度制御モジュール 空気浄化手段 粒子除去フィルタ チャンバ 図 請求項 水分除去モジュール フィルタ 浄化ユニット 環境制御 温度 構成 汚染物質 処理装置 処理対象物 温度制御手段 インターフェース 環境 閉空間 ウエハ 発明 温度制御 半導体製造プロセス 手段 温度制御装置 前記空気浄化手段 構成図 ガス状汚染化学物質 汚染化学物質 開口部 ロードポート 実施形態 乾燥空気 記載 空間 前記閉空間 性能低下 水分 装置 ファン 上記 ウエハポッド 外部 フィルタ等 水分量 汚染ガス等 製造プロセス 上記図 ガス状物質 内部 通常クリーンルーム 特徴 配管 処理対象設置部 天井近傍 粒子状 当該処理装置 物質 ＨＥＰＡフィルタ等 洗浄装置等 説明 清浄 圧状態 湿度 露光装置 機 成膜装置 温度条件 酸性ガス アルカリガス 有機ガス プロセス 低湿度 冷温水式等 設定温度範囲 実施 形態 性能 ＶＬＳＩ等 ゼオライト等 機等 塵等 生成等 電気式 機械式 水分吸着性 回転ローター式 維持管理 維持管理コスト コスト 課題 目的 該閉空 符号 夫 ｂ 吸着 微粒子 外側 床面近傍 前記インターフェース 半導体チップ 構成部分 酸化膜 設置位置 性能劣化 天井 乾燥 量 ウエハ取り出し口 グレーチング床 コスト低減 室内空間 一定範囲 ウエハ表面 住空間 技術分野 技術 塵 ＶＬＳＩ 高密度化 部分 エア源 種類 複数種類 作業員 チャンバ内部 良品 原因 スピンナー 容積 要求 築造 上述 寿命 圧 同一 周囲 出し入れ ウェア 程度 ａ 側 代わり 破線 ポンプ 公知 シリカゲル 材料 素材 例 状態 精度 静電気 発生 悪影響 効果 図面\n",
      "\n",
      "===== # 3, Topic : 14, p : 9.4462 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 杭 中間層 支持層 上杭 図 トルク値 位置 実施形態 ヒンジ部 境界部 Ｎ値 ヒンジ構造 請求項 回転圧入 上記実施形態 回転圧入杭 上記 地盤 発明 所要トルク値 耐震性能 上端部 先端 測定値 上下 Ｌ モーメント 先端部 施工方法 補強部 鋼管杭 軸方向 水平変位 状態 実施 突出量 地表 ヒンジ ｂ 根入れ長 回転圧入機 上下両端部 補強 予備調査 精度 支持杭 主部 回転圧入量 値 方法 想定距離 地震 対策 陸 深度 厚み ａ 分 誤差分 トルク測定データ 形態 ～図 測定 想定値 説明 全長 通常 上部 構成 特徴 上記構造 構造 耐震性 耐力 施工 施工性 下端部 ＰＨＣ杭 上下両端 突出量分 測定データ ヒンジ接合 所要長 耐震性向上 上記事情 上記工法 課題 施工位置 施工対象 鋼管 所望 素材 効果 軸耐力 距離Ｌ 先端翼 技術分野 技術 対象 完成 完成状態 破損 事例 形成 目的 手段 径寸法 やっとこ 直径 変形 応用 格別 断面 剛性 複数 他 状況 等 図面 挙動 符号\n",
      "\n",
      "===== # 4, Topic : 8, p : 7.7452 %\n",
      "Topic words : 効果, 砂, 側方, Ａ, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 外壁面 外側出隅Ｅ 段差部 建物 外側空洞部 スリットＳ 空洞部 合部 内側出隅Ｅ 建物外方 建物外部 ゴンドラレール 方立て ガラス板 露出面 角部 立て 出隅Ｅ 建物外壁面用建材 内側空洞部 図 縦材 上記外側出隅 溝部 横材 外部圧 上記外側出隅Ｅ 外側出隅 スリット 縦方向 横方向 面 車輪案内通路 Ｒ角 外壁面用建材 ピン角 内側出隅 平面断面図 車軸懸架部 化粧材 内側出隅Ｓ 騒音 建物内側 上記スリットＳ 発明 断面構造 ガラス 当該スリットＳ Ｅ 左右 実施形態 風 当該外壁面 凸 頂壁ｂ 当該空洞部 ゴンドラ 曲面 方向 試験体 段差部Ｅ 形態 建物内部 一対 側壁ａ 当該段差部 建材 上記方立て 内部 縦向き 頂部 方立て取り付け面法線方向 車輪 該露出面 例 スリットＳ近傍 背面 レール溝 課題 該段差部 図示例 実験例 建物内側部分 接続部分 板 露出面 内壁ｃ 実験 断面斜視図 説明 表面 上記 モデル ° 上記図 建物内外 窓清掃用 ターンテーブル 試験体ＰＰ 試験体ＲＲ 試験体ＰＲ 仕上げ材 背面圧 実施 ターンテーブル風上 ターンテーブル開口部下 共鳴器 Ｒ状 枠 側端縁 同士 相互 気密 奥 寸法 他方 風向 測定 影響 θ 表 効果 音圧レベル 取り付け作業 ゲッチンゲン型閉鎖風洞 風洞代表点 図示 発生 発生騒音 外形輪郭形状 下層階 上層階 矩形状 風騒音 音響的共鳴器 発生源 技術分野 技術 融通性 風圧 カーテンウォール 圧力差 圧力 シール部材 風圧等 ゴム製 調圧弁 添付図面 組立体 仮想線 曲率 °ピッチ 形状 図面 カーテンウォール 一般 お互い 相当 境界 変動 容積 悪影響 種 走行 空間 手段 特徴 単体 特長 中央 半径 ～ 丸み 種類 ＲＰ φ パイプ マイクロホン °～ 範囲 風速 ｓ 対策 横向き 作用 圧化 符号\n",
      "\n",
      "===== # 5, Topic : 8, p : 7.9543 %\n",
      "Topic words : 効果, 砂, 側方, Ａ, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 支保工 鋼管矢板 鋼管矢板井筒 図 ブラケット 井筒部 目支保工 水中 井筒 フラットバー 多段支保工 鋼管矢板井筒基礎工法 式ブラケット 多段水中支保工 周側曲面頂部 工程 a 施工法 鋼管矢板曲面頂部 水平部アーム 地盤 隔壁部 前記鋼管矢板井筒 鋼管 腹起材 矢板井筒基礎工法 ｂ 溶接 溶接等 補強アーム ボルト 水中溶接等 尺アーム 地盤Ａ 短尺アーム 内側補強アーム 発明 気 水中溶接 アーム 施工 垂直アーム 前記頂部 設置 水 説明図 水平部 底盤コンクリート 手段 垂直部アーム 所定 上端部 Ｌ 支保工設置 水中支保工 上記鋼管矢板井筒 説明 水中コンクリート 切梁材 工期 ｃ 鋼板矢板井筒基礎下部 フロー図 鋼板矢板 鋼板矢板連結部 作業 上端 工法 等 補強 鋼板矢板井筒部内 施工精度 前記曲面頂部 ユニットＬ 多段 部材 地組 周側連結部 井筒外側 実施例 連結部材 取り付け台 所謂 概略斜視図 井筒内地盤 腹起継ぎ手部 ユニット 構造 地組ヤード 下段部 井筒内外 補強材 ボルト等 多段支保工施工法 土砂 下端 －（ 火打材 所定位置 位置 前記 上段部 端部 締結部 中間部 要部 工程順 下段支保工 上記支保工 形状 水Ｂ 外側補強アーム 周面側 水中掘削 装着 冶具 目 楕円形状 水替え 継手部材 所定間隔 一段支保工 腹起継ぎ手等 段数 部分 置 下方 作業能率 水圧差 角形状 底盤 斜視図 短縮 工期短縮 上端部内周面側 切梁 ブラケット機能 固定ブラケット 地盤Ｂ 図面 他端 概略説明図 足場板Ｃ等 目的 平面 状態 精度 橋梁下部 機能等 設置工程 目的地盤Ａ 施工工数 構成材 式構成 施工期間 橋梁下部工事 足場板 位置精度 尺部材 式ブラッケト 作業工程 Ｃ 水Ｃ 海上工事等 平面図 拡大図 ダイバー等 気中地組ヤード 中央断面図 橋梁下部形成 精度アップ 楕円形 足場板Ｄ 架設例 河川 排水 掘削 ～ 交互 課題 欠点 コストダウン 寸法 クレーン 外側 一端 一緒 機能 キャンバー 隙間 一体 段階 ブラッケット 分割ユニット 水圧 向上 概略 冶具Ａ Ｈ形鋼形状 所定水位 固定ブラッケト a）～（ｃ 水圧対抗手段 半円形状 ・スタンションＬ 下段 差 アップ ブラッケト a）,（ｂ 掘削深度 最小段数 段数低減 三角形状 組み立て架設 固定状態 安定維持構造 筋違的機能 コストアップ ポンツー船上 安全性 ～（ｃ ポンツーン船上 信頼性 技術分野 技術 ポンツーン 矢印イ 内外 保持具 水位 分割 Ｄ 矢印 ダイバー 装着スピード 最終 種 主因 外 調整 置換 b 回転 取り外し 置用 側面 熱 楔状 本 ４つ 障害 レベル ｄ ａ 支点 スタンション ー 指示 バランス 現状 元 鎖線 ブロック 順序 効果 圧力 側壁 特徴 段 符号 ・・フラットバー ・・キャンバー\n",
      "\n",
      "===== # 6, Topic : 14, p : 9.2086 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 型枠 パネルユニット 周辺枠フレーム フレーム メッシュ状フレーム パネル 構造物 屋根 図 屋根フレーム 構築 構築方法 枠面 ノード鋼板 合成構造 コンクリート 下地材 上記型枠 金属薄板 羽根材 請求項 前記周辺枠フレーム コンクリートパネル 側型枠 鉄筋コンクリート造 方法 発明 剛性 フレーム交点部 フレーム断面 ドーム状屋根 プレストレス 発泡樹脂 構造 鉄筋コンクリート造フレーム フレーム構築 前記型枠 上面 端部 上記 交点部 上面型枠 メッシュ状 技術 実施形態 隅部同士 下面型枠 底型枠 上側型枠 屋根構造 ドーム状 面内補剛部材 鋼棒 ドーム状構造物 工程 相互 メッシュ状フレーム構築用 アーチ状 構築法 上端部 プレキャストコンクリート部材 施工 前記パネルユニット 面 上下面 作業 ＰＣ鋼棒 構造図 屋根フレーム構築用 ブレース状 屋根材 剛強 コンクリート打設孔 技術分野 問題点 組立て設置 外 膜張力 メッシュ状空間 扇形状 フープ状 取付け 軽量 下面 平面拡大図 平面図 壁体等 コスト 手間 工期 参照 解体撤去 ユニット形態 発泡 面外 梁部材 合成材 鋼製 構成 鉄筋 コンクリート製ドーム屋根 主要構造体 分 手段 スタッド ～ 準備作業 簡便化 交点 上面開口 密閉空間 壁材 補剛効果 径間屋根 特開 組立構法 ドーム 羽根材同士 前記発泡樹脂 接合部 下端部 断面図 面外剛性 コンクリート打設 ＰＣ鋼棒等 前記プレストレス せん断補剛筋 外装材 説明 取り付け 課題 既往 一定 特徴 記載 強度 応力 前記 支保工 直前 状態 鉄筋コンクリート 精度 部材 撤去 設コンクリート 概略立面図 前記アーチ 硬化性 縦断面図 ウレタン系発泡樹脂 形態 解体撤去作業 熱硬化性樹脂 正面図 前記シース 外方 面積 建物 重量 I － 公報 側壁 中心 II 同一 組み込み 外面 図面 複数 伝達 版 施工性 形状 組立て 設置 短縮 主筋 解体 ダイヤフラムタイプ 削減 段階 プレストレストコンクリート製単層ラチスドーム 鋼材等 溶接等 ポンプ等 せん断補強筋 当該アーチ 実施例 ブレース形状 せん断剛性 導入作業 上記構成 複合化 軽量化 上端 上下 効果 謂シェル効果 取付け金物 金物作成 くり形状 箱形状 裏当て 精度管理 接続金具 シース ポストテンション 管理 間隔 対角線方向 中間部位 くり 現場サイト 外周縁 中央 間隔Ｌ 幅寸 鉄筋籠 中央上部 鉄骨 鋼管 一般 現状 事前 種 公知 一対 要領 仲介 製作 目的 合一 図示 保持 正方形 四隅 向き フランジ ｂ 通常 常温 温度 予熱 荷重 一辺 環状 下方 略 両側 配置 仰角 点線 次 下部 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 8.5323 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 固定構造 ポストスライド式支承 上部構造 図 沓 係合凹部 ベースプレート 支承 係合体 鋼棒 ジャッキＪ 支承本体 ポストスライド施工 ジャッキ 材 ポストスライド式支承Ｓ 収縮 下部構造 支承Ｓ 構造図 中間沓 Ｂ 当該上部構造 上沓 橋軸方向 凹部 固定ボルト 下部構造Ｂ ゴム体 上部構造Ｇ 位置 構造 状態 係合操作 継ぎ体 当該ポストスライド式支承 位置状態 ＰＣ鋼棒 Ｓ 常態収縮変形 力 実施形態 ＰＣ構造 橋桁Ｇ 収縮変形 ゴム支承本体 係合部 固定用 取付け板 断面図 体 乾燥収縮 発明 前記支承本体 コンクリート 前記ベースプレート ボルト 態様 係合 ～図 収縮方向 係合子 方向 施工 橋桁 当該収縮 スライド移動方法 縦断面図 プレストレス力 ボルト挿通孔 実施 該ジャッキＪ クリープ収縮 橋 部分 ねじ構造 中央位置 偏心位置 構成 センターホールジャッキＪ 偏心移動位置 金属体 センターホールジャッキ 例 先端 ＰＣ 取付け操作 コンクリート造 ねじ棒 乾燥収縮等 中央 クリープ変形 偏心位置状態 ねじ 材固定構造 上部分 ねじ孔 腕杆 中間部 上面 弾性変形 施工性 左方 他 操作 平面構造 係 ボルト固定 該ねじ プレストレストコンクリート構造 橋梁 該係合体 ソールプレート 前部 ｃ 側面 Ｇ 挿通孔 橋梁構造 詳細構造 中間位置 中間位置Ｊ 該コンクリート桁 橋桁等 Ｊ ジャッキＪ側 該下沓 径 該支承本体 形態 ｂ 太径 支承Ｓ部分 該中間沓 細幅部 広幅部 肩部 側面部 傾斜部 側壁部 壁部 ＰＣ鋼線 プレストレス導入 荷重 板 前記 該ＰＣ鋼棒 右方 ＰＣ橋 変位 収縮変位 上部取付け鋼板 移動 後部 】( 収縮量 温度収縮 上記構成 橋軸直角方向 橋台等 鋼製 プレストレス橋 水平断面図 矩形状 油圧ジャッキ等 説明 変形 孔 ｄ 後端面部 Ａ 先 取付け用 上部アンカー部材 前端面 中央部 定常位置 橋桁中央部 施工要領 現場施工 大型化 プレストレス プレストレス 積層ゴム体 油圧シリンダージャッキ 要領図 前記下沓 プレストレストコンクリート 型枠 ＰＣ材 鋼板 下部取付け鋼板 該ピン挿通孔部 図例 プレストレス力 右方側Ｓ ＰＣ線材 該係合凹部 調整施工 施工手順 操作性 端部 偏心状態 上記実施形態 橋桁用 構築 次 特徴 下 主体 機構 Ｈ 縁 拡大図 作動下沓 躯体 鉄筋コンクリート橋 取付け態様 ピン挿通孔 桁Ｇ 橋脚 技術 層 手間 範囲 ゴム支承系 ゴム素材 ゴム層 下部アンカー部材 先端部 引込み操作 嵌め込み操作 矢視図 作用性 作用 ピン せん断変形 偏心距離 横方向 イ方向 引込み力 偏心モーメント 部分構成 後端面 端面 平面構成 押し引き作用線 矩形平板状 プレストレストコンクリート造 技術分野 時点 該支承 課題 目的 図面 Ｉ 上面板 下面板 螺 ナット 後端 参照 首部 両端 橋脚Ｂ 該橋脚Ｂ 反対側 効果 端面部 － 縦剛性 鉄骨造 耐荷重性 水平剛性 水平変位 平面形状 上記実情 貫通状 作動 前端 設置 導入 距離 形状 架設現場 縮小化 技術的範囲 設置空間 設置手間 所定距離 凸形状 適用例 嵌め込み作業 作動過程 基本的技術思想 調整 伸長ロッド 荷重伝達 駆動装置 定位置状態 所定 加硫成形 ポリテトラフルオロエチレン樹脂 接面積 過程 設計変更 作業 本質的事項 支承上 内部 通常 手段 Ｘ Ｙ α なり 厚み 同等 両側 後記 一体 溶接 １つ 対称 該引張 該取付 受圧面 座金 一定 隙間 関係 支承部 図上 都合 該型枠 装着 上方 結 進行 所期 該態様 機能 円形 工程 符号\n",
      "\n",
      "===== # 8, Topic : 8, p : 7.8684 %\n",
      "Topic words : 効果, 砂, 側方, Ａ, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 先部 Ｈ型鋼 鋼製支保工 図 コンクリートアーチシェル コンクリート壁 下方 コンクリート トンネル掘削 トンネル 基部 前記鋼製支保工 下方部分 アーチ部周辺 説明 掘削方向 部分 説明図 拡大説明図 アーチ状 トンネル掘削工法 支保工 外周部 切羽 アーチ部 結地山 発明 トンネル坑内 掘削機 工法 既設 Ａ 掘削 側壁部 作業 孔 前記 前記Ｈ型鋼 鋼製支保工部分 曲率 アースオーガ 安定性 作業効率 下面 正面図 鉄筋構造体 周方向 拡大正面図 断面 状態 幅 一つ 】(Ａ 断面図 曲率半径 スリット スリット部分 次 Ｂ 上位 前記コンクリートアーチシェル 前記既設 連続状 Ｃ コストダウン 鉄筋 配設作業 図面 バックホウ 上部 硬性 Ｆ 種類 土質 場所 特徴 金網 数 寸法 実施 形態 形状 補強構造 前記事情 前記目的 Ｄ Ｅ Ｇ 課題 上述 同一 符号 延 技術分野 技術 下面全域 目的 ー 号 上記 現状 手段 鋼材 部材 箇所 下部 最小限 効果\n",
      "\n",
      "===== # 9, Topic : 14, p : 10.0623 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : アンカー 緩衝機構 土留壁 アンカー力 アンカー台座 自由長部 緩衝梁 アンカーヘッド 土留構造 自由長部ａ 力 アンカーａ 実施 アンカー体部ａ 図 地盤 緩衝機能 材料 アンカー体部 前記アンカー 発明 土留壁ｂ 実施例 形態 腹起し 土留工法 台座 孔長 作用土留壁 底部 支点部材 説明 頭部 説明図 弾性体 変位 アンカー長 アンカー工 アンカー材料 材 アンカー体部末端 前記土留壁 前記アンカーヘッド ＰＣ鋼 鋼棒 目的 変形 掘削側 連続梁状 アンカー台座ｄ アンカー台座本体 機構 前記緩衝機構 アンカーヘッドｅ 該アンカーヘッド 硬質地盤ｃ 作用緩衝機構 杭 イ ロ 断面図 グラウンドアンカー設計 想定滑り面 変化 分 作用 板材 調節範囲 建築地盤アンカー設計施工指針 親杭 横矢板 荷重計 ばね 中央部 面 隣地境界 硬質地盤 地盤条件 間隔 逆 工費 工期 ハ 効果 参照 地盤工学会 剛性 ＰＣ鋼棒 例 抵抗力 設計 弾性材料 部材 支点 腹起しｆ 配置実施 弾性範囲 Ｈ形鋼等 板状 所定 位置 クサビ 解説 － 平成 p 課題 次 線 選択 部分 特徴 範囲 図面 内容 通常 一体 鎖線 側 弾性材 伸び量 施工基準 材料費 日本建築学会 円筒状 荷重 伸び 略水平方向 ワッシャ－タイプ ゴム板 量δ 断面剛性 伸びただ 防食性 適用条件 必要性 皿ばね コイルばね 直径方向 ナットタイプ 技術分野 技術 基準 指針 問題点 ロードセル 結シルト ネジ溝 地山側 巻き取り 直径 炭素繊維 支保工 原則 ＪＧＳ 根拠 一定 影響 余地 数 一つ 手段 上記 前面 方法 他 要因 伝達 鉄板 幾つ 関係 種類 斜め 鋼材 鋳物 ニ 最初 幅 鋼板 複数 空間 不連続 短縮 距離 制約 符号 ・・アンカーヘッド\n",
      "\n",
      "===== # 10, Topic : 14, p : 8.5207 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 工事用仮設エレベータ 昇降台 仮設エレベータ 昇降式ラック 図 伸縮リンク 収納ピット ケージ 油圧ジャッキ 前記工事用仮設エレベータ 建築資材 積み荷 電動ポンプ 発明 前記 操作盤 ハンドパレット 階 ｃ 説明 昇降ラック 昇降 前記昇降式ラック 昇降式 床 積載能力 人力 目 ラック ｂ 使用状態説明図 タワー 当該工事用仮設エレベータ 昇降手段 ｆ 複数段 脚部 状態 建築 フォークリフト 該仮設エレベータ 説明図 ｅ レベル 高層建物 建物 前記昇降台 一段積み 運搬効率 ｄ 前記昇降ラック 往復 パレット当たり Ｘ ｅ駆動用 昇降速度 積載量 課題 当該エレベータ 効率 例 該ケージ 操作 作業員 作業者 上下方向 前記建築資材 エレベータ籠 前記操作盤 伸縮 使用状態 構築 ～ 工期 効果 程度 籠 駆動 前記ケージ 該昇降台 駆動電動機 前記タワー 積載能力範囲 操作レバー 高層階 当該建物 該伸縮リンク 前記底面 該収納ピット 当該フォークリフト 駆動モータ 台車 幅 容積 床下 図面 運搬 正面図 側面図 様子 該タワー 使用方法 フォークリフト等 上層階 上階 該荷取階 手段 壁繋ぎ材 ワイヤー等 技術分野 技術 鉄骨フレーム 分程度 ＡＬＣ板 尺物 格荷重 往復回数 上記課題 範囲 床面 同一符号 壁 側面 底面 上記 収容面積 符号 搬入 外壁 該一対 トン 引出し コスト 厚み 比重 重量 状況 要旨 下段 位置 実施 形態 理解 部分 一対 ポスト 下 ガイド 上部 側壁 他 上面 公知 所要 任意 短縮 上段\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.6839e-37, 8.4047e-34, 3.6231e-32, 4.0688e-17, 4.2832e-17, 4.3490e-39,\n",
      "         2.0319e-43, 6.1292e-08, 2.0949e-19, 5.2821e-24, 6.0934e-33, 4.3292e-28,\n",
      "         1.4118e-22, 1.7413e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 967.6998\n",
      "Test epoch : 1 Average loss: 927.1873\n",
      "PP(train) = 2316.365, PP(valid) = 2376.933\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3312e-43, 6.7580e-27, 4.7306e-09, 2.0785e-20, 4.0600e-06, 9.5257e-41,\n",
      "         7.2633e-27, 1.9279e-16, 4.7467e-17, 1.7910e-23, 8.3816e-31, 1.8390e-21,\n",
      "         3.8479e-07, 4.7042e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 967.5477\n",
      "Test epoch : 2 Average loss: 927.0730\n",
      "PP(train) = 2312.470, PP(valid) = 2374.347\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5057e-25, 1.4767e-26, 1.1754e-07, 4.5866e-15, 5.5142e-11, 3.8262e-37,\n",
      "         1.3230e-27, 1.0000e+00, 3.8476e-12, 7.9555e-23, 2.4235e-21, 1.3341e-23,\n",
      "         3.1677e-18, 1.6013e-21, 1.2614e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 967.4116\n",
      "Test epoch : 3 Average loss: 926.9482\n",
      "PP(train) = 2308.389, PP(valid) = 2371.916\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5309e-31, 3.4292e-32, 1.5754e-10, 4.3119e-08, 2.9239e-03, 5.1116e-37,\n",
      "         2.2988e-33, 5.0603e-03, 7.8974e-11, 1.9377e-14, 5.6189e-25, 6.8596e-26,\n",
      "         2.0451e-07, 1.3178e-14, 9.9202e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0446, 0.1038, 0.0502, 0.0603, 0.0671, 0.1074, 0.0921, 0.0593,\n",
      "         0.0523, 0.0622, 0.0556, 0.0510, 0.0710, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 967.0351\n",
      "Test epoch : 4 Average loss: 926.8167\n",
      "PP(train) = 2304.144, PP(valid) = 2369.489\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[0.0000e+00, 2.9157e-31, 2.4919e-22, 3.7895e-33, 2.0452e-18, 1.1071e-34,\n",
      "         5.1091e-41, 6.9240e-09, 3.2614e-15, 3.7428e-26, 3.0275e-29, 2.2121e-30,\n",
      "         8.4649e-18, 1.1377e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 966.8001\n",
      "Test epoch : 5 Average loss: 926.6797\n",
      "PP(train) = 2299.676, PP(valid) = 2366.904\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1719e-27, 1.6069e-24, 8.9956e-15, 5.2667e-15, 1.0747e-05, 1.1129e-25,\n",
      "         1.7747e-27, 3.4779e-01, 4.1729e-08, 3.2934e-08, 2.7994e-26, 1.0513e-25,\n",
      "         3.2924e-02, 1.3871e-17, 6.1928e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0830, 0.0626, 0.0844, 0.0496, 0.0670, 0.0767, 0.0960, 0.0726, 0.0590,\n",
      "         0.0485, 0.0792, 0.0524, 0.0468, 0.0799, 0.0422]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 966.6058\n",
      "Test epoch : 6 Average loss: 926.5424\n",
      "PP(train) = 2294.902, PP(valid) = 2364.039\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3106e-32, 5.7600e-35, 1.4259e-09, 2.8032e-28, 4.4011e-10, 4.2550e-23,\n",
      "         1.2439e-25, 2.2171e-08, 1.8565e-11, 1.9668e-21, 5.4771e-27, 3.8123e-28,\n",
      "         1.3684e-01, 5.0969e-26, 8.6316e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0769, 0.0481, 0.1010, 0.0521, 0.0577, 0.0673, 0.0989, 0.0914, 0.0619,\n",
      "         0.0573, 0.0605, 0.0558, 0.0526, 0.0707, 0.0479]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 966.3112\n",
      "Test epoch : 7 Average loss: 926.4068\n",
      "PP(train) = 2290.144, PP(valid) = 2361.232\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7960e-29, 1.3114e-23, 1.2918e-02, 8.3560e-18, 4.6028e-09, 2.4037e-33,\n",
      "         9.4258e-35, 1.1570e-12, 1.7525e-15, 1.8325e-21, 1.1256e-25, 3.6453e-26,\n",
      "         1.1066e-09, 1.1643e-20, 9.8708e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0448, 0.1033, 0.0505, 0.0604, 0.0667, 0.1071, 0.0922, 0.0597,\n",
      "         0.0524, 0.0620, 0.0555, 0.0511, 0.0711, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 966.1535\n",
      "Test epoch : 8 Average loss: 926.2709\n",
      "PP(train) = 2285.533, PP(valid) = 2358.539\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5251e-36, 3.8636e-17, 1.4902e-02, 7.9940e-08, 1.0021e-06, 9.7530e-26,\n",
      "         2.7936e-31, 5.9115e-03, 1.0160e-04, 4.9634e-14, 1.1724e-23, 2.0955e-22,\n",
      "         1.2738e-05, 3.3170e-21, 9.7907e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0451, 0.1028, 0.0505, 0.0606, 0.0668, 0.1069, 0.0918, 0.0598,\n",
      "         0.0524, 0.0623, 0.0555, 0.0510, 0.0713, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 965.8096\n",
      "Test epoch : 9 Average loss: 926.1347\n",
      "PP(train) = 2281.040, PP(valid) = 2355.936\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7830e-41, 2.7819e-41, 3.2685e-17, 4.5417e-25, 4.4540e-14, 6.0862e-40,\n",
      "         0.0000e+00, 4.3422e-23, 4.2974e-17, 3.2912e-31, 3.6714e-43, 3.5258e-31,\n",
      "         5.1269e-18, 1.1382e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 965.5454\n",
      "Test epoch : 10 Average loss: 925.9978\n",
      "PP(train) = 2276.502, PP(valid) = 2353.286\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.7062e-31, 3.7686e-26, 9.9890e-01, 4.8474e-27, 2.9578e-17, 1.0344e-32,\n",
      "         3.4790e-31, 3.5633e-12, 9.2196e-14, 1.9415e-14, 1.7292e-38, 7.9174e-33,\n",
      "         8.3215e-13, 8.0625e-19, 1.1021e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0898, 0.0443, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0948,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 965.3586\n",
      "Test epoch : 11 Average loss: 925.8642\n",
      "PP(train) = 2271.971, PP(valid) = 2350.661\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2549e-30, 4.1162e-22, 2.7888e-19, 3.7522e-06, 9.0785e-10, 1.5602e-28,\n",
      "         1.1376e-29, 2.4959e-05, 1.8365e-04, 2.0512e-24, 2.2946e-14, 6.3374e-11,\n",
      "         3.2044e-04, 4.1353e-05, 9.9943e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 965.1806\n",
      "Test epoch : 12 Average loss: 925.7312\n",
      "PP(train) = 2267.452, PP(valid) = 2347.971\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6957e-36, 1.6489e-33, 4.2663e-16, 8.6321e-26, 1.9501e-08, 6.7816e-40,\n",
      "         1.8913e-41, 6.8250e-12, 2.1281e-12, 3.4546e-28, 1.5361e-36, 6.2395e-22,\n",
      "         4.9041e-18, 2.8022e-35, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 964.8054\n",
      "Test epoch : 13 Average loss: 925.6004\n",
      "PP(train) = 2263.047, PP(valid) = 2345.407\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5142e-31, 3.4788e-25, 3.9300e-19, 1.6061e-20, 6.2173e-05, 7.3218e-29,\n",
      "         1.0662e-24, 5.2936e-03, 9.7015e-01, 1.3046e-19, 2.3276e-19, 6.0946e-32,\n",
      "         2.2444e-04, 8.4594e-16, 2.4273e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0569, 0.0670, 0.0454, 0.0626, 0.0311, 0.0760, 0.0640, 0.0918, 0.0572,\n",
      "         0.0688, 0.0843, 0.0649, 0.0511, 0.1295, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 964.6290\n",
      "Test epoch : 14 Average loss: 925.4704\n",
      "PP(train) = 2258.688, PP(valid) = 2342.881\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0594e-42, 1.5196e-19, 2.6232e-18, 2.0049e-12, 6.7415e-09, 3.6328e-40,\n",
      "         1.8865e-25, 5.2135e-10, 4.8179e-11, 5.1402e-14, 1.5192e-26, 2.4766e-15,\n",
      "         8.0729e-01, 1.1852e-10, 1.9271e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0542, 0.0684, 0.0821, 0.0593, 0.0450, 0.0660, 0.0625, 0.0825, 0.0725,\n",
      "         0.0851, 0.0513, 0.0542, 0.0578, 0.0665, 0.0925]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 964.3697\n",
      "Test epoch : 15 Average loss: 925.3418\n",
      "PP(train) = 2254.432, PP(valid) = 2340.425\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1408e-29, 1.5386e-20, 3.1235e-07, 6.4798e-17, 9.6130e-08, 4.7425e-23,\n",
      "         4.2481e-29, 2.7420e-06, 2.6254e-03, 2.6467e-22, 1.2235e-20, 8.6498e-13,\n",
      "         9.7866e-01, 5.3928e-17, 1.8715e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0738, 0.0766, 0.0604, 0.0416, 0.0648, 0.0547, 0.0793, 0.0744,\n",
      "         0.0929, 0.0486, 0.0531, 0.0584, 0.0647, 0.1079]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 964.2127\n",
      "Test epoch : 16 Average loss: 925.2143\n",
      "PP(train) = 2250.115, PP(valid) = 2337.902\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0991e-40, 5.9450e-37, 8.5075e-16, 2.9755e-28, 2.9400e-16, 5.4580e-40,\n",
      "         4.2185e-38, 2.7837e-04, 2.1026e-10, 5.4551e-25, 5.9876e-35, 2.3067e-30,\n",
      "         1.7979e-15, 7.8888e-27, 9.9972e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 963.7974\n",
      "Test epoch : 17 Average loss: 925.0852\n",
      "PP(train) = 2245.862, PP(valid) = 2335.384\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0574e-19, 6.1785e-33, 3.1179e-13, 5.1399e-12, 2.5875e-15, 7.2045e-26,\n",
      "         2.1688e-21, 2.1354e-09, 9.9998e-01, 1.5625e-14, 7.8493e-23, 3.3878e-31,\n",
      "         1.0900e-12, 5.9056e-20, 1.7690e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 963.7770\n",
      "Test epoch : 18 Average loss: 924.9582\n",
      "PP(train) = 2241.640, PP(valid) = 2332.874\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0343e-40, 1.0831e-37, 1.5207e-20, 1.2270e-17, 7.6919e-08, 8.4139e-28,\n",
      "         3.1073e-25, 6.3850e-14, 1.0000e+00, 1.1683e-28, 1.1471e-38, 4.1976e-28,\n",
      "         3.4394e-15, 4.6425e-23, 1.3223e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 963.4841\n",
      "Test epoch : 19 Average loss: 924.8352\n",
      "PP(train) = 2237.443, PP(valid) = 2330.419\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6778e-37, 1.7607e-36, 2.7130e-03, 7.8835e-30, 3.1726e-12, 3.2242e-35,\n",
      "         4.9045e-44, 6.1271e-07, 5.4284e-03, 1.0846e-27, 3.8625e-28, 3.0770e-29,\n",
      "         1.7247e-29, 8.9933e-27, 9.9186e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0816, 0.0446, 0.1037, 0.0503, 0.0600, 0.0669, 0.1072, 0.0923, 0.0594,\n",
      "         0.0524, 0.0621, 0.0556, 0.0511, 0.0712, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 963.1297\n",
      "Test epoch : 20 Average loss: 924.7090\n",
      "PP(train) = 2233.383, PP(valid) = 2328.030\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2368e-41, 1.0693e-25, 2.7627e-12, 3.8555e-12, 1.3518e-15, 3.2910e-23,\n",
      "         2.1889e-33, 9.9551e-01, 1.8514e-10, 8.4005e-21, 1.3267e-19, 5.3567e-31,\n",
      "         7.7644e-18, 1.4019e-32, 4.4911e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1052, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0371, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 962.9691\n",
      "Test epoch : 21 Average loss: 924.5854\n",
      "PP(train) = 2229.330, PP(valid) = 2325.671\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5801e-33, 1.1408e-24, 1.4913e-21, 2.9105e-16, 8.5164e-03, 3.5610e-33,\n",
      "         1.0256e-24, 4.9663e-21, 1.5105e-17, 2.2967e-23, 1.6089e-27, 5.5969e-28,\n",
      "         2.7125e-14, 4.0639e-25, 9.9148e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.0444, 0.1036, 0.0501, 0.0604, 0.0669, 0.1076, 0.0925, 0.0592,\n",
      "         0.0526, 0.0621, 0.0557, 0.0510, 0.0708, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 962.7661\n",
      "Test epoch : 22 Average loss: 924.4630\n",
      "PP(train) = 2225.249, PP(valid) = 2323.269\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0928e-25, 1.6909e-36, 2.2229e-09, 4.5965e-10, 6.8757e-14, 6.6815e-36,\n",
      "         1.9588e-40, 2.8912e-07, 1.6441e-06, 1.4494e-17, 9.3778e-23, 3.0450e-28,\n",
      "         2.0983e-15, 1.2897e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 962.4947\n",
      "Test epoch : 23 Average loss: 924.3419\n",
      "PP(train) = 2221.214, PP(valid) = 2320.916\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.8006e-34, 1.4216e-21, 7.8348e-20, 1.4712e-21, 1.5380e-03, 7.4659e-24,\n",
      "         6.2267e-27, 3.7824e-02, 3.5348e-05, 6.4305e-15, 1.2943e-21, 1.4904e-21,\n",
      "         1.4497e-03, 2.7321e-13, 9.5915e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0820, 0.0461, 0.1020, 0.0501, 0.0610, 0.0680, 0.1065, 0.0901, 0.0593,\n",
      "         0.0519, 0.0638, 0.0553, 0.0506, 0.0719, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 962.3221\n",
      "Test epoch : 24 Average loss: 924.2220\n",
      "PP(train) = 2217.220, PP(valid) = 2318.564\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5835e-42, 5.5118e-31, 1.5144e-24, 1.5155e-25, 6.3637e-12, 2.0620e-38,\n",
      "         2.9465e-37, 1.0000e+00, 6.1889e-10, 3.1210e-30, 3.6647e-25, 7.6739e-16,\n",
      "         9.1235e-15, 3.7189e-32, 1.9313e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 962.0366\n",
      "Test epoch : 25 Average loss: 924.1015\n",
      "PP(train) = 2213.336, PP(valid) = 2316.278\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0999e-37, 6.5200e-35, 7.0750e-13, 1.3349e-17, 8.2526e-05, 1.4121e-33,\n",
      "         3.7391e-28, 9.2797e-09, 3.2833e-06, 1.1211e-18, 3.7012e-24, 7.9147e-26,\n",
      "         1.3355e-14, 9.3809e-29, 9.9991e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 961.8270\n",
      "Test epoch : 26 Average loss: 923.9831\n",
      "PP(train) = 2209.461, PP(valid) = 2314.029\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.9492e-44, 5.0166e-43, 3.5491e-08, 1.7267e-16, 7.0105e-14, 6.7119e-32,\n",
      "         2.8026e-45, 5.1168e-07, 4.0033e-11, 2.1252e-24, 1.2217e-18, 2.0825e-28,\n",
      "         2.3719e-18, 5.0765e-34, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 961.5634\n",
      "Test epoch : 27 Average loss: 923.8646\n",
      "PP(train) = 2205.609, PP(valid) = 2311.764\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0486e-29, 6.2541e-19, 2.3521e-05, 5.2929e-13, 2.1225e-04, 2.3423e-27,\n",
      "         3.1245e-34, 6.0316e-03, 1.3286e-05, 2.1602e-22, 1.2662e-19, 3.2001e-25,\n",
      "         3.0895e-09, 1.3975e-08, 9.9372e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0446, 0.1040, 0.0502, 0.0602, 0.0671, 0.1074, 0.0920, 0.0593,\n",
      "         0.0522, 0.0623, 0.0555, 0.0510, 0.0710, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 961.3949\n",
      "Test epoch : 28 Average loss: 923.7478\n",
      "PP(train) = 2201.709, PP(valid) = 2309.451\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1805e-32, 2.3553e-31, 8.1679e-09, 4.2438e-20, 1.7265e-01, 8.7687e-27,\n",
      "         7.7185e-40, 7.1376e-09, 2.9404e-11, 1.5505e-19, 2.4583e-26, 3.7263e-18,\n",
      "         9.0119e-17, 2.5471e-19, 8.2735e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0745, 0.0455, 0.0902, 0.0484, 0.0659, 0.0674, 0.1086, 0.0960, 0.0572,\n",
      "         0.0585, 0.0635, 0.0570, 0.0506, 0.0700, 0.0466]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 961.2065\n",
      "Test epoch : 29 Average loss: 923.6325\n",
      "PP(train) = 2197.844, PP(valid) = 2307.168\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7096e-43, 7.6145e-29, 1.4260e-12, 2.5779e-21, 2.2561e-26, 1.6421e-29,\n",
      "         7.5591e-36, 2.5735e-15, 6.6218e-14, 2.4255e-28, 6.8271e-31, 2.3456e-32,\n",
      "         5.7506e-21, 3.0166e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 960.9422\n",
      "Test epoch : 30 Average loss: 923.5182\n",
      "PP(train) = 2194.083, PP(valid) = 2304.981\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3250e-28, 6.6857e-38, 9.9357e-16, 2.5308e-32, 2.2469e-19, 0.0000e+00,\n",
      "         1.3747e-42, 4.3866e-11, 1.0000e+00, 3.6536e-24, 7.3724e-41, 8.8663e-32,\n",
      "         1.9183e-19, 1.2860e-26, 2.7739e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 960.6806\n",
      "Test epoch : 31 Average loss: 923.4047\n",
      "PP(train) = 2190.396, PP(valid) = 2302.857\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3091e-37, 4.0341e-41, 5.8833e-21, 1.0911e-19, 2.5545e-26, 1.4095e-22,\n",
      "         8.8695e-32, 3.5641e-18, 2.1416e-16, 1.0898e-18, 2.6148e-27, 1.1317e-26,\n",
      "         2.5758e-21, 1.3611e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 960.6341\n",
      "Test epoch : 32 Average loss: 923.2934\n",
      "PP(train) = 2186.701, PP(valid) = 2300.720\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1604e-32, 1.8409e-35, 2.8351e-18, 6.9598e-24, 1.5704e-18, 1.6146e-30,\n",
      "         1.4649e-28, 7.3370e-13, 1.0000e+00, 4.6108e-27, 2.0493e-28, 9.0387e-29,\n",
      "         2.2008e-09, 4.5618e-31, 4.3004e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 960.3787\n",
      "Test epoch : 33 Average loss: 923.1823\n",
      "PP(train) = 2182.997, PP(valid) = 2298.523\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7588e-31, 2.5015e-35, 8.8708e-31, 5.5946e-26, 1.6366e-06, 7.9576e-31,\n",
      "         4.9037e-33, 3.3267e-06, 3.1318e-08, 4.7024e-17, 3.6998e-34, 1.6100e-29,\n",
      "         2.9480e-09, 3.1662e-18, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 960.1398\n",
      "Test epoch : 34 Average loss: 923.0701\n",
      "PP(train) = 2179.330, PP(valid) = 2296.341\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9502e-32, 1.8303e-23, 1.4624e-04, 7.5247e-23, 1.4072e-05, 4.6098e-38,\n",
      "         6.9628e-23, 5.2978e-03, 9.9454e-01, 6.3261e-08, 8.9247e-24, 1.3968e-33,\n",
      "         5.9861e-13, 8.7614e-17, 1.2751e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0563, 0.0676, 0.0443, 0.0628, 0.0306, 0.0761, 0.0630, 0.0916, 0.0570,\n",
      "         0.0691, 0.0848, 0.0650, 0.0510, 0.1312, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 959.9277\n",
      "Test epoch : 35 Average loss: 922.9593\n",
      "PP(train) = 2175.756, PP(valid) = 2294.264\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0152e-35, 1.6448e-34, 2.8945e-09, 5.7592e-23, 3.1487e-12, 5.6866e-35,\n",
      "         4.1493e-28, 9.9988e-01, 1.2056e-04, 1.4600e-24, 1.8561e-28, 1.0718e-24,\n",
      "         1.4972e-13, 6.4433e-26, 7.4092e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 959.7081\n",
      "Test epoch : 36 Average loss: 922.8488\n",
      "PP(train) = 2172.250, PP(valid) = 2292.227\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5638e-32, 3.6834e-32, 2.7289e-08, 1.9330e-14, 2.4361e-03, 2.5464e-24,\n",
      "         7.4532e-28, 1.0909e-04, 3.4120e-09, 4.7007e-20, 1.7947e-22, 4.6617e-20,\n",
      "         2.1237e-09, 1.4998e-21, 9.9745e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0816, 0.0444, 0.1041, 0.0502, 0.0602, 0.0669, 0.1076, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0708, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 959.5109\n",
      "Test epoch : 37 Average loss: 922.7402\n",
      "PP(train) = 2168.666, PP(valid) = 2290.121\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7217e-35, 5.8035e-25, 8.2469e-14, 1.2567e-19, 7.1247e-04, 6.6501e-29,\n",
      "         2.5399e-38, 2.2028e-04, 8.0947e-02, 3.7663e-19, 7.5307e-30, 4.6525e-32,\n",
      "         9.1812e-01, 8.4541e-29, 2.0062e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0491, 0.0742, 0.0733, 0.0611, 0.0405, 0.0658, 0.0549, 0.0803, 0.0734,\n",
      "         0.0921, 0.0508, 0.0541, 0.0582, 0.0685, 0.1036]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 959.2756\n",
      "Test epoch : 38 Average loss: 922.6326\n",
      "PP(train) = 2165.073, PP(valid) = 2287.990\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2888e-42, 1.0740e-28, 1.2196e-08, 5.3363e-30, 1.1782e-04, 4.1436e-42,\n",
      "         1.2442e-35, 3.4252e-05, 6.2766e-10, 5.0582e-23, 2.2298e-32, 1.1493e-34,\n",
      "         5.4513e-19, 3.7125e-21, 9.9985e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 958.9966\n",
      "Test epoch : 39 Average loss: 922.5250\n",
      "PP(train) = 2161.606, PP(valid) = 2285.995\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.5034e-38, 1.4400e-23, 3.4285e-08, 1.3398e-21, 1.5479e-08, 1.9415e-35,\n",
      "         8.5090e-36, 2.9980e-04, 9.9969e-01, 1.3164e-14, 9.6742e-31, 6.2938e-24,\n",
      "         6.3438e-06, 1.5990e-09, 1.7285e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 958.8242\n",
      "Test epoch : 40 Average loss: 922.4190\n",
      "PP(train) = 2158.182, PP(valid) = 2284.036\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8643e-29, 2.3443e-21, 5.8580e-11, 8.3771e-23, 4.7136e-18, 3.0436e-25,\n",
      "         8.3358e-27, 8.4737e-04, 8.1297e-06, 1.8446e-25, 1.0971e-18, 1.2019e-18,\n",
      "         2.3646e-02, 2.1304e-25, 9.7550e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0450, 0.1037, 0.0505, 0.0597, 0.0670, 0.1060, 0.0921, 0.0598,\n",
      "         0.0531, 0.0618, 0.0556, 0.0513, 0.0709, 0.0425]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 958.7265\n",
      "Test epoch : 41 Average loss: 922.3153\n",
      "PP(train) = 2154.713, PP(valid) = 2282.023\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2563e-34, 1.3264e-38, 3.4856e-23, 4.9124e-26, 3.7817e-12, 7.6102e-40,\n",
      "         9.3576e-37, 2.0902e-11, 1.7310e-23, 7.4654e-32, 4.7909e-31, 2.8072e-31,\n",
      "         2.0449e-16, 1.3847e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 958.4988\n",
      "Test epoch : 42 Average loss: 922.2138\n",
      "PP(train) = 2151.270, PP(valid) = 2280.026\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0766e-42, 1.3570e-27, 1.0628e-18, 8.8858e-20, 2.5930e-14, 5.9125e-32,\n",
      "         2.1121e-31, 9.4961e-07, 9.9999e-01, 1.5264e-26, 8.8319e-37, 1.8827e-33,\n",
      "         3.2583e-10, 2.4983e-13, 4.9947e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 958.2397\n",
      "Test epoch : 43 Average loss: 922.1095\n",
      "PP(train) = 2147.881, PP(valid) = 2278.039\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5109e-18, 2.7903e-28, 1.8843e-07, 1.8231e-20, 5.9068e-06, 1.2166e-17,\n",
      "         6.1176e-26, 2.9520e-01, 1.5308e-10, 1.7750e-14, 8.7825e-18, 2.4858e-23,\n",
      "         1.9283e-03, 2.8717e-09, 7.0287e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0839, 0.0586, 0.0879, 0.0494, 0.0665, 0.0751, 0.0994, 0.0756, 0.0587,\n",
      "         0.0483, 0.0768, 0.0529, 0.0472, 0.0786, 0.0410]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 958.1699\n",
      "Test epoch : 44 Average loss: 922.0072\n",
      "PP(train) = 2144.540, PP(valid) = 2276.115\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4928e-35, 3.6514e-37, 1.7476e-16, 5.5388e-37, 2.3872e-20, 0.0000e+00,\n",
      "         5.3249e-44, 9.5748e-29, 1.7455e-18, 1.4074e-36, 2.3297e-33, 7.0065e-44,\n",
      "         5.5198e-34, 3.8683e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 957.9006\n",
      "Test epoch : 45 Average loss: 921.9058\n",
      "PP(train) = 2141.250, PP(valid) = 2274.240\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3465e-32, 1.6615e-32, 1.3318e-20, 1.9718e-26, 1.2559e-12, 5.3746e-22,\n",
      "         7.1887e-42, 1.7327e-12, 2.0583e-09, 1.3515e-21, 3.5069e-26, 3.3944e-25,\n",
      "         2.2864e-22, 4.4046e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 957.7191\n",
      "Test epoch : 46 Average loss: 921.8064\n",
      "PP(train) = 2137.877, PP(valid) = 2272.277\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8962e-40, 4.8693e-19, 4.6148e-18, 9.6399e-26, 3.5227e-13, 9.3750e-28,\n",
      "         1.2580e-21, 8.3011e-18, 1.4150e-10, 2.5002e-22, 1.2902e-34, 4.9070e-33,\n",
      "         1.0981e-12, 2.2910e-19, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 957.5745\n",
      "Test epoch : 47 Average loss: 921.7036\n",
      "PP(train) = 2134.540, PP(valid) = 2270.267\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2782e-26, 4.0683e-22, 2.9004e-17, 9.0292e-17, 4.2193e-04, 7.7350e-33,\n",
      "         1.7688e-28, 1.1837e-08, 1.4608e-18, 3.9722e-26, 5.8692e-37, 8.6123e-32,\n",
      "         1.8955e-18, 3.6228e-24, 9.9958e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 957.3696\n",
      "Test epoch : 48 Average loss: 921.6052\n",
      "PP(train) = 2131.325, PP(valid) = 2268.436\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3039e-25, 3.1691e-28, 6.2453e-17, 2.8186e-22, 1.9227e-10, 4.2267e-33,\n",
      "         8.9372e-24, 2.9450e-12, 4.8873e-18, 4.6533e-20, 2.4232e-24, 9.7995e-19,\n",
      "         1.7265e-23, 2.6338e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 957.2313\n",
      "Test epoch : 49 Average loss: 921.5069\n",
      "PP(train) = 2128.135, PP(valid) = 2266.618\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9765e-34, 1.0042e-28, 1.4982e-15, 3.3064e-18, 3.0029e-06, 5.5577e-26,\n",
      "         7.0077e-22, 5.3496e-10, 6.5252e-09, 3.9846e-09, 1.0841e-20, 4.0437e-12,\n",
      "         1.7620e-05, 6.1754e-11, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 956.9661\n",
      "Test epoch : 50 Average loss: 921.4085\n",
      "PP(train) = 2124.892, PP(valid) = 2264.730\n",
      "Writing to ./topicwords/14-topwords_e50.txt\n",
      "Topic 0: 角度θ つぎ 緊張 位置ずれ 定着構造 撮像画像 ダンパ 鋼 継手板 原子力発電所\n",
      "Topic 1: つぎ 位置ずれ 角度θ コンクリート打設後 柱状 定着構造 撮像画像 電磁弁 原子力発電所 ダム\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 技術的範囲 態様 特許請求 周知 前記実施形態 衝撃 構成要素 縦 姿勢 柱状\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: 位置ずれ つぎ 角度θ ダンパ 定着構造 セグメントリング 撮像画像 電磁弁 原子力発電所 コンクリート打設後\n",
      "Topic 6: つぎ 角度θ セグメントリング コンクリート打設後 緊張 ダンパ 位置ずれ 定着構造 撮像画像 鋼\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 図\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: つぎ 角度θ 位置ずれ ダム コンクリート打設後 鋼 縦 閾値 原子力発電所 差異\n",
      "Topic 10: 位置ずれ つぎ 角度θ ダンパ 撮像画像 定着構造 設計通り 緊張 コア 鋼\n",
      "Topic 11: 角度θ つぎ 位置ずれ 定着構造 鋼 撮像画像 ダンパ 緊張 コンクリート打設後 セグメントリング\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: つぎ 位置ずれ 角度θ 鋼 柱状 緊張 継ぎ目 撮像画像 行い 定着構造\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 14, p : 9.1723 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ステップＳＴ 評価対象 建物 用途変換 事業性評価 事業性 評価 用途変換評価装置 演算部 前記評価対象 条件 法的条件 地域条件 地域 住宅 オフィス 用途 サブルーチン処理 改修項目 オフィス賃料 事業性評価手段 窓先空地 地域条件判定手段 住宅賃料 上層増築 窓 前記事業性評価手段 入力部 法的条件判定手段 サブルーチン 処理 図 賃貸オフィスビル 賃料水準 住戸割りプラン 対象 確認申請 前記建物 収益性 発明 用途変換評価 データ記憶部 確認 集合住宅 住宅賃料水準データベース 居室条件 データ 代替進入口 特許文献 マンション賃料 改修パターン 用途転換 フローチャート オフィス賃料データベース 開口 耐震性能 判定 耐震評価 建物条件 オフィスビル 上層増築判定手段 上記開閉窓 実施 形態 演算 改修費 用途混在 データベース 用途変換評価プログラム 表示部 賃貸対象面積 容積率 耐震補強 メインルーチン処理 事業期間 空室率 可能性 前記地域条件判定手段 不動産 オフィス用 構造評価 判定処理 用途地域 所在地 規模修繕 否 当該建物 担保性 オフィス賃料水準データベース 住宅転用 建物診断 前記地域判定手段 プログラム 賃金水準 賃貸ビル 面積 支出総計 収入総計 規模模様替え 物理的適性 増築 実施形態 部分 余裕 特許 採光 立地条件 採光条件 同一条件 制約条件 開閉窓 演算処理 等 法 建物改修履歴 事業リスク 住戸割り 図面 住宅建設可能地域 投資対象物 賃貸集合住宅 建物性能 記法的条件判定手段 当該地域 改修履歴データ 手段 補強 開閉 確認処理 課題 既存建物 マンション 居室 ボイド 支出 ００７ 不動産投資 周辺賃料データベース 不動産特定共同事業法 改修コスト 改修投資効果 資産価値 ライフサイクルコスト ビル 日本国内 号公報 竣工年 ブロック図 建物一体 投資商品 投資信託 改修内容 不動産投資商品 不動産登記法 建築基準法 面積等 説明 土地 １つ 所定 年数 所要 各戸 修繕計画作成装置 技術 家賃 低下 内容 構造 範囲 空室リスク 基本図面データ 現地調査データ 投資利回り 法制度 収益率 所在地データ 性能 証券取引法 建物リフォーム支援システム 既存遡及項目 建築基準法施行令 不動産担保価値 傾向 債権譲渡特例法等 専有面積等 証券化 不動産金融商品 上層階 適性 拡大傾向 ベースビル 下落率 低下率 流動化 活性化 不動産不可分 基本図面 現地調査 不動産特性 登記 参照 設計 構成 駅 東京 記載 検討 特許請求 インターフェース等 階段等 税金等 活用可能範囲 開口拡大 情報化対応 施工計画立案 信託受益権 建築安全条例 家賃値下がりリスク 上記発明 デフレ傾向下 費用算出 既存施設 概算費用 想定期間 開口新設 技術分野 金利政策 設定 一体 リニューアル工事 立地 敷地形状 情報 条例 都心 同一 階段 仕様設定 法規制 仕様 都心居住 効果 地価 外装 方法 ＳＰＣ Ｓｐｅｃｉａｌ Ｐｕｒｐｏｓｅ Ｃｏｍｐａｎｙ 整備 欧米 面 融資 原則 前提 目的 コンピュータ 任意 順序 収支 例 他 寮 機能 ポテンシャル キーボード ポインティングデバイス 最寄り駅 ＣＲＴ Ｃａｔｈｏｄｅ Ｒａｙ Ｔｕｂｅ ＬＣＤ Ｌｉｑｕｉｄ ＣｒｙｓｔａｌＤｉｓｐｌａｙ 要件 道路 ｖｏｉｄ 空隙 追加 法定 変更 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 10.8808 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 関係履歴 情報 情報データベース 関係履歴オブジェクト データベース 利用者 情報オブジェクト 関係履歴情報 利用料金 利用 利用者Ａ 関係 データベース利用料金 利用者端末 操作関係履歴 参考関係履歴 前記関係履歴情報 データベースサーバー 関係履歴情報部 前記データベース 利用者データベース 関係履歴オブジェクトα 登録関係履歴 データベース利用者 データベース利用 履歴 オブジェクト 利用サービス 当該情報 情報オブジェクトＸ 利用関係 実施形態 請求項 顧客企業 検索 参照関係 図 情報量 回答関係履歴 引用関係履歴 料金 データベース情報 利用形態 情報検索 データベース運営者 発明 利用者ＩＤ 前記利用料金 活用度合い 情報閲覧 前記利用者端末 前記関係履歴記憶手段 運営者 種類 情報本体部 参照先 操作 上記関係履歴オブジェクト 情報内容 データベースシステム 利用内容 各種関係履歴 情報蓄積量 量 識別情報 コンテンツ データベース利用サービス 関係履歴記憶手段 コンテンツＢ 上記操作関係履歴 参考 計算 当該情報オブジェクト 関係レベル要素 個数 登録情報 情報登録 関係履歴生成手段 利用量 重み付け データベース活用 利用者認証 画面 要素 情報部 上記実施形態 コンテンツＨ 情報オブジェクトＹ 利用料金計算 情報生成 形態 参照 引用関係 参照元 アカウント情報 操作内容 前記料金決定手段 情報ダウンロード 料金体系 システム 情報活用度合い 各種情報 技術情報 検索キーワード コンテンツＩ コンテンツＪ コンテンツＫ 特徴 図表Ｍ 情報相互 人員情報 格納情報 蓄積量 上記 登録 参照先情報部 閲覧 Ｓ 参照先要素 利用価値 利用頻度 利用状況 情報検索システム オブジェクトＸ 実施 メッセージＰ メッセージＱ 構成 データＬ 重み付けテーブル オブジェクトＹ 記載 処理 データ構成 コンテンツＧ 営業客先情報 電子掲示板情報等 例 表示 料金決定手段 顧客 構成図 簡易閲覧 詳細閲覧 Ｙ 本願発明者 説明 他 インターネット等 電子掲示板 キーワード検索 計算方法 顧客端末 コンテンツ登録 当該顧客企業 キーワード検索等 相互 印刷 別 記憶手段 ログインアカウント 重み付け値 当該検索キーワード コンテンツ閲覧等 ダウンロード 入力 概要 フローチャート 回数 回答メッセージ 生成量 サービス 業務Ｅ 投稿メッセージ 組織Ｄ 顧客企業宛 回答メッセージＱ 親メッセージ 子メッセージ ネットワーク 目的 手段 ステップ リンク 生成 夫 アクセス 有効活用 投稿メッセージＰ 記憶装置 内容 単位記憶量当り 業務等 ダウンロード等 発生量 サービス画面 登録済み 請求書 上記図 回答 元 パスワード等 印刷等 名称等 タイトル等 構成例 計算手法 計算タイミング 課題 現時点 メリット 上記データベースシステム ～ ２つ 付箋 一覧 前回 評価 インセンティブポイント 技術 各種 組織 値 技術分野 可能性 必然性 重要度 ディスプレイ画面 貢献度 ＡＳＰ形式 容量化 出願人 特願 複数種類 ＡＳＰ パスワード 頻度 あたり別 表示順序 タイミング 増加個数 失 対価 不満 コスト 低下 － 関連 別個 社員 編集 ポインタ 上述 従事 統計 旨 性質 通常 コード 順 所定 基礎 プリンタ 向け 課金 自体 ポイント 効果 図面 符号\n",
      "\n",
      "===== # 3, Topic : 8, p : 7.6532 %\n",
      "Topic words : 効果, 砂, 側方, Ａ, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 浄化剤 注入 地下水 浄化 汚染土壌 位置浄化工法 揮発性有機塩素化合物 注入試験 図 地盤 硝酸性窒素 温度 有機塩素化合物 浄化効果 標準砂注入試験 注入パイプ 地下水中 特許文献 水素供与体 実施例 位置浄化 汚染物質 地下水汚染 注入圧力 前記浄化剤 － 発明 実施形態 有機酸 井戸 土壌 地盤温度 範囲 注入量 方法 濃度 請求項 注入対象土 ℃ 浄化剤注入 工法 微生物 対象地盤 加熱温度 注入試験装置 浄化速度 温水 有機化合物 影響範囲 注入用パイプ 注入圧 加熱タンク 試験 圧力 実施 汚染範囲外 注入剤 浄化対象区域 浄化対象エリア 効果 汚染 浄化促進 標準砂 有機炭素濃度 管路 移動性 汚染地域 加熱条件 注入温度 均一性 ℃注入温度 号公報 変化 水素 浄化工法 加熱 分解微生物 地盤加熱 拡散範囲 分解性ポリマー 温度低下 炭素 帯水層 位置浄化方法 前記汚染土壌 ｃｉｓ－ＤＣＥ 汚染箇所 分解速度 注入井戸 砂 ＴＣＥ ＰＣＥ 浄化期間 窒素 揚水井戸 浄化剤そのもの 対象土 酸化剤 ｎ－ヘキサデカン 分解 浄化対象箇所 ｌ浄化剤 グラフ 粘性 ＶＯＣ 概念図 塩素化反応 実験方法実験条件 ＤＣＥ 浄化完了 地下水汚染物質 揮発性有機物質 原位置浄化剤 培養温度 融点 ～ 状態 栄養塩類 リチャージ ＶＯＣ濃度変化 浸透性 ポリ－ 参照 ℃， 拡散方法 拡散 透水性 透過性浄化壁 ＴＯＣ濃度変化 注入ポンプ 分子状水素 移動 水素イオン 操作性 嫌気状態 注入法 注入完了 温水注入 揚水リチャージ 分解性高分子 ＶＯＣ汚染 有機系 加熱方法 特開 試験体 注入材 汚染状況 間隙比 加温 計測井戸 温度変化 薬液注入工法 移動速度 分解抑制剤 土 有機酸塩 水処理 水みち 特徴 ℃～ マイターボックス 嫌気性雰囲気 条件 嫌気性微生物群 地盤条件砂質シルト 実験方法初期濃度 固体水素供与体 ｃｉｓ－ＤＣＥ濃度変化 分解性ポリマ－（炭素 分解活性 説明図 有機物 ｌ 地盤加熱併用 界面活性剤 特開平 ℃範囲 作用 酢酸 記載 浸透状況 微生物活性 試験施工 分解至適温度 温度特性 揚水リチャージ工法 汚染地下水 －ヒドロキシアルカノエート 均一地盤 例 ＣＯＤ濃度 接触効率 地盤条件砂層 ＴＣＥ濃度変化 ＰＣＥ濃度変化 説明 促進 液体 培養方法 影響範囲等 形態 水素置換 上記実施形態 物質 地下水位 シルト質土壌 －ヒドロキシ酪酸 課題 ジクロロエチレン ｃｍ リン 排水管路 技術 揚水 塩 群 固体 近傍 砂層 ℃、 ギ酸 ステアリン酸 カルボン酸 井戸設置コスト ℃、温度上昇 気分解 温水リチャージ 排気管路 液体状 揚水リチャージ循環 硝酸還元菌 対策工法 炭素数 塩化炭素 微生物由来 プレパクトコンクリート用 排水パイプ 除去速度 溶解度変化 中間生成物 レンズ状 液状体 吸引井戸近傍 マンガン酸カリウム 活性向上 短縮化 溶解度 基準値 環境基準値 酸化還元電位 タンパク質 増殖 目的 ＶＣ ビニルクロライド 限界 ｍｐａ ｓ バイオサーファクタント 図面 ℃） コック 上蓋 下 上部 現場 式 圧 溶液 ℃○　 ℃→ 液体状態 粘性土 拡散領域 吸引除去 ＶＯＣ共 有機物量 ＶＯＣ類 移動距離 デンプン系高分子 持続期間そのもの 必要設置本数 孔径φ 吸引 そのもの ＴＯＣ 色素判別法 脂肪族ポリエステル樹脂 作業環境 圧送ポンプ 細孔 接触回数 高圧ポンプ 必須測定項目 透水係数 コンプレッサー等 技術分野 熱伝導 鉄粉 持続 問題点 存在確認 液状 ヒーター 確認 色素 上昇 底盤 投入口 コンプレッサー 程度小石 荒目 割裂注入 電気ヒーター 水平方向 マイクロ波 程度 目 密閉ガラス瓶 静置培養 脱着 イン・シチュ・バイオレメディエーション 下流 オゾン 過酸化水素 毒性 斯 手段 性質 固定 コントロール 面 生物 トリクロロエチレン テトラクロロエチレン ジクロロメタン 周囲 アルコール ポリビニルアルコール Ｍａｎｎｏｓｙｌｅｒｙｔｈｒｉｔｏｌｌｉｐｉｄｓ Ｒｈａｍｎｏｌｉｐｉｄｓ リポペプチド 事前 通常 時点 任意 手順 固化 あたり 利用 マイクロウェーブ 掘削 混入 試薬 基質 ボルト 金網 流出 布 各層 下端 溢出水 色 限度 試料 土質 モールド 算出 Ｋａｒｏｌ Ｐｉｒｓｏｎ 使用 上述 窒菌 窒速度 流速 既知 蒸気 内部 働き 主体 発生 減少 広範囲 関係 ＯＲＰ\n",
      "\n",
      "===== # 4, Topic : 14, p : 8.6066 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ソイルセメント柱列壁 地下壁 芯材 Ｈ型鋼材 合成地下壁 壁厚 鋼材 図 型鋼材 ソイルセメント 実施 面内剪断抵 前記ソイルセメント柱列壁 地下工事 所定本数 面内剪断 形態 壁筋 地下掘削 発明 前記鋼材 ソイルセメント柱列壁芯材 アングル型鋼材 本数 耐震壁 ～図 面内剪断抵抗力 Ｘ字状 両端 スタッドボルト Ｉ型鋼材等 合理化 斜視図 芯材同士 応力負担材 状態 他 正面図 Ｖ字状 鉄筋コンクリート製 壁厚ｔ 部分横断面図 前記実施 ＲＣＳ合成地下壁概要 逆Ｖ字状 説明 複数 要素 構成 ボルト 同士 中間 溶接 有効壁厚ｔ 取り付け本数 表面 必要壁厚ｔ 応力負担用 内側 工事 鋼材表面 短縮 背景技術 構造体 孔 前述 工期 作用 耐震性 有効利用 土留め 特許文献 土水圧 低減 コンクリート 抵抗 種々 破断斜視図 目的 所定本数ごと 図－ 前記目的 前記両端 地盤 構築 課題 コストダウン 剛性 図面 部分 短縮化 技術分野 ブレース構造 日本建築学会大会講演梗概集 逆 上端位置 下端位置 上端部 下端部 せん断実験山浦一郎 連結部材 連結一体化 連結強度 部分断面 研究概要 研究 村田義行 藤原達夫 阪井眞人 中村良雄 金子治 施工条件 強度 剛接合 一般 関係 参照 中国 手段 特徴 一体 要旨 範囲 例 符号\n",
      "\n",
      "===== # 5, Topic : 7, p : 8.5597 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 測定 簡易測定法 コンクリート 簡易測定 塩分 塩分量 硬化コンクリート 詳細測定 塩分量簡易測定器 塩分量簡易測定法 雰囲気温度 塩分量 水 比例定数ａ 温度 塩 抽出 抽出水 図 比例定数 詳細測定法 塩化物量 可溶性塩分 孔粉 回帰式 塩化物 二酸化炭素ガス 塩分量簡易測定 試料 詳細分析法 抽出水温度 発明 フリーデル 二酸化炭素 測定法 ＪＣＩ法 固定化塩分 簡易測定器 簡易塩分測定器 方法 塩分量測定法 混合液 簡易塩分量測定器 簡易測定方法 カンタブ 詳細分析 精製水 係数 塩分量簡易測定方法 可溶性塩分量 試料容器 関係 塩分測定用 塩分量詳細測定法 測定方法 測定精度 ハンマードリル 精度 ～ Ｏ 現場 孔 （社）日本コンクリート工学協会 試料採取 － 塩素イオン 水温 検知部 コンクリート粉 上記 法 比例直線 回帰分析 塩分浸透量 ℃ セメント 特開 混合溶液 構造物 飛来塩分 塩分分布 所定 実験 Ｈ 回帰 汎用性 市販 値 設定条件下 比較 コンクリート粉末 コンクリート微粉 横軸 縦軸 強度コンクリート 分析方法 上澄み水 温度条件 鉄筋腐食 ＪＣＩ 指数関数 詳細値 蒸留水 外気温度 分析過程 説明 課題 特徴 任意 Ａｌ 生成量 前記温度条件下 範囲 混合 手法 前記回帰式 単位セメント量 混合液溶液 上澄み液 試料採取用 ａｘ 生成物 孔程度 ガスボンベ 化学式 ドリル 容器 抽出水温度 コア採取 孔数 相関係数ｒ 試料調整 相関性 調査対象部材 種類 上記特開 イオン 安価 実施 形態 同等 補正 影響 ℃、 両者 効果 ＣａＯ 印 決定係数ｒ ＪＣＩ－ＳＣ 健全性 セメント成分 部材表面 部材断面 換算値 設定条件 危険性 高性能ＡＥ減水剤 程度 直線 単位容積質量 精度向上 太平洋マテリアル株式会社製 規制値 化学組成 範囲程度 調査箇所 換算表 技術分野 基礎データ 技術 ｍ 海岸地域 滴定 特許文献 簡便手法 ミネラルウォーター 所定位置 ビニールテープ 位置 μｍ 電子天秤 商品名 ～図 曝露実験 データ ＯＨ 判定 塩害 発生 一つ 欠点 全量 比率 誤差 目的 手段 充填 一般 下 原則 初期 作業 径 最低 重量 前述 マーキング ＪＩＳ Ｚ 標準 距離 ノギス 黄色 青色 色 各々 比 正比例 ｙ 逆数 使用 送り込み 発泡 バブリング 反応 次 ＣＯ ＣａＣＯ ＣａＣｌ ℃） 各種 図面\n",
      "\n",
      "===== # 6, Topic : 14, p : 8.5406 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 既存スラブ架構 既存スラブ架構Ａ フラットスラブ架構 スラブ架構 補強スラブ スラブ 補強スラブＢ フラットスラブ架構Ｃ スラブ改造 実施形態 前記既存スラブ架構 既存スラブ 図 キャピタル 既存 柱 補強部材 前記フラットスラブ架構 スラブ改造工法 補強スラブＢ部分 炭素繊維シート 特徴構成 工法 請求項 梁 ＰＣ圧着用鉄骨 前記既存スラブ ラーメン構造 既存スラブ架構Ｂ 前記補強スラブ キャピタル用アンカー筋 改造 縦断正面図 部分 フラットスラブ架構Ｄ 補強用アンカー筋 前記補強部材 鉄板 構成 発明 上面 補強スラブＣ 下面 複数 下方 近傍部分 アンカー筋 補強筋 前記キャピタル 図外 型枠 改造対象 前記柱 補強スラブ打設時 スラブ改造工程 フラットスラブ完成 実施 形態 支持部材 支持構造 状態 ＰＣ鋼棒 柱部分 建物 キャピタル用鉄板 天井 天井高 キャピタル用 コンクリート打設用孔 免震装置 前記梁 説明 上面側 平面図 繊維状シート 説明図 炭素繊維 コンクリート 荷重 近傍 構成部品 ワイヤーソー コンクリート打設 残り部分 ～ 先 前記柱部分 作業 改造工法施工 構造 下面側 変更作業 免震装置Ａ 最初 力 ナット 対象 各種 チャンネル状 上下方向 コンクリート打設時 特許文献 設 貫通孔 収縮モルタル 誘導架台 斜視図 前記複数 鉄骨 コンクリート打設後 横断平面図 構築 支持 ロ つぎ キャピタル設置 設置 型作業 型枠代わり 配筋 施工作業 ワイヤーソー駆動装置 切断撤去作業 課題 施工 地震 図面 イ 最後 方向 符号 手順 程度 上端 周囲 厚み 重複説明 床版天端レベル 特許 切断 複数本 矢印方向 溝状 合成樹脂製 号公報 上記公報 能率化 簡素化 床仕上げ 垂直プーリ 水平プーリ 建物Ｄ 切断状態 技術分野 技術 問題点 込口 切削器具 支保手段 外側面 プレストレス 接着剤 欠点 参照 開示 改良 余地 目的 新規 手段 エレベータ 階段 安価 軽量 通常 グリース 穿設 一対 どうし 隙間 強度 支保工 両端 ガイドプーリ 作用 ストッパ 応力 上方 一連 例 逆 金属 板材 種々 改変 コッタ\n",
      "\n",
      "===== # 7, Topic : 14, p : 8.3768 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 軸 撹拌軸 汚染物質浄化用杭 浄化層 浄化 ドラム 前記撹拌軸 軸撹拌翼 浄化剤 特許文献 杭造成装置 外軸撹拌翼 外軸 支持層 地盤 浄化層形成用撹拌翼 固化材 図 杭 特許請求 ドラム吐出口 原地盤 下部吐出口 撹拌用 前記ドラム 号公報 造成工法 特許 有害物質浄化剤 浄化壁 前記浄化剤 浄化方法 撹拌翼 軸方向 汚染物質浄化 駆動装置 地盤改良 有害物質 前記浄化層形成用撹拌翼 周辺地盤 発明 地盤改良用撹拌装置 前記外軸撹拌翼 汚染物質浄化剤 支持力 請求項 外周 土壌 汚染物質 前記 前記内軸撹拌翼 オーガー装置 地下水 装置 断面図 管路 土 駆動軸 前記杭造成装置 撹拌翼構造 浄化能力 参照 範囲 外管ケーシング 前記ドラム吐出口 鉄還元剤 前記支持層 遮蔽壁 金属系還元剤 図～ 回転軸 ＶＯＣ 実施形態 地盤改良装置 噴射撹拌器 高圧用 低圧用 前記下部吐出口 固化材供給用 地盤強度 汚染土壌浄化能力 地盤改良工法 方法 硬化杭 基礎杭 前記ドラム外周 相対撹拌作用 － ｂ 反対方向 粘性土層 前記ドラム内部 特公昭 中心部 石炭灰 ケーシング 構造 地盤強度低下 ～図 説明 下端 汚染除去 原位置土 土質安定材 ～ 駆動力 汚染対象領域 汚染地下水 化合物 無機凝集剤 方向 粘性土 強度回復 注入噴射腕 注入噴射孔 課題 周辺 悪影響 目的 正 外部 特徴 実施 形態 図面 変位 外周部 外側 揮発性有機化合物 断面構造 掘削土壌 捕捉方法 混合物 施工方法 建造物 金属 特開平 円柱状 スラリー状 技術分野 技術 内部 結体 中空ロッド 撹乱オーガ 鋭敏比 混合 硫化水素 問題点 作業エリア 一つ 地中 セメント ベントナイト 主体 内外 相互 ソイルモルタル 経過 アルカリ性 表面積 燐 斯 手段 周囲 記載 セメントスラリー スイベルジョイント 配管 ａ 現場 所定 深度 工程 効果 内側 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 9.2564 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 梁鉄筋 梁 柱 梁接合部 定着 接合部 鉄筋 梁主筋 鉄筋コンクリート造柱 鉄筋定着具 鉄筋コンクリート造 柱鉄筋 鉄筋コンクリート造柱梁接合部 図 鉄筋コンクリート梁 柱主筋 定着ナット 先端部 定着板 Ａ－Ａ断面図 柱梁接合部 配筋構造 補強筋 レベル 鉄筋端部 ねじ鉄筋 梁同士 梁主筋同士 実施例 ｂ コンクリート 部材断面 － 鉄筋コンクリート造建築物 梁鉄筋端部 梁中央 複数 断面図 例 発明 具体的構成 レベル差 平面図 上端筋 下端筋 定着具 垂直断面図 立断面図 太径鉄筋 一端部 接合部耐力 鉄筋集約定着タイプ 複数梁 梁せい 鋼板 配筋 外側鉄筋 ネジフシ鉄筋 ｃ 特徴 鉄筋コンクリート造建物 水平方向 該鉄筋コンクリート造 特開平 号参照 継手金物 施工性 定着板等 定着手段 該ねじ鉄筋 ロックナット リング 建物 該鉄筋コンクリート柱 定着板使用タイプ 定着ナット使用タイプ 該柱鉄筋 該梁鉄筋 上下 外側 鋼板巻き 他端部 該接合部 一直線 継手 種類 貫通孔 柱頭柱脚 加工 階高 建築物 実施 上下端レベル 説明 一方向 内側 効率 外形 多方向 不足 ｄ 設計 加工図 補強鋼板 分解平面図 コンクリート等 ぶりコンクリート 部材 レベル合わせ 垂直方向 一つ 部位 ａ 他方 課題 所定 強度 部分 重量 環境 負荷 図面 参照 外方向 等 透孔 部品種類数 該リング 横側面 横幅 加工ミス 工事管理 設計管理 コーン破壊 コーン状 技術分野 技術 螺合し 部品 荷重増加 手段 付加価値 工事 発生 位置 サイズ 資源 無駄遣い 該梁 干渉 形態 外周 間隔 恐れ 力 同等 球状 効果 一定 天井 空間 体積 向上 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 8.6519 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 上層部 下層部 居住部 共用部 コア部 建築物 立体駐車設備付建築物 立体駐車設備空間 上記コア部 実施例 居住者等 該コア部 図 立体駐車設備 駐車スペース 中央部 天井部 共用廊下 エレベータ 平面図 上層部用エレベータ 高層建築物 乗換階 上記上層部 地下部 該共用部 建築計画 立体駐車場設備 居住空間 地下階等 該下層部 上記立体駐車設備空間 耐火部 耐火天井部 周辺部 居住部空間 縦方向 階段 機械式立体駐車場 所定階 上層階 当該建築物 上記実施例 該耐火部 側面部 地下階 ボイド部 空間 該機械式立体駐車場 駐車場 下層部用エレベータ 該乗換階 該立体駐車設備空間 外周部 該建築物 該上層部 該中央部 上記下層部 外部通路 上記 通風等 上層部用 該共用廊下 平面計画 立断面図 案内通路 該所定階 下層部用 発明 ～ 隅部 当該地下建築物 立体駐車場設備空間 コ字型状 隔壁 位置 等 コア部分 上記共用部 実施 荷物用 耐火隔壁 水平投影面積 居住者 部分 下層部部分 駐車位置 耐火性 中間位置 上記ボイド部 該Ｔ字型コア部 周囲 配置計画 形状 下層階用エレベータ 各階 当該駐車スペース 高層建築物等 該ボイド部 階段等 Ｌ字型状 中心部 該側面部 上記地下階 字 高層 該上層部用エレベータ 床下天井部 住環境スペース 遮音性 上記エレベータ ～図 駐車台数等 地上建築物 居住 中央部側 配置 隔絶状態 平面位置 縦動線 居住環境 該中央部側 採光 境 壁 設備シャフトＳ等 楕円形 居住スペース 居住スペース空間 眺望 当該部分 該駐車スペース 例 住環境 スペース 縦動線用 床下 側 停止階 最下階 Ｔ字 天井部分 上記縦方向 波型形等 Ｔ字型形状 コ字 建 高層化 Ｈ型形 ハート型形 月型形 長方形状 Ｌ字 低層用エレベータ ビル組み込み形式 該エレベータ 特徴 該上 車 出入口 連結 車路 他 該 箇所 奥行き 個々 階数 Ｉ字 切欠ドーナツ型形 変形楕円形等 号公報 該シャフト 居住棟 上記公報 開放空間 移動手段 シャフト 圧迫感 中央部分 該設備シャフトＳ等 ガラス等 外的空間 説明 常用 通り 耐火壁 上記欠点 上記構成 精神的ストレス 円形 昇降式リフト 多角形 外周部分 側面部側 外周 コ 専用エレベータ 面積 上下方向 該耐火壁 当該階層 筒状 外壁面 変形外壁面 別棟 ー 目的 課題 アプローチ 側面周囲 外周長 移動 手段 形式 界壁 変形円形 開放端 四角形状 技術分野 技術 特開平 容積率 階層 居戸数 建設料金 距離 アクセス 換気 程度 内側 増加 形態 地域 該隔壁 建物 範囲 略 三角形 五角形 六角形 等辺 方法 効果 住戸 大型 敷地 図面 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 10.9871 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 設備室 設備機器 設備収容構造 階層 収容空間 支持架台 空間 支持床 建物 階層Ｆ 図 空き空間 空間Ｓ 開口 支持構造体 鉛直方向 階高 複数 複数階層 既存 中途階層 平面図 スラブ 支持フレーム 縦断面図 床片 発明 配電設備 Ｌ 階層Ｆ１ 空気調整装置等 支持床間 前記設備収容構造 配管 収容空間Ｓ 配管用開口 階高ｈ 天井側 設備等 階層分 空気調整設備 配管用 空気調節装置 空気調節設備 各種設備機器 設備室自体 空調設備室 複数種 階層～ 高層建物 部材 ～ 整備用空間 オフィスビル 数 実施形態 階層Ｆ１～ 居住空間 各種空気調整設備等 ｄ 階層Ｌ 使用空間 新規 ａ ｂ 多目的 居住空間等 前記支持架台 階層おき 最下階層 情報通信設備等 層 他 各階 整備用開口 事務所用 整備用開口Ｆ１～Ｆ 免震空間 配電機器 方向 大型コンピュータ 特許文献 各種装置等 等 Ｓ 説明 寸法 目的 図面 間隔 状態 ｃ 荷重 上層階 上下階 ～Ｌ ショッピングセンタ等 斜視図 ショッピングセンタ ショッピングビル等 建物改築方法 複数層 改装 一般 種 改築 データセンタ 課題 程度 実施 形態 支柱 ｈ 図示 事務所ビル 配管群 方法 分 各層Ｌ メッシュ状 格子状 施工方法 技術分野 技術 リニューアル工事 小型化 ～（ｄ 特開 号公報 隅部 金属板 汎用性 任意箇所 固定部材 積載荷重 ２つ 上方 参照 － 欄 地上 場所 細切れ 手段 交換 所定 １つ 剛性 増加 補強 内側 部分 順 効果 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.1428e-43, 1.2232e-25, 1.7574e-13, 4.9742e-21, 1.4348e-15, 2.0493e-25,\n",
      "         2.5634e-31, 7.9302e-01, 1.0941e-12, 6.4969e-19, 3.8446e-32, 3.4077e-22,\n",
      "         4.6158e-13, 2.4690e-21, 2.0698e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0845, 0.0898, 0.0633, 0.0462, 0.0760, 0.0878, 0.0838, 0.0518, 0.0552,\n",
      "         0.0404, 0.1060, 0.0468, 0.0398, 0.0901, 0.0384]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 998.0508\n",
      "Test epoch : 1 Average loss: 1136.8121\n",
      "PP(train) = 2273.350, PP(valid) = 2317.046\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2932e-30, 6.3658e-20, 1.6302e-10, 1.1607e-14, 9.9966e-01, 1.8756e-21,\n",
      "         4.4991e-24, 2.6261e-08, 3.1109e-04, 1.8365e-14, 7.8105e-22, 1.9516e-19,\n",
      "         1.7067e-05, 7.8847e-19, 1.3601e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 997.9292\n",
      "Test epoch : 2 Average loss: 1136.7140\n",
      "PP(train) = 2270.214, PP(valid) = 2315.482\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3779e-35, 1.0490e-24, 2.6144e-04, 6.1327e-19, 1.2284e-08, 2.4887e-27,\n",
      "         1.4813e-24, 3.9758e-06, 2.3176e-14, 2.7880e-17, 2.1999e-25, 6.1465e-14,\n",
      "         3.1328e-13, 8.1685e-18, 9.9973e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 997.7144\n",
      "Test epoch : 3 Average loss: 1136.6070\n",
      "PP(train) = 2266.364, PP(valid) = 2313.707\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7133e-25, 2.1391e-18, 5.0675e-13, 2.2072e-07, 5.5769e-04, 5.4094e-27,\n",
      "         1.1608e-24, 2.4438e-08, 1.7418e-03, 6.7384e-18, 1.8877e-27, 2.2065e-20,\n",
      "         9.6495e-23, 8.4060e-21, 9.9770e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1075, 0.0924, 0.0593,\n",
      "         0.0524, 0.0620, 0.0556, 0.0510, 0.0709, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 997.5023\n",
      "Test epoch : 4 Average loss: 1136.5001\n",
      "PP(train) = 2262.339, PP(valid) = 2312.094\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2795e-28, 1.3830e-15, 1.4235e-11, 3.8441e-10, 3.5612e-09, 1.0554e-29,\n",
      "         1.3072e-34, 9.9784e-01, 7.3181e-05, 2.3930e-25, 2.7684e-22, 3.8612e-30,\n",
      "         1.1518e-03, 4.0931e-27, 9.3007e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1192, 0.0438, 0.0366, 0.0939, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 997.2404\n",
      "Test epoch : 5 Average loss: 1136.3930\n",
      "PP(train) = 2258.205, PP(valid) = 2310.469\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1056e-27, 1.1690e-14, 8.9541e-05, 1.7114e-08, 4.4193e-02, 4.8887e-18,\n",
      "         5.2830e-27, 8.9316e-01, 4.9216e-02, 3.5512e-15, 1.8295e-20, 5.1482e-09,\n",
      "         7.0013e-12, 8.5800e-21, 1.3340e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0805, 0.0997, 0.0543, 0.0453, 0.0767, 0.0906, 0.0784, 0.0481, 0.0536,\n",
      "         0.0404, 0.1147, 0.0460, 0.0381, 0.0945, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 997.0402\n",
      "Test epoch : 6 Average loss: 1136.2861\n",
      "PP(train) = 2254.012, PP(valid) = 2308.813\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7951e-31, 8.4284e-26, 6.8599e-06, 2.8922e-18, 5.2298e-12, 7.5170e-26,\n",
      "         4.2409e-25, 1.3577e-09, 3.9169e-08, 1.5431e-21, 7.0223e-26, 7.9222e-20,\n",
      "         9.9999e-01, 5.7542e-21, 1.4061e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 996.7643\n",
      "Test epoch : 7 Average loss: 1136.1803\n",
      "PP(train) = 2249.649, PP(valid) = 2307.044\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7995e-29, 6.4787e-11, 4.6974e-06, 4.4561e-15, 6.2164e-02, 1.5163e-31,\n",
      "         2.6066e-13, 4.5479e-04, 1.0997e-07, 5.8871e-18, 4.0831e-21, 1.2340e-22,\n",
      "         6.2960e-14, 3.6614e-11, 9.3738e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0791, 0.0448, 0.0991, 0.0496, 0.0622, 0.0671, 0.1080, 0.0937, 0.0586,\n",
      "         0.0545, 0.0626, 0.0561, 0.0509, 0.0706, 0.0432]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 996.6057\n",
      "Test epoch : 8 Average loss: 1136.0704\n",
      "PP(train) = 2245.414, PP(valid) = 2305.322\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5383e-24, 7.3963e-27, 2.0580e-10, 2.0374e-15, 9.0368e-03, 6.6302e-37,\n",
      "         2.0183e-28, 9.6757e-01, 1.0082e-08, 2.5192e-17, 3.9766e-17, 2.3378e-18,\n",
      "         9.5715e-09, 5.2453e-14, 2.3397e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0832, 0.1031, 0.0552, 0.0445, 0.0790, 0.0916, 0.0780, 0.0449, 0.0533,\n",
      "         0.0377, 0.1173, 0.0443, 0.0370, 0.0933, 0.0374]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 996.2220\n",
      "Test epoch : 9 Average loss: 1135.9672\n",
      "PP(train) = 2241.236, PP(valid) = 2303.750\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2120e-30, 4.8275e-22, 3.0362e-13, 1.4385e-11, 4.9327e-13, 3.3743e-34,\n",
      "         1.6946e-31, 4.2236e-06, 1.1178e-11, 4.9534e-26, 1.3873e-18, 1.1803e-31,\n",
      "         4.8895e-10, 5.0649e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 996.0066\n",
      "Test epoch : 10 Average loss: 1135.8639\n",
      "PP(train) = 2237.143, PP(valid) = 2302.205\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0342e-27, 6.1598e-22, 3.0469e-05, 1.8756e-15, 3.2968e-05, 2.4352e-32,\n",
      "         9.4774e-25, 2.9517e-11, 7.2699e-01, 3.5270e-15, 4.9885e-24, 1.8712e-23,\n",
      "         5.9024e-24, 5.0995e-21, 2.7294e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0632, 0.0611, 0.0569, 0.0601, 0.0372, 0.0746, 0.0741, 0.0936, 0.0586,\n",
      "         0.0652, 0.0790, 0.0634, 0.0519, 0.1129, 0.0481]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 995.7587\n",
      "Test epoch : 11 Average loss: 1135.7577\n",
      "PP(train) = 2232.994, PP(valid) = 2300.519\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0486e-27, 9.2580e-11, 4.3641e-10, 2.2344e-19, 1.8890e-11, 3.8755e-34,\n",
      "         5.2146e-22, 1.9080e-10, 1.0000e+00, 1.2474e-30, 8.3431e-13, 2.2528e-20,\n",
      "         3.5162e-08, 8.1012e-18, 6.2673e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 995.6059\n",
      "Test epoch : 12 Average loss: 1135.6562\n",
      "PP(train) = 2228.866, PP(valid) = 2298.858\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8585e-32, 1.0387e-34, 7.9791e-20, 6.7872e-18, 2.5722e-06, 3.4739e-39,\n",
      "         1.5119e-37, 3.7665e-14, 6.0797e-15, 1.7777e-30, 4.0297e-32, 1.2938e-24,\n",
      "         6.4774e-24, 2.4186e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 995.2789\n",
      "Test epoch : 13 Average loss: 1135.5554\n",
      "PP(train) = 2224.813, PP(valid) = 2297.262\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4053e-26, 5.0043e-13, 9.9989e-01, 8.0173e-17, 8.5209e-07, 9.5257e-33,\n",
      "         2.8586e-31, 1.9290e-13, 2.7625e-06, 1.1040e-09, 1.7733e-29, 1.2855e-21,\n",
      "         3.9572e-07, 2.3595e-19, 1.0183e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 995.1043\n",
      "Test epoch : 14 Average loss: 1135.4559\n",
      "PP(train) = 2220.879, PP(valid) = 2295.782\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3720e-25, 1.0955e-09, 1.1557e-06, 2.8490e-16, 5.5718e-11, 2.4142e-25,\n",
      "         2.3092e-24, 4.4419e-08, 2.0862e-12, 1.7216e-23, 2.9207e-22, 1.7212e-23,\n",
      "         2.2263e-06, 2.0757e-15, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 994.8934\n",
      "Test epoch : 15 Average loss: 1135.3581\n",
      "PP(train) = 2216.949, PP(valid) = 2294.293\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5353e-32, 4.2491e-23, 4.1220e-12, 4.4211e-18, 5.9089e-14, 2.5313e-39,\n",
      "         2.2022e-33, 4.3832e-10, 1.8443e-13, 4.3568e-20, 1.7867e-21, 2.0490e-28,\n",
      "         2.2301e-18, 1.5573e-31, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 994.6282\n",
      "Test epoch : 16 Average loss: 1135.2615\n",
      "PP(train) = 2212.989, PP(valid) = 2292.733\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6413e-26, 2.3639e-18, 1.2667e-01, 1.0358e-09, 1.6052e-02, 2.3568e-17,\n",
      "         1.5000e-19, 8.0437e-01, 5.8411e-08, 2.7124e-12, 2.9628e-20, 2.3511e-12,\n",
      "         1.5809e-09, 7.6319e-10, 5.2908e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0827, 0.0990, 0.0554, 0.0479, 0.0799, 0.0843, 0.0795, 0.0501, 0.0581,\n",
      "         0.0411, 0.1067, 0.0457, 0.0395, 0.0923, 0.0379]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 994.4247\n",
      "Test epoch : 17 Average loss: 1135.1622\n",
      "PP(train) = 2209.065, PP(valid) = 2291.122\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3514e-23, 7.4214e-17, 9.8911e-07, 1.4296e-20, 5.2993e-02, 1.2140e-27,\n",
      "         1.0988e-21, 9.2880e-01, 1.3713e-06, 4.1232e-14, 2.1004e-22, 3.0491e-17,\n",
      "         2.0285e-12, 2.7608e-13, 1.8208e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.1005, 0.0547, 0.0444, 0.0802, 0.0909, 0.0794, 0.0468, 0.0532,\n",
      "         0.0394, 0.1154, 0.0451, 0.0375, 0.0922, 0.0388]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 994.2135\n",
      "Test epoch : 18 Average loss: 1135.0680\n",
      "PP(train) = 2205.291, PP(valid) = 2289.724\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1894e-32, 8.3409e-12, 9.8494e-01, 3.6794e-08, 2.4180e-12, 2.3355e-31,\n",
      "         9.9924e-25, 4.7146e-05, 3.1361e-03, 1.9300e-11, 5.3508e-33, 2.9959e-09,\n",
      "         5.4128e-09, 6.9307e-23, 1.1872e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0891, 0.0447, 0.0713, 0.0835, 0.0487, 0.0752, 0.0756, 0.0943,\n",
      "         0.0582, 0.0624, 0.0479, 0.0512, 0.0864, 0.0354]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 993.9140\n",
      "Test epoch : 19 Average loss: 1134.9730\n",
      "PP(train) = 2201.557, PP(valid) = 2288.314\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.6668e-37, 4.7360e-26, 7.9099e-08, 9.3153e-14, 9.1774e-13, 4.5223e-09,\n",
      "         3.3593e-27, 1.0000e+00, 1.0901e-07, 9.7258e-18, 5.2255e-25, 9.7992e-21,\n",
      "         1.0621e-10, 7.0073e-17, 4.7842e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 993.7398\n",
      "Test epoch : 20 Average loss: 1134.8802\n",
      "PP(train) = 2197.697, PP(valid) = 2286.773\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5900e-26, 1.4893e-17, 7.8879e-14, 4.0909e-19, 9.9985e-01, 2.7823e-35,\n",
      "         4.1967e-23, 1.2336e-04, 9.5645e-10, 2.4631e-18, 9.4112e-16, 5.5780e-20,\n",
      "         1.3782e-11, 2.8465e-22, 2.6334e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 993.5150\n",
      "Test epoch : 21 Average loss: 1134.7865\n",
      "PP(train) = 2194.002, PP(valid) = 2285.324\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7481e-27, 8.4397e-22, 1.2546e-15, 7.6935e-15, 8.8361e-01, 3.0535e-36,\n",
      "         4.4674e-17, 1.4561e-02, 5.4601e-02, 3.3924e-27, 2.2237e-26, 6.0107e-20,\n",
      "         2.1662e-09, 5.3072e-08, 4.7228e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0475, 0.0497, 0.0447, 0.0402, 0.0888, 0.0673, 0.1042, 0.1063, 0.0468,\n",
      "         0.0896, 0.0690, 0.0605, 0.0464, 0.0661, 0.0728]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 993.2719\n",
      "Test epoch : 22 Average loss: 1134.6950\n",
      "PP(train) = 2190.387, PP(valid) = 2283.979\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5294e-38, 3.0265e-20, 4.7642e-15, 7.6728e-25, 2.7666e-11, 5.3685e-32,\n",
      "         3.1930e-24, 2.9672e-07, 1.5427e-12, 1.3308e-22, 7.2617e-36, 1.1702e-21,\n",
      "         2.4811e-21, 2.8906e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 993.0488\n",
      "Test epoch : 23 Average loss: 1134.6029\n",
      "PP(train) = 2186.822, PP(valid) = 2282.648\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4106e-35, 9.1496e-24, 2.7500e-11, 7.5662e-23, 8.0171e-13, 3.0937e-31,\n",
      "         1.4190e-41, 1.4814e-11, 2.1342e-09, 7.2116e-20, 1.0062e-36, 3.8767e-36,\n",
      "         1.2235e-14, 2.3478e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 992.8146\n",
      "Test epoch : 24 Average loss: 1134.5129\n",
      "PP(train) = 2183.172, PP(valid) = 2281.259\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6326e-20, 5.0096e-26, 8.6932e-01, 4.9459e-11, 1.7804e-02, 2.6817e-22,\n",
      "         5.5244e-25, 1.1022e-01, 1.6012e-04, 2.6368e-16, 9.5637e-18, 1.3465e-22,\n",
      "         1.0255e-09, 2.5441e-13, 2.4943e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0767, 0.0911, 0.0457, 0.0677, 0.0843, 0.0527, 0.0762, 0.0720, 0.0885,\n",
      "         0.0563, 0.0676, 0.0479, 0.0496, 0.0874, 0.0363]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 992.6335\n",
      "Test epoch : 25 Average loss: 1134.4254\n",
      "PP(train) = 2179.500, PP(valid) = 2279.802\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.6610e-25, 2.7793e-25, 2.1985e-17, 5.7202e-21, 1.9884e-11, 6.9779e-21,\n",
      "         3.1135e-31, 9.9592e-01, 2.3609e-11, 3.5864e-15, 7.8073e-19, 1.7068e-22,\n",
      "         4.2223e-11, 1.2746e-26, 4.0841e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1053, 0.0545, 0.0443, 0.0791, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0370, 0.1191, 0.0439, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 992.4811\n",
      "Test epoch : 26 Average loss: 1134.3379\n",
      "PP(train) = 2175.881, PP(valid) = 2278.383\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1046e-31, 1.5792e-27, 1.2139e-05, 2.2122e-17, 1.3566e-13, 6.3420e-26,\n",
      "         1.1028e-30, 2.0704e-11, 1.0555e-05, 5.6255e-29, 1.8041e-25, 2.5285e-21,\n",
      "         3.2091e-07, 4.5211e-18, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 992.2176\n",
      "Test epoch : 27 Average loss: 1134.2507\n",
      "PP(train) = 2172.445, PP(valid) = 2277.122\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6020e-33, 1.0830e-23, 9.1505e-10, 4.9960e-09, 5.4128e-01, 2.0448e-24,\n",
      "         5.6232e-29, 1.0310e-11, 6.1836e-07, 1.8503e-20, 4.7635e-22, 9.3115e-20,\n",
      "         7.8840e-02, 1.1635e-14, 3.7988e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0579, 0.0492, 0.0637, 0.0449, 0.0769, 0.0676, 0.1037, 0.1017, 0.0532,\n",
      "         0.0769, 0.0649, 0.0591, 0.0496, 0.0669, 0.0639]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 991.9546\n",
      "Test epoch : 28 Average loss: 1134.1684\n",
      "PP(train) = 2169.015, PP(valid) = 2275.913\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0096e-35, 2.4421e-13, 1.8991e-10, 1.0306e-15, 2.2353e-14, 3.1812e-27,\n",
      "         5.3748e-23, 1.1590e-05, 4.8802e-10, 1.0128e-23, 7.1278e-32, 4.3721e-15,\n",
      "         1.2426e-17, 1.7327e-25, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 991.8602\n",
      "Test epoch : 29 Average loss: 1134.0821\n",
      "PP(train) = 2165.552, PP(valid) = 2274.563\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2084e-26, 1.7371e-20, 2.6189e-10, 1.2191e-13, 9.3770e-15, 1.4758e-30,\n",
      "         9.8777e-22, 1.3967e-10, 5.0767e-08, 9.1430e-23, 2.4430e-34, 3.8657e-25,\n",
      "         4.9916e-12, 3.7903e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 991.5947\n",
      "Test epoch : 30 Average loss: 1133.9992\n",
      "PP(train) = 2162.060, PP(valid) = 2273.213\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0678e-30, 2.2534e-26, 3.4267e-20, 2.8202e-24, 6.9593e-30, 1.0986e-29,\n",
      "         3.0837e-28, 6.2517e-16, 9.9999e-01, 6.4446e-25, 1.7059e-28, 4.0562e-26,\n",
      "         9.6750e-11, 1.0571e-19, 9.8365e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 991.4348\n",
      "Test epoch : 31 Average loss: 1133.9174\n",
      "PP(train) = 2158.642, PP(valid) = 2271.938\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9200e-27, 3.3091e-22, 1.3536e-08, 1.7408e-22, 7.7174e-07, 5.7705e-35,\n",
      "         3.1231e-25, 9.9881e-01, 3.4988e-21, 7.2175e-18, 1.4412e-25, 1.7684e-26,\n",
      "         2.1287e-14, 3.2665e-12, 1.1929e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 991.2221\n",
      "Test epoch : 32 Average loss: 1133.8348\n",
      "PP(train) = 2155.347, PP(valid) = 2270.746\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0535e-27, 1.7066e-11, 3.6288e-09, 2.2131e-11, 3.4867e-10, 5.0569e-20,\n",
      "         4.9682e-23, 5.6100e-01, 1.4916e-03, 3.0437e-20, 1.0284e-23, 2.5410e-17,\n",
      "         1.7599e-10, 2.6472e-20, 4.3751e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0848, 0.0741, 0.0741, 0.0480, 0.0718, 0.0822, 0.0913, 0.0622, 0.0572,\n",
      "         0.0442, 0.0919, 0.0499, 0.0434, 0.0852, 0.0398]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 990.9915\n",
      "Test epoch : 33 Average loss: 1133.7538\n",
      "PP(train) = 2151.986, PP(valid) = 2269.483\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1718e-24, 1.9352e-19, 1.9182e-07, 1.1961e-14, 7.8853e-12, 1.2020e-27,\n",
      "         4.4278e-19, 2.1804e-05, 2.2953e-07, 3.0431e-15, 2.0149e-30, 6.5878e-21,\n",
      "         9.4541e-10, 2.2563e-18, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 990.7995\n",
      "Test epoch : 34 Average loss: 1133.6729\n",
      "PP(train) = 2148.683, PP(valid) = 2268.239\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9025e-33, 1.2678e-18, 7.4709e-07, 1.1684e-15, 5.8986e-11, 9.7397e-30,\n",
      "         1.9616e-25, 7.0313e-05, 1.4528e-07, 7.4666e-24, 5.7332e-14, 3.4784e-23,\n",
      "         5.3577e-04, 5.9600e-14, 9.9939e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 990.5526\n",
      "Test epoch : 35 Average loss: 1133.5931\n",
      "PP(train) = 2145.448, PP(valid) = 2267.051\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1733e-28, 2.7563e-14, 4.7039e-08, 1.8397e-14, 6.3731e-05, 8.7537e-28,\n",
      "         4.9385e-23, 9.9483e-01, 7.4152e-07, 5.5231e-15, 6.6751e-13, 3.0418e-15,\n",
      "         3.9600e-03, 1.3105e-23, 1.1406e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1054, 0.0545, 0.0443, 0.0789, 0.0922, 0.0768, 0.0438, 0.0532,\n",
      "         0.0372, 0.1189, 0.0439, 0.0366, 0.0938, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 990.4213\n",
      "Test epoch : 36 Average loss: 1133.5171\n",
      "PP(train) = 2142.205, PP(valid) = 2265.877\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7200e-32, 2.7123e-19, 9.6144e-20, 4.3665e-10, 1.4808e-09, 3.0436e-28,\n",
      "         1.5328e-25, 1.6593e-10, 1.0000e+00, 6.1944e-18, 1.3268e-20, 2.4197e-28,\n",
      "         9.7580e-14, 3.9829e-20, 1.1166e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 990.2613\n",
      "Test epoch : 37 Average loss: 1133.4378\n",
      "PP(train) = 2138.933, PP(valid) = 2264.612\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8652e-18, 7.2786e-05, 3.5001e-02, 4.4218e-04, 1.6550e-04, 9.6228e-30,\n",
      "         5.9719e-14, 3.8166e-07, 8.9960e-01, 6.4715e-02, 4.3006e-08, 4.8537e-10,\n",
      "         7.5805e-08, 9.1234e-15, 5.7760e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0568, 0.0697, 0.0434, 0.0656, 0.0336, 0.0751, 0.0647, 0.0894, 0.0583,\n",
      "         0.0694, 0.0801, 0.0647, 0.0505, 0.1288, 0.0498]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 990.0468\n",
      "Test epoch : 38 Average loss: 1133.3628\n",
      "PP(train) = 2135.696, PP(valid) = 2263.409\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1666e-34, 5.5026e-27, 4.3029e-18, 3.2715e-12, 4.2387e-10, 7.6023e-28,\n",
      "         1.6229e-37, 9.9028e-01, 1.3776e-04, 1.3697e-19, 1.3733e-26, 3.1494e-27,\n",
      "         1.6296e-08, 5.0529e-22, 9.5785e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1048, 0.0547, 0.0443, 0.0790, 0.0921, 0.0772, 0.0440, 0.0532,\n",
      "         0.0371, 0.1187, 0.0440, 0.0367, 0.0938, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 989.7567\n",
      "Test epoch : 39 Average loss: 1133.2844\n",
      "PP(train) = 2132.583, PP(valid) = 2262.284\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9692e-20, 5.5032e-18, 2.6792e-06, 2.4823e-05, 1.5929e-01, 1.9206e-26,\n",
      "         2.4622e-21, 6.5861e-07, 1.0745e-07, 1.6310e-21, 4.2474e-19, 3.2811e-17,\n",
      "         1.3754e-16, 4.8592e-16, 8.4068e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0751, 0.0454, 0.0913, 0.0485, 0.0654, 0.0674, 0.1085, 0.0957, 0.0574,\n",
      "         0.0580, 0.0634, 0.0569, 0.0506, 0.0701, 0.0462]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 989.7041\n",
      "Test epoch : 40 Average loss: 1133.2071\n",
      "PP(train) = 2129.513, PP(valid) = 2261.185\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6353e-34, 4.9021e-32, 2.4927e-06, 1.1726e-09, 8.6531e-03, 6.7859e-27,\n",
      "         9.9349e-34, 8.8111e-13, 4.0244e-04, 3.6370e-16, 2.2391e-13, 2.0128e-28,\n",
      "         1.1984e-08, 1.2813e-18, 9.9094e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0814, 0.0444, 0.1036, 0.0501, 0.0604, 0.0669, 0.1076, 0.0925, 0.0592,\n",
      "         0.0526, 0.0621, 0.0557, 0.0510, 0.0708, 0.0417]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 989.3875\n",
      "Test epoch : 41 Average loss: 1133.1329\n",
      "PP(train) = 2126.390, PP(valid) = 2260.024\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0620e-29, 2.8483e-13, 1.1423e-02, 7.9146e-13, 3.8223e-12, 7.2979e-26,\n",
      "         1.3176e-17, 9.1880e-06, 9.5744e-01, 1.7352e-15, 1.0338e-25, 1.8075e-17,\n",
      "         1.5079e-14, 4.7070e-09, 3.1123e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0572, 0.0669, 0.0456, 0.0627, 0.0315, 0.0756, 0.0643, 0.0920, 0.0576,\n",
      "         0.0688, 0.0838, 0.0648, 0.0512, 0.1287, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 989.2561\n",
      "Test epoch : 42 Average loss: 1133.0620\n",
      "PP(train) = 2123.178, PP(valid) = 2258.816\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.0539e-20, 1.4755e-27, 1.6109e-06, 1.8232e-15, 1.3521e-05, 6.7466e-25,\n",
      "         1.3201e-18, 5.8211e-10, 9.9925e-01, 6.1207e-17, 2.5000e-28, 4.4220e-27,\n",
      "         1.1176e-15, 1.4937e-09, 7.3030e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 989.0853\n",
      "Test epoch : 43 Average loss: 1132.9879\n",
      "PP(train) = 2120.099, PP(valid) = 2257.668\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3647e-20, 8.0250e-15, 2.8307e-03, 5.0904e-12, 6.0630e-11, 3.9502e-24,\n",
      "         1.3689e-18, 3.3539e-10, 3.0142e-02, 1.2447e-12, 1.6324e-20, 8.1005e-19,\n",
      "         2.6492e-03, 1.7350e-20, 9.6438e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0452, 0.1017, 0.0508, 0.0590, 0.0673, 0.1058, 0.0925, 0.0596,\n",
      "         0.0530, 0.0627, 0.0560, 0.0512, 0.0724, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 988.9249\n",
      "Test epoch : 44 Average loss: 1132.9180\n",
      "PP(train) = 2117.170, PP(valid) = 2256.698\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7166e-33, 2.6819e-13, 7.1983e-10, 1.9881e-11, 7.2990e-03, 7.2701e-29,\n",
      "         6.7774e-36, 9.1305e-03, 2.6189e-02, 2.2637e-32, 3.3433e-28, 2.3268e-19,\n",
      "         3.3584e-12, 1.2222e-13, 9.5738e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0454, 0.1011, 0.0505, 0.0596, 0.0676, 0.1061, 0.0921, 0.0593,\n",
      "         0.0529, 0.0631, 0.0559, 0.0510, 0.0724, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 988.6319\n",
      "Test epoch : 45 Average loss: 1132.8452\n",
      "PP(train) = 2114.192, PP(valid) = 2255.617\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3156e-20, 7.6586e-13, 1.0926e-17, 3.3242e-10, 8.9955e-06, 1.0546e-25,\n",
      "         1.4077e-25, 1.8818e-08, 1.6211e-09, 2.9101e-19, 8.0135e-28, 2.9943e-17,\n",
      "         3.3528e-07, 2.1004e-16, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 988.5611\n",
      "Test epoch : 46 Average loss: 1132.7767\n",
      "PP(train) = 2111.170, PP(valid) = 2254.544\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7573e-27, 2.0538e-25, 9.7920e-01, 4.8463e-19, 5.3341e-10, 6.6666e-39,\n",
      "         9.2228e-22, 1.7746e-02, 8.0908e-07, 2.2306e-11, 2.5566e-29, 8.6926e-29,\n",
      "         3.0573e-03, 7.1545e-23, 2.9454e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0761, 0.0902, 0.0445, 0.0710, 0.0838, 0.0491, 0.0749, 0.0747, 0.0940,\n",
      "         0.0579, 0.0630, 0.0477, 0.0509, 0.0866, 0.0355]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 988.3000\n",
      "Test epoch : 47 Average loss: 1132.7062\n",
      "PP(train) = 2108.153, PP(valid) = 2253.411\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5577e-30, 4.5983e-23, 5.8372e-20, 1.4268e-18, 8.9899e-03, 2.7515e-24,\n",
      "         4.3608e-30, 1.1680e-06, 1.4344e-07, 2.5585e-14, 4.7248e-28, 1.1022e-33,\n",
      "         9.9098e-01, 1.2607e-14, 2.6599e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0742, 0.0758, 0.0603, 0.0416, 0.0646, 0.0543, 0.0792, 0.0743,\n",
      "         0.0939, 0.0484, 0.0530, 0.0584, 0.0643, 0.1096]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 988.1459\n",
      "Test epoch : 48 Average loss: 1132.6363\n",
      "PP(train) = 2105.278, PP(valid) = 2252.420\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.3295e-30, 9.8523e-18, 8.8690e-04, 3.3636e-09, 9.9664e-01, 3.3189e-18,\n",
      "         5.8196e-15, 5.0840e-04, 5.8453e-06, 2.0156e-13, 1.2633e-22, 9.9560e-20,\n",
      "         1.9627e-03, 9.7869e-21, 5.8208e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0422, 0.0382, 0.0958, 0.0657, 0.1065, 0.1083, 0.0451,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 988.0146\n",
      "Test epoch : 49 Average loss: 1132.5676\n",
      "PP(train) = 2102.324, PP(valid) = 2251.332\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2298e-30, 1.4341e-22, 8.9634e-23, 4.5155e-22, 4.0341e-13, 6.6782e-32,\n",
      "         1.1770e-31, 5.2071e-12, 6.9789e-04, 7.3368e-21, 5.9052e-35, 1.8135e-30,\n",
      "         5.7119e-14, 6.7297e-19, 9.9930e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 987.7626\n",
      "Test epoch : 50 Average loss: 1132.5039\n",
      "PP(train) = 2099.439, PP(valid) = 2250.376\n",
      "Writing to ./topicwords/15-topwords_e50.txt\n",
      "Topic 0: 角度θ 原子力発電所 水溶性 撮像画像 柱状 実験例 添加 セグメントリング 継手板 電磁弁\n",
      "Topic 1: 角度θ 技術的範囲 柱状 原子力発電所 位置ずれ セグメントリング 添加 電磁弁 撮像画像 実験例\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 要旨 技術的範囲 態様 後述 姿勢 地面 大型 特許請求 構成要素 前記複数\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: 角度θ セグメントリング 原子力発電所 位置ずれ 電磁弁 柱状 撮像画像 継手板 技術的範囲 実験例\n",
      "Topic 6: 角度θ セグメントリング 技術的範囲 カメラ 原子力発電所 撮像画像 柱状 設計通り 継手板 ｘ軸\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 図\n",
      "Topic 8: 効果 砂 側方 Ａ ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: 技術的範囲 角度θ 閾値 原子力発電所 カメラ 通行 セグメントリング 柱状 番号 組み立て\n",
      "Topic 10: 角度θ 原子力発電所 設計通り 実験例 セグメントリング 撮像画像 閾値 位置ずれ 水溶性 技術的範囲\n",
      "Topic 11: 角度θ 原子力発電所 セグメントリング 水溶性 カメラ 撮像画像 技術的範囲 実験例 設計通り 係止\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: 角度θ 技術的範囲 原子力発電所 柱状 位置ずれ 配置例 セグメントリング 設計通り 実験例 撮像画像\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 7, p : 8.4959 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 油圧シリンダー 図 溝開ユニットＵ 縦溝Ｍ デイスク 刃体 サブソイラ 溝 円筒体 縦溝 排水用溝開装置 進行方向 方向 請求項 実施例 盛り土部分Ｄ 溝開装置 圃場表面 ピストンロッド 作業 油圧シリンダケース リンク片 溝開ユニット 左右 前記油圧シリンダー 外部油圧操作レバー 縦軸芯Ｓ廻り 発明 左右方向 外部油圧取出用 左右両側 機体 リンク機構 後端部 溝Ｍ 向き 側 土壌表面 特許文献 角パイプ 後部 排水溝 状態 リフトアーム 回転軸 圃場 水 トラクタ 回動横軸 盛り上がり部分 フォーク部 部分 ミッションケース 皿状 縦溝両側 支持枠 牽引車両 上方 仮想線 左側 縦軸芯 要部 電動シリンダー等 油圧操作レバー 支柱 油圧機器 前記油圧シリンダケース 上下方向 上下 フレーム 往路 構成 右側 Ｓ 油圧ホース 軸芯Ｓ廻り 外部油圧取出機構 排水性 変速装置 盛り土部分 側面図 平面図 断面図 排水用 土 左 右 Ｄ 前記円筒体 板体 上部 複動シリンダー 作業者 回転動力 横方向 横 機体後部 図示外 前記フォーク部 Ｍ 前端部 盛り上がり部Ｄ サブフレーム 特開昭 リフトロッド 説明 排水 土中 手段 後方 特徴 アクチュエータ 記載 輪 バルブ 実線 反対 符号 前記リフトアーム 前記サブソイラ ロワーリンク プレート ピン 複動式 中央部 上面前端部 偏芯カム機構 盛り土Ｄ状態 円板状 機構 トップリンク 土Ｄ 横移動 湾曲面 回転 前記デイスク 外部機器 前記ロワーリンク 前記支柱 前記ピン 振動式サブソイラ 円板プレート サブゾイラ 効果 課題 所望 前輪 ｂ スクレーパ 後端 一体 位置 端 ボルト 背面図 一端側下部 他端側下部 幅広状 字状 サブソイラ単独 機枠 作業姿勢 作業跡 前方 下部 平面 技術分野 背景技術 雨水等 機体前部 図示省略 左前方 湾曲凹部 右前方 稲藁 号公報 バッテリ電源 昇降スイッチ 油路 フロントローダ 前部 凹部 相違点 幅調節 暗渠 直前 雑草 欠点 － 開示 簡易 目的 片側 最良 形態 順 圃場内 周知 エンジン 車体 側部 マスト 参照 軸受 他方 同士 台座 ２つ 角度 様子 単一 作用 矢印 関係 同一 中心 下方 動き a 正規 間隔 上記 一方向 図面\n",
      "\n",
      "===== # 2, Topic : 12, p : 7.9822 %\n",
      "Topic words : 施工, 内側, Ａ, 図面, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : コンクリート 鋼管 コンクリート誘導路 鋼管本体 コンクリート流出口 充填 コンクリート充填用鋼管 コンクリート充填鋼管工法 コンクリート充填 流出口 コンクリート充填方法 請求項 図 該コンクリート誘導路 コンクリート上面 誘導管 コンクリート充填工法 発明 コンクリート流出 コンクリート充填状態 隔壁 前記コンクリート誘導路 充填工法 工法 一定間隔 コンクリート誘導管 管体 誘導路 半円筒 割り筒状 鋼管底部 コンクリート鋼管工法 中空筒状 コンクリート充填用鋼管造 縦向き 鋼管上端 管 円筒状 圧入工法 コンクリート断面 方法 間隔 断面 コンクリート打設高 上端 直径 前記コンクリート流出口 内面 向き 縦方向 充填方法 特徴 揚重機 誘導路用管 断面図 コンクリート圧送 連結手段 前記鋼管 記載 水平方向 上下方向 誘導路開口部 筒状 コンクリート投入口 該誘導管 鋼管本体底部 躯体 ～ コンクリートポンプ 柱等 コンクリート表面 コンクリートバケット コンクリート注入 鋼管下部 鋼管内面 構成 実施例 例 連結材 該鋼管 鉛直状 半円筒状 厚肉部 上方 口部 開口直径 コンクリート圧送ポンプ 平面図 状態 上面 円筒 作業 使用 圧入口 相互拘束効果 ＣＦＴ造 下端 程度 一体 複数 縁 ｆ 上下 底部 補強用 縦断面図 半円筒形 半円筒隔壁 構築物 特許文献 流動性 該誘導路用管 開口部 部 割り状 該鋼管本体 縦 Ａ－Ａ線断面図 Ｂ－Ｂ線断面図 トレミー管 説明 上述 空間 交互 側面 梁 ホース 漏斗状 平板状 千鳥状 方形状 角形状 造 技術 骨材 柱 開口率 長手方向 ～図 斜視図 ブレース接合部 梁連結部分 課題 投入 目的 手段 内部 形態 実施 図面 ホッパー 実験 表 配置 発明方法 細骨材 前記鋼管内 技術分野 背景技術 ＲＣ造 ＳＲＣ造 技術基準 耐力及び変形性能 柱頭部 分布状態 使用状態 ＣＦＴ 効果 耐震性能 耐火性能 都市ハウジング協会発行 軸圧縮耐力 中心部分 構造形式 強度発現 フレキシブルホース ダイアフラム ダイアフラム下 社団法人 設備費 調整 調整作業 外殻 外 位置 最上端 上位置 一般 特性 均質 密 隙間 一体化 施工 設時 分離 品質 空隙 平成 解説 開示 距離 確保 進行 最低 他 工程 遅延 コスト 増大 設 操作 該方法 提供 所期 付近 形成 最良 主体 全長 重量 次 前述 材料 調合 設速度 分 硬化 放射状 対称 他方 側 同上 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 8.8348 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 矩形推進管 袋体 断面トンネル 材 行矩形推進管 注入材 先行矩形推進管 推進管 切削部 掘削機 拡幅部 材切削用拡幅ビット 工体 地盤切削用拡幅ビット 行推進管 滑材 施工 トンネル 前記矩形推進管 施工方法 先行推進管設置工程 行推進管設置工程 矩形断面 設置 断面 矩形 図 推進 袋体注入工程 材注入工程 前記拡幅部 推進管用掘削孔 地盤 土留め 面版 推進管設置工程 注入 行矩形推進管設置工程 行矩形推進管側 該矩形推進管 実施例 部 前記 止水構造 前記袋体 前記袋体注入工程 管体 推進施工 断面図 矩形トンネル 切削部注入工程 用 矩形推進管同士 該先行矩形推進管 切削溝 前記先行矩形推進管 体 揺動カッター 断面トンネル用 列 主材 土留め構造 該切削部 側 断面トンネル施工用 行 複数 断面トンネルａ 外郭 推進工法 面版側部 断面土留め推進管列 薬液注入等 前記切削部 前記切削部注入工程 設トンネル 側部 発明 地下トンネル 推進延長 袋体内 構築 側方 空面 水 上下 トンネル函体 上下方向 進 左端 該推進管 トンネル軸 参照 設トンネルｃ 背面 流体 正面図 止水施工 注入量 状況 背面地盤 説明図 横断面 構築用 地中 止水 地盤側 可能性 材料 該覆工体 工程 繊維材料 地盤改良 改良地盤 説明 外周 状態 掘削溝 前記袋体内 ボックストンネル 右端 掘削範囲 施工列 設置～ 揺動 外郭部 止水性 進施工 セメント系材料 該袋体 双方 下段 方法 揺動軸 該拡幅部 延伸方向 設計注入量 設 鋼製プレート 仮設施工 施工コスト 内部 開削工法 シールド工法 工期 夫 左右 凹部 止水性能 地盤改良ｂ 地盤改良工事 長期化 摩擦抵抗 外郭付近 地下立体交差トンネル 地盤性状 揺動軸回り 山留め壁 特許文献 ａ ホース状 上床版 底版 対策工 該掘削機 課題 工費 上記 中央 方向 後 進軸方向 同士 前記流体 発泡ウレタン等 材料費 発泡ウレタン 水平方向 アラミド繊維 アクリル繊維 地下 障害 交通量 プレート 次 薬液 亀裂 手段 効果 進抵抗 離隔 形成 範囲 ベントナイト 部分 隙間 上端 上段 側部後方 抵抗 正面形状 左端列 右端列 複数列 確実性 セメント 外形形状 位置 外形 技術分野 背景技術 地上交通 多様化 内部プレート 持分範囲 分割範囲 ゴム製材料 配置位置 取り付け位置 外形寸法 収縮モルタル 特開 号公報 問題点 対策 地 所定間隔 地山 所定 余堀 両側 道路 鉄道 下 増加 適用 鉄筋 コンクリート 設施工 概要 撤去 充填 コーキング 三重 － 開示 一つ 高騰 最良 形態 構成 役割 硬化 最後 組合せ 後続 平面 自己 均等 中心 短縮 正方形 層 地質 置換 一端 性質 段 該余掘 経過 厚 表面 側壁 他方 ボルト 図面 他 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 8.9720 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 弾性シール材 杭 補助部材 鋼管杭 杭壁 ガイド部材 接面 図 杭列 請求項 圧入 部材 止水 杭同士 水平断面図 下側 地山 前記弾性シール材 形成方法 発明 式圧入施工機械 土留め 補助部材先端部 形態例 先端 記載 補助部材内部 方向 断面略Ｖ字状 形状 土留め機能 ｂ 先端抵抗 裏側面 鋼管杭同士 土留め壁 端部 チャック部 水 前記補助部材 既設鋼管杭 止水機能 略丸形状 面 断面形状 断面 略多角形状 下部 地面 マスト部 断面略Ｖ字形状 オーガスクリュー 略三角形状 圧入機 略 形態 状態 部分 摺動性 前記杭 水壁 要部断面図 貫通孔 同士 該杭 連続気泡 方法 材 矢板 下端 壁状構造物 透水壁 傾斜面 土砂 隙間 ａ 鉛直断面図 側面 管状 地下水 断面略三角形状 断面略丸形状 前記ガイド部材 断面略Ｖ字形 反対側 連続気泡構造 側 回転圧入 地山内 力 該鋼管杭 接合部 特徴 透水性 断面略四角形状 先端部 ドラム 磁石 上側面 機能 既設 他方 土圧 ２つ ｃ 正面図 抵抗力 既設杭掴持部 凹部 両方 引き抜き 多角形状 地 上記 継手 抵抗 止水処理 グラウト材 地下連続壁等 埋設 課題 鋼材 手段 内部 列 形状等 凸部 溝部 主要部 下端部 特許文献 スライドベース Ｉ形鋼 Ｈ形鋼 鋼矢板 道路擁壁等 Ｉ形鋼等 テープ状 円筒状 曲面状 テーパー状 Ｌ形鋼 説明 間隙 場所 設置 磁力 実施 突条部 押圧面 ローラー 中空構造 継手構造 円弧形 外周形状 管形状 錘形状 等 上記構造物 Ｈ型矢板等 斜視図 当該方法 箱型矢板 前記オーガスクリュー 地盤 ～ 供給 上部 水圧 表面 素材 生地 パイプ クランプ サドル 収納 セメントミルク等 技術分野 背景技術 接合部分 周辺環境 仮設足場 足場 軸方向 特開 号公報 ポリエステル繊維 交通 影響 水上 確保 参照 目的 摩擦 － 開示 コスト 強度 恐れ 水底 斜め 効果 水面 最良 使用 中央 溝部 空洞 材料 スリングベルト 棒状 他 下方 ヒンジ 応力 程度 先 内側 列方向 左右 向き 連結 所定 作業 ケミカルグラウト 図面 符号\n",
      "\n",
      "===== # 5, Topic : 7, p : 8.0516 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 剥落防止部材 工コンクリート 樹脂板 コンクリート 屈曲部 剥落防止構造 トンネル内壁面 工 方向 コンクリート収縮 継目 コンクリート剥落 剥落 コンクリート打設面 面 幅方向前端 トンネル コンクリート打設時 係止 幅方向後端 設コンクリート 長手方向 前記樹脂板 前記剥落防止部材 図 幅方向中央部 コンクリート打設空間 施工方法 周方向 幅方向両端 中央部 継目部分 セントル 前記屈曲部 剥落防止効果 側 コンクリート打設面側 収縮クラック発生 ｂ 樹脂 幅方向両側 請求項 状態 片 面側 トンネル延 密着状態 工コンクリート同士 形状 トンネル左右 前段 字状 前記継目 施工 同士 該屈曲部 前端面 後端 該樹脂板 上記剥落防止部材 発明 次段 補強構造 左右方向 排水溝 コンクリート収縮等 ヘ字状 コンクリート同士 周側 トンネル内周 先端 他方 説明図 補修費用 地山 ＮＡＴＭトンネル トンネル火災 要部縦断面図 コンクリート表面 吹付コンクリート 同一平面 平面 幅長 突出端 トンネル壁面 調査 記載 両側 部材 表出面 仕上げ面 支持力 帯状 掘削 継目間隔 説明 幅 周面 妻板 クラック 取付強度 凹凸 合成樹脂 パネル内側 湧き水 前記係止 施工コスト 支持板 ＮＡＴＭ 取り付けクリアランス等 樹脂製シート 形態 複数 ～ ｂ側 端部同士 斜視図 正面図 収縮性 延 後端部分 表面 トンネル内面 トンネル内壁 押さえ板 燃性 実施 特徴 強度 膨出部 中心 角度範囲 開口 外観 可能性 山岳トンネル等 ロックボルト 左右端側 Ｕ字状 Ｖ字状 Ｔ字状 仕上がり面 汎用重機 特許文献 シート 後付け 取り付け 帯板形状 工用型枠 課題 工面 影響 目的 図面 浮き 補修等 工法 略密着状態 地 ひび割れ部分 パネル等 発生 費用 掘削機等 略矩形状 ひび割れ等 アーチ状 予測部分 欠損部分 先端部分 三角形状 円形状 埋設状態 補修用鋼板 設スパーン 湧き水等 掘削機 型枠 工事 事故 内側 手段 変位 簡易 延焼 設圧力 外側 効果 セット 角度α範囲 地山面 地山側 押出し長尺材 支持脚 検査費用 鋼製可動式 口金開口形状 支保能力 耐腐食性 楕円形状 小片形状 ひび割れ コスト 火災 上記 防水シート 付け 技術分野 背景技術 アンカーボルト パネル自体 止め効果 硬質塩化ビニル シールドマシン シールド工法 前方部位 後方部位 上記状況 目地材 防水層 埋設済み 吹付 支保機能 湧水 ずり搬出 特開平 号公報 水 通行者 三角形 止め 展着 スパーン エアーフェンス New Austrian Tunnelling Method 支保工 断面 亀裂 地質 ムラ 切羽 ダンプトラック 下記 個 台車 － 開示 鉄道 道路 赤外線 音波 漏水 浸食 応力 欠点 対応 穴 車両 該前段 流入 最良 翼状 斜め 後述 例 セグメント 浸水 手順 セントルフォーム 間隙 所定 期間 養生 参照 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 7.7862 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 泥土除去用バケット 底面開口部 底泥 バケット本体 バケット 回転軸 前記泥土除去用バケット 孔 ケーシング オーガー 底面 回転方向 蓋体 前記底面開口部 図 掘削ビット 回転 前記バケット本体 孔底 前記オーガー 除去 開口縁 上面開口部 発明 容器部 スライム 側縁 掘削 前記泥土除去用バケット内部 孔方法 底泥除去作業 回転状態 軸部 完全除去 請求項 端部 状態 方向 排土用スクリュー 前記ケーシング 孔底付近 回転方向他側縁 底泥除去 泥土 泥土除去用バケット内部 地盤 前記回転軸 前記 開口部 孔作業 泥土除去作業 先端 軸端部 ｂ アースドリル工法 泥土排出用バケット ストッパ 孔壁保護 開放状態 方法 該各底面開口部 掘削土砂 内部 孔外 コンクリート ベノト工法 底部 孔底部 工法 前記蓋体 土砂除去 孔内水 方向回転 工事機械 安定液 土砂 該回転軸 ケーシング付きオーガー 工程 バケット本体内部 外径 円筒中心 矢印方向 バケット操作 地上部 用ディープウェル 開放状態図 孔工事 孔壁 スライム除去作業 全量バケット本体 掘削用重機 詳細構造 円筒容器状 閉鎖状態図 扇形 遊嵌状態 相似形 特徴 ａ 作業 掘削作業 ケーシング内径 揚水井戸工事 重機 回転停止状態 口径 複数 一体 前記掘削ビット 廃泥土 リバース工法 施工 中心 先端部 上部中心 発明方法 動作方法 オーガー先端部 ドリルビット コンクリート杭 止め用ストッパ 孔土 孔終了 底面位置 施工順序 補強用ブラケット 課題 水処理用プラント 鉛直孔 水平孔 斜孔 トレミー管 容器内部 コンクリート工事 実施例 アタッチメント式 支持突起 下端部 作業能率 逆側 下部内周部 説明 直径 放射状 記載 ｇ センタライザ 作業効率 掘削用具 ディープウェル スクリュー 取り込み作業終了 円筒状 上面 施工能率 ターンテーブル先端 程度 土質 処理 手間 目的 手段 上記 該蓋体 ロ ハ ニ 同一 入れ替え 泥水 内側 鋼管コンクリート 一対 施工条件 Ｖ字形 施工管理 Ｌ字形ストッパ 筒状 上部外周 上部周縁 連結構造 能率 内径 実施 代表的工法 地盤透水性 技術分野 背景技術 ハンマーグラブ グラブ落下 ストレーナー管 産業廃棄物 地盤振動 浮遊土 開閉操作 サクションポンプ コスト高 スパイラル翼 問題点 外周 添付図面 鋼管 引き抜き 引き抜き段階 鉄筋籠 フィルター砂利 下部 管理 図面 ドリリングバケット 選定 礫 一般 開示 衝撃 大 欠点 組合せ 揚土 こぼれ 部分 イ 所定 深度 到着 効果 現場 一方向 逆転 脱着 使用 最良 形態 チップ 周囲 上方 時点 ｃ ｄ ｅ 引き上げ ｆ 廻り ４つ ２つ 形状 材質 上端 同軸 羽根 容量 昇降 調合 固化 品質 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 7.6117 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 音検査用打撃装置 ハンマー ハンマーシャフト ハンマーヘッド 音 圧縮コイルバネ アクチュエータシャフト 検査対象壁面 可動鉄心 検査 前記ハンマーシャフト ハンマー軸 衝突音 図 特許文献 ソレノイド 作動音 音検査対象物 音検査法 バネ 装置 音検査 励磁用コイル 打撃 コイルバネ DCソレノイド ハンマー駆動 検査対象 検査対象物体 収縮位置 フェルトリング エアシリンダ バネホルダ 検査方法 検査者 圧力センサ フレーム バネ力 機構部 発明 前進端 コンクリート 後退位置 衝撃緩衝材 手持ち型ハンマー 後退端 打撃装置 前記アクチュエータシャフト 可動範囲 ハンマー打音 ストッパー 作動行程解説図 駆動音 音発生性能 上記 Ｏリング 検査法 固定鉄心 ピストンロッド 号公報 打撃力 手持ちハンマー 上記ハンマーシャフト 記載 レバー 側面図 前記圧縮コイルバネ 高圧空気 慣性マス モータ 先端 リング形 後部 後部フレーム 緩衝作用 車載用 軸受け穴 a 発生 初期位置 排気音 音聴取 上記ソレノイド 衝突 前端 圧縮 可動行程 コンクリート構造物 待機位置 衝撃吸収性素材 目的 リング 前部縦壁 先端部 前進 連結部 正面図 前記可動鉄心 穴 特開 反撥力 カム バネオフセット型エアシリンダ 構造 電磁石 一体 後端 ゴムワッシャ 緩衝材 保持角度 空気シリンダ 振動特性 課題 背面 発泡ゴム エアシリンダ側 シリンダ形 構造物 初期収縮位置 後端壁面 後端内壁面 説明 構成 手 作動 b 前方 機械的騒音 力 慣性 前面 前端内壁面 後端部 後部縦壁 壁面下部 面 フェルト 背面図 中立位置 消音構造 手持ち 範囲 モータ駆動式 モータ駆動カム 電源部 正確性 底面図 手持ち作業 前部フレーム 上記課題 上記目的 技術分野 背景技術 剥離 空洞 通電 欠点 下向き 横向き 上向き 種々 作業姿勢 響き 座 円筒 保持姿勢 運動慣性 ばね力 開昭 使用姿勢 背面側 床面 技術的範囲 円盤形 中心穴 初期状態 使用目的 騒音 技術的課題 凹形側面形状 特開平 前面側 実施例 実施形態 動力源 制御弁 出力信号 突出方向 大型 消費電力 大型化 雑音音量 動力 自由度 雑音 形態 方向 中心 肉体的労力 低下 内部 ジャンカ 表面 有無 一定 人力 油圧 空気圧 一方向 解除 押圧 ソレノイドアクチュエータ 開示 ギヤ タイミング オフタイミング 手段 箇所 妨害 効果 両面 比率 能率 最良 低減 向上 ボルト ナイロンブッシュ 円形 動作 下 制約 改変 図面 c d 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 7.9809 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 鋼管柱 誘導管 下端開口 コンクリート 管 下端開口部昇降機構 下端 コンクリート充填鋼管柱 下端部 図 充填 下端開口部 コンクリート充填 上端側 前記鋼管柱 前記下端開口部昇降機構 コンクリート充填作業 該誘導管 請求項 上端部 鋼管柱内充填 鋼管柱底部 状態 線材 下端部単管 部 単管 コンクリート充填方法 揚重機 前記誘導管 柱 下端部開口 上端開口 発明 機構 充填工法 コンクリート打設上面 ウインチ 縦断面図 上記請求項 コンクリート充填工法 取り機構 工法 手動 コンクリート充填装置 上端 下側単管 該鋼管柱 下端補強リング 方法 コンクリート充填鋼管柱造 下端開口部外周 装置 ホッパー 前記下端開口 軸方向 コンクリート打設口 鋼管柱上部 下端側 断面図 下側 充填作業 コンクリート鋼管柱造 ｄ 底部 前記巻き取り機構 最下端 昇降機構 上端補強リング 圧入工法 充填方法 作業 柱上部 口部 コンクリート圧送ポンプ 漏斗状 コンクリート投入用ホッパー コンクリート導入筒 ホース 記載 特徴 コンクリート充填完了 最下端部 取り 柱頭部 上部単管 下端外周 上部 コンクリート打設高 コンクリート打設装置 コンクリート打設方法 コンクリート打設面 コンクリート打設口高 上側単管 巻き取り機構 ＣＦＴ造 縦向き 長手方向 相互拘束効果 鉛直状 ブレース接合部 複数 固定状態 構成 同上 手動式 止め突起 補強リング 下端部内周 コンクリートポンプ 特許文献 実施例 支持部材 トレミー管 上端内周 一端側 コンクリートバケット 説明 設 投入 構造 ～ 上側 造 止め用 部分省略拡大断面図 技術 上端部外面 作業効率 圧入口 ダイアフラム下 ～図 正面図 揚重し 課題 程度 目的 形態 図面 符号 両者 フランジ ｂ 上下 滑車 人力 前述 概略 ＲＣ造 ＳＲＣ造 軸圧縮耐力 技術分野 背景技術 技術基準 手動ウインチ 耐力及び変形性能 ＣＦＴ 効果 実施 電動ウインチ 例 耐震性能 耐火性能 フレキシブルホース 都市ハウジング協会発行 ホース先端 固定的状態 外径 最大径 電動機 構築物 構造形式 強度発現 流動性 ダイアフラム 空隙発生 社団法人 左右両側 ワイヤーロープ 部分 一般 特性 均質 密 隙間 所要 一体化 施工 上方 設時 分離 品質 平成 解説 開示 梁 床 上面 距離 確保 工程 遅延 コスト 増大 提供 手段 上述 所期 中空 他方 管状 調整 目視 最良 形状 自重 外力 一対 側面 上昇 次\n",
      "\n",
      "===== # 9, Topic : 14, p : 8.4835 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : シールドマシン 立坑 流動化処理土 管 水 到達立坑 シールドマシン到達予定位置 図 シールド機 垂直断面 到達方法 ｂ 止弁 スキンプレート 前記到達立坑 面板 前記シールド機 水圧計 到達 到達立坑側 水平断面 到達用 保持材 シールドマシンストッパ 泥水処理設備 泥管 排出処理 工程 排水設備 注入材 前記管 発明 内部 止水 排泥管 流動化処理土等 後方 上端部 地盤 セグメント 排水処理 特許文献 断面 実施 断面図 土水圧 流動化保持材 形態 方法 側 面板カッタ 等 注入 排水処理装置 － ａ 地盤改良 年次学術講演会講演概要集 水圧 泥水圧 到達機構 土砂 圧力 停止 カッタ 前記シールド機内 流れ チャンバ 側壁 ソイルモルタル等 上端 実施例 進行方向後方 前記保持材 姿勢 泥水 前記 前記水 坑壁 掘削土 ソイルモルタル 土木学会 計測値 筒状 底版 支持具 テールシール 位置 説明 ストッパ 材質 方向 後端部 コンクリート等 切羽水圧 自然水圧 図面 進 ヒューム管等 止水機能 上端部付近 下端部 前端部 セラミック管 コンクリート製 後端部付近 ｂ等 地下水 Ｂ－Ｂ Ａ－Ａ 例 地下水等 構築方法 エントランス装置等 土砂等 補足注入 技術分野 背景技術 程度 参照 ｐｐ 課題 工期 目的 ｃ ｄ ｅ ｆ 管内 強度 流入 表面 素材コンクリート壁 梁 ｂ側 特許請求 変更例 修正例 鋼製リング 矢印Ｃ 矢印Ｆ 矢印Ｇ 実施形態 技術的思想 技術的範囲 地山 圧力確保 設置 切羽 切削 連通 進停止 工事費 鏡切り 坑口金物 セラミック 切削物 設置量 添付図面 範囲 開示 費用 手段 特徴 地上 ｇ 水中 効果 最良 部材 既往 下部 周囲 鋼材 アンカ 所定 水位 同等 周 空間 性質 部分 外周 傾き 時点 上部 材料 既設 節減 短縮 全長 業者 範疇 各種 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 9.0156 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 固化体ブロック せん断抵抗壁 球状固化体 構造物 液状化防止構造 既設構造物 鋼管矢板 構造物基礎地盤 芯材 図 地盤 抵抗芯材 基礎 螺旋状壁 コンクリート構造体 基礎版 固化体 基礎構造 方向 注入固化工法 構造物周辺 液状化層 抵抗 特許文献 壁体 構造体 構造 付帯構造物 構造物中心 直下地盤 構造物直下 所定 団子状 固化体直径 三角形状 実施例 前記固化体ブロック 液状化 周辺 状 せん断抵抗性 リング状コンクリート せん断変形 荷重 中心 壁状 石油タンク等 発明 周辺構造物 薬液注入 コンクリート構造 せん断抵抗 リング状 地震 中心位置 柱体状 抵抗部材 前記構造物基礎 地盤改良 前記構造物 略団子状 地盤改良体 液状化対策工 地盤固化改良 断面図 変形抵抗 本体構造物 地盤注入 説明図 形状 直径 タンク 継手管 複合構造壁 液状化対策 説明 タンク基礎直下 注入率 液状化等 略団子状柱体 外周 集合体 シートパイル 該せん断抵抗壁 地盤ブロック 円筒状 構造安定性 球状 注入薬液 位置 砂質土 状態 螺旋 対策工 石油タンク 周辺地盤 半径方向 前記基礎 螺旋状壁自体 号公報 液状化現象 液状化発生 タンク等 配管架台等 略三角形状 中心方向 土質性状 屋外タンク貯蔵所基礎 注入薬液量 改良ブロック 浸透性注入薬液 周囲 柱体形状 屋外タンク貯蔵所基礎等 地盤範囲 芯材頭部 技術 継手 地表面付近 リング 周数 断面 注入量 構築例 せん断変形抑制工法 対策工法 薬液注入速度 注入孔 円筒形石油タンク せん断変形抑制工法等 円形中心 所定間隔 注入速度 周方向 頭部 剛性 屋外タンク貯蔵所 参照 効果 重 公知 ｂ 分布荷重 荷重分布 支持地盤 注入装置 注入作業 荷重作用方向 コンクリート製 中心位置Ｃ 鋼管杭 等 特許 地中杭 複数本 口径鋼管矢板 せん断力 平面円形中心位置 配置方向 鋼製円筒形石油タンク 地盤内部分 工事コスト 間隔 側圧ｑ 注入用多段パッカー 例 止水壁 浸透性 所定位置 改良範囲 連続性 充填材 連続方向 下端 上述 頂点 構成 ａ 対象砂質土 鋼矢板 地表面 外周面 薬液使用量 所定版厚 基準 目的 発生 強度 部分拡大断面図 台形分布荷重 側断面図 安全性 平面円形 困難性 略Ｃ字形断面 幅方向 変位方向 円周方向 長手方向 施工面 弾性シーリング材 配管架台 複数周 地震動作用方向 砂質土地盤 底版外周 シリカ系水溶液型薬液 所定範囲 所定量 最大荷重 平面視円形 水平荷重作用 三角形形状 分布状態 平面配置 トレンチ等 モルタル等 五角形等 エラスタイト等 範囲 継手部 奇数周 多角形 地震動方向成分 シートパイル強度 多段パッカー プレストレス 配置状態 種々 配管 － 課題 土圧 厚み 最良 形態 鉄筋コンクリート製 ～ ストレーナ 複数 所定剛性 挙動 サイズ 対象範囲 技術分野 背景技術 孔本数 先行技術 完成系 プレストレス材料 対象 中間部 地上部 浸透度 微粒子シリカ 特開 特開平 外形形状 最小本数 施工条件 慣性力 形成ピッチ 設ピッチ 規制基準 Ｎ値 沈下 不同沈下 プレストレス 問題点 上記目的 添付図面 m 円環 m程度 地震動 白矢印 なす角度 最大 間隙水圧 位相差 位相 図面 個所 外側 上端 改善 開示 内部 軽減 同等 手段 特徴 目 溶液 バランス ° 角 開口 T 向上 φ 設本数 設計 四角形 線上 丸み 表面 変化 代 設位置 近傍 上部 下部 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2748e-37, 2.1089e-22, 9.9999e-01, 3.0120e-12, 1.0192e-19, 1.0356e-42,\n",
      "         5.1333e-21, 5.3689e-06, 7.7930e-25, 1.2523e-33, 3.1183e-30, 3.1648e-21,\n",
      "         1.3607e-10, 8.0780e-27, 3.5265e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1062.8923\n",
      "Test epoch : 1 Average loss: 1010.7732\n",
      "PP(train) = 2214.410, PP(valid) = 2343.348\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8245e-27, 9.6971e-19, 3.1216e-13, 2.4236e-15, 3.2214e-10, 1.8698e-39,\n",
      "         7.5639e-27, 1.4094e-12, 5.6623e-05, 2.3076e-13, 4.9271e-27, 1.7600e-27,\n",
      "         5.5680e-17, 4.1079e-06, 9.9994e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1062.7227\n",
      "Test epoch : 2 Average loss: 1010.6861\n",
      "PP(train) = 2211.948, PP(valid) = 2342.059\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9463e-19, 4.6839e-08, 2.0694e-06, 1.2906e-10, 1.0415e-16, 3.7333e-12,\n",
      "         6.4474e-15, 2.9678e-03, 2.3223e-10, 6.0483e-19, 1.2609e-19, 7.0968e-15,\n",
      "         1.5879e-05, 1.5913e-10, 9.9701e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0445, 0.1042, 0.0502, 0.0602, 0.0670, 0.1075, 0.0922, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1062.6584\n",
      "Test epoch : 3 Average loss: 1010.5752\n",
      "PP(train) = 2208.284, PP(valid) = 2339.799\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4421e-20, 9.5070e-16, 7.6561e-01, 2.0172e-13, 4.8563e-04, 1.7888e-19,\n",
      "         2.7367e-18, 2.3291e-01, 8.0905e-08, 2.0715e-20, 9.0835e-24, 1.1561e-29,\n",
      "         3.2525e-13, 3.7460e-22, 1.0011e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0786, 0.0944, 0.0470, 0.0648, 0.0839, 0.0570, 0.0763, 0.0672, 0.0839,\n",
      "         0.0530, 0.0734, 0.0474, 0.0479, 0.0892, 0.0361]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1062.3806\n",
      "Test epoch : 4 Average loss: 1010.4614\n",
      "PP(train) = 2204.413, PP(valid) = 2337.529\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6022e-23, 6.3530e-24, 1.6105e-20, 1.6513e-11, 2.5027e-14, 1.0741e-39,\n",
      "         3.7477e-24, 4.2354e-06, 9.9999e-01, 1.1045e-14, 1.3792e-27, 7.2813e-24,\n",
      "         3.8914e-11, 7.9467e-08, 5.3137e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1062.1248\n",
      "Test epoch : 5 Average loss: 1010.3408\n",
      "PP(train) = 2200.902, PP(valid) = 2335.631\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1595e-38, 9.8217e-30, 9.7056e-01, 1.0541e-07, 5.4954e-15, 7.1406e-27,\n",
      "         6.2920e-15, 2.9436e-02, 4.3145e-09, 8.2145e-15, 1.9539e-35, 4.9046e-18,\n",
      "         4.3819e-13, 1.1720e-15, 1.7840e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0763, 0.0905, 0.0446, 0.0707, 0.0840, 0.0494, 0.0750, 0.0743, 0.0935,\n",
      "         0.0575, 0.0636, 0.0477, 0.0507, 0.0868, 0.0354]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1061.8601\n",
      "Test epoch : 6 Average loss: 1010.2240\n",
      "PP(train) = 2197.180, PP(valid) = 2333.641\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.1564e-32, 9.7228e-15, 4.6955e-04, 1.1884e-13, 9.4321e-01, 4.0964e-11,\n",
      "         1.8642e-16, 1.0180e-09, 5.6055e-02, 4.5126e-10, 2.4413e-26, 2.1430e-06,\n",
      "         1.9946e-19, 4.8435e-15, 2.6092e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0456, 0.0491, 0.0425, 0.0394, 0.0905, 0.0666, 0.1041, 0.1080, 0.0459,\n",
      "         0.0929, 0.0684, 0.0607, 0.0461, 0.0651, 0.0753]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1061.7117\n",
      "Test epoch : 7 Average loss: 1010.1020\n",
      "PP(train) = 2193.125, PP(valid) = 2331.278\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1441e-23, 4.9726e-15, 3.4844e-05, 4.0568e-13, 2.5843e-04, 1.4623e-09,\n",
      "         1.1435e-09, 1.5587e-05, 9.9902e-01, 1.2256e-07, 1.8870e-12, 9.6793e-24,\n",
      "         5.9328e-04, 3.0840e-08, 7.8529e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0498]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1061.4506\n",
      "Test epoch : 8 Average loss: 1009.9860\n",
      "PP(train) = 2189.173, PP(valid) = 2329.053\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0720e-29, 1.0457e-17, 2.4766e-19, 4.6345e-30, 9.9837e-01, 2.5995e-35,\n",
      "         3.5072e-33, 3.4716e-04, 6.8046e-19, 1.7322e-26, 3.0285e-28, 2.2586e-26,\n",
      "         2.5944e-13, 4.8733e-21, 1.2873e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1061.1124\n",
      "Test epoch : 9 Average loss: 1009.8656\n",
      "PP(train) = 2185.586, PP(valid) = 2327.089\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.4039e-23, 3.0087e-13, 3.5530e-16, 9.0191e-21, 1.2822e-07, 3.2989e-14,\n",
      "         2.2430e-26, 4.9519e-07, 7.9240e-19, 1.9857e-19, 5.6237e-20, 2.0113e-19,\n",
      "         2.7895e-14, 9.6017e-14, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1060.9455\n",
      "Test epoch : 10 Average loss: 1009.7474\n",
      "PP(train) = 2181.865, PP(valid) = 2325.030\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.3069e-27, 4.8154e-18, 4.0706e-06, 8.3248e-19, 6.4824e-16, 1.0459e-30,\n",
      "         2.9733e-21, 6.9310e-16, 7.3233e-09, 4.3748e-22, 2.1971e-27, 1.4951e-24,\n",
      "         1.2042e-18, 5.0361e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1060.6572\n",
      "Test epoch : 11 Average loss: 1009.6344\n",
      "PP(train) = 2178.112, PP(valid) = 2322.979\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.4604e-40, 1.9735e-16, 1.1837e-11, 6.1382e-11, 1.1887e-28, 3.5418e-33,\n",
      "         4.5407e-21, 1.0000e+00, 8.5924e-17, 1.5717e-29, 9.7259e-27, 1.0348e-34,\n",
      "         2.2029e-18, 5.5192e-28, 3.9857e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1060.4392\n",
      "Test epoch : 12 Average loss: 1009.5187\n",
      "PP(train) = 2174.404, PP(valid) = 2320.883\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0247e-31, 7.3389e-28, 1.1247e-16, 2.2029e-12, 4.6136e-08, 2.3786e-26,\n",
      "         3.3836e-16, 9.9981e-01, 1.9105e-18, 3.0499e-16, 1.3939e-27, 8.8650e-23,\n",
      "         6.1650e-08, 4.0167e-20, 1.8817e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1060.2036\n",
      "Test epoch : 13 Average loss: 1009.4089\n",
      "PP(train) = 2170.905, PP(valid) = 2319.041\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6686e-25, 4.4840e-28, 2.0516e-18, 5.5934e-12, 9.9999e-01, 1.2894e-21,\n",
      "         6.6043e-42, 5.9713e-06, 4.4154e-12, 2.0197e-14, 1.4849e-24, 3.5198e-23,\n",
      "         2.2813e-28, 1.1063e-16, 5.0192e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1059.9741\n",
      "Test epoch : 14 Average loss: 1009.2978\n",
      "PP(train) = 2167.279, PP(valid) = 2317.011\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8490e-21, 2.2323e-26, 8.6950e-01, 4.0715e-13, 2.3063e-12, 6.6279e-25,\n",
      "         1.3897e-25, 8.5993e-12, 4.5856e-18, 9.0034e-15, 4.7707e-25, 8.0776e-22,\n",
      "         4.1409e-16, 1.5165e-34, 1.3050e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0773, 0.0826, 0.0498, 0.0689, 0.0810, 0.0509, 0.0791, 0.0780, 0.0900,\n",
      "         0.0578, 0.0627, 0.0491, 0.0515, 0.0849, 0.0363]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1059.7791\n",
      "Test epoch : 15 Average loss: 1009.1855\n",
      "PP(train) = 2163.726, PP(valid) = 2315.006\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9497e-12, 2.4036e-19, 3.6921e-06, 1.5061e-06, 4.5345e-04, 1.9910e-24,\n",
      "         4.4503e-19, 6.0705e-01, 5.6274e-02, 2.2595e-15, 5.8547e-20, 4.3173e-21,\n",
      "         1.5170e-09, 2.8672e-10, 3.3621e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0831, 0.0789, 0.0686, 0.0483, 0.0702, 0.0841, 0.0873, 0.0601, 0.0568,\n",
      "         0.0442, 0.0964, 0.0498, 0.0428, 0.0893, 0.0401]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1059.5148\n",
      "Test epoch : 16 Average loss: 1009.0765\n",
      "PP(train) = 2160.212, PP(valid) = 2313.052\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4919e-25, 4.9178e-15, 1.2303e-02, 4.6341e-21, 4.5273e-04, 1.0527e-06,\n",
      "         1.2219e-14, 9.0359e-01, 5.9614e-09, 2.5656e-19, 5.7353e-11, 4.2024e-14,\n",
      "         1.3785e-06, 2.3985e-12, 8.3657e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0839, 0.0988, 0.0577, 0.0454, 0.0781, 0.0899, 0.0797, 0.0472, 0.0544,\n",
      "         0.0386, 0.1130, 0.0451, 0.0381, 0.0924, 0.0376]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1059.3451\n",
      "Test epoch : 17 Average loss: 1008.9704\n",
      "PP(train) = 2156.744, PP(valid) = 2311.153\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9661e-34, 3.0390e-16, 2.3335e-15, 2.2449e-18, 2.7065e-13, 4.1163e-24,\n",
      "         1.1981e-31, 5.1592e-08, 1.6450e-07, 1.0520e-22, 5.6916e-29, 3.7033e-33,\n",
      "         2.1091e-10, 3.4978e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1059.0565\n",
      "Test epoch : 18 Average loss: 1008.8641\n",
      "PP(train) = 2153.369, PP(valid) = 2309.337\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.1937e-37, 1.3294e-18, 1.6896e-05, 3.5231e-10, 2.8882e-06, 5.2253e-27,\n",
      "         8.3430e-28, 7.4384e-01, 1.6749e-12, 3.8869e-29, 1.9505e-04, 5.4429e-17,\n",
      "         1.9771e-01, 4.3717e-17, 5.8238e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0770, 0.0965, 0.0622, 0.0489, 0.0705, 0.0869, 0.0753, 0.0528, 0.0589,\n",
      "         0.0467, 0.0989, 0.0475, 0.0421, 0.0883, 0.0475]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1058.8413\n",
      "Test epoch : 19 Average loss: 1008.7601\n",
      "PP(train) = 2150.118, PP(valid) = 2307.627\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.6529e-29, 9.7917e-12, 2.2478e-02, 9.9440e-18, 2.3160e-17, 8.3162e-26,\n",
      "         3.0393e-30, 8.0516e-06, 3.5505e-16, 4.3937e-23, 5.0553e-16, 1.0655e-18,\n",
      "         2.1490e-12, 1.8386e-23, 9.7751e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0451, 0.1025, 0.0507, 0.0606, 0.0665, 0.1068, 0.0921, 0.0601,\n",
      "         0.0525, 0.0621, 0.0555, 0.0511, 0.0713, 0.0413]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1058.6566\n",
      "Test epoch : 20 Average loss: 1008.6604\n",
      "PP(train) = 2146.736, PP(valid) = 2305.782\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8222e-22, 4.9410e-23, 1.8140e-03, 1.0167e-10, 9.9807e-01, 2.9697e-16,\n",
      "         1.1230e-17, 4.0398e-05, 1.6211e-08, 1.4199e-24, 6.7827e-22, 2.0488e-21,\n",
      "         2.8825e-08, 3.7364e-13, 7.8854e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0451,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1058.5194\n",
      "Test epoch : 21 Average loss: 1008.5589\n",
      "PP(train) = 2143.321, PP(valid) = 2303.863\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3057e-39, 1.6007e-20, 4.3682e-18, 1.6107e-21, 8.1326e-09, 2.7013e-33,\n",
      "         6.4741e-39, 2.6544e-17, 1.6996e-07, 1.5987e-18, 3.3940e-16, 1.3069e-18,\n",
      "         4.8302e-12, 5.2198e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1058.1956\n",
      "Test epoch : 22 Average loss: 1008.4588\n",
      "PP(train) = 2140.127, PP(valid) = 2302.179\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0018e-32, 2.2693e-23, 1.8473e-19, 8.7860e-21, 6.2072e-18, 4.9047e-36,\n",
      "         1.5887e-21, 1.0389e-07, 5.7148e-21, 2.5477e-18, 5.9591e-18, 5.4391e-28,\n",
      "         4.1110e-15, 2.2484e-21, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1058.0013\n",
      "Test epoch : 23 Average loss: 1008.3563\n",
      "PP(train) = 2136.950, PP(valid) = 2300.479\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.9483e-27, 1.4245e-21, 3.1602e-05, 6.3414e-15, 7.8350e-15, 3.9596e-22,\n",
      "         2.9279e-30, 4.0269e-09, 3.9267e-13, 3.9130e-14, 2.4638e-25, 6.4244e-25,\n",
      "         9.3744e-09, 5.7305e-15, 9.9997e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1057.8005\n",
      "Test epoch : 24 Average loss: 1008.2592\n",
      "PP(train) = 2133.683, PP(valid) = 2298.686\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3886e-29, 4.7989e-20, 8.8030e-03, 1.8834e-19, 9.9054e-07, 7.6216e-21,\n",
      "         7.6597e-20, 3.2300e-12, 2.7134e-07, 2.8729e-16, 2.3116e-24, 1.3887e-22,\n",
      "         8.2437e-01, 4.5668e-12, 1.6683e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0536, 0.0693, 0.0810, 0.0596, 0.0449, 0.0657, 0.0615, 0.0821, 0.0730,\n",
      "         0.0859, 0.0511, 0.0540, 0.0579, 0.0665, 0.0938]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1057.6187\n",
      "Test epoch : 25 Average loss: 1008.1632\n",
      "PP(train) = 2130.515, PP(valid) = 2296.985\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2951e-17, 3.8987e-16, 5.6457e-10, 6.2652e-24, 1.6938e-16, 2.3403e-19,\n",
      "         1.3928e-17, 2.0238e-02, 1.1961e-15, 7.4570e-30, 2.4824e-25, 1.6479e-17,\n",
      "         1.1040e-06, 1.5354e-25, 9.7976e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0819, 0.0452, 0.1032, 0.0502, 0.0605, 0.0675, 0.1070, 0.0911, 0.0593,\n",
      "         0.0520, 0.0629, 0.0554, 0.0508, 0.0714, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1057.4514\n",
      "Test epoch : 26 Average loss: 1008.0653\n",
      "PP(train) = 2127.372, PP(valid) = 2295.259\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2796e-30, 1.2790e-13, 8.9384e-21, 8.3341e-23, 2.0304e-17, 8.2968e-24,\n",
      "         9.4738e-32, 1.1325e-12, 2.0047e-07, 1.1517e-23, 6.5367e-34, 1.3834e-19,\n",
      "         1.1304e-10, 5.1150e-18, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1057.1732\n",
      "Test epoch : 27 Average loss: 1007.9664\n",
      "PP(train) = 2124.205, PP(valid) = 2293.491\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6159e-26, 1.0902e-32, 3.9829e-23, 6.7264e-27, 1.8691e-22, 4.1576e-29,\n",
      "         6.7167e-28, 7.6590e-11, 1.4451e-09, 8.0846e-23, 1.3045e-36, 4.0752e-20,\n",
      "         1.0000e+00, 7.5484e-15, 4.2391e-17]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1056.9955\n",
      "Test epoch : 28 Average loss: 1007.8739\n",
      "PP(train) = 2121.156, PP(valid) = 2291.894\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4681e-24, 7.0351e-03, 1.0248e-05, 4.7844e-08, 9.6542e-01, 2.3414e-24,\n",
      "         2.8123e-17, 2.4926e-05, 5.2822e-07, 6.4453e-27, 2.8384e-17, 7.2918e-19,\n",
      "         5.8721e-03, 2.5039e-13, 2.1640e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0458, 0.0481, 0.0433, 0.0388, 0.0943, 0.0657, 0.1063, 0.1074, 0.0457,\n",
      "         0.0930, 0.0669, 0.0600, 0.0460, 0.0627, 0.0761]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1056.7953\n",
      "Test epoch : 29 Average loss: 1007.7820\n",
      "PP(train) = 2118.159, PP(valid) = 2290.309\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9995e-33, 2.6343e-17, 1.7020e-23, 4.7667e-20, 1.0000e+00, 2.7975e-31,\n",
      "         1.1814e-33, 5.6442e-09, 4.8378e-16, 1.5628e-17, 6.4717e-19, 1.7053e-20,\n",
      "         1.6363e-16, 2.5777e-12, 8.6435e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1056.6175\n",
      "Test epoch : 30 Average loss: 1007.6945\n",
      "PP(train) = 2115.194, PP(valid) = 2288.795\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0179e-22, 1.6891e-19, 2.4831e-05, 7.7576e-10, 4.5755e-04, 6.9108e-26,\n",
      "         4.0909e-19, 1.6066e-03, 4.5225e-08, 1.5325e-11, 2.9971e-09, 7.1801e-14,\n",
      "         1.3845e-01, 1.1672e-14, 8.5946e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0768, 0.0483, 0.1009, 0.0521, 0.0577, 0.0674, 0.0988, 0.0913, 0.0619,\n",
      "         0.0573, 0.0606, 0.0558, 0.0526, 0.0707, 0.0480]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1056.3725\n",
      "Test epoch : 31 Average loss: 1007.6022\n",
      "PP(train) = 2112.152, PP(valid) = 2287.133\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.8698e-22, 4.5752e-05, 2.0076e-01, 1.4809e-10, 5.3185e-02, 5.5350e-19,\n",
      "         7.1484e-08, 1.2838e-09, 1.7650e-06, 2.8586e-05, 6.3902e-15, 3.9228e-12,\n",
      "         2.1628e-08, 6.5445e-17, 7.4598e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0792, 0.0521, 0.0849, 0.0539, 0.0669, 0.0636, 0.1014, 0.0907, 0.0652,\n",
      "         0.0560, 0.0632, 0.0549, 0.0515, 0.0743, 0.0421]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1056.2672\n",
      "Test epoch : 32 Average loss: 1007.5122\n",
      "PP(train) = 2109.182, PP(valid) = 2285.532\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7455e-18, 2.3795e-22, 1.5343e-08, 4.8152e-11, 7.2497e-10, 1.3600e-21,\n",
      "         1.0942e-23, 1.8251e-09, 1.5533e-14, 2.3081e-21, 2.1133e-26, 5.3537e-18,\n",
      "         7.9013e-08, 2.7805e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1056.0359\n",
      "Test epoch : 33 Average loss: 1007.4240\n",
      "PP(train) = 2106.251, PP(valid) = 2283.972\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3570e-20, 4.8510e-16, 4.8583e-14, 7.7480e-10, 1.2568e-07, 1.4848e-24,\n",
      "         6.2164e-28, 9.9786e-01, 1.0736e-06, 3.8145e-12, 4.4949e-13, 1.4601e-22,\n",
      "         2.1349e-03, 8.6448e-10, 1.7528e-19]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1055, 0.0544, 0.0443, 0.0790, 0.0922, 0.0768, 0.0437, 0.0531,\n",
      "         0.0371, 0.1192, 0.0438, 0.0366, 0.0939, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1055.8892\n",
      "Test epoch : 34 Average loss: 1007.3346\n",
      "PP(train) = 2103.364, PP(valid) = 2282.450\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4483e-34, 2.2875e-20, 2.3496e-09, 7.3323e-04, 4.2338e-07, 6.1495e-09,\n",
      "         7.1113e-29, 5.5097e-01, 1.3469e-03, 4.7415e-14, 3.8931e-16, 1.0375e-14,\n",
      "         2.1779e-05, 1.9694e-14, 4.4693e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0847, 0.0734, 0.0746, 0.0481, 0.0717, 0.0819, 0.0916, 0.0626, 0.0572,\n",
      "         0.0444, 0.0913, 0.0500, 0.0436, 0.0850, 0.0399]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1055.5918\n",
      "Test epoch : 35 Average loss: 1007.2458\n",
      "PP(train) = 2100.511, PP(valid) = 2280.956\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.2291e-25, 1.4955e-12, 6.8807e-18, 1.9032e-15, 8.0305e-11, 4.8993e-27,\n",
      "         4.0836e-21, 9.5138e-01, 2.6429e-09, 1.2796e-18, 6.3905e-17, 1.8301e-15,\n",
      "         4.8365e-02, 9.5023e-26, 2.5204e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.1046, 0.0557, 0.0453, 0.0772, 0.0914, 0.0761, 0.0452, 0.0544,\n",
      "         0.0390, 0.1151, 0.0445, 0.0377, 0.0929, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1055.4694\n",
      "Test epoch : 36 Average loss: 1007.1596\n",
      "PP(train) = 2097.716, PP(valid) = 2279.504\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9646e-13, 1.3738e-14, 9.3096e-05, 1.2381e-13, 1.0317e-09, 1.7109e-23,\n",
      "         3.6931e-27, 2.1290e-14, 3.3508e-07, 2.1658e-05, 1.5362e-18, 4.5494e-18,\n",
      "         5.1277e-13, 2.5606e-04, 9.9963e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1055.3110\n",
      "Test epoch : 37 Average loss: 1007.0749\n",
      "PP(train) = 2094.845, PP(valid) = 2277.938\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2234e-21, 2.0789e-24, 2.4410e-24, 1.5064e-10, 3.0243e-20, 3.0037e-29,\n",
      "         1.0372e-26, 3.1875e-08, 3.3939e-08, 8.1519e-22, 8.5282e-25, 2.4949e-25,\n",
      "         1.6812e-18, 7.0505e-26, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1055.1097\n",
      "Test epoch : 38 Average loss: 1006.9951\n",
      "PP(train) = 2091.992, PP(valid) = 2276.467\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.1438e-25, 1.9186e-24, 1.7882e-11, 3.2861e-10, 2.9073e-11, 2.6896e-16,\n",
      "         5.0333e-24, 2.3065e-01, 1.9534e-13, 9.9680e-16, 2.4862e-21, 1.7671e-26,\n",
      "         4.3391e-10, 8.8339e-17, 7.6935e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0836, 0.0552, 0.0914, 0.0496, 0.0652, 0.0734, 0.1013, 0.0791, 0.0589,\n",
      "         0.0492, 0.0734, 0.0536, 0.0481, 0.0770, 0.0411]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1054.9293\n",
      "Test epoch : 39 Average loss: 1006.9125\n",
      "PP(train) = 2089.278, PP(valid) = 2275.110\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0804e-25, 2.1593e-14, 8.4114e-08, 3.7019e-03, 5.6650e-04, 1.9636e-19,\n",
      "         1.6522e-22, 4.5144e-01, 1.3726e-02, 9.5659e-20, 1.1038e-12, 2.0358e-20,\n",
      "         5.2198e-01, 1.2702e-10, 8.5880e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0648, 0.0902, 0.0678, 0.0548, 0.0578, 0.0793, 0.0665, 0.0631, 0.0663,\n",
      "         0.0638, 0.0767, 0.0508, 0.0492, 0.0804, 0.0686]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1054.7254\n",
      "Test epoch : 40 Average loss: 1006.8315\n",
      "PP(train) = 2086.596, PP(valid) = 2273.745\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.4481e-36, 7.1152e-26, 3.3770e-16, 3.1408e-24, 2.5375e-17, 1.1882e-26,\n",
      "         0.0000e+00, 3.2925e-20, 3.6350e-14, 1.7753e-26, 4.0203e-28, 4.8633e-28,\n",
      "         1.2612e-17, 2.1507e-14, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.996, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1054.4959\n",
      "Test epoch : 41 Average loss: 1006.7502\n",
      "PP(train) = 2083.877, PP(valid) = 2272.320\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8426e-29, 7.2937e-22, 4.4994e-10, 7.5978e-20, 1.6641e-09, 4.3517e-22,\n",
      "         5.8097e-28, 3.5321e-16, 1.3005e-12, 5.0464e-16, 1.0033e-20, 2.0812e-15,\n",
      "         3.2908e-09, 3.1558e-32, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1054.3817\n",
      "Test epoch : 42 Average loss: 1006.6720\n",
      "PP(train) = 2081.200, PP(valid) = 2270.948\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0098e-23, 6.3215e-31, 7.7254e-14, 1.8203e-06, 4.5238e-16, 7.8959e-34,\n",
      "         4.4424e-28, 9.9953e-01, 4.6934e-04, 3.6358e-22, 4.2952e-32, 3.2204e-32,\n",
      "         1.0628e-06, 3.5108e-17, 8.5403e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0443, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1054.1438\n",
      "Test epoch : 43 Average loss: 1006.5937\n",
      "PP(train) = 2078.552, PP(valid) = 2269.613\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5320e-18, 1.4714e-07, 3.7654e-11, 1.8380e-09, 2.5740e-16, 1.2013e-26,\n",
      "         5.9486e-15, 9.9998e-01, 7.1404e-08, 8.0889e-16, 4.3924e-25, 1.2406e-22,\n",
      "         9.2472e-17, 2.1769e-13, 2.3978e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1054.0390\n",
      "Test epoch : 44 Average loss: 1006.5147\n",
      "PP(train) = 2075.909, PP(valid) = 2268.220\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8820e-24, 2.7757e-25, 2.9930e-17, 1.0783e-09, 1.7008e-08, 1.9940e-33,\n",
      "         8.1380e-16, 1.1229e-02, 9.8877e-01, 2.3306e-12, 3.2574e-21, 1.4195e-19,\n",
      "         4.5139e-12, 3.6890e-08, 2.0202e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0564, 0.0678, 0.0444, 0.0627, 0.0307, 0.0762, 0.0631, 0.0912, 0.0570,\n",
      "         0.0689, 0.0850, 0.0649, 0.0509, 0.1310, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1053.8596\n",
      "Test epoch : 45 Average loss: 1006.4371\n",
      "PP(train) = 2073.291, PP(valid) = 2266.879\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7948e-30, 6.9555e-33, 3.8539e-16, 4.6757e-25, 4.7298e-25, 1.4255e-34,\n",
      "         1.4465e-39, 4.7189e-12, 1.0322e-12, 1.2240e-19, 2.0493e-26, 1.1334e-21,\n",
      "         3.9053e-26, 5.7597e-30, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1053.6323\n",
      "Test epoch : 46 Average loss: 1006.3579\n",
      "PP(train) = 2070.697, PP(valid) = 2265.552\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.9413e-32, 5.1807e-26, 2.8022e-10, 1.9119e-18, 9.1368e-12, 2.5120e-23,\n",
      "         5.4709e-24, 1.6765e-10, 1.0000e+00, 6.7305e-17, 8.2469e-33, 8.1216e-24,\n",
      "         2.8252e-22, 1.6263e-20, 1.6680e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1053.4667\n",
      "Test epoch : 47 Average loss: 1006.2850\n",
      "PP(train) = 2068.122, PP(valid) = 2264.271\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2097e-22, 8.3807e-29, 1.0058e-13, 1.2898e-20, 9.7457e-12, 1.6521e-21,\n",
      "         8.6401e-28, 8.9342e-01, 4.6875e-16, 6.0268e-17, 4.4913e-26, 1.3095e-29,\n",
      "         5.9869e-22, 3.0229e-22, 1.0658e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0841, 0.0972, 0.0588, 0.0453, 0.0776, 0.0900, 0.0805, 0.0477, 0.0542,\n",
      "         0.0388, 0.1124, 0.0454, 0.0382, 0.0921, 0.0377]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1053.3200\n",
      "Test epoch : 48 Average loss: 1006.2095\n",
      "PP(train) = 2065.564, PP(valid) = 2262.936\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8895e-30, 8.4495e-24, 9.9547e-01, 2.5852e-12, 1.2535e-12, 1.1349e-34,\n",
      "         6.4076e-08, 4.5286e-03, 1.2154e-11, 3.1657e-19, 8.9288e-18, 5.0060e-12,\n",
      "         1.3693e-17, 1.1260e-16, 6.1390e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0900, 0.0443, 0.0714, 0.0840, 0.0485, 0.0748, 0.0752, 0.0947,\n",
      "         0.0581, 0.0625, 0.0477, 0.0511, 0.0865, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1053.1765\n",
      "Test epoch : 49 Average loss: 1006.1383\n",
      "PP(train) = 2063.098, PP(valid) = 2261.740\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.9795e-24, 1.2007e-13, 8.1134e-10, 5.7046e-18, 2.6594e-10, 4.2785e-19,\n",
      "         2.2196e-25, 7.0971e-01, 2.6852e-18, 2.1213e-17, 3.8565e-23, 1.0464e-21,\n",
      "         2.9029e-01, 5.2560e-09, 2.1471e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0736, 0.0987, 0.0620, 0.0501, 0.0678, 0.0861, 0.0717, 0.0536, 0.0606,\n",
      "         0.0501, 0.0949, 0.0479, 0.0433, 0.0871, 0.0524]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1052.9377\n",
      "Test epoch : 50 Average loss: 1006.0635\n",
      "PP(train) = 2060.524, PP(valid) = 2260.385\n",
      "Writing to ./topicwords/16-topwords_e50.txt\n",
      "Topic 0: 水溶性 角度θ 組み立て 原子力発電所 実験例 継手板 放射線 電磁弁 配置例 係止\n",
      "Topic 1: 組み立て 番号 角度θ 位置ずれ 水溶性 配置例 原子力発電所 電磁弁 実験例 放射線\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 説明 発明 図\n",
      "Topic 3: 大型 特許請求 リング状 組み立て 幅方向 蓋 負担 番号 水和反応 周側\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 図\n",
      "Topic 5: 組み立て 角度θ 位置ずれ 水溶性 原子力発電所 蓋 電磁弁 取り外し 油圧 設計通り\n",
      "Topic 6: 角度θ 組み立て 蓋 配置例 設計通り 原子力発電所 係止 水溶性 継手板 電磁弁\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 説明 発明 図\n",
      "Topic 8: 効果 砂 Ａ 側方 ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: 組み立て 角度θ 番号 原子力発電所 通行 水溶性 油圧 蓋 映像 配置例\n",
      "Topic 10: 組み立て 水溶性 設計通り 角度θ 原子力発電所 実験例 位置ずれ 電磁弁 係止 継手板\n",
      "Topic 11: 水溶性 角度θ 組み立て 配置例 係止 設計通り 原子力発電所 実験例 番号 放射線\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 発明 図\n",
      "Topic 13: 組み立て 角度θ 配置例 位置ずれ 取り外し 水溶性 原子力発電所 設計通り 締め付け 係止\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 8, p : 8.0267 %\n",
      "Topic words : 効果, 砂, Ａ, 側方, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 車輪止め 荷台落下防止柵兼用車輪止め 車輪 荷台 貨物用車両 図 脚部 落下防止用 止め部材 コ字形枠 支柱 発明 荷台落下防止柵 防止 転落防止装置 作業 積載物 請求項 安全性 正面図 合成樹脂製パイプ 台座部 開口部 ネット 柵 安全帯用 トラック Ａ 車両 タイヤ 外周部 実施形態 状態 車 Ｂ 管継手 地面 特許文献 装置 作業等 側面図 人員 記載 辺 結束紐 前記特許文献 柱状部 作業者 輪止め 硬質塩ビ管 部材 軽量 設置 ロープ 転落 特許 形態 建築土木用車両 安全帯 端縁 作業性 クランプ構造 ブロック体 設置状態 墜落防止設備 端部 説明 網 要旨 身体 ｂ 底面図 建築土木現場 輪車 中央部 下記特許文献 同上側面図 安全レベル 建築土木施工現場 現場 等 同上底面図 現場作業所 作業員 軽量性 実施 トラック等 労働安全衛生規則 クレーン車等 必要性 柔軟性 危険性 施工 資材 トレーラ車 搬出 基準 各々 不測 事態 足場 三角柱 楔状 課題 上方 横 図面 ＶＰ サイズ 端 ｃ 位置 穴 綱 タイヤ当り部材 転落事故 労働者 連輪 ｄ等 前記不都合 前記目的 技術分野 背景技術 大型トラック 軽量化 番外側 目的 立設固定 床面積 使用 張設 使用法 ｄ 向上 時 目 ～ 箇所 事情 取り扱い 号 乗用車 開示 滑り 重量 機構 配慮 手段 組 衝撃 圧迫 複数 影響 走行 効果 最良 土台 両側 水平 両端 挿通用 直径 横向き 符号\n",
      "\n",
      "===== # 2, Topic : 14, p : 8.4000 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 接合側 水シート 縁部 帯状袋部 部 接合部 接合部構造 前記帯状袋部 前記接合側 シール材 開放端部 帯状シート 接合方法 側端 項 所定幅 密閉室 前記 平面視中央部 平面視中央部寄り 水性 端 前記シール材 密閉性 請求項 前記切欠きの縁部 前記他方 他方 押出溶着部 密着固定部 接合構造 図 上記 緩衝材 接合作業 押え板 発明 該開放端部 部材 斜線部 板 端縁 記載 同一素材 作用効果 水型護岸 幅Ｗ 水 前記密閉室 コ字状 締結部材 ボルト シート材 平面視コ字状 一定幅 所定幅Ｗｓｔ 一定 形態 角ワッシャー 平面視 穴 位置 構造 特許文献 エアポンプ 等 空間 状態 既設 実施 部分 ナット 直線状 説明 幅Ｗｓｔ 設置幅 溶着 補修 態様 厚み 自然石等 平面視直線状 構成 所定幅Ｗｉ 平面視長方形 ボルト等 水跳ね 所定幅Ｗｉ 所定幅Ｗｏ ポリ塩化ビニル 構造物 傾斜堤 施工現場 構成要素 設置 確認 例 技術 技術的範囲 設置面積 破損部分 方法 前記合成樹脂 内部構造 箇所 符号 図示 ～ 平面図 課題 水性試験方法 熱溶着 項記載 耐腐食性等 効果 管状室 合成ゴム等 同一部分 表面素材 管理型廃棄物処分場等 技術分野 背景技術 上記構成 先行技術文献 止水性 密着 合成樹脂 Ａ－Ａ断面図 ケーソン等 － 新規 各項 最良 該押 裂け目 形状 単体 表面 止水性クッション 堤体 現場条件 一定空気量 上記課題 エアホース 上記任意 上記説明 製 築堤表面 透視図 アスファルト系ブチルテープ 鋼製 コンクリート製 有害物質 特開 号公報 本願発明 図面 添付図面 長方形 左右方向 方向 上端辺 上端 貫通穴 軸力 ラバースポンジ ステンレス合金 減少分 締め込み Ｗｉ 任意 具体的寸法 水域 造成 堤内 堤外 漏出 一般 矢板 砕石 場所 概要 更新 ケース 目的 手段 理解 他 該穴 既存 亀裂 選択 表示 双方 ポリエチレン 処理 ＥＰＤＭ ＰＶＣ ＳＵＳ 荷重 各部 径 Ｍ Ｗｏ 値 次 有無\n",
      "\n",
      "===== # 3, Topic : 14, p : 8.2335 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 中央ブロック 周辺ブロック 分割坑 ワイヤーソー設置孔 分割ブロック ワイヤーソー コンクリート構造物 サイドブロック ダム堤体 縁切り 先進ブロック ボーリング 開口部 連続縁切りボーリング 先進坑 前記先進ブロック 背面側端面 背面側端部 ブロック 実施形態 図 開口方法 前記周辺ブロック 側 バースター孔 ボーリング孔 中央ブロック縁切り工程 作業坑 形状 バースター 拡幅部 角部 バースター孔 前記コンクリート構造物 ボーリング装置 計画断面 当該先進ブロック 中央ブロック排出工程 工程 周辺領域 当該周辺ブロック 縦ボーリング孔 背面側 横ボーリング孔 ａ 方向 拡幅部ブロック 作業 側面 縦断面図 特許文献 引き切り 前記 鉄球 前記先進坑 既設コンクリート構造物 底面 形成 左右 前記分割ブロック側 排出 分割 施工状況 横方向 周辺 既設構造物 上流側 下流側 前記分割ブロック 前面 表面側 奥側 発明 上面 所定形状 ｂ 形成方法 排出作業 施工 バースター 既設 貫通孔 前記ワイヤーソー設置孔 ボーリング径 ケーブル通し穴 上流側端部 ダム 施工性 断面形状等 柱状体 当該ワイヤーソー設置孔 位置 上段 油圧ジャッキ 形態 押し切り 幅方向中間部 周面 説明 正面図 前記ワイヤーソー 当該先進坑 複数 プーリー 予定箇所 状態 開口 断面図 所定 下側角部 実施 設置箇所等 品質施工 施工サイクル 施工期間 前面側 鉛直方向 当該ワイヤーソー 既設コンクリート 精度 簡易 ００７ 中段分割坑 開口縁部 内壁面 前記バースター孔 分割数 前記実施形態 規模 説明図 斜視図 排出方法 手間 背面 方法 バースター装置 領域 面 中心部 隅部 課題 中段 縦方向 規模等 断面矩形 切り 振動 他方 排出状況 隙間 ワイヤーソーマシン 参照 Ａ－Ａ断面図 Ｂ－Ｂ断面図 上段中央 断面台形状 号公報 破砕作業 切り出し 自由度 左右二つ 切断作業 摩擦抵抗 数 切り出し寸法 破砕体 引き切り状況 作業スペース 搬出作業 上方 － 上下 中間 亀裂 施工費 水平方向 特許 当該 配置 前記課題 油圧 任意形状 割方法 縦断面 装置 先行技術文献 上流 油圧割岩機 油圧供給機 技術分野 背景技術 四隅 回数 クリアランス 配置状況 ピッチ 内径 伴 両端 押圧力 符号 下段 上流側面 下流側面 錐台状 表面 幅 径 切断 スリット状 排水路増設 短縮化 切断用 個所数 低減化 特開 部材厚 任意 合計９つ 図示省略 短縮 構成要素 切削 周囲 四辺 概要 手段 特徴 効果 図面 目的 ～ 上側 他 φ 等間隔 程度 近傍 付近 挿入 延長線 外縁 交点 円弧 中点 先 時点 順序 供用 悪影響 前述 趣旨 範囲 変更 バースターヘッド\n",
      "\n",
      "===== # 4, Topic : 10, p : 7.7831 %\n",
      "Topic words : 組み立て, 水溶性, 設計通り, 角度θ, 原子力発電所, 実験例, 位置ずれ, 電磁弁, 係止, 継手板\n",
      "Input : ペースト組成物 強度 強度ペースト組成物 ペースト 張力繊維 質量 セメント 圧縮強度 繊維 減水剤 無機質微粉 骨材 ～ 実施例 細骨材 ｍ 鉱物組成 泡剤 粒径 実施形態 － 質量部 流動性 粒度 μｍ 粒群 比較例 シリカフューム 組成物 繊維径 ブレーン比表面積 鋼繊維 スランプフロー試験 Ａ 合計量 発明 高性能減水剤 Ａ量 特許文献 金属繊維 平均粒子径 混合物 水 試験 圧縮強度試験 単位量 Ｓ量 強度材料 量 微粉 構造単位 分散性 繊維長 コンクリート Ｃ ｇ 残分 強度試験 図 配合量 配合組成 未満 珪石粉 アスペクト比 モル比 ファイバーボール ポリカルボン酸系 強度化 材料 表 試験方法 有機繊維 炭素繊維 常温養生 石灰石粉 スランプフロー 粒子径－通過分積算 砕石粉 水セメント比 分散状態 繊維分散性 材齢 上記 骨材等 粒度分布 スラグ粉等 Ｎ ｍｍ 外割り 状態 剤 圧縮強度試験方法 熱養生 型枠 粒子径 炭化水素基 通過分積算 イオン配合型界面活性剤 体積 ｃｍ 石灰石骨材 繊維材料 強度繊維補強コンクリート 細骨材等 じん性 ミキサ ）－（ スラグ 測定 メチル基 試験材齢 積分値 ＡＦ量 目標圧縮強度 高性能ＡＥ減水剤 ポリカルボン酸系高性能減水剤 アラミド繊維 スランプフロー試験方法 特開 号公報 群 密度 ＪＩＳ 商品名 ＰＯＰ 高性能ＡＥ減水剤等 キルン 強度試験用供試体 Ｈ－ＮＭＲスペクトル ステンレス繊維 ガラス繊維 アラミド繊維等 Ｓ 上記高張力繊維 基準 写真 当たり Ｏ ペースト用ミキサ 石灰石 粉末度試験方法 スラグ粉 粒子径分布 施工性 珪石 値 上記無機質微粉 強度発現性状 上記実施形態 ひび割れ抑制機能 建設発生土 ナフタレンスルホン酸系 アミノスルホン酸系 現場施工 膨張材 アモルファス合金繊維等 下記実施例 程度 参照 早期 傾向 ＰＯＥ シグナル 粒子屈折率 ＬＡ－ 物理試験方法 セメント協会標準試験方法 含有量 石灰石微粉 特定粒 上記セメント 硬化体 形態 石炭灰 アルキル鎖 流動性確保 じん性セメント系材料 収縮低減剤 補強等 鉄筋量 凝結促進剤 凝結遅延剤 散乱式粒子径分布測定装置 ボールミル等 矢印Ａ 自由度 シリカフューム含有量 粗粒率 課題 技術 ×Ｆｅ リグニン系 ポリエーテル系等 単位水量 パン型ミキサ 鉄筋使用量 Ｋ－ μｍ網 ＮＭＲ測定装置 養生装置 粉砕 試料 目標スランプ 型枠面内側 配合割合 供試体 軸強制練りミキサ ポゾラン質微粉 ＳＰキルン等 疎水性シリカ 耐久性パネル 使用材料 鋼製 製造方法 固形分濃度 形状等 ポリマーディスパージョン等 グラウトミキサ等 橋梁等 高炉ダスト等 施工 高炉スラグ 先行技術文献 ヘキサメタリン酸ナトリウム水溶液 込型ゲージ 銅ガラミ等 電融ジルコニア等 化学分析方法 上記シリカフューム 測定終了 試料分散媒 軸強制練りミキサ 円柱供試体 溶媒屈折率 電気炉酸化スラグ 技術分野 背景技術 説明 標準 通常 範囲 製造 原料 実機キルン 石膏 所定 ＮＳＰキルン ダスト 通り 砕石 Ｒ ×ＳｉＯ ×Ａｌ 曲線 Ｗ 作製 評価 部分 コンクリート工学年次論文集 構造部材 東京製綱株式会社製 器研究所製 コンクリート製品 金属シリコン 晶質 ボーグ式 堀場製作所製 ＢＲＵＫＥＲ製 上水道水 合成樹脂粉末 下限値 吸水率 封緘状態 目標 硬化 Ｍｉｅ散乱理論 軽量化 製造箇所 製造品 自己収縮 ペーストスラリー ブロック耐震壁 静弾性係数 化学成分 フレッシュ性状 形状 発明者 特定 焼成自体 下限 ｇ未満 粉砕機 アルカリ溶液 ポリアルキレン誘導体 ＰＣ梁 橋梁 下記 ×ＣａＯ）－（ レーザー回折 テフロンシート 実験的検討 削減 要求 記載 条件 Ｖｏｌ ｐｐ 概要 既存 運搬 制約 設計 目的 手段 効果 図面 上限 粘性 操作 調合 クリンカー フェロシリコン 排ガス 副産物 主成分 ＳｉＯ 観点 均一 種類 増 ポリマーエマルジョン 砂利 補修 内容 準備 ポルトランドセメント ＳＯ ＪＣＡＳ ＳＦ Ｖ２ 出力 ホモジナイザー 演算 ｉ 砕砂 珪砂 モース 参考 メタノール ＡＶＡＮＣＥ ポリオキシプロピレン ポリオキシエチレン 付近 ＣＷ 目視 作り方 中心 温度 スチレンボード 拘束 ｂ ａ 塊 ファーバーボール 偏在\n",
      "\n",
      "===== # 5, Topic : 14, p : 9.0134 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 箱形パイプ部材 箱型パイプルーフ 函形地中構造体 箱形パイプルーフ 地中 単位パイプ部材 到達立坑 調整パイプ部材 前記箱形パイプ部材 前記箱型パイプルーフ 発進立坑 矩形断面 側部調整パイプ部材 側辺部 先端部 パイプルーフ 側 上辺部 構築方法 前記函形地中構造体 先端 実施形態 底辺部 図 内側 箱形パイプ部 土砂 発進立坑側 ｂ 前記調整パイプ部材 函形パイプルーフ 前記内側 前記単位パイプ部材 前記側部調整パイプ部材 地下道 ｃ スパン 端面 後端部分 後端部 到達立坑側 当該箱型パイプルーフ 周方向 特許文献 方法 列 推進 押し出し方向 土留壁 略示縦断面図 土留部材 発明 断面形状 外周面 継手部 状態 ａ 安息角θ 押しジャッキ 該箱形パイプ部材 圧入方向 勾配線Ｌ 安息角 フリクションカット部材 切羽面 分 該箱型パイプルーフ 勾配線 外周形状 断面 力壁 スパン分 正方形断面 先端側 方向Ｘ 上辺部内側 推進機 幅 地盤 側方 部分 参照 押し出し方向Ｘ 当該先端 作業 工程 程度 推進作業 発進架台 下方地盤 下方 技術 解体作業 撤去作業 鉄道 規模 下部 掘削排除 都市部 形態 道路 外側 フリクションカッタープレート 内部 ｂ内側 構成部材 圧入方向Ｘ 安息角Ｌ 長手方向 後端 コンクリート製 施工法 上記実施形態 掘削作業 ジャッキ 号公報 横断面図 ～（ｃ 上部交通 ストラット 面積化 筒体 施工方法 説明 既存 上方 特許 ～ 公知 当該スパン Ｘ 押し出し 略正方形 略示正面図 略示側面図 ジャッキ等 作業空間 先行技術文献 θ 内部掘削機構 土留鋼矢板 技術分野 背景技術 工法 影響 工期 課題 次 上述 所定 土質 両側 側面 推進設備 土砂Ｘ 中空内部 プレキャストコンクリート製 上記目的 押出機構 オーガ等 相当程度 並列状態 特開平 目的 構成 図示省略 既設 支保工 効率 記載 手間 － 概要 敷地 コスト 削減 開発 手段 効果 図面 鉤状 一体 装置 周囲 前方 短縮 最後 種々 変更 状況 符号\n",
      "\n",
      "===== # 6, Topic : 2, p : 8.2298 %\n",
      "Topic words : 参照, 位置, 配置, 構造, 技術分野, 形態, 手段, 説明, 発明, 図\n",
      "Input : 軸部 孔部 掘削爪 棒状弾性体 掘削工具 掘削部 前記軸部 溝 ホルダ 前記孔部 前記掘削爪 支持孔 外周面 周面 掘削チップ 図 係合部 チップ保持部 弾性力 掘削孔 挿入方向 発明 テーパ 実施形態 止め力 断面形状 方向 掘削爪軸部 特許文献 形状 弾性変形 軸方向 後端側 刃部 隆起部 前記棒状弾性体 地盤 孔部Ａ 挿入方向Ａ 前記ホルダ 掘削爪脱着 止めリング 縦断面図 ｂ 請求項 基部 状態 ホルダ支持孔 回転軸 円形 樹脂リング 挿入口 先細り 損傷 効果 先端側 摩擦力 下端部 羽部 前端側 脱着 略半身 位置決め溝部 課題 記載 樹脂リング装着部 前記溝 断面図 特開 号公報 製作手間 周 通り挿入方向 略垂直方向 先端面 説明 オーガヘッド 隙間 土砂 技術 該軸部 構成 長手方向 文献 形態 結体 保持構造 内壁面 縦溝 略 上記溝 先行技術文献 後端 圧縮力 技術分野 背景技術 略円柱状 略一定間隔 アースオーガ 羽部 形成 労力 先端 内面 － 該孔部 奥 双方 特徴 図面 一方向 他方 合計 装着 片側 扁平形状 斜視図 側面図 ～図 上記テーパ 垂直抗力 樹脂等 テーパ付き 所定間隔 長円状 楕円形 多角形 中央部分 機械加工 タングステンカーバイド 中空円形 構造 装置 オーガスクリュー 複数 参照 ａ 柱状 磨耗 交換 経済 概要 混入 反面 コスト 手段 例 コバルト 一端 端 角度 離脱 金属 ゴム Ｘ 一つ 両側 あて 材質 本数 重量 逆 部位 押し込み 符号\n",
      "\n",
      "===== # 7, Topic : 14, p : 7.5649 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : カートリッジ フィルター 漏水防止用 タンク 図 パッキング材 濾過ユニット 側面用漏水防止材 仕切板 端部用漏水防止材 漏水防止鉄板 請求項 下段カートリッジ 網体 上段カートリッジ タンク固定ボルト 濁水 処理 乾燥植物体 前記カートリッジ 固定ボルト 固定金具 濾過材料 発明 漏水防止材 枠体 弾性部材 フィルター有効面積 記載 脚 正面図 注入口 吐出口 周囲 スポンジ等 側面 実施形態 端部 フィルター材 内部 上下 内面 付け金具 円柱状 上端部 設置 側面図 特許文献 水平調整用 特徴 沈殿物 変形例 側 下端部 外周面 フィルター設置用 上端部外側面 濾過材 低減防止 タンク内面 設置状態 成形体 池 低減 鉄板 濁水処理池用 縦断面図 汚泥 前記固定ボルト ゴム 下 形態 外周側 前記仕切板 前記網体 上下面 課題 交換 接合部 底部 周側 有効面積 濁水処理ウォータータンク 濁水処理池 前記フィルター ボルト カートリッジ方式 前記カートリッジ内部 下段 該濁水処理池 説明 ネット 側部 構成 並列 台座 凹部 斜面 上段 ポーラス材等 濁水処理対策 下部 上部 位置 鉄網 濁水浄化処理効果 ノッチタンク 設置位置 鉛直設置 下部外側面 拡大図 分解図 平面図 流入口 排水口 特許 先行技術文献 沈砂池設置用地 技術分野 背景技術 形状等 細部構造等 尺状 ロープ 支持部材 複数 手間 ａ ｂ 図示 円筒状 バイオログナチュラルフィルター 外側面 両側 通過面 水圧 影響 段数 水槽 ループ状 繊條 熱可塑性合成樹脂 効果 下端 水効果 汚泥付着 汚泥回収 土砂工事 外面 号公報 特殊加工 下面周囲 外面上部 バイオログ 登録商標 鋼製 下面 自然沈降 接点 列数 沈砂池 掘削許可 対応 都市土木 対応流量 モールコード 相互接点 方向 該複数 概要 鋼材 溶接 既存 手段 図面 同質 下方 一対 重ね 三面 縁 お互い 洗浄 ローテーション ごと 向き 流れ 密度 一定 水深 均等 粒子 負担 持続 重力 底面 他 繊維 ノズル 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 7.9460 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 液分離槽 ドラム本体 回転式ドラムフィルタ ドラム 処理水 処理 濾過部材 洗浄水 フロック 処理水中 洗浄水回収部材 洗浄装置 洗浄 図 実施形態 濁水処理装置 水 沈降槽 放流水 濁水 放流槽 槽 排出樋 固体分 フロック排出口 下流側 上流側 濁水処理 液分離 邪魔板 装置 構成 液分離法 ドラム状 屋根板 内面側 発明 筒状枠体 液分離装置 目 エアー 回転軸 水位センサ 処理水槽 脱水処理 開閉装置 洗浄処理 沈降槽等 断面模式図 正面模式図 上面模式図 外周面 水位 槽外 回転 分離精度 技術 金網 制御装置 凝集材 流下方向 処理量 処理現場 凝集材投入装置 水質 放流先 当該ドラム本体 整流板 攪拌槽 沈降性 水質基準 凝集沈殿物 処理水量 汚濁水 連通管 底部 洗浄水量 濁水処理技術 部材 処理量等 スクリーン 角錐状 円錐状 濾過障害 処理対象 処理目的 処理済 処理目標 処理コスト 下端部 該筒状枠体 特許文献 せん断力 粒径 外面側 状態 量 濾過材 清浄水 金網等 機構 ２つ 有機性汚泥 斜視図 ～図 食品処理施設 一端部 他端部 水平方向 形態 一般廃棄物処理施設等 放流水中 内側 内部 外側 上部 下方 １つ 目詰まり 目開き 水道水 清浄水 潜り堰 緑化基盤 沈降 水槽 高分子凝集材 傾斜下端側 洗浄効果 間欠洗浄 耐久性等 説明 方法 理由 ＳＳ 環境基準等 無機系凝集材 水頭差 工事現場等 現場 目的 空気 所定 三極センサ等 洗浄用圧縮空気 産業廃棄物 傾斜下端 繊維等 材料等 Ｍｅｓｈ等 流量等 清水等 攪拌槽等 工事現場 液体分 固形分 先行技術文献 技術分野 背景技術 沈殿池 夾雑物 水質環境 土粒子 駆動軸 環境 状況 工事 同等 シックナー 重力 概要 課題 力 下 複数 効率 図面 外観 所定量 斜め ｐＨ 制御 フロート マイナス 土粒子群 ～ エアー抜き機構 工事敷地 駆動電力 スイッチ機構 貯留容積 特開 号公報 特徴点 特徴 清澄度 効果 半開き状態 材料 乱流 符号Ｆ 中性域 電気代 スイッチ 三極 検知信号 符号 Ｆ 前段 支障 悪影響 河川 湖沼 海域 単独 単位 あたり 参照 ウェッジワイヤ － 種々 一つ 影響 速度 他 粘性 砂 石 ビニール 事情 手段 下部 メンテナンス 噴出 気 成長 蛇口 モータ 外部 位置 数 性状 ３つ ４つ 観点 濾布等 ポンプ 頻度 遷移 上下動 電極 プラス 上方 共通\n",
      "\n",
      "===== # 9, Topic : 4, p : 7.6753 %\n",
      "Topic words : 上方, ｃ, 発明, 参照, 力, 技術分野, 形態, 手段, 説明, 図\n",
      "Input : 廃棄物 車輌運搬ケージ 地下貯留構造物 貯留物 リフト固定ガイド 地下貯留施設 連絡通路 昇降架台 図 車輌出入り用枠部 貯留 車輌 クレーン 外周側壁 処分場 廃棄物処分場 昇降用ガイドレール 地表部 クレーンレール 運搬車輌 請求項 スロープ 埋立 発明 埋立レベル 埋立地盤 躯体 実施形態 内側 斜路 車輌乗り入れ用 車輌運搬ケージ側部 廃棄物貯留 係合部材 廃棄物埋立レベル 底面 廃棄物運搬車輌 前記 込み式 電動式 昇降設備 通路 作業 昇降用ガイド 埋立作業 車輌運搬リフト 平面図 レベル 地下貯留構造物躯体 前記連絡通路 前記車輌運搬ケージ 組立構造 記載 鉄筋コンクリート壁 側面図 躯体壁 コンクリート壁 土砂扱い量 内側面 処分場内空寸法 車輌運搬ケージ着地 前記昇降用ガイドレール 廃棄物埋立設備 該車輌運搬ケージ ガイドレール 鉄筋コンクリート造 コンクリート 内側底面 盛替え 凹所 容量 廃棄物埋立方法 貯留容量 凹所内 リフト クレーンワイヤー 廃棄処分作業 該車輌出入り用枠部 有価物 面 傾斜道路 建設資材 用地面積 形態 ゲート 車輌昇降リフト導入 特許文献 処分場底面 前記凹所内 形式 盛替え手順 作業機械 外周 埋立面 下 要旨 ダンプトラック 埋立地盤面 仕切壁 前記クレーン トラス構造 鉄筋コンクリート造り 鋼製 ガイド鋼材 横支持材 頻度 スロープ連絡部 前記図 正面図 量 内方 縮減 上昇 傾斜 ｂ 埋立作業関連 上記凹所内 コンクリート斜路 下部昇降架台 使用対象 地表 地盤面 斜路走行 壁 前記外周則壁 該クレーンレール 地表面 コンクリート製通路 前記特許文献 斜路スペース 同上正面図 説明 地面 掘削 鉄筋 構築 実施 同上平面図 縦断正面図 土砂 部分 下部 設置替え 躯体外方 躯体内方 盛替え頻度 図中１ 走行面 クレーン機能付きバックホウ 下記特許文献 前記凹所 前記目的 設備 重機 詰土砂 砕石 地上 上方 課題 坂路 ロス 例 進捗 支障 空間 陸 段差 脇 開放 閉鎖 図面 モータ ドラム 間隔 左右 程度 遊び ピット 垂直部材 側部 組立 技術分野 背景技術 外方 梁材 使用法 危険性回避 締固め効果 ＧＬ± ＧＬ－ スロープ部分 軽量ブロック 特開 － 号公報 目的 方法 転圧 圧 跡地利用 沈下 不同沈下 効果 柱支柱 クラブトローリー形式 段状 分解除去 利用 場外 図示 ブルドーザ ローラ 複数 開示 壁間 内部 不都合 手段 方式 項目 改良 投入 投下 終了 障害 埋め立て 出し入れ 最良 鉄骨 台車 参照 上下 切断 ガイドローラ 上端 観音開き 外側 跳ね橋 自体 一括 重量 位置 積層 再生 片側 都合 下地 ブルドーザ・バックホウ ガタツキ 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 7.8357 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 突出部材 シールドトンネル トンネル 図 セグメント 前記突出部材 実施形態 適用例 土塊重量 モーメント 土塊 トンネル底部 前記シールドトンネル 技術 突出 施工条件 効果 防止 地盤 曲げモーメント セグメントリング 形態 最大値 】－－－適用例 施工 先端部 既存構造物等 防止効果 集中 空 下方 例 側方土圧 土 トンネル内空 各種施工条件 当該突出部材 シールドトンネル内空側 シールドトンネル外周面 斜め上方 トンネル施工 深度 シールドトンネル外周表面 設置位置 トンネル断面 上記適用例 集中抑制効果 空中心ｃ 解析 対策 前記突出長 前記適用例 浮力 構造 地下水位 設置位置ｇ 部材 集中抑制 トンネル軸方向 シールドトンネル内空 シールド工法 前記モーメント 重量 特許文献 側部 断面図 くさび状 空中心 断面形状 発明 設置 シールドトンネル直上 前記セグメント 抑制効果 斜め下方 インバート部 ＫＮ ｍ 実施 突出長 突出部材直上 領域 くさび効果 前記断面力 土圧 地中 状況 トンネル右側断面 トンネル左側断面 角パイプ 固定 力 当該先端部 固定形態 上述 低減 障害物等 トンネル掘削 トンネル下部 トンネル側部 方向 固定手段 上記形態 周辺土塊 セグメント底部 － 外周位置 前記内空中心 地下構造物 前記二つ 前記領域 アーチアクション 前記内空 トンネル周方向 トンネル進行方向 前記 荷重 １つ 深度分 適用範囲 既存地下構造物 上方 状態 前記内空中心ｃ シールドマシン 欠損等 上記 パターンａ 断面力 固定箇所 抑制手段 外周位置ｄ 内壁面 上下方向 課題 鋼材 施工環境 等 オープンシールド工法 岩盤 礫 地上 地下 範囲 シールド外径 断面図参照 断面図パターンｂ セグメントリング２つ 地上部 前記浮力 アンカー 号公報 支保機能 土塊体積 土塊密度 土塊群 挿入口 外側 時計回り 外側部 都市部 有効土圧 シールリング等 各種既存 セグメントリング１つおき 地盤条件 説明 目的 変形 元 パターンｇ 土粒子同士 セグメント厚 コッター 作業 内側 ～ 各種設計指針等 地上部占有 コンクリート等 斜視図 ～図 側面図 鉄道線路等 手段 パターンｄ エレクター等 鉄筋等 油圧ジャッキ等 先行技術文献 工法 乗算値 コンクリート函体 外側面 浮力対抗手段 全土圧 技術分野 背景技術 アンカー長 鉄道線路 函体周囲 道路 コスト 垂線 凸 挙動 図面 間隔 性状 梁 右側 左側 力係数Ｋｖ 岩石 Ａ Ｂ 上記課題 内側面 板状鋼材 地盤状況 軟弱地盤 地上環境 所定長 Ｈ型鋼 開削工法 アーチ作用 欠損 直上 アンカー支持層 範囲幅 鋼棒 都市土木 グラウト材 特開平 特開 プレート材 経済的設計 厚 分野 密着性 示方基準 コッター自体 交通量 側面 作用 鉄筋コンクリート柱 圧入作業 圧入 固化剤 ボルト締結 優越状態 桁高 揚圧力 制約 恐れ 現象 概要 懸念 地層 盛土 カウンターウェイト 妨げ 余裕 提供 特徴 －－－　 配置 連結 他 構築 部分 複数 各々 両側 偏心 由来 地表 考慮 鋼管 剛性 動作 根元 設 溶接 キン 水密 支点 Ｌ 前述 支保工 前提 水圧 正負 正 負 要旨 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9978e-26, 3.0712e-19, 1.0002e-10, 1.6033e-17, 2.0603e-05, 3.4737e-28,\n",
      "         5.6019e-25, 3.6543e-02, 7.4287e-05, 4.9601e-14, 7.6494e-34, 2.1212e-29,\n",
      "         5.6071e-14, 1.6466e-19, 9.6336e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0821, 0.0459, 0.1022, 0.0501, 0.0609, 0.0679, 0.1066, 0.0902, 0.0593,\n",
      "         0.0518, 0.0637, 0.0553, 0.0506, 0.0718, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1011.3688\n",
      "Test epoch : 1 Average loss: 1069.4710\n",
      "PP(train) = 2060.185, PP(valid) = 2133.895\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.9077e-20, 9.9682e-01, 2.0372e-08, 1.6442e-09, 4.1423e-07, 5.1056e-26,\n",
      "         2.1525e-19, 5.9980e-09, 5.0572e-12, 2.6330e-11, 1.2227e-17, 7.9064e-08,\n",
      "         3.0949e-03, 6.5529e-19, 8.0014e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1093, 0.0480, 0.0399, 0.1057, 0.0449, 0.0465, 0.0730, 0.0406, 0.0766,\n",
      "         0.0767, 0.0375, 0.0459, 0.0599, 0.1148, 0.0807]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1011.3510\n",
      "Test epoch : 2 Average loss: 1069.3895\n",
      "PP(train) = 2057.487, PP(valid) = 2132.643\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3351e-24, 1.3604e-21, 2.5493e-06, 3.1523e-19, 8.4495e-09, 6.2305e-31,\n",
      "         3.5397e-17, 3.9123e-10, 7.7600e-01, 1.0569e-24, 3.5018e-19, 9.6145e-16,\n",
      "         2.2399e-01, 2.4594e-10, 2.5917e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0550, 0.0699, 0.0507, 0.0632, 0.0330, 0.0743, 0.0616, 0.0901, 0.0614,\n",
      "         0.0752, 0.0756, 0.0630, 0.0534, 0.1135, 0.0602]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1011.0657\n",
      "Test epoch : 3 Average loss: 1069.3063\n",
      "PP(train) = 2054.087, PP(valid) = 2131.224\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.9625e-28, 1.4801e-20, 6.0950e-08, 1.7402e-11, 7.2601e-11, 9.7477e-20,\n",
      "         1.6310e-23, 4.1881e-02, 1.2689e-10, 8.0578e-12, 2.5935e-25, 1.1072e-12,\n",
      "         2.9080e-05, 1.4344e-16, 9.5809e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0822, 0.0462, 0.1019, 0.0501, 0.0610, 0.0681, 0.1065, 0.0898, 0.0593,\n",
      "         0.0518, 0.0640, 0.0552, 0.0505, 0.0720, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1010.8975\n",
      "Test epoch : 4 Average loss: 1069.2223\n",
      "PP(train) = 2050.672, PP(valid) = 2130.037\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9401e-21, 1.5970e-17, 4.0503e-17, 2.7108e-12, 1.7801e-08, 5.6111e-19,\n",
      "         4.6324e-15, 2.0547e-14, 3.9068e-04, 7.6407e-21, 9.9383e-17, 2.2143e-11,\n",
      "         1.1128e-07, 8.4715e-13, 9.9961e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1010.7420\n",
      "Test epoch : 5 Average loss: 1069.1370\n",
      "PP(train) = 2047.032, PP(valid) = 2128.728\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9623e-22, 9.6511e-22, 1.3408e-11, 2.2486e-08, 1.6918e-02, 2.0706e-12,\n",
      "         2.9722e-23, 9.8051e-01, 3.7905e-05, 4.3209e-19, 5.4654e-17, 5.0866e-17,\n",
      "         2.5299e-03, 1.3683e-31, 1.3030e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0826, 0.1044, 0.0543, 0.0443, 0.0795, 0.0919, 0.0774, 0.0445, 0.0531,\n",
      "         0.0378, 0.1182, 0.0442, 0.0368, 0.0934, 0.0376]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1010.4236\n",
      "Test epoch : 6 Average loss: 1069.0539\n",
      "PP(train) = 2043.272, PP(valid) = 2127.366\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0971e-23, 3.0365e-30, 9.9909e-01, 1.4185e-11, 2.8845e-08, 2.7599e-30,\n",
      "         6.9335e-29, 9.1759e-15, 2.5918e-04, 2.9183e-19, 8.5188e-35, 9.6283e-14,\n",
      "         1.6336e-07, 3.8662e-12, 6.5399e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0898, 0.0442, 0.0716, 0.0839, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1010.2016\n",
      "Test epoch : 7 Average loss: 1068.9723\n",
      "PP(train) = 2039.700, PP(valid) = 2126.217\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3621e-23, 6.5731e-24, 3.6167e-14, 7.5150e-23, 3.6480e-02, 1.3623e-27,\n",
      "         1.2481e-20, 1.0565e-15, 9.6352e-01, 4.9020e-25, 8.9390e-17, 2.8358e-25,\n",
      "         2.1526e-10, 1.0761e-07, 1.1067e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0558, 0.0668, 0.0443, 0.0619, 0.0318, 0.0759, 0.0644, 0.0928, 0.0567,\n",
      "         0.0703, 0.0842, 0.0651, 0.0510, 0.1283, 0.0507]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1009.9975\n",
      "Test epoch : 8 Average loss: 1068.8896\n",
      "PP(train) = 2036.057, PP(valid) = 2124.931\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0146e-25, 2.0404e-24, 7.1210e-07, 6.7683e-24, 6.9937e-02, 3.3337e-20,\n",
      "         1.5277e-21, 9.3006e-01, 3.0506e-14, 2.2114e-26, 1.1784e-14, 6.4130e-28,\n",
      "         2.3602e-09, 6.5974e-22, 7.9405e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0806, 0.1008, 0.0539, 0.0442, 0.0809, 0.0909, 0.0794, 0.0469, 0.0529,\n",
      "         0.0398, 0.1157, 0.0452, 0.0374, 0.0921, 0.0392]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1009.7201\n",
      "Test epoch : 9 Average loss: 1068.8095\n",
      "PP(train) = 2032.446, PP(valid) = 2123.676\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9750e-33, 1.3900e-13, 4.6965e-05, 3.1691e-05, 1.4864e-07, 3.2076e-17,\n",
      "         3.4557e-24, 2.1424e-02, 9.5512e-01, 2.6785e-22, 1.7526e-19, 1.0955e-10,\n",
      "         3.4124e-06, 1.8196e-14, 2.3374e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0573, 0.0676, 0.0455, 0.0623, 0.0316, 0.0764, 0.0642, 0.0908, 0.0572,\n",
      "         0.0682, 0.0850, 0.0646, 0.0509, 0.1291, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1009.4685\n",
      "Test epoch : 10 Average loss: 1068.7304\n",
      "PP(train) = 2028.926, PP(valid) = 2122.529\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7000e-21, 1.0714e-18, 2.7121e-14, 1.9004e-14, 3.7778e-08, 4.0800e-22,\n",
      "         6.2129e-16, 1.4759e-12, 3.3446e-04, 1.9899e-12, 1.1654e-23, 1.2643e-17,\n",
      "         6.4918e-02, 1.8362e-10, 9.3475e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0794, 0.0461, 0.1028, 0.0511, 0.0590, 0.0671, 0.1034, 0.0919, 0.0606,\n",
      "         0.0546, 0.0613, 0.0557, 0.0518, 0.0708, 0.0444]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1009.2765\n",
      "Test epoch : 11 Average loss: 1068.6523\n",
      "PP(train) = 2025.438, PP(valid) = 2121.378\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3436e-24, 2.7560e-17, 8.9613e-17, 5.0024e-14, 4.8154e-11, 2.6036e-37,\n",
      "         1.1274e-21, 2.5789e-07, 9.9670e-01, 1.3353e-24, 3.9630e-11, 2.8406e-22,\n",
      "         3.7964e-06, 1.7965e-22, 3.2996e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0444, 0.0628, 0.0305, 0.0760, 0.0630, 0.0919, 0.0570,\n",
      "         0.0692, 0.0846, 0.0651, 0.0511, 0.1311, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1009.0159\n",
      "Test epoch : 12 Average loss: 1068.5762\n",
      "PP(train) = 2021.996, PP(valid) = 2120.243\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.4918e-16, 8.2835e-22, 9.1600e-01, 5.4651e-04, 3.2151e-10, 4.1910e-18,\n",
      "         1.3213e-25, 2.1856e-12, 7.8306e-02, 2.5278e-16, 2.4784e-11, 9.3543e-17,\n",
      "         5.1516e-03, 1.0864e-23, 4.6875e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0744, 0.0883, 0.0446, 0.0712, 0.0777, 0.0505, 0.0741, 0.0770, 0.0916,\n",
      "         0.0595, 0.0641, 0.0492, 0.0515, 0.0897, 0.0367]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1008.8784\n",
      "Test epoch : 13 Average loss: 1068.5011\n",
      "PP(train) = 2018.546, PP(valid) = 2119.090\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7031e-28, 3.4777e-16, 8.7574e-15, 3.5656e-20, 2.9844e-06, 6.0944e-20,\n",
      "         3.2005e-17, 4.3246e-12, 9.9079e-01, 6.0583e-17, 8.5367e-11, 8.9203e-24,\n",
      "         2.7562e-14, 3.1244e-18, 9.2037e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0564, 0.0672, 0.0447, 0.0628, 0.0306, 0.0760, 0.0633, 0.0920, 0.0570,\n",
      "         0.0692, 0.0844, 0.0651, 0.0511, 0.1307, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1008.6135\n",
      "Test epoch : 14 Average loss: 1068.4282\n",
      "PP(train) = 2015.116, PP(valid) = 2117.897\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5877e-22, 2.1423e-24, 2.6897e-23, 4.9158e-19, 1.0000e+00, 5.3973e-29,\n",
      "         1.9670e-12, 1.4035e-13, 3.6576e-15, 1.7931e-12, 6.9718e-24, 5.9279e-13,\n",
      "         7.7727e-16, 3.0640e-22, 1.8734e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1008.4512\n",
      "Test epoch : 15 Average loss: 1068.3543\n",
      "PP(train) = 2011.811, PP(valid) = 2116.795\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0934e-29, 3.2257e-19, 1.0205e-04, 4.0005e-10, 5.7700e-04, 7.4751e-26,\n",
      "         2.2273e-21, 9.9931e-01, 2.6223e-06, 2.0173e-08, 5.2272e-20, 2.1925e-12,\n",
      "         3.2842e-06, 2.6366e-12, 9.6415e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0543, 0.0442, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1008.1533\n",
      "Test epoch : 16 Average loss: 1068.2798\n",
      "PP(train) = 2008.620, PP(valid) = 2115.786\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1817e-21, 3.9795e-16, 5.1085e-19, 6.5332e-18, 3.3148e-06, 5.3718e-24,\n",
      "         1.8194e-27, 1.0000e+00, 1.1573e-15, 3.6566e-18, 4.7516e-20, 4.0794e-30,\n",
      "         2.9284e-14, 1.1399e-24, 7.0530e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1007.9996\n",
      "Test epoch : 17 Average loss: 1068.2088\n",
      "PP(train) = 2005.324, PP(valid) = 2114.691\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4933e-29, 1.7250e-22, 4.8861e-03, 1.0057e-33, 4.3945e-08, 1.0387e-25,\n",
      "         3.5713e-25, 9.5765e-01, 1.2856e-16, 1.1320e-15, 3.7483e-30, 1.6960e-24,\n",
      "         9.9463e-11, 8.4158e-21, 3.7465e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0836, 0.1025, 0.0559, 0.0447, 0.0786, 0.0912, 0.0781, 0.0452, 0.0537,\n",
      "         0.0377, 0.1166, 0.0444, 0.0372, 0.0933, 0.0372]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1007.6805\n",
      "Test epoch : 18 Average loss: 1068.1400\n",
      "PP(train) = 2002.097, PP(valid) = 2113.674\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6992e-25, 1.6598e-15, 2.1313e-09, 1.6062e-09, 3.7027e-01, 5.0962e-19,\n",
      "         3.0134e-31, 1.8426e-05, 3.9161e-06, 1.0302e-14, 6.0869e-21, 2.6320e-24,\n",
      "         4.6509e-12, 4.7461e-15, 6.2971e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0666, 0.0465, 0.0760, 0.0461, 0.0728, 0.0677, 0.1092, 0.0998, 0.0545,\n",
      "         0.0662, 0.0650, 0.0582, 0.0498, 0.0687, 0.0530]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1007.4465\n",
      "Test epoch : 19 Average loss: 1068.0719\n",
      "PP(train) = 1998.964, PP(valid) = 2112.663\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9294e-16, 2.9724e-19, 2.1944e-11, 1.1951e-20, 5.8869e-11, 1.8314e-10,\n",
      "         2.8576e-22, 3.7125e-03, 6.7312e-10, 8.6612e-20, 2.8977e-22, 1.8463e-15,\n",
      "         2.1150e-15, 2.2090e-17, 9.9629e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0445, 0.1041, 0.0502, 0.0602, 0.0670, 0.1075, 0.0921, 0.0593,\n",
      "         0.0523, 0.0621, 0.0555, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1007.3200\n",
      "Test epoch : 20 Average loss: 1068.0035\n",
      "PP(train) = 1995.726, PP(valid) = 2111.540\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1271e-29, 2.0436e-12, 1.1040e-13, 5.7123e-17, 4.2761e-12, 1.7644e-35,\n",
      "         4.2679e-29, 1.4593e-13, 5.0276e-06, 3.3468e-27, 6.3929e-32, 4.0285e-19,\n",
      "         3.8282e-08, 5.7774e-32, 9.9999e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1007.0486\n",
      "Test epoch : 21 Average loss: 1067.9357\n",
      "PP(train) = 1992.677, PP(valid) = 2110.562\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0310e-17, 3.2445e-18, 1.0672e-05, 9.9763e-01, 7.6727e-09, 5.6805e-07,\n",
      "         4.0503e-25, 3.3811e-05, 4.7671e-17, 9.0074e-20, 1.0695e-24, 4.7563e-25,\n",
      "         7.6870e-07, 7.7503e-17, 2.3246e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0417, 0.0551, 0.0918, 0.0503, 0.0527, 0.0525, 0.0533, 0.0686,\n",
      "         0.1349, 0.1029, 0.0485, 0.0563, 0.0522, 0.0905]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1006.9146\n",
      "Test epoch : 22 Average loss: 1067.8691\n",
      "PP(train) = 1989.661, PP(valid) = 2109.622\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.7737e-24, 4.5993e-03, 6.6233e-03, 8.9161e-12, 5.0538e-07, 3.1658e-10,\n",
      "         5.2835e-15, 9.8751e-01, 3.7917e-06, 1.2429e-13, 3.1176e-10, 6.3835e-11,\n",
      "         1.9488e-04, 2.2802e-13, 1.0707e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0836, 0.1051, 0.0543, 0.0446, 0.0790, 0.0917, 0.0770, 0.0439, 0.0534,\n",
      "         0.0373, 0.1183, 0.0439, 0.0368, 0.0941, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1006.6970\n",
      "Test epoch : 23 Average loss: 1067.8045\n",
      "PP(train) = 1986.513, PP(valid) = 2108.546\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.0878e-19, 2.6782e-22, 9.3992e-04, 6.4895e-15, 2.7448e-14, 5.3577e-17,\n",
      "         8.9945e-28, 1.2691e-08, 1.8437e-02, 2.6812e-09, 7.9679e-14, 7.4678e-11,\n",
      "         9.8058e-01, 1.8975e-13, 4.1738e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0485, 0.0744, 0.0754, 0.0607, 0.0411, 0.0648, 0.0541, 0.0792, 0.0744,\n",
      "         0.0934, 0.0488, 0.0532, 0.0584, 0.0653, 0.1084]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1006.4976\n",
      "Test epoch : 24 Average loss: 1067.7403\n",
      "PP(train) = 1983.535, PP(valid) = 2107.581\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.7199e-27, 3.3337e-21, 2.1898e-17, 9.9619e-27, 3.7533e-14, 1.0719e-18,\n",
      "         1.6658e-18, 1.0000e+00, 6.2681e-08, 8.3323e-22, 1.5626e-20, 2.3944e-16,\n",
      "         3.1202e-09, 9.8828e-21, 4.0509e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1006.2986\n",
      "Test epoch : 25 Average loss: 1067.6755\n",
      "PP(train) = 1980.619, PP(valid) = 2106.658\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4213e-13, 9.6666e-19, 4.6031e-15, 2.2346e-08, 7.0620e-08, 3.5897e-20,\n",
      "         6.4124e-23, 4.1479e-05, 4.3464e-05, 1.7503e-09, 1.9074e-21, 9.8505e-23,\n",
      "         9.9991e-01, 1.0429e-08, 1.1664e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0482, 0.0744, 0.0761, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0938, 0.0482, 0.0529, 0.0585, 0.0643, 0.1099]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1006.1007\n",
      "Test epoch : 26 Average loss: 1067.6136\n",
      "PP(train) = 1977.680, PP(valid) = 2105.741\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4491e-19, 2.8835e-13, 7.8083e-01, 5.8386e-10, 1.7683e-01, 1.6558e-06,\n",
      "         7.2701e-14, 5.1745e-08, 3.0657e-12, 3.3278e-09, 1.1073e-09, 4.0478e-10,\n",
      "         4.2336e-02, 1.2760e-13, 2.2276e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0690, 0.0811, 0.0456, 0.0647, 0.0849, 0.0526, 0.0799, 0.0819, 0.0838,\n",
      "         0.0657, 0.0635, 0.0508, 0.0513, 0.0819, 0.0432]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1005.9413\n",
      "Test epoch : 27 Average loss: 1067.5540\n",
      "PP(train) = 1974.737, PP(valid) = 2104.794\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0513e-20, 7.1692e-28, 2.0496e-02, 1.2072e-15, 3.4218e-04, 4.1166e-28,\n",
      "         5.7147e-20, 9.7916e-01, 4.1466e-11, 9.1713e-19, 3.1440e-25, 5.9978e-14,\n",
      "         2.1726e-08, 7.8470e-13, 5.9019e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1054, 0.0542, 0.0447, 0.0793, 0.0912, 0.0769, 0.0442, 0.0538,\n",
      "         0.0374, 0.1179, 0.0440, 0.0368, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1005.6822\n",
      "Test epoch : 28 Average loss: 1067.4942\n",
      "PP(train) = 1971.834, PP(valid) = 2103.903\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.0868e-18, 3.3717e-23, 3.0646e-17, 1.8082e-16, 3.7791e-09, 4.4601e-30,\n",
      "         3.0948e-32, 1.0000e+00, 2.2731e-13, 6.4660e-13, 2.1672e-28, 4.2233e-25,\n",
      "         1.6512e-21, 9.5888e-18, 5.2400e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1005.4982\n",
      "Test epoch : 29 Average loss: 1067.4305\n",
      "PP(train) = 1969.095, PP(valid) = 2103.045\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4293e-35, 6.3793e-23, 1.2170e-06, 2.7218e-28, 3.3267e-22, 1.1623e-24,\n",
      "         1.1136e-19, 1.3372e-09, 8.4520e-15, 2.2332e-24, 3.4199e-23, 3.6027e-27,\n",
      "         3.3838e-18, 7.7423e-25, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 1005.3032\n",
      "Test epoch : 30 Average loss: 1067.3717\n",
      "PP(train) = 1966.267, PP(valid) = 2102.157\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.7237e-20, 3.7899e-22, 1.1464e-13, 7.4725e-08, 9.9417e-15, 1.5196e-22,\n",
      "         7.5466e-24, 1.5891e-03, 9.9841e-01, 2.2093e-29, 2.8820e-30, 1.5628e-24,\n",
      "         3.6569e-10, 2.0685e-20, 9.8995e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0674, 0.0443, 0.0628, 0.0304, 0.0760, 0.0629, 0.0918, 0.0570,\n",
      "         0.0692, 0.0847, 0.0651, 0.0510, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 1005.1307\n",
      "Test epoch : 31 Average loss: 1067.3168\n",
      "PP(train) = 1963.405, PP(valid) = 2101.260\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3292e-18, 1.4639e-22, 2.9454e-17, 7.6506e-16, 1.0286e-12, 3.1317e-25,\n",
      "         4.2242e-19, 1.5909e-17, 9.9937e-01, 1.0779e-18, 2.7618e-20, 1.4991e-16,\n",
      "         2.9311e-12, 9.3288e-15, 6.2757e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 1004.9583\n",
      "Test epoch : 32 Average loss: 1067.2575\n",
      "PP(train) = 1960.631, PP(valid) = 2100.339\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.3248e-21, 3.1026e-25, 1.8933e-17, 8.9484e-19, 1.3295e-07, 5.6926e-24,\n",
      "         1.0705e-20, 3.2505e-02, 9.6231e-01, 3.6583e-20, 1.3383e-22, 2.0504e-21,\n",
      "         7.8447e-12, 3.1626e-20, 5.1812e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0571, 0.0684, 0.0449, 0.0623, 0.0316, 0.0767, 0.0637, 0.0900, 0.0570,\n",
      "         0.0680, 0.0857, 0.0644, 0.0507, 0.1300, 0.0494]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 1004.6921\n",
      "Test epoch : 33 Average loss: 1067.2007\n",
      "PP(train) = 1957.979, PP(valid) = 2099.568\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3250e-27, 1.8534e-25, 1.6415e-15, 1.1871e-21, 3.8168e-10, 1.8243e-34,\n",
      "         6.6428e-37, 1.4675e-07, 1.2854e-03, 2.8138e-09, 1.9714e-28, 8.0366e-19,\n",
      "         4.2238e-15, 4.6063e-31, 9.9871e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1042, 0.0502, 0.0600, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 1004.4466\n",
      "Test epoch : 34 Average loss: 1067.1462\n",
      "PP(train) = 1955.326, PP(valid) = 2098.815\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.7232e-24, 3.0652e-11, 1.6263e-16, 3.7025e-24, 1.6567e-08, 2.7714e-24,\n",
      "         3.2445e-29, 9.3101e-12, 2.9373e-13, 6.3947e-12, 1.5309e-21, 1.8514e-26,\n",
      "         2.6943e-12, 9.6840e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 1004.3724\n",
      "Test epoch : 35 Average loss: 1067.0908\n",
      "PP(train) = 1952.569, PP(valid) = 2097.927\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.1173e-15, 2.1679e-05, 3.4194e-18, 6.4062e-12, 8.3411e-05, 1.5139e-23,\n",
      "         9.4507e-26, 6.3307e-02, 8.4091e-01, 5.4222e-15, 1.1750e-20, 1.0595e-04,\n",
      "         1.9033e-05, 2.1394e-09, 9.5549e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0604, 0.0674, 0.0493, 0.0609, 0.0349, 0.0769, 0.0679, 0.0888, 0.0576,\n",
      "         0.0656, 0.0850, 0.0633, 0.0506, 0.1227, 0.0486]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 1004.2248\n",
      "Test epoch : 36 Average loss: 1067.0388\n",
      "PP(train) = 1949.893, PP(valid) = 2097.075\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.3324e-32, 8.9506e-29, 5.5504e-18, 1.7682e-14, 1.5322e-20, 4.7800e-30,\n",
      "         1.1836e-31, 2.1299e-25, 6.0949e-18, 5.5628e-23, 3.4762e-21, 3.8614e-27,\n",
      "         8.0568e-28, 2.1323e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 1004.0392\n",
      "Test epoch : 37 Average loss: 1066.9874\n",
      "PP(train) = 1947.273, PP(valid) = 2096.338\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8200e-35, 2.1218e-34, 7.1986e-04, 7.5210e-19, 4.3353e-22, 9.7830e-39,\n",
      "         7.2774e-31, 1.8884e-02, 2.0783e-16, 6.8335e-13, 9.0725e-21, 4.7081e-27,\n",
      "         1.1987e-02, 4.0343e-23, 9.6841e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0815, 0.0455, 0.1029, 0.0504, 0.0603, 0.0675, 0.1063, 0.0911, 0.0596,\n",
      "         0.0525, 0.0627, 0.0555, 0.0509, 0.0714, 0.0419]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 1003.7362\n",
      "Test epoch : 38 Average loss: 1066.9343\n",
      "PP(train) = 1944.789, PP(valid) = 2095.660\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.2330e-30, 1.4163e-22, 2.1096e-32, 3.1678e-17, 1.0000e+00, 9.0642e-26,\n",
      "         4.6596e-14, 3.6139e-08, 3.2226e-09, 1.2178e-29, 5.7945e-26, 2.6190e-22,\n",
      "         4.7187e-17, 3.1979e-20, 5.5718e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 1003.6348\n",
      "Test epoch : 39 Average loss: 1066.8844\n",
      "PP(train) = 1942.141, PP(valid) = 2094.867\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.2875e-28, 5.1640e-14, 1.2182e-08, 7.2159e-10, 4.3040e-06, 1.0007e-15,\n",
      "         5.7902e-20, 1.7488e-07, 1.2144e-10, 4.9901e-16, 1.7122e-10, 4.1731e-11,\n",
      "         3.4384e-20, 4.3870e-14, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 1003.4877\n",
      "Test epoch : 40 Average loss: 1066.8346\n",
      "PP(train) = 1939.481, PP(valid) = 2093.988\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1755e-22, 1.1135e-24, 2.7495e-09, 1.2899e-23, 3.6905e-20, 2.9131e-30,\n",
      "         4.8251e-20, 1.0000e+00, 8.3163e-16, 2.4898e-28, 3.9949e-06, 2.7761e-19,\n",
      "         1.5574e-08, 8.0394e-30, 1.3109e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 1003.3434\n",
      "Test epoch : 41 Average loss: 1066.7848\n",
      "PP(train) = 1937.010, PP(valid) = 2093.296\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1488e-21, 2.6432e-20, 1.8565e-18, 3.5853e-07, 1.2614e-10, 6.2296e-21,\n",
      "         7.9031e-26, 3.8178e-08, 1.0537e-04, 6.3339e-11, 2.0204e-21, 2.7649e-17,\n",
      "         1.2613e-12, 5.5165e-18, 9.9989e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 1003.1295\n",
      "Test epoch : 42 Average loss: 1066.7347\n",
      "PP(train) = 1934.593, PP(valid) = 2092.638\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3308e-25, 1.1184e-24, 1.7536e-12, 1.6498e-12, 2.0046e-19, 4.4039e-27,\n",
      "         5.2635e-22, 5.7356e-11, 1.0949e-10, 1.5218e-23, 6.0361e-27, 2.8877e-16,\n",
      "         3.2939e-09, 3.4324e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 1002.9382\n",
      "Test epoch : 43 Average loss: 1066.6828\n",
      "PP(train) = 1932.086, PP(valid) = 2091.863\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.6550e-24, 4.1863e-21, 1.8619e-04, 3.5037e-16, 4.0495e-10, 4.5509e-25,\n",
      "         1.9470e-16, 8.6833e-01, 1.0315e-01, 4.1106e-12, 3.0375e-20, 2.8437e-18,\n",
      "         3.3993e-06, 1.5580e-22, 2.8330e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0809, 0.0995, 0.0548, 0.0466, 0.0719, 0.0906, 0.0769, 0.0487, 0.0542,\n",
      "         0.0403, 0.1143, 0.0465, 0.0386, 0.0976, 0.0386]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 1002.7336\n",
      "Test epoch : 44 Average loss: 1066.6359\n",
      "PP(train) = 1929.639, PP(valid) = 2091.160\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9730e-17, 4.4668e-18, 1.5209e-04, 9.3505e-05, 1.0570e-05, 4.0118e-22,\n",
      "         2.6968e-16, 2.4109e-07, 3.7685e-01, 6.4862e-23, 6.3453e-27, 2.3338e-22,\n",
      "         6.2289e-01, 8.6340e-18, 1.8213e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0520, 0.0730, 0.0632, 0.0626, 0.0375, 0.0700, 0.0582, 0.0851, 0.0687,\n",
      "         0.0853, 0.0607, 0.0583, 0.0566, 0.0858, 0.0830]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 1002.5828\n",
      "Test epoch : 45 Average loss: 1066.5896\n",
      "PP(train) = 1927.203, PP(valid) = 2090.461\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8020e-31, 5.7043e-21, 4.0706e-09, 6.2571e-07, 9.9535e-01, 1.3812e-24,\n",
      "         3.6292e-25, 6.5873e-07, 4.6486e-03, 6.2087e-30, 1.7060e-21, 4.5095e-21,\n",
      "         1.5589e-10, 1.7758e-19, 3.7007e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0422, 0.0382, 0.0955, 0.0658, 0.1064, 0.1084, 0.0451,\n",
      "         0.0939, 0.0673, 0.0601, 0.0456, 0.0623, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 1002.4203\n",
      "Test epoch : 46 Average loss: 1066.5435\n",
      "PP(train) = 1924.812, PP(valid) = 2089.807\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.5635e-36, 9.5174e-19, 2.1432e-10, 1.3099e-23, 3.9928e-16, 1.0049e-19,\n",
      "         4.7059e-25, 9.9966e-01, 2.8477e-12, 1.4029e-17, 9.3379e-24, 1.0904e-19,\n",
      "         8.4677e-23, 4.0280e-20, 3.3864e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0544, 0.0442, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 1002.2332\n",
      "Test epoch : 47 Average loss: 1066.4984\n",
      "PP(train) = 1922.410, PP(valid) = 2089.120\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4148e-23, 9.4187e-14, 1.9384e-07, 3.6002e-03, 6.1401e-02, 1.1161e-17,\n",
      "         3.0180e-24, 4.5273e-09, 1.3190e-07, 2.3370e-18, 1.7362e-17, 2.4703e-17,\n",
      "         5.9245e-11, 1.8448e-16, 9.3500e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0790, 0.0448, 0.0990, 0.0497, 0.0621, 0.0671, 0.1078, 0.0935, 0.0587,\n",
      "         0.0547, 0.0627, 0.0561, 0.0509, 0.0705, 0.0434]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 1002.0714\n",
      "Test epoch : 48 Average loss: 1066.4531\n",
      "PP(train) = 1919.993, PP(valid) = 2088.401\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.2380e-23, 5.3031e-23, 2.6513e-04, 7.7285e-12, 9.9968e-01, 5.1488e-28,\n",
      "         3.0752e-19, 5.9227e-05, 1.1943e-21, 1.6150e-13, 1.7985e-21, 5.2663e-17,\n",
      "         4.3595e-12, 8.3406e-15, 3.7263e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 1002.0056\n",
      "Test epoch : 49 Average loss: 1066.4095\n",
      "PP(train) = 1917.638, PP(valid) = 2087.717\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1472e-27, 4.9535e-23, 3.6496e-14, 2.5925e-15, 2.4342e-17, 3.2578e-30,\n",
      "         9.9976e-39, 8.2561e-01, 1.5754e-01, 1.6820e-16, 1.4965e-31, 7.6242e-27,\n",
      "         1.6713e-02, 3.0090e-25, 1.2997e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0788, 0.0992, 0.0537, 0.0477, 0.0683, 0.0903, 0.0751, 0.0503, 0.0548,\n",
      "         0.0421, 0.1130, 0.0475, 0.0394, 0.0999, 0.0400]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 1001.7179\n",
      "Test epoch : 50 Average loss: 1066.3645\n",
      "PP(train) = 1915.512, PP(valid) = 2087.222\n",
      "Writing to ./topicwords/17-topwords_e50.txt\n",
      "Topic 0: 角度θ 水溶性 経路 継手板 柱状 蓋 定着構造 組み立て 公報 界面活性剤\n",
      "Topic 1: 蓋 経路 柱状 角度θ 発破 水溶性 組み立て ダム 位置ずれ チャンバー\n",
      "Topic 2: 参照 位置 配置 構造 技術分野 形態 手段 特許文献 説明 発明\n",
      "Topic 3: 低減 立坑 ダンパー 設置位置 側部 蓋 土壌 不都合 スキンプレート 斜め\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 説明 特許文献\n",
      "Topic 5: 蓋 角度θ くさび 柱状 経路 発破 継手板 水溶性 免震構造 組み立て\n",
      "Topic 6: 蓋 角度θ 緊張 発破 Ｂ－Ｂ断面図 柱状 くさび ダム 継手板 水溶性\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 手段 特許文献 説明 発明\n",
      "Topic 8: 効果 砂 Ａ 側方 ｂ 端部 技術分野 形態 手段 説明\n",
      "Topic 9: 蓋 組み立て 角度θ 経路 ダム 不都合 水溶性 土壌 柱状 通行\n",
      "Topic 10: 水溶性 蓋 角度θ 発破 継手板 経路 緊張 部品等 柱状 ダンパー\n",
      "Topic 11: 角度θ 水溶性 蓋 経路 くさび 発破 継手板 柱状 部品等 定着具\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 手段 説明 特許文献 発明\n",
      "Topic 13: 角度θ 蓋 柱状 経路 発破 組み立て 不都合 Ｂ－Ｂ断面図 水溶性 位置ずれ\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 7, p : 7.6296 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 特許文献, 説明, 発明\n",
      "Input : 穿孔 弾性波 地質探査方法 打撃面 穿孔用ビット 図 打撃 ロッド 前記打撃用治具 方法 地山 撃用治具 受振器 先端面 穿孔機 ハンマー 前記穿孔機 前記打撃面 弾性波速度 打撃用治具 切羽前方 切羽面 前記ロッド 前記受振器 初動波 切羽Ｓ 地質探査システム 観測波形 前記穿孔用ビット 切羽面Ｓ 平坦面 穿孔作業 トンネル空間 前記ハンマー 請求項 打撃力 外周面 速度 軸方向 前記穿孔 速度検層 信号発生用センサ 先端 アップホール法 連続打撃 打撃部 ダウンホール法 ｃｈ 系列波形 ハンマー打撃 発明 前記平坦面 波形 探査方法 系列波形図 立ち上がり位置 初動波到達 地質探査 地 上記請求項 脚部 データ ｃｈ ノイズ 位置 作業 前記ダウンホール法 孔壁 ドリルジャンボ 前記切羽面 半径方向 端面 観測データ ダウンホール法 ｃｈ～ 穿孔用重機 前記脚部 穿孔ロッド 特許文献 縦断面図 測定作業 ドリフタ 状態 前記穿孔作業 前記 弾性波平均速度 前記打撃部 トンネル切羽面Ｓ 弾性波測定 不良地山 正面図 弾性波速度値 運転 側面 記載 曲線 部分 前記速度検層 地質 測定 前記アップホール法 切羽Ｓ前方 環境下 相互補完 比較調査 下端面 課題 トンネル掘削 ねじ孔 下記特許文献 上端面 平均速度 手間 間隔 成分 断面 速度値 設置位置 側 速度構造 挿入 回収 ボルト 前方 図示例 号公報 連結部分 読み取り精度 計測器 方法とも ～図 区間速度値 ～ 説明 技術 任意 特徴 ずれ 切羽崩落災害等 断面円形状 トンネル等 断面形状 大型ハンマー アップホール法 トンネル坑内 孔底面 上記 一定方向 水平方向 短尺ロッド トンネル工事現場 側面図 先行技術文献 所定位置 測定ポイント 技術分野 背景技術 事前 調査 － 概要 前述 通り 分解 組立て 人力 接触 図面 B 形態 外側 固定 複数 内側 部材 例 機器 基準 近傍 影響 比較 値 上記課題 ブロック状 凹凸状 角形状 掘削 計測 読み取り 変形例 ２つ目 ４つ目 ６つ目 金属製 プラスチック製 ゴム製 ローレット加工 挟持金具 継ぎノミ方式 トラブル防止 妥当性 特開平 特開 下端両側 滑り防止 整合性 支保パターン 受振位置 実用化 油圧ユニット ガイドセル 延出し 両側 後付け 他方側 加工 ブルマン 登録商標 所定 ２つ ６つ ショットマーク ｓ付近 形状 観点 ニーズ 一つ 地盤 各種 稼働 振動 制御 手段 効果 詳説 A 実施 ケーブル 台車 シャンクロッド 下方 一対 全長 チャンネル 木製 鉄製 構成 次 手順 ドリフター 番号 判読 特定 解析 他 傾き 同等 符号\n",
      "\n",
      "===== # 2, Topic : 4, p : 7.7482 %\n",
      "Topic words : 上方, ｃ, 発明, 参照, 力, 技術分野, 形態, 手段, 説明, 特許文献\n",
      "Input : 補強部 円筒状周壁 ＰＣタンク ＰＣ防液堤 前記円筒状周壁 ＰＣ鋼材 下端部 繊維補強コンクリート 上端部 鉛直方向 繊維補強モルタル 当該円筒状周壁 パネル状 基礎版 上端部Ａ 図 前記補強部 タンク本体 外周 周方向 前記タンク本体 前記ＰＣ防液堤 前記基礎版 定着部材 強度繊維補強材 構築方法 鉛直断面図 プレストレス 型枠 下端部Ｂ 防液堤 要部 実施形態 アンカーボルト 取付孔 コンクリート ＰＣ ジベル筋 容易化 液密性 プレキャスト工法 配設予定位置 発明 状態 Ｕ字状 基礎杭Ｋ 当該ＰＣ防液堤 ＰＣＬＮＧタンク 外槽 槽 緊張力 部位 強度 発生 変形例 当該パネル状 ひび割れ 直線状 三角形状 モーメント コスト 特許文献 シース管 ＬＮＧタンク 外周表面 コンクリート製 施工 頂部 中間部 側 上端部近傍 液圧 下端部近傍 ＰＣ製 プレストレス導入 方向 側部保冷材Ｑ側表面 外周縁部 Ａ 説明 プレストレス 材料コスト 一体 特性値 ＬＮＧ 隙間 前記定着部材 前記アンカーボルト 上端 両方 前記外周 平面図 前記取付孔 下端側 冷熱抵抗緩和部 液化天然ガス 当該型枠 特徴 ａ 基部 図示 杭 薄肉断面 殻式ＬＮＧタンク 水平断面図 側部保冷材Ｑ 鉛直配置 コンクリート打設 導入 部分 設 対応部分 アンカーボルト等 コンクリート打設後 形態 Ａ－Ａ線 ｂ Ｎ ｍｍ 符号 ナット 該防液堤 配置予定位置側 抵抗力 壁本体 変形 Ｂ 圧縮強度 ひび割れ発生強度 緊張作業等 当該隙間 先行技術文献 特許 技術分野 背景技術 配設 技術思想 外周面 容量化 製 施設 形状 鉄筋コンクリート 屋根 課題 材料 範囲 ＬＮＧ貯留 側部 参照 上方 配設作業 Ｋ 内槽 プレキャスト工法 外方 設置作業 発生防止 粒状パーライト等 有効利用 貯留 号公報 目的 上記目的 図面 添付図面 明細書 思想 グラスウール 面 敷地 観点 金属 周囲 プレストレストコンクリート 地震 概要 分 事情 手段 締結 ポイント 効果 請求 要旨 変更 複数 ＰＵＦ 下部 性能 鉄筋 下面 程度 分布 挿入 工期 量 削減 手間\n",
      "\n",
      "===== # 3, Topic : 8, p : 7.9131 %\n",
      "Topic words : 効果, 砂, Ａ, 側方, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 杭打ち機用防音装置 杭打ち機 杭 音漏れ 音漏れ防止部材 反射音低減部材 前記杭打ち機 反射音 蓋部材 前記杭 前記音漏れ防止部材 カバー 上蓋部材 隙間 杭打設時 図 紙材 状 特許文献 前記カバー 騒音 前記反射音低減部材 反射音どうし 凹凸状 鋼管杭 防音用 号公報 紙製 発明 形態 前記隙間 Ａ 実施 前記蓋部材 低減効果 上下開口 段ボール 凹凸部 杭径 杭長 効果 上記 内面 チューブ Ｂ 鋼製カバー ハンマー部 特開平 作用効果 角筒状 特開 － 軽量 技術 図外 紙 設時 騒音低減性能 上部開口 下部開口 反響音 リサイクル率向上 防音対策 鉛直断面図 密着性 断面四角形 接触 内部 リブ 装置 概略斜視図 部分 取扱い 揚 作業 騒音低減効果 吸音作用等 配置態様等 筒 筒状 構成 現場 特徴 音域 卓越周波数 円形断面 断面円形 説明 上端 課題 使用 エアー 複数 形状 上下方向 先行技術文献 断面多角形 径円盤状 円柱状 円筒状 リング状 凹凸形状 特開昭 ハンマー 等 開平 作用 方向等 上下縁 上下逆 技術分野 背景技術 外部 目的 図面 下部 波状 吸音材 吹付 代わり 側 利用可能性 加工性 実用新案登録 対策 重量物 駆動源 部分Ａ 平面板 四角柱状 積層枚数 上記構成 外周 ～ 参照 概要 開発 手段 下端 地上 地中 外観 表裏 強度 構造 材料 遮音 調整 所期 動作 一般 一端 端 他端 外面 間隔 数量 組み合わせ 別 産業 符号\n",
      "\n",
      "===== # 4, Topic : 8, p : 8.2634 %\n",
      "Topic words : 効果, 砂, Ａ, 側方, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 溝壁 音波センサ 掘削溝 センサ位置決め部材 音波溝壁測定装置 センサ 端面 先端部 図 鉛直フレーム 音波 移動装置 水平距離 走行体 ワイヤ 障害物 溝底 形状 領域 形態 掘削上面 実施 基端部 上端部 溝幅方向 地中連続壁 前記センサ位置決め部材 反射波 発明 方向 先端 前記溝壁 送波器 近傍 支持部材 水平姿勢 走行手段 下端部近傍 先端部近傍 地中構造物 側壁測定装置 基端部近傍 地表面 精度 距離測定 地中埋設物等 前記掘削溝 鉛直面 前記 水平部材 上面 部分 面 前記鉛直フレーム 溝塀 溝芯 作業 一定速度 地中障害物Ｆ 掘削孔 当該センサ 音波発振器 鉛直上方 鉛直状 走行体本体 鉛直距離 ワイヤドラム 前記先端部 受波器 装置 地中連続壁等 ～図 測定方位 該超音波センサ 水平軸 特許文献 下端部 形状把握 下方 距離 構成 使用状態 状態 シーブ 上方 前記移動装置 上記 具 支柱 レール 側方 位置 深度方向 回動動作 ピース ウィンチ 概略 ａ 一対 構造 所望位置 水平方向 鉛直下方向 信頼性 背面 記録機 枠材 横臥部材 説明 キャプタイヤケーブル 課題 伸縮ブーム先端 該鉛直フレーム 手段 上端 近傍両側 背面側 地表 先行技術文献 技術分野 背景技術 精度確認 受信器 背面図 前面側 地盤 減衰 目的 該ワイヤ 特徴 ｂ 下面 角度 平面図 回転軸 ピース等 構築予定位置 精度確認作業 中間位置 側壁 伸長動作 回転機構 コンクリート造 鉄筋コンクリート造 特開 号公報 安定液 矩形杭 分野 繰り出し操作 鉄筋籠 設置方法 実施形態 段階 所定 否 － 概要 比重 手前 効果 図面 事例 一般 本数 先 両者 間隔 ホイール 柱状 程度 ストッパ 背 正規 品質 趣旨 範囲 種々 変更 符号\n",
      "\n",
      "===== # 5, Topic : 14, p : 8.2007 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 溝壁 音波センサ 掘削溝 音波孔壁測定器 掘削孔 ワイヤ 深度 水平距離 センサ 孔壁 ワイヤ固定点Ｃ 溝底 図 スタビライザー 距離 規定深度範囲Ｘ 音波 ワイヤ固定点 精度管理 形状 押圧装置 地中連続壁 規定深度範囲 精度管理方法 支持具 位置 測定 発明 具 振り子状 範囲 深度方向 深度位置 作業 溝芯 精度 安定液 実施 前記孔壁 孔底 形態 深度領域 揺れ ウィンチ 記録機 工程 ｂ 計測作業 状態 溝幅 測定作業 特許文献 形状変化 孔口 ａ 本体 動力源 ワイヤ固定位置Ｃ 配置位置 所定 送波器 溝底近傍 Ｘ 測定方位 地中連続壁等 挿通孔 地上 地中構造物 方向 反射波 所定深度 地盤 対流 繰り出し 距離計測 管理者 コンクリート造 油圧ジャッキ 処理工程 一定速度 揺動 音波発振器 平面視位置 側壁測定装置 該ワイヤ 前記ワイヤ 当該センサ 該超音波センサ 水平方向 説明 課題 効率 構造 構成 内部 程度 部分 近傍 直上位置 平面視四角形状 位置出し 受信器 方法 平面視断面 記録 先行技術文献 技術分野 背景技術 鉛直状 否 キャプタイヤケーブル 目的 早期 事例 ドラム 受波器 上面 動力 数量 変化 上段 形状把握 上記実施形態 該スタビライザー 丸杭 矩形杭 凹凸等 回転機構 伸長動作 側壁 特開 号公報 中間部 当該 ～図 分野 側面各々 先端面 持状態 垂直性 上記 段階 － 概要 挿入 手段 特徴 所望 効果 図面 概略 一般 上端 一対 ケーブル 口径 径 置状態 下面 ２つ 本数 個数 影響 現象 一体 重量 作動 先 待ち時間 溝口 趣旨 種々 変更 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 8.3155 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 杭 杭体 騒音低減材 騒音低減型杭 前記杭体 空気袋 挿入体 空気 前記騒音低減材 鋼管杭 シート状材料 材 騒音 前記空気袋 設置方法 反響音 放射音 杭そのもの 杭内部 内部 前記杭体そのもの 材料 前記挿入体 図 騒音低減性能 筒状 接着材 発明 吸音材 特許文献 杭径 シート状 段ボール 方法 前記シート状材料 Ａ Ｂ 振動 前記吸音材 － 号公報 外表面 そのもの 前記接着材 鋼管杭そのもの 効果 内面 鋼管杭内部 騒音低減効果 形態 騒音低減対象 実施 円柱状 円筒状 同軸状 上記 棒状 設時 断面図 段ボール加工 周辺環境 廃棄物 特開 バルブ付きチューブ 設置 側断面図 外側 鋼管杭全長 開口部 騒音源 接触音 特徴 リング状 作用効果 質量体 弾性体 技術 表面 混成材料 加工性 空気入れ用 負荷 縮減 形状 工程 紙 概略斜視図 積層枚数等 説明 参照 現場 波状 反射音 円形断面 角形断面 ハンマー 課題 作業 対策 設置範囲 Ａ－Ａ線 Ｂ－Ｂ線 Ｃ－Ｃ線 端部 尺方向 先行技術文献 開口端部 特開昭 段ボール等 周方向 等 防音用 長手方向 尺方向所定 技術分野 背景技術 目的 図面 隙間 箇所 作用 外部 表裏 強度 構造 調整 所期 両端 中間部 特開平 周面 制振作用 利用可能性 プラスチック等 吸音作用 分解性 防音対策 防音装置 尺棒状 内表面 外表面 実用新案登録 グラスウール ロックウール 上端外周 吸音手段 所定 開平 支持力 振動エネルギー 全長 上記構成 拡径方向 膨張圧 種類 カバー ～ ブロック 間隔 内側 概要 開発 手段 金属 木質 ゴム 樹脂 ウレタン 布 素材 ａ 動作 段階 状態 寸法 最後 撤去 除去 代わり 例 自体 両方 費用 産業 符号\n",
      "\n",
      "===== # 7, Topic : 7, p : 7.3986 %\n",
      "Topic words : －, Ａ, 位置, ２つ, 技術分野, 形態, 手段, 特許文献, 説明, 発明\n",
      "Input : 建物 固有周期 剛性分布 健全性判定用 分布 質量分布 質量 剛性分布設定方法 健全性確認方法 建物モデル 固有振動数 応答 剛性分布設定システム 剛性 健全性 回帰式 実施形態 式 設定 解析 判定 学習型応答推定機能 構造ヘルスモニタリングシステム モデル 台形分布 図 補正 初期情報 センサ 軒高 Ｓ造 最大加速度分布 固有周期算出手段 固有周期算出工程 応答推定 最大変位分布 健全性判定用質量分布 確認 前記 剛性分布算出手段 加速度 動的解析モデル 最大速度分布 建物応答 情報 健全性判定用質量 ＳＲＣ造 設計モデル 建物固有周期 補正係数 固有値解析 質点系モデル 刺激関数 建物階数 剛性分布算出工程 係数値 応答情報 ベイズ更新 発明 剛性分布係数Ｎ 剛性分布係数Ｎ倍 解析モデル センサ設置階 実験式 多層構造 事前分布 構造設計 値 剛性分布ｋ 剛性分布決定手段 剛性分布決定工程 Ｓ造建物モデル 固有振動数分布 構造物 補正係数算出手段 建物軒高算出手段 剛性算出手段 補正係数算出工程 剛性算出工程 適合性 最大層間変位分布 建物健全性判定システム Ｓ造建物 剛性分布作成工程 応答解析 層 構造種別 行列 変位 振動解析モデル 質点系解析モデル 固有周期比較手段 建物構造入力工程 階層 観測層 建物健全性判定装置 刺激関数算出手段 刺激関数算出工程 応答加速度 加速度応答 ｐ 推定 ａ ｂ θ 建物構造設計 対数回帰式 剛性行列Ｋ 数 質量分布ｍ 前記建物 標準階高 ｋ 線形応答 階 建物設計用 事後分布ｐ 特許文献 事後分布 前記補正係数 設定情報 パラメータ 階数 方法 設定値 高層建物 損傷 地震 加速度波形 層間変位 Ｔ 構造形式 健全性評価 各層 上記 一様分布 ＳＲＣ造建物モデル ｎ 振動センサ 周期Ｔ モデルイメージ 強非線形応答 最大応答 Ｓｔｅｐ ００７ 関数 システム Ｎ ＳＲＣ造建物 安全性 信頼性 階層数 ｍ ＲＣ 固有角振動 手段 土木構造物 ～ 推定値 Ｄ 建物層数 ｑ 設計 自由度系 刺激関数φ 振動計測 建物階層数 加速度センサ 初期モデル情報 モード応答相対加速度 ベイズ 更新 補正倍率 マンション等 設計情報 質量ｍ モデルパラメータ 動的解析情報 オフィスビル 参照 専門家 一定 相関 構造設計者 観測応答加速度波形ｙ 振動特性 真値 ｙ Ｍ 計算 相対変位 修正設計モデル 初期条件 建物各層 線形モデル 加速度ｙ ]↑Ｔ∈Ｒ 形態 耐震性評価 評価 高層 構造体 確率モデル 変化 下層 関係 低層 建て 速度 モード応答相対加速度ベクトル 前記軒高 ｍ次 応答ｙ∈Ｒ 質量行列Ｍ 台形形状 剛性ｋ ｊ 例 パラメータ数 波形情報 扱い質量 対象構造物 上層 ｃ シミュレーション Ｋ ｓ Ф 推定値ｑ（^） 横軸 縦軸 地動計測用 標準階 尤度関数 構造ヘルスモニタリング 減衰係数行列Ｃ 学習 センサ設置位置 変位差 検出情報 特開 号公報 ^（ハット ～ｎ φ 一般化 波形 初期パラメータ 事前情報 隣接階 精度 把握 各階 データ Ａ 観測データ ｆ 線 ｔｏｎ 作成 手法 修正 システム自体 地動 期待値 モード 最大点 観測データＤ 先行技術文献 t)]↑Ｔ 地動ｕ データ収録処理装置 技術分野 背景技術 説明 建築 事前 任意 定理 － 課題 目的 特徴 自動 独立 n m 傾向 Ｈ 範囲 逆数 ずれ 次 損傷検出手法 主要モード 歪み等 一般 Ｐ Ｐ（^） 形状 損傷検知 損傷個所 ｊ次 被災状況 被害状況 確率変数 Ｐ（^）） 対象 上記事情 フロー図 Ｃ 分散σ t ～（チルダ 計算例 度合い 早期 劣化 変形 同一 場所 予測 ケーブル 配線 一つ 分析 概要 判断 該軒高 π 効果 図面 他 時刻 平均 現実 影響 行 中層 下記 相似 全階 程度 誤差 通常 はずれ 選択 趣旨\n",
      "\n",
      "===== # 8, Topic : 14, p : 7.4700 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ヒータ管 側部ヒータ設備 枝管 温度 流量 ヘッダー管 ヒータ 測定値 管 前記ヒータ管 温水 データ処理装置 運転管理 地下タンク 回収側管体 運転状態 電磁流量 側 前記回収側管体 ヒータ運転 温度計 運転 ヒータ運転管理システム 積算流量 流量計 供給側管体 温水回収側 前記 分岐配管 前記温度計 外管 運転管理方法 流量調整 温水供給側 測定部 図 管理 ヒータ運転管理方法 温度測定 前記側部ヒータ設備 積算流量計 ヒータ設備 値 実施形態 側部ヒータ設備等 前記流量計 前記供給側管体 維持管理 特許文献 地盤 仕切弁 発明 配管 加温設備 内管 フレキシブルホース 断熱材 警報装置 タンク 前記測定値 測定 ストレート配管 自動温度測定 オリフィス U字管 底版 設備 接触式 端部 管タイプ 特許 上記 警報 接触 低温液化ガス 範囲 温度状態 前記データ処理装置 良否 所定 地中連続壁 調整 損失熱量 温度測定間隔 ａ 構成 付け型温度センサー 例 周囲 データ 循環ポンプ 警報等 図示 ｂ 号 特開平 号公報 マンホール 作業 流動障害 躯体 運転開始 鉄筋コンクリート製 循環水量 説明 外面 モニタリング 構造 オリフィス等 バイメタル式温度計 鋼製屋根 凍結範囲 °反対側 分岐配管等 集水層 鉛直方向 周方向 － 流れ コスト 判定 周囲地盤 配管作業 施工性 供給箇所 温水循環式 タンク稼働 タンクあたり 値未満 コントロール室 循環ポンプ等 形態 通信部等 側壁 外側 地上 ～ 個々 等量 課題 蓋 特徴 閉塞 影響 モニター 取付部 通信部 制御部 液化天然ガス 液化石油ガス 間隔 図面 範囲外 取付状態 記憶部 地下構造物 先行技術文献 同様フレキシブルホース 技術分野 背景技術 技術的範囲 砕石等 警報ランプ LNG LPG 概略 内部 貯留 近傍 順 付近 圧力損失 作業性 表面 管内 目視 ステップ 取付け 取外し 断面 無線 有線 両方 底版下 構造形式 長手方向 技術的思想 コントロール 周 取付 周辺地盤 m程度 月程度 制御信号 ℃未満 変更例 修正例 山留 筒状 熱交換 m 水圧差 差 両側フランジ トラブル原因 周辺 孔 当該孔 添付図面 構築 内面 メンブレン 下方 水位 参照 環状 バルブ 可 延長 位置 対策 概要 仕様 計算 誤差 直径 照明 目的 手段 効果 径方向 遠隔 設定 一定 コンピュータ 継 機能 オペレータ 調査 補修 アラーム 代わり ケース 一端 他端 常設 差分 コンクリート 業者 本願 範疇 各種 符号\n",
      "\n",
      "===== # 9, Topic : 8, p : 7.8664 %\n",
      "Topic words : 効果, 砂, Ａ, 側方, ｂ, 端部, 技術分野, 形態, 手段, 説明\n",
      "Input : 鉄骨柱 鉄筋カゴ 杭ユニット 杭 リング部材 実施形態 杭頭鋼管 杭構造 孔 上杭ユニット 係止鉄筋 円弧状鉄筋 Ｂ 図 Ａ 固定部材 架台 組立方法 上架台 主筋 外側部材 フープ筋 係止部 前記 ボルト孔 上記実施形態 前記鉄筋カゴ 凸状円弧面 前記リング部材 杭頭 断面 異形鉄筋 内側部材 発明 プレート 部材 状態 位置調整 杭頭剛性 本体部 軸方向 Ｈ形鋼 請求項 フラットバー アングル クレーン 凹状円弧面 前記鉄骨柱 中心部 孔底 内側 ボルト 断面図 丸鋼 台座部 工程 フック 方向 周方向 曲率半径 櫓 立断面図 杭底 柱 地組ヤードＧ 脚部 頂部 ジャッキ 接合プレート 変形例 部分断面図 方法 鉛直方向 地組ヤード 杭工事 杭長 精度 上部 コンクリート 略円弧状 鉄筋鉄骨コンクリート製 前記下架台 接合構造 上端部 略同一 特許文献 形態 技術 結束線 ルーズホール 構成 電動ウィンチ レール状部材 前記固定部材 下げ方法 端面 水平方向 間隔 地盤 ウェブ 中心 中央部分 十字状 位置 側面図 板状支持部材 支持部材 軸心 径方向内側 前記フープ筋 込み 効果 Ｃ 矢印Ｗ 孔径 ピース部材 トレミー管 円環状 傾斜計 建入れ チェーンブロック 補助フック Ｈ 端部 位置調整手段 外側 説明 構真柱 記載 複数 フランジ 置部 参照 トラバーサー 上記特許文献 上記 関係 Ｄ 矢印Ｈ 接合 所定位置 取付け位置 位置関係 上端部分 調整作業 組み立て方法 下記特許文献 例 先行技術文献 技術分野 背景技術 課題 地上 工期 図面 他方 等間隔 荷重 ワイヤー 作用 位置決め Ｇ 矢印Ｌ 径 中央 下げ 外径 掛合溝 特開 号公報 手段 治具 ジャッキアップ 板 巻回し ～（Ｄ 引抜抵抗力 アングル等 凹部 － 概要 影響 目的 円周 ２つ 図示 下部 建物 設柱 単一 辺 一対 張出し 下面 内径 上面 直径 寸法 工場 内部 目盛 スペース 起し 用 尺 別 形状 程度 ピッチ 製造 各種 要旨 範囲 態様 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 8.7436 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : ベルトコンベア昇降装置 ベルトコンベア 昇降部 ベルトコンベアＡ 昇降機構 支持部 例 スライド材 下枠 桁 ガイド材 チェーンブロック 実施例 】[ベルトコンベア昇降装置 枠組足場 作業 枠組足場Ｂ 昇降装置 チェーン 例]　実施例 機構 上桁 下部 作業効率 昇降幅 装置 発明 図 桁材 両端部 上部 枠状 尺状 受台 個別電子計算機 管理電子計算機 連続ベルトコンベア 】[ベルトコンベア昇降装置]＜ 作業員 特許文献 ２つ 枠 枠状体 】[昇降機構 支持枠 両端部付近 説明図 設置 支持 説明 トンネル 技術 部材 】[支持部 電子計算機 連結機構 Ａ 側面図 正面図 工事車両 形鋼 中間部 脚部 課題 重機 空間 地上 リブ 下方 ブラケット ターンテーブル部 作業コスト 高所 牽引式 特開 号公報 台車 受け替え 孔壁 支持台車 Ｂ 坑内 上下 各種 両者 所定 上方 複数 傾斜 支持台 高所作業車 作業工程 工程 操作 筒状 高所作業車等 クランプ機構 ギア機構 モーター機構 下部手摺 先行技術文献 技術分野 背景技術 ターンテーブル 中央下部 フレーム下部 他 設備 アンカー 金具 － 次 組み合わせ 効果 構成 人力 図面 ａ ｂ 材料 側 上部手摺 荷重 任意 山岳トンネル工事 中央上部 円筒形 連結 テールピース台車 製作コスト 施工コスト 水平手摺 ズリ運搬用 可能性 トラックミキサ等 安全性 フレーム下面 垂直方向 直交方向 電動ウインチ 油圧ウインチ 占有面積 坑外 方法 最大高 最大 角形鋼管 内部形状 ギア 対象物 操作端 使用方法 ＣＡＤデータ 施工 切羽 抵触 天井 延伸 側壁 バックホウ 概要 欠点 工期 遅延 原因 確保 レール 構造 ジャッキ 目的 手段 上記 特徴 状態 既存 軽量 移動 形態 滑落 力 円柱 外形 本体 公知 重量 能力 後方 配置 上昇 撤去 単体 架台 変更 駆動 有線 無線 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.7194e-18, 1.6532e-14, 2.3813e-02, 5.2701e-07, 4.6729e-02, 2.7615e-11,\n",
      "         1.7323e-18, 1.7184e-05, 9.2733e-01, 2.0002e-03, 7.2094e-21, 6.0824e-19,\n",
      "         4.8655e-05, 6.2580e-15, 6.5567e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0563, 0.0672, 0.0444, 0.0620, 0.0331, 0.0751, 0.0652, 0.0927, 0.0574,\n",
      "         0.0704, 0.0835, 0.0648, 0.0511, 0.1263, 0.0507]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 983.0473\n",
      "Test epoch : 1 Average loss: 1029.9408\n",
      "PP(train) = 1996.306, PP(valid) = 2115.631\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8495e-27, 9.8535e-23, 1.4303e-10, 2.2716e-21, 7.4775e-13, 4.2126e-25,\n",
      "         1.5786e-26, 1.1001e-15, 5.7383e-10, 4.3923e-26, 9.8364e-27, 9.3901e-13,\n",
      "         7.3474e-30, 2.0435e-23, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 982.8907\n",
      "Test epoch : 2 Average loss: 1029.8897\n",
      "PP(train) = 1993.966, PP(valid) = 2114.767\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8856e-17, 5.8597e-12, 3.7620e-05, 1.5439e-13, 2.6604e-04, 1.0429e-13,\n",
      "         8.2080e-12, 9.9630e-01, 3.6632e-05, 2.6436e-15, 1.7444e-11, 9.1376e-08,\n",
      "         3.2689e-03, 3.6975e-16, 8.6367e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1055, 0.0544, 0.0443, 0.0790, 0.0922, 0.0768, 0.0437, 0.0532,\n",
      "         0.0371, 0.1190, 0.0439, 0.0366, 0.0939, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 982.8164\n",
      "Test epoch : 3 Average loss: 1029.8369\n",
      "PP(train) = 1991.258, PP(valid) = 2113.960\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9619e-21, 5.0494e-24, 2.4979e-07, 1.0392e-06, 3.3814e-04, 3.4153e-18,\n",
      "         4.1609e-18, 9.9932e-01, 3.3598e-04, 5.3418e-23, 3.8369e-20, 7.4522e-14,\n",
      "         1.0333e-07, 7.0764e-18, 3.8468e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0443, 0.0791, 0.0922, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 982.5684\n",
      "Test epoch : 4 Average loss: 1029.7832\n",
      "PP(train) = 1988.237, PP(valid) = 2113.090\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.0772e-23, 4.1663e-18, 6.3502e-02, 1.2441e-06, 4.2855e-08, 1.3914e-12,\n",
      "         2.8148e-12, 1.6938e-09, 9.3000e-01, 1.8509e-06, 5.1450e-15, 4.5336e-06,\n",
      "         2.5361e-06, 2.0123e-10, 6.4830e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0576, 0.0687, 0.0447, 0.0636, 0.0327, 0.0741, 0.0641, 0.0912, 0.0591,\n",
      "         0.0687, 0.0832, 0.0641, 0.0513, 0.1280, 0.0488]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 982.3953\n",
      "Test epoch : 5 Average loss: 1029.7309\n",
      "PP(train) = 1985.111, PP(valid) = 2112.237\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0303e-26, 1.4922e-20, 1.2997e-05, 5.0566e-07, 6.5244e-04, 5.3032e-22,\n",
      "         2.7278e-15, 4.8943e-06, 9.9170e-01, 1.4387e-25, 5.5809e-18, 1.9829e-08,\n",
      "         9.5323e-07, 2.9982e-04, 7.3283e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0563, 0.0672, 0.0446, 0.0628, 0.0306, 0.0760, 0.0632, 0.0920, 0.0570,\n",
      "         0.0692, 0.0845, 0.0651, 0.0511, 0.1308, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 982.1634\n",
      "Test epoch : 6 Average loss: 1029.6765\n",
      "PP(train) = 1981.992, PP(valid) = 2111.398\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0395e-13, 2.1435e-16, 6.2677e-06, 5.2524e-19, 9.7517e-01, 2.1299e-11,\n",
      "         2.3728e-15, 6.0833e-11, 1.1482e-02, 2.9388e-14, 1.3331e-02, 2.6741e-10,\n",
      "         4.6032e-08, 1.3881e-21, 6.5336e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0454, 0.0489, 0.0423, 0.0384, 0.0945, 0.0659, 0.1048, 0.1080, 0.0454,\n",
      "         0.0928, 0.0680, 0.0601, 0.0455, 0.0632, 0.0768]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 982.0489\n",
      "Test epoch : 7 Average loss: 1029.6229\n",
      "PP(train) = 1978.872, PP(valid) = 2110.562\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1055e-12, 1.6448e-19, 3.4330e-15, 1.1813e-06, 2.0978e-05, 6.6130e-41,\n",
      "         2.2897e-27, 9.9998e-01, 4.7375e-10, 8.1158e-20, 1.8756e-10, 3.8684e-26,\n",
      "         1.5244e-08, 4.3194e-08, 3.4020e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 981.8207\n",
      "Test epoch : 8 Average loss: 1029.5728\n",
      "PP(train) = 1975.750, PP(valid) = 2109.759\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7396e-23, 5.5069e-10, 5.5998e-16, 5.6770e-11, 9.9915e-01, 1.1731e-14,\n",
      "         4.8373e-19, 5.7391e-04, 3.9997e-06, 8.6987e-11, 1.1380e-17, 4.4453e-11,\n",
      "         3.6960e-11, 2.6895e-04, 2.5268e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 981.6373\n",
      "Test epoch : 9 Average loss: 1029.5193\n",
      "PP(train) = 1972.620, PP(valid) = 2108.872\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6052e-10, 1.0332e-17, 1.4841e-10, 4.6250e-12, 2.8306e-09, 5.1446e-22,\n",
      "         2.5305e-15, 1.0000e+00, 4.9447e-12, 5.0769e-07, 1.1835e-17, 5.9171e-21,\n",
      "         1.9055e-08, 1.1607e-11, 1.2207e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 981.4216\n",
      "Test epoch : 10 Average loss: 1029.4696\n",
      "PP(train) = 1969.602, PP(valid) = 2108.129\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9802e-20, 1.0806e-06, 1.0000e+00, 2.9178e-10, 6.5007e-21, 1.0418e-16,\n",
      "         1.0815e-13, 5.1718e-12, 9.3212e-10, 2.3421e-12, 5.8596e-21, 8.1681e-20,\n",
      "         3.3651e-18, 4.3947e-25, 1.9028e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 981.2453\n",
      "Test epoch : 11 Average loss: 1029.4208\n",
      "PP(train) = 1966.640, PP(valid) = 2107.395\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2536e-12, 5.7173e-10, 1.5024e-06, 2.0781e-14, 9.9424e-01, 2.8392e-12,\n",
      "         3.0382e-16, 9.6681e-07, 6.3208e-07, 1.2236e-13, 3.1162e-13, 7.7951e-14,\n",
      "         1.6338e-03, 1.0614e-15, 4.1255e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0449, 0.0479, 0.0424, 0.0382, 0.0957, 0.0657, 0.1065, 0.1083, 0.0451,\n",
      "         0.0938, 0.0671, 0.0601, 0.0456, 0.0621, 0.0766]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 980.9753\n",
      "Test epoch : 12 Average loss: 1029.3743\n",
      "PP(train) = 1963.628, PP(valid) = 2106.630\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.1098e-40, 2.0568e-20, 4.8385e-07, 1.6522e-18, 9.0315e-11, 6.0183e-26,\n",
      "         4.1267e-25, 1.0000e+00, 8.6866e-09, 1.0046e-12, 6.0845e-23, 4.8014e-16,\n",
      "         3.5923e-10, 1.7483e-24, 2.3068e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 980.7618\n",
      "Test epoch : 13 Average loss: 1029.3246\n",
      "PP(train) = 1960.780, PP(valid) = 2105.934\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8351e-28, 1.3267e-14, 1.4582e-07, 1.0159e-16, 2.3117e-11, 3.6452e-17,\n",
      "         1.0317e-12, 1.5334e-03, 2.2402e-04, 1.3734e-20, 7.8916e-19, 2.5848e-20,\n",
      "         2.0543e-11, 1.1398e-11, 9.9824e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1042, 0.0502, 0.0601, 0.0670, 0.1075, 0.0922, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 980.5680\n",
      "Test epoch : 14 Average loss: 1029.2766\n",
      "PP(train) = 1957.884, PP(valid) = 2105.195\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0663e-13, 2.0891e-09, 3.1547e-15, 2.5367e-12, 9.7593e-01, 8.4825e-14,\n",
      "         5.1136e-12, 1.6596e-04, 4.5348e-13, 6.4436e-10, 1.5815e-16, 3.1233e-10,\n",
      "         4.9385e-14, 2.9819e-04, 2.3608e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0455, 0.0479, 0.0431, 0.0384, 0.0951, 0.0658, 0.1068, 0.1081, 0.0454,\n",
      "         0.0928, 0.0672, 0.0601, 0.0458, 0.0624, 0.0757]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 980.4220\n",
      "Test epoch : 15 Average loss: 1029.2315\n",
      "PP(train) = 1954.917, PP(valid) = 2104.387\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1701e-23, 3.0820e-22, 4.8355e-08, 1.2238e-24, 4.4782e-11, 1.5830e-18,\n",
      "         1.2515e-27, 1.1487e-04, 8.5544e-19, 7.6228e-19, 3.1194e-21, 2.7604e-16,\n",
      "         5.5281e-12, 1.0536e-27, 9.9989e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 980.2160\n",
      "Test epoch : 16 Average loss: 1029.1850\n",
      "PP(train) = 1952.195, PP(valid) = 2103.729\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8581e-18, 1.7365e-22, 3.4581e-03, 2.8714e-18, 4.5247e-16, 5.6168e-25,\n",
      "         2.5192e-22, 1.1142e-12, 1.8991e-19, 8.4044e-24, 2.5474e-21, 1.1530e-17,\n",
      "         2.4418e-12, 1.2388e-24, 9.9654e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0445, 0.1041, 0.0503, 0.0602, 0.0668, 0.1074, 0.0923, 0.0594,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 980.0379\n",
      "Test epoch : 17 Average loss: 1029.1413\n",
      "PP(train) = 1949.412, PP(valid) = 2103.070\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.4156e-18, 6.7516e-22, 2.5641e-08, 1.6716e-03, 7.5849e-16, 2.4596e-12,\n",
      "         8.3665e-16, 2.3338e-02, 1.0388e-08, 4.7300e-11, 2.1400e-21, 9.7499e-01,\n",
      "         2.5655e-11, 6.4132e-15, 1.2699e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0621, 0.0630, 0.0811, 0.0550, 0.0553, 0.1195, 0.0465, 0.0776, 0.0972,\n",
      "         0.0678, 0.0453, 0.0659, 0.0589, 0.0671, 0.0378]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 979.8650\n",
      "Test epoch : 18 Average loss: 1029.0990\n",
      "PP(train) = 1946.521, PP(valid) = 2102.315\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8216e-26, 1.0819e-21, 3.2248e-15, 9.9071e-01, 8.0544e-17, 1.5815e-33,\n",
      "         6.4432e-25, 5.7927e-11, 1.3062e-11, 3.7965e-19, 1.8648e-19, 2.6908e-25,\n",
      "         9.2945e-03, 4.4168e-22, 7.8010e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0420, 0.0552, 0.0916, 0.0502, 0.0528, 0.0524, 0.0534, 0.0687,\n",
      "         0.1348, 0.1023, 0.0485, 0.0563, 0.0522, 0.0909]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 979.6654\n",
      "Test epoch : 19 Average loss: 1029.0565\n",
      "PP(train) = 1943.871, PP(valid) = 2101.721\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.4503e-29, 7.0274e-11, 9.9974e-01, 1.4084e-15, 8.0625e-20, 1.5376e-21,\n",
      "         5.1437e-11, 5.0487e-10, 8.8063e-11, 9.9834e-15, 1.6320e-17, 2.4156e-20,\n",
      "         2.6232e-04, 9.5609e-22, 1.2002e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 979.5213\n",
      "Test epoch : 20 Average loss: 1029.0149\n",
      "PP(train) = 1941.193, PP(valid) = 2101.090\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7180e-31, 1.1415e-22, 9.5656e-18, 1.0188e-24, 9.9975e-01, 1.0521e-19,\n",
      "         1.7890e-22, 2.3559e-15, 2.3217e-16, 1.2221e-23, 1.3428e-33, 1.2275e-21,\n",
      "         2.3739e-08, 1.5276e-19, 2.4623e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 979.2903\n",
      "Test epoch : 21 Average loss: 1028.9744\n",
      "PP(train) = 1938.451, PP(valid) = 2100.419\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.5220e-23, 1.5586e-16, 9.9972e-01, 4.3319e-10, 2.4048e-04, 4.5866e-30,\n",
      "         3.9715e-24, 5.2062e-07, 6.8110e-08, 2.2180e-23, 1.6933e-17, 1.1988e-20,\n",
      "         3.5897e-05, 1.4047e-11, 7.8537e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0898, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 979.1129\n",
      "Test epoch : 22 Average loss: 1028.9335\n",
      "PP(train) = 1935.880, PP(valid) = 2099.838\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5437e-18, 5.1977e-13, 3.3973e-08, 3.1351e-18, 3.1922e-07, 1.5357e-15,\n",
      "         3.7496e-19, 5.0801e-15, 1.2017e-20, 9.5212e-12, 9.8451e-22, 1.2911e-17,\n",
      "         1.4114e-12, 4.5660e-20, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 978.9666\n",
      "Test epoch : 23 Average loss: 1028.8944\n",
      "PP(train) = 1933.210, PP(valid) = 2099.197\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.6317e-24, 1.0845e-20, 1.1939e-21, 4.2937e-16, 3.0650e-17, 8.7050e-24,\n",
      "         7.8713e-28, 4.8727e-29, 1.8314e-16, 6.2759e-21, 3.4036e-24, 1.6843e-15,\n",
      "         7.6138e-12, 9.4998e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 978.8054\n",
      "Test epoch : 24 Average loss: 1028.8551\n",
      "PP(train) = 1930.577, PP(valid) = 2098.557\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7724e-38, 3.2948e-20, 4.8173e-11, 1.1367e-30, 6.6807e-22, 3.9295e-35,\n",
      "         1.1132e-38, 9.0454e-24, 1.7882e-17, 6.7588e-28, 7.1839e-28, 2.3043e-26,\n",
      "         6.5552e-15, 7.3309e-22, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 978.5804\n",
      "Test epoch : 25 Average loss: 1028.8185\n",
      "PP(train) = 1928.100, PP(valid) = 2098.063\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0820e-14, 8.2791e-16, 9.9803e-01, 3.3879e-21, 2.2536e-07, 5.0363e-17,\n",
      "         9.6248e-22, 5.3010e-08, 4.9979e-09, 1.9720e-03, 1.3235e-13, 2.9439e-16,\n",
      "         1.1037e-15, 9.9060e-11, 7.4550e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0759, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0948,\n",
      "         0.0582, 0.0622, 0.0478, 0.0511, 0.0865, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 978.4557\n",
      "Test epoch : 26 Average loss: 1028.7826\n",
      "PP(train) = 1925.567, PP(valid) = 2097.517\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.2402e-22, 3.8770e-23, 7.1770e-01, 1.6359e-07, 2.8207e-01, 9.1678e-35,\n",
      "         1.3955e-21, 7.5848e-05, 4.5778e-13, 5.9750e-09, 2.1493e-17, 2.4466e-20,\n",
      "         1.0160e-05, 8.6279e-16, 1.4389e-04]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0667, 0.0767, 0.0445, 0.0611, 0.0889, 0.0538, 0.0843, 0.0851, 0.0784,\n",
      "         0.0679, 0.0649, 0.0519, 0.0505, 0.0803, 0.0448]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 978.2020\n",
      "Test epoch : 27 Average loss: 1028.7486\n",
      "PP(train) = 1922.946, PP(valid) = 2096.912\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.8524e-31, 1.3192e-15, 3.9918e-08, 5.7046e-16, 9.9227e-01, 3.8892e-21,\n",
      "         1.9043e-29, 9.8639e-05, 7.6312e-03, 5.6086e-18, 9.5875e-29, 1.3587e-20,\n",
      "         3.0214e-14, 1.9218e-12, 1.1193e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0480, 0.0422, 0.0383, 0.0952, 0.0658, 0.1063, 0.1083, 0.0451,\n",
      "         0.0938, 0.0673, 0.0602, 0.0456, 0.0625, 0.0765]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 978.1097\n",
      "Test epoch : 28 Average loss: 1028.7155\n",
      "PP(train) = 1920.598, PP(valid) = 2096.498\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0152e-20, 4.6189e-15, 2.0830e-11, 5.7799e-13, 2.0228e-15, 2.8922e-22,\n",
      "         2.4054e-16, 2.9126e-02, 1.5640e-23, 8.2145e-20, 2.9008e-30, 7.7321e-05,\n",
      "         1.3506e-09, 5.0882e-19, 9.7080e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0820, 0.0456, 0.1027, 0.0502, 0.0607, 0.0677, 0.1068, 0.0906, 0.0593,\n",
      "         0.0519, 0.0633, 0.0554, 0.0507, 0.0716, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 977.9385\n",
      "Test epoch : 29 Average loss: 1028.6785\n",
      "PP(train) = 1918.052, PP(valid) = 2095.857\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.9342e-15, 2.1203e-23, 2.2001e-13, 1.5509e-12, 4.2759e-13, 2.8639e-27,\n",
      "         1.0579e-12, 9.9999e-01, 7.3137e-13, 1.1899e-05, 1.1233e-17, 1.5304e-18,\n",
      "         4.7588e-09, 1.6836e-11, 4.3291e-20]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 977.7711\n",
      "Test epoch : 30 Average loss: 1028.6444\n",
      "PP(train) = 1915.525, PP(valid) = 2095.249\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8140e-24, 2.4239e-08, 2.2678e-08, 1.1237e-09, 9.9977e-01, 4.5960e-13,\n",
      "         3.8027e-23, 1.6342e-05, 4.5307e-08, 9.4523e-07, 2.5659e-11, 1.2618e-04,\n",
      "         7.7982e-19, 1.2935e-15, 8.2464e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 977.6078\n",
      "Test epoch : 31 Average loss: 1028.6116\n",
      "PP(train) = 1913.234, PP(valid) = 2094.842\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8136e-23, 1.1050e-17, 2.4092e-05, 3.2175e-20, 4.6197e-10, 6.3883e-27,\n",
      "         1.4627e-23, 3.4502e-12, 1.8533e-08, 1.6862e-16, 2.0909e-15, 8.4662e-21,\n",
      "         3.0263e-09, 8.7790e-22, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 977.3965\n",
      "Test epoch : 32 Average loss: 1028.5775\n",
      "PP(train) = 1910.922, PP(valid) = 2094.365\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.2785e-21, 2.9544e-10, 2.5559e-10, 2.2821e-24, 9.9999e-01, 2.1847e-22,\n",
      "         3.5136e-26, 2.6807e-08, 3.6105e-14, 3.4116e-19, 7.2646e-24, 6.2414e-21,\n",
      "         1.2642e-05, 4.4868e-10, 2.1542e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 977.2827\n",
      "Test epoch : 33 Average loss: 1028.5450\n",
      "PP(train) = 1908.427, PP(valid) = 2093.777\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4791e-18, 3.5361e-24, 8.2620e-08, 1.0311e-25, 1.6691e-09, 2.3682e-20,\n",
      "         1.1016e-23, 9.8182e-01, 1.8545e-06, 1.2727e-17, 8.4471e-16, 1.8267e-19,\n",
      "         3.5696e-13, 2.6181e-13, 1.8179e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0835, 0.1041, 0.0551, 0.0444, 0.0789, 0.0919, 0.0775, 0.0443, 0.0533,\n",
      "         0.0373, 0.1182, 0.0441, 0.0368, 0.0936, 0.0371]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 977.0862\n",
      "Test epoch : 34 Average loss: 1028.5152\n",
      "PP(train) = 1906.112, PP(valid) = 2093.343\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.4749e-11, 1.0984e-12, 3.2755e-07, 1.1566e-11, 1.4132e-14, 1.0796e-21,\n",
      "         2.0459e-20, 3.2738e-09, 1.1067e-07, 1.3746e-15, 2.9541e-22, 1.7185e-14,\n",
      "         6.1004e-09, 9.4797e-01, 5.2025e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0410, 0.0345, 0.0743, 0.0912, 0.0749, 0.0877, 0.0495, 0.0622, 0.1284,\n",
      "         0.0524, 0.0656, 0.0495, 0.0606, 0.0905, 0.0377]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 976.9230\n",
      "Test epoch : 35 Average loss: 1028.4840\n",
      "PP(train) = 1903.805, PP(valid) = 2092.851\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2356e-17, 4.9039e-19, 4.6991e-12, 1.5172e-13, 1.4123e-03, 4.6834e-18,\n",
      "         1.5410e-16, 3.9128e-13, 1.7207e-07, 8.3753e-15, 1.1455e-26, 9.1741e-18,\n",
      "         2.9319e-07, 9.9859e-01, 2.5528e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0393, 0.0339, 0.0725, 0.0936, 0.0754, 0.0885, 0.0472, 0.0606, 0.1330,\n",
      "         0.0522, 0.0655, 0.0489, 0.0609, 0.0912, 0.0374]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 976.8188\n",
      "Test epoch : 36 Average loss: 1028.4522\n",
      "PP(train) = 1901.463, PP(valid) = 2092.328\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6454e-08, 3.6829e-10, 5.4163e-03, 2.5913e-14, 2.6913e-04, 6.9589e-17,\n",
      "         1.8542e-19, 9.6316e-01, 2.5452e-02, 1.5066e-14, 1.1472e-04, 2.8379e-20,\n",
      "         9.4057e-07, 1.9014e-07, 5.5864e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0828, 0.1041, 0.0544, 0.0449, 0.0774, 0.0916, 0.0769, 0.0449, 0.0535,\n",
      "         0.0379, 0.1178, 0.0445, 0.0371, 0.0948, 0.0373]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 976.6705\n",
      "Test epoch : 37 Average loss: 1028.4248\n",
      "PP(train) = 1899.231, PP(valid) = 2091.936\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1004e-15, 4.6540e-14, 8.5137e-09, 1.1685e-12, 4.3550e-13, 1.3566e-20,\n",
      "         4.4468e-15, 1.0000e+00, 8.2859e-14, 2.3821e-11, 1.3371e-20, 6.5620e-20,\n",
      "         3.1649e-11, 2.7630e-28, 2.9506e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 976.5078\n",
      "Test epoch : 38 Average loss: 1028.3954\n",
      "PP(train) = 1896.959, PP(valid) = 2091.483\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.6616e-22, 2.7588e-31, 1.4861e-07, 1.2348e-03, 4.1062e-06, 6.2301e-16,\n",
      "         8.5450e-16, 2.6889e-03, 8.4360e-06, 2.7575e-15, 2.3309e-10, 5.3421e-17,\n",
      "         2.1074e-08, 1.4225e-13, 9.9606e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0445, 0.1041, 0.0502, 0.0601, 0.0670, 0.1074, 0.0921, 0.0593,\n",
      "         0.0523, 0.0621, 0.0556, 0.0510, 0.0709, 0.0415]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 976.3214\n",
      "Test epoch : 39 Average loss: 1028.3678\n",
      "PP(train) = 1894.706, PP(valid) = 2091.047\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5742e-28, 9.1749e-14, 2.9657e-04, 1.2807e-03, 6.7331e-01, 1.8041e-17,\n",
      "         3.0941e-13, 1.4059e-06, 4.0226e-09, 9.7024e-12, 4.5040e-05, 4.3078e-16,\n",
      "         2.6266e-05, 3.2504e-01, 2.4662e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0445, 0.0443, 0.0521, 0.0530, 0.0918, 0.0750, 0.0847, 0.0928, 0.0664,\n",
      "         0.0804, 0.0691, 0.0582, 0.0519, 0.0729, 0.0629]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 976.1795\n",
      "Test epoch : 40 Average loss: 1028.3402\n",
      "PP(train) = 1892.540, PP(valid) = 2090.637\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.6005e-14, 4.3547e-05, 3.7142e-01, 2.7104e-10, 5.0823e-01, 2.9624e-09,\n",
      "         2.2220e-20, 6.1539e-05, 8.8930e-02, 4.1197e-13, 2.7249e-11, 3.3427e-07,\n",
      "         1.4730e-07, 1.9096e-15, 3.1314e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0582, 0.0640, 0.0456, 0.0522, 0.0835, 0.0611, 0.0917, 0.0954, 0.0629,\n",
      "         0.0773, 0.0684, 0.0570, 0.0496, 0.0775, 0.0558]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 976.0152\n",
      "Test epoch : 41 Average loss: 1028.3168\n",
      "PP(train) = 1890.341, PP(valid) = 2090.271\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4328e-26, 3.5541e-18, 9.9997e-01, 1.6056e-16, 1.5037e-21, 1.1254e-22,\n",
      "         2.3200e-16, 1.4461e-16, 3.8413e-13, 1.3735e-25, 9.0805e-18, 2.0072e-24,\n",
      "         3.1683e-05, 1.9023e-12, 4.1117e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 975.8746\n",
      "Test epoch : 42 Average loss: 1028.2889\n",
      "PP(train) = 1888.079, PP(valid) = 2089.778\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6093e-18, 2.5763e-16, 1.1964e-16, 6.0573e-12, 9.0694e-01, 9.8763e-24,\n",
      "         5.5981e-22, 2.7463e-07, 2.7203e-12, 2.4931e-11, 3.8974e-12, 4.7028e-09,\n",
      "         1.4857e-14, 3.2663e-07, 9.3058e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0476, 0.0478, 0.0461, 0.0393, 0.0924, 0.0662, 0.1074, 0.1075, 0.0465,\n",
      "         0.0896, 0.0671, 0.0600, 0.0463, 0.0632, 0.0729]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 975.7587\n",
      "Test epoch : 43 Average loss: 1028.2595\n",
      "PP(train) = 1885.995, PP(valid) = 2089.368\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3495e-13, 8.1351e-19, 1.0000e+00, 5.7017e-22, 7.5243e-11, 1.3041e-23,\n",
      "         4.7788e-20, 9.3308e-14, 3.2982e-16, 5.7710e-23, 2.6491e-18, 1.4869e-15,\n",
      "         6.4808e-14, 4.6814e-18, 1.0114e-19]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 975.6557\n",
      "Test epoch : 44 Average loss: 1028.2361\n",
      "PP(train) = 1883.911, PP(valid) = 2089.029\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.8064e-18, 1.1648e-14, 3.0254e-17, 6.1031e-13, 2.4704e-02, 6.4546e-17,\n",
      "         3.0622e-14, 4.2523e-11, 5.8603e-11, 1.4491e-24, 6.0953e-16, 4.1255e-14,\n",
      "         1.3210e-18, 1.3712e-16, 9.7530e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0807, 0.0445, 0.1022, 0.0500, 0.0609, 0.0670, 0.1077, 0.0929, 0.0590,\n",
      "         0.0532, 0.0622, 0.0558, 0.0510, 0.0707, 0.0421]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 975.4673\n",
      "Test epoch : 45 Average loss: 1028.2122\n",
      "PP(train) = 1881.696, PP(valid) = 2088.588\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.9407e-15, 2.8666e-05, 2.4161e-07, 5.7255e-09, 2.7509e-03, 1.6798e-13,\n",
      "         3.6549e-13, 2.4290e-05, 9.8836e-01, 5.0262e-07, 8.1025e-11, 1.9196e-20,\n",
      "         5.2701e-03, 3.7259e-10, 3.5685e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0446, 0.0628, 0.0306, 0.0759, 0.0631, 0.0920, 0.0571,\n",
      "         0.0694, 0.0843, 0.0650, 0.0511, 0.1305, 0.0500]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 975.3050\n",
      "Test epoch : 46 Average loss: 1028.1862\n",
      "PP(train) = 1879.623, PP(valid) = 2088.222\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.5665e-21, 1.8680e-02, 3.7616e-08, 4.9015e-11, 9.8093e-01, 9.4376e-25,\n",
      "         6.2084e-11, 6.2197e-09, 1.6210e-04, 1.7934e-14, 1.0384e-16, 3.6834e-11,\n",
      "         2.1983e-04, 2.8871e-06, 9.3169e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0456, 0.0480, 0.0422, 0.0389, 0.0948, 0.0655, 0.1062, 0.1067, 0.0456,\n",
      "         0.0939, 0.0666, 0.0600, 0.0459, 0.0630, 0.0770]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 975.1030\n",
      "Test epoch : 47 Average loss: 1028.1629\n",
      "PP(train) = 1877.612, PP(valid) = 2087.923\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5625e-16, 2.9851e-08, 1.2355e-02, 4.5765e-02, 2.1578e-10, 8.0665e-11,\n",
      "         9.2600e-07, 3.1836e-01, 6.2351e-01, 4.4712e-12, 9.7641e-13, 8.2499e-10,\n",
      "         4.0361e-07, 1.8394e-22, 5.1560e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0651, 0.0783, 0.0490, 0.0588, 0.0438, 0.0811, 0.0684, 0.0723, 0.0580,\n",
      "         0.0599, 0.0973, 0.0579, 0.0473, 0.1155, 0.0475]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 975.0404\n",
      "Test epoch : 48 Average loss: 1028.1424\n",
      "PP(train) = 1875.509, PP(valid) = 2087.583\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9453e-01, 3.3654e-06, 5.3896e-06, 1.6821e-01, 1.4855e-03, 1.0140e-07,\n",
      "         1.8306e-16, 6.6984e-05, 2.5927e-10, 1.0552e-08, 4.8353e-16, 2.1431e-15,\n",
      "         1.2986e-05, 2.1360e-07, 5.3568e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0806, 0.0513, 0.0674, 0.0716, 0.0554, 0.0597, 0.0729, 0.0906, 0.0660,\n",
      "         0.0719, 0.0751, 0.0694, 0.0590, 0.0625, 0.0465]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 974.8729\n",
      "Test epoch : 49 Average loss: 1028.1202\n",
      "PP(train) = 1873.385, PP(valid) = 2087.188\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3642e-20, 4.7051e-10, 4.2316e-26, 3.7470e-15, 9.7358e-01, 3.9791e-08,\n",
      "         1.1962e-19, 1.0066e-09, 2.6418e-02, 3.3202e-08, 3.2260e-19, 4.3761e-15,\n",
      "         6.7124e-22, 2.5879e-21, 2.3883e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0484, 0.0423, 0.0387, 0.0933, 0.0661, 0.1054, 0.1082, 0.0454,\n",
      "         0.0935, 0.0678, 0.0604, 0.0458, 0.0635, 0.0760]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 974.7796\n",
      "Test epoch : 50 Average loss: 1028.0988\n",
      "PP(train) = 1871.419, PP(valid) = 2086.878\n",
      "Writing to ./topicwords/18-topwords_e50.txt\n",
      "Topic 0: 土壌 角度θ 汚染土壌 アルカリ性 定着具 継手板 定着構造 短期間 チャンバー 締め付け\n",
      "Topic 1: 土壌 角度θ チャンバー アルカリ性 タイル 免震構造 定着具 蓋 部品等 発破\n",
      "Topic 2: 参照 位置 配置 技術分野 構造 形態 特許文献 手段 説明 発明\n",
      "Topic 3: 土壌 欠点 付近 計算 ロ 周 各々 基準 排気口 現地\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 手段 特許文献 説明\n",
      "Topic 5: 土壌 角度θ 定着具 くさび 礫 免震構造 蓋 継手板 汚染土壌 アルカリ性\n",
      "Topic 6: 角度θ 土壌 くさび 定着具 蓋 アルカリ性 礫 継手板 タイル 締め付け\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 特許文献 手段 説明 発明\n",
      "Topic 8: 効果 砂 Ａ 側方 ｂ 端部 技術分野 形態 手段 特許文献\n",
      "Topic 9: 土壌 角度θ 礫 汚染土壌 定着具 計算 蓋 排気口 タイル アルカリ性\n",
      "Topic 10: 土壌 角度θ 短期間 計算 定着具 アルカリ性 部品等 礫 汚染土壌 継手板\n",
      "Topic 11: 土壌 角度θ 定着具 くさび チャンバー 部品等 礫 タイル アルカリ性 継手板\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 特許文献 手段 説明 発明\n",
      "Topic 13: 土壌 角度θ 定着具 短期間 多角形 礫 くさび 計算 締め付け 樹脂等\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 4, p : 7.8326 %\n",
      "Topic words : 上方, ｃ, 発明, 参照, 力, 技術分野, 形態, 手段, 特許文献, 説明\n",
      "Input : 基準墨 基準レベル 位置 グローバル座標 位置座標 ローカル座標 ＧＰＳ観測局 誤差 前記基準墨 座標 墨出し ＧＰＳ座標算出部 建物 ＳＴＥＰ 位置情報 位置精度 測定情報データベース ＧＰＳ電波 誤差算出部 ＧＰＳ情報データベース 前記ＧＰＳ観測局 基準 相対位置座標 風速 基準値 変位 加速度 座標変換部 Ｙ ＧＰＳ基準局 ＧＰＳ電波情報 設計情報データベース 墨出し位置 Ｚ 前記建物 系列データ Ｘ 墨出し座標算出部 図 測定グローバル座標 精度 Ｒ ＧＰＳ受信手段 検知 前記誤差算出部 設計 Ｇ 所定 水平方向 風向風速 入力部 実施形態 レベル誤差 加速度情報 測定対象期間 平均風速 前記変位検知手段 レーザ位置測定器 標準偏差 変動係数 処理端末 精度監視システム 当該ＧＰＳ観測局 基準点 風向風速情報 測定対象 座標変換 墨出しレベル 平均加速度 前記墨出し座標算出部 ＧＰＳ装置 傾きθ 変位判別手段 Ｎ節 変換 座標データ 前記 ＧＰＳ座標取得部 レベル 前記ＧＰＳ位置座標取得部 設計グローバル座標 水平誤差 方法 座標軸 分析対象 入力 発明 上層階 出力部 測定 ねじれ 平均値 上記 閾値 最上階 相対測位 設計情報 記相対位置座標 風向 ～ＳＴＥＰ 式 監視システム 前記ＧＰＳ基準局 システム 閾値以下 上階 監視方法 鉛直方向 位置測定器等 下階 水平方向誤差 説明 強風 影響 方向 修正量 Ｎ－ 処理 前記入力部 鉛直方向誤差 作業 地震 振動 フローチャート 工程 データベース ｃｏｓθ ｓｉｎθ －ｒ 上層 ～ 期間 方位等 風速計 現場 グランドレベル ねじれ角φ 誤差分 下層 共振 修正 風向風速計 特許文献 手段 ｒ 加速度計 否 上部 形態 夫 差 風 上昇 流れ 構成部 構成 受信環境 Ｚ軸方向 レーザ距離計 高層建物 所定値 設計図 課題 状態 特徴 帯 図面 外力 変形 原点 δ 梁 緯線 経線 角度 スチールテープ等 ディスプレー等 パーソナルコンピュータ等 レーザ式 複数点 単独測位 装置 Ｚ成分 複数 φ ＸＹ成分 技術分野 背景技術 計測方法 特許請求 柱梁架構 複数層 通り芯 通り線 特開平 号公報 近傍 現場近傍 パーソナルコンピュータ 柱 構築 進行 周囲 開示 目的 効果 最良 緯度 経度 海抜 風向き 範囲 Ａ Ｂ ハードディスク ＣＰＵ メモリ プログラム 鉄骨 夜間 日射 上下 極座標 ａ ｂ 水平面 状況 符号\n",
      "\n",
      "===== # 2, Topic : 4, p : 7.3828 %\n",
      "Topic words : 上方, ｃ, 発明, 参照, 力, 技術分野, 形態, 手段, 特許文献, 説明\n",
      "Input : 膜型アクチュエータ アクチュエータ用圧電素子 センサ用圧電素子 複層膜型アクチュエータ 電圧発生手段 電圧 膜型圧電素子 アクチュエータ センサ 印加電圧 制御手段 膜 圧電素子 振動 板バネ 前記アクチュエータ用圧電素子 前記センサ用圧電素子 空気バネ 制御信号 請求項 リード線 振動板 壁材 コア部材 圧電セラミック 膜状 面 膜型化 歪量 機器類 床面 図 外周面 空気バネ構造 発明 前記コア部材 形態 実施 梁 基本構成 記載 構成 前記印加電圧 置台 出力 建築部材 力 接合面 電圧値 複数 前記制御信号 スピーカ エポキシ樹脂 フランジ エポキシ樹脂等 衝撃 複層 特許文献 ポリイミドフィルム 方向 強度 特徴 センサ出力 平面状 梁構造 手段 Ｗ 音源出力手段 Ｂ 両面 稼動 歪 薄膜化 電圧情報 矩形圧電セラミック繊維 構造 Ａ 側面 片面 ００７ 振動対策 下側取付台 上側取付台 電極 変形量 加速度センサ 説明 シート状 バネ鋼 振動面 ポリイミドフィルム印刷電極 中央部付近 平面 複数個 制御ブロック 床材等 支持剛性 接着剤 接着材 位置別 上フランジ 基本構造 支持部材 天井材 ～ 側 上面 下面 Ｓ 矢印 振動情報 振動状態 前記複数 追加設置工事等 振動抑制 振動状況 追加設置工事 設置場所 先行技術文献 装置構成 上記構成 技術分野 背景技術 繊維状 靴 建物 躯体等 課題 他方 センシング 伝播 面積 他端 一体 話し声等 機能 電気通信大学認知機能工学科ロボティクス講座下条 音源 剛性 明研究室ホームページ 衝撃力 積層状態 電磁力 アクティブクッション 重量物 上記 積層 例 応用例 矢印Ｗ 上下方向 柱 ブロック 装置 上下 補助柱 検出及 抑制 底 歩行 自体 概要 目的 効果 図面 用途 弾性 同士 作用 図示 ウェブ 両側 １つ 任意 方法 作業 施工 隣室 音 一般 機構 形状 通常 伸縮 符号 ポリイミドフルム\n",
      "\n",
      "===== # 3, Topic : 14, p : 7.8174 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : コンクリート養生型枠 コンクリート 養生 保水部 コンクリートＣ 型枠本体 表面 表面部 水 コンクリート表面 水抜き孔 水中養生 実施形態 コンクリート部材 養生工程 水シート 透水性 保水性 余剰水 養生方法 型枠 前記保水部 コンクリート養生 養生水 水分 前記型枠本体 型工程 設コンクリート コンクリートＣ側表面 孔 型枠設置工程 当該コンクリート養生型枠 コンクリート打設工程 散水養生 ブリージング水 壁部材 コンクリートＣ表面 コンクリート系板材 発明 内側面 当該型枠 被膜養生剤 面板 工程 コンクリート硬化体 通水 Ｃ 材料 図 養生効果 前記実施形態 前記コンクリート 養生箇所 特許文献 形態 設コンクリートＣ 厚み 耐久性 強度 コンクリート等 水和 実施 環境 前記表面部 コンクリートＣ側面 シート状 下端部 前記 シリカ系含浸剤 板材 鉛直面 接面 コンクリート上面 管材 効果 目詰まり セメント粒子 コンクリート打設後 硬化体 方法 むら 圧力 形状 範囲 透水係数 等 表面乾燥 工コンクリート等 水和反応 床版コンクリート 構築方法 側面 課題 脱 排水 一対 部材 布 供給 浸透性防錆剤 撥水効果 緻密性 水和生成物 多孔質 ひび割れ抵抗性 浸透性防錆剤等 水中 水酸化カルシウム 構築 厚み分 止水性能 実施例 透水性フィルム状物質等 合成樹脂系板材 板状 板 防錆効果 説明 目的 通過 気泡 ～ 位置 発泡スチロール系板材 合成樹脂系材料 設後 貫通孔 吸水性ポリマー 孔間隙率 多孔板 前記課題 プレキャスト部材 内側 擁壁 先行技術文献 技術分野 背景技術 観点 外部 斜面 水槽 飽和 溶液 図面 配置 剛性 状態 Ｎ ｍｍ 所望 流出 フィルム 所定 隙間 物質 フライアッシュ等 合成繊維 軽量骨材 温度ひび割れ抑制 拡大断面図 自然有機繊維 温度差 斜視図 適用箇所 乾燥 抑制 特開平 号公報 問題点 鋼製 部分 圧縮強度 上面 有効成分 質量保存 移動経路 表層部分 構成要素 設計変更 内部 底面 水平面 処理 多量 外側 複数 － 概要 記載 手段 特徴 概略 カルバート 壁面 トンネル 主体 両面 間隔 側圧 木製 一端 他端 ％～ ↑-↑ ｓｅｃ ポーラスモルタル ポーラスコンクリート モルタル 石材 軽石 粉 凹凸 外面 無数 役割 参照 量 上方 全面 液体 追加 空隙 機能 現場 逸散 品質 美観 耐力 変形 影響 前述 趣旨 裏面 下面 周囲 工場 符号\n",
      "\n",
      "===== # 4, Topic : 4, p : 7.6290 %\n",
      "Topic words : 上方, ｃ, 発明, 参照, 力, 技術分野, 形態, 手段, 特許文献, 説明\n",
      "Input : ストラット付ＰＣ箱桁 床版 ストラット 波形鋼板 接合部 床版コンクリート コンクリート 面 張出し床版 図 ストラット側 上床版 上下床版 肌隙 実施例 力 上記ストラット 鋼板 上下床版コンクリート 床版構造 粉粒体 付着力 谷部 波形 鋼板側面 箱桁断面 上記波形鋼板 付着切れ 側面図 波形鋼板ウエブ 張出し床版構造 側 ウエブ ＰＣ箱桁 肌隙防止構造 発明 波形鋼板ウエブ桁 耐久性 上記肌隙防止構造 接合部分 スタッド 箱形断面桁 接着表面積 珪砂 防止 谷部両側 特許文献 貫通孔 上部構造物自重 Ａ部 橋軸方向 リブ付床版構造 部分 粒径 横断面図 山部 接着剤 防食性 拡大図 腹板 上記粉粒体 接着性 軸 下部構造物 水 強度 エポキシ樹脂系接着剤 下方部分 張出し床版長 水効果 張出し上床版 込み性 上記谷部 構造性 改訂版 縦断面図 軽減化 線Ｂ－Ｂ 線Ｃ－Ｃ 形態 技術 方向 鉄筋 細砂 複数 ～図 ～ 説明 － 位置 背景技術 実施 鋼桁橋 例 縮小化 小型化 構造形式 複合構造 粒度 軸方向力 噛合効果等 下部構造物幅 ＰＣ道路橋計画マニュアル 同等 塗布量 頭部 接合方式 橋梁 形 程度 要因 溶植後 各種 コンクリート型枠 軸経 下端 下端部近傍 経済性 有効性 親和性 実施形態 再現性 上記事情 上記目的 上記スタッド 先行技術文献 効果 技術分野 型枠 省力化 斜め 支柱 等間隔 参照 一般 工費 施工 課題 結露 水滴 雨水 剪断力 図面 溶植前 図示 要素 符号 ｍ アコーデオン効果 社団法人プレストレスト・コンクリート建設業協会 縮小 塗布方法 組み立て解体作業 号公報 降伏点 頭部直径 頭部厚 相違点 号珪砂 広幅員 プレストレス 工期短縮 特開 目的 機械的性質 プレストレストコンクリート 軽量 コンクリートウエブ 平成 ｐ 概要 欄 利点 角度 手段 特徴 耐力 Ｎ ｍｍ Ｎｍｍ 伸び 寸法 双方 規定\n",
      "\n",
      "===== # 5, Topic : 2, p : 7.3761 %\n",
      "Topic words : 参照, 位置, 配置, 技術分野, 構造, 形態, 特許文献, 手段, 説明, 発明\n",
      "Input : ロックボルト 回転用冶具 地山 孔 係合部 フランジ 地 筒体 請求項 図 プレート ロックボルト施工 基端部 ボルト 締め付け 前記回転用冶具 雌ネジ レンチ掛け部 前記フランジ 前記地山 防水シート ネジ ナット 前記 前記筒体 軸心 孔Ｈ 発明 定着剤 ロックボルト基端部 端部 記載 側面図 施工 螺合 シール材 実施形態 回転 孔径 Ｒ 筒状 締め付けプレート 地山表面 シャフト部 前記ロックボルト 前記孔 外径 突起 中心 基端面 特許文献 特徴 螺合長 表面 回転軸 円周部 本体 矢視断面図 Ｈ 面 構成 正面図 上面図 形態 円筒状 円盤状 等角度間隔 位置 課題 円形 トルク 状態 ４つ 形状 耐荷性能 上面 説明 技術 断面 背面 側 径 定着材 防水シートＲ 係合し フランジ側 ナット等 形状等 施工方法 先行技術文献 技術分野 背景技術 板状 円柱状 通常 内部 座金 角 工具 延長線 程度 強度 曲面 下面 細部構造等 モルタル等 四角形等 トルクレンチ等 トンネル周囲 山岳トンネル工法 突出長 ナット頭部 ナット自体 °間隔 特殊形状 方法 特開平 号公報 水漏れ 角柱形 周囲 形成位置 頭部 角柱 トルクレンチ 詰め物 道具 工程 工夫 ワッシャー 幅 － 概要 先 かなり 分 凹凸 手段 効果 図面 最良 上端 一体 先端 サイズ あまり 裏面 直径 正六角形 上下 取り付け 穿孔 口 ～ 効率 次 作業 柄 数 符号\n",
      "\n",
      "===== # 6, Topic : 14, p : 8.3711 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 水位 水位保持管 試験孔用水 試験孔用水Ｗ 水位測定用水Ｗ 水位測定用水 前記気密水槽 挿入部材 試験孔 気密水槽 前記水位測定用水 前記試験孔用水Ｗ 蓋部材 透水試験装置 前記水位測定用水Ｗ 水位変化 Ｗ 他端開口 前記試験孔 図 気密状態 前記挿入部材 前記試験孔用水 水位センサー 注水管 水位保持手段 水位変化測定 前記 試験孔Ｈ 地盤 前記注水管 前記試験孔Ｈ 該気密水槽 該試験孔用水Ｗ 一端開口 水面Ｗ 状態 該試験孔 該試験孔Ｈ 地盤Ｇ Ｈ 発明 該定位水位保持管 透水性 符号 大気連通管 請求項 b 透水試験方法 前記他端開口 ｂ 水面 Ｇ 模式図 特許文献 部材 水位測定用 上述 蓋部材自体 Ｂ 気密性 円筒状 管 該挿入部材 浸透度合い 前記蓋部材 a 測定現場 水 部分断面図 部分 測定精度 斜視図 略等断面積 現場 連通管 孔塩化ビニル管 参照 号公報 該蓋部材 開口 分解斜視図 配置位置等 b)(c 一定 使用状態 ｃ 浸透 棒状 Ａ 形状 装置 シール部材 案内管 下 感度 作業 構成 棒 略一定 ～(c 断面積 Ｂ参照 丸棒 該連通管 該案内管 前記棒状 本明細書 特開 挿脱可能 下端面 断面形状 音波式 説明 構造 ａ 特徴 一体 側面 図面 表面張力 筒状 水平断面積 案内管固定砕石 ビル等 特許 手段 a)(b 角筒状 周辺地盤 先行技術文献 c 円筒 符号Ｈ 技術分野 背景技術 四角筒 図示 固定 － 課題 上部 方向 効果 位置決め ワンタッチ 操作 装着 外観 形態 形式 空気 公知 他 浸透量 角棒 上側部分 下端 方法 音波距離計 Ａ参照 建設現場 底付き トンネル ダム 近傍 関係 概要 反面 否 目的 括弧 番号 要素 記述 記載 事態 実施 排水 現象 手間 ネジ 該全体 １つ ２つ 他方 該第 接触 該開口 流入 影響 両方 車 都合 フロート タイプ\n",
      "\n",
      "===== # 7, Topic : 14, p : 7.5928 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : グラウンドアンカー アンカー孔 コンクリートダム コンクリート構造物 補強部材 せん断 図 前記コンクリート構造物 補強方法 グラウト材 コンクリート構造物側 前記グラウンドアンカー 保持手段 メカニカルアンカー 境界部 力 自由長部 前記アンカー孔 孔部 コンクリート 緊張力 孔工程 面部 解析ケース 先端部 上端部 せん断力 索条部材 管状部材 アンカー体部 亀裂等 ダム 継ぎ目部等 天端 ケース 変形例 前記せん断 概略図 側 解析 亀裂 発明 略中心 説明図 所定 グラウンドアンカー打設工程 前記アンカー 位置 下端部 深層部 後端部 本体部 岩盤 外周面 手段 該アンカー孔 定着材 状態 テンドン 該グラウンドアンカー 等 前記コンクリート構造物側 コンクリートダム等 地盤 緊張 周側 形態 緊張力付与工程 補強部材設置工程 孔機 周面 アフターボンド 地震 実施 防錆機能 ＦＥＭ解析 解析定数 振動 隙間 部分 説明 ｂ 外周側 前記地盤 補強 パッカー ロッキング振動 該せん断 上記 鋼管 鉛直方向 重力式コンクリートダム 剛性 楔 円筒状 定着部材 重力式コンクリートダム等 継ぎ目部 表 せん断抵抗力 先端 手段等 底部 材 前記 角筒状 定着具 方法 上下 解析モデル 解析モデル図 構成 水 浮力 周囲 長期 特許文献 シース管 圧縮荷重 ～ 設置工程 工程 課題 段差 ｌ φ ｔ タイプ せん断波速度 せん断弾性係数 ～図 タイプ等 材料等 ワイヤー等 上端 経年変化等 ～ケース 該自由長部 背面側 機能 提体 技術分野 背景技術 先行技術文献 既設 新設 上部 虞 通常 設 外力 変位 図面 クレーン ａ 一体 内部 凹部又 下方 下端 ジャッキ 地点 メカニカルシール 最大鉛直応力 上下方向 基礎地盤 重力 設工程 シース管内 径方向外方 複数箇所 複数本 特開 号公報 線径 応力 効果 間隔ごと 耐腐食性 材料 ～（ｆ 中空環状 径 ｆ 単位重量 ポアソン比 ヤング率 土留壁 作用効果 テンドン 自重 水圧 形式 各種 劣化 － 概要 費用 理由 現状 目的 特徴 一定 経過 全幅 強度 カーボン 鋼材 操作 制限 両端 ｃ 内面 ヒンジ ばね ｄ ｅ 径又 小径 前面 貯水 各々 種類 対策 符号\n",
      "\n",
      "===== # 8, Topic : 14, p : 7.7380 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : アンカー アンカー孔 コンクリートダム 案内溝 挿入方法 滑車 天端 テンドン 前記アンカー 支持台 図 前記 駆動源 前記アンカー孔 コンクリート構造物 発明 支持軸 形態 実施 開口縁部 孔機 クレーン 先端部 グラウンドアンカー 機能 コンクリート構造物等 部分拡大図 ダム 駆動 支持フレーム 防食機能 後端部 上端部 シース管 緊張力 材 前記滑車 状態 ワイヤ 凹凸 動力伝達部材 塑性変形 下端部 深層部 一対 作用効果 防食材 鉛直方向 接線方向 上下方向 水平方向 岩盤 底部 自由長部 荷重 グラウト材 等 挿入 前記案内溝 該アンカー 方法 上記 ～ 程度 表面 減速機付モータ 位置 ドラム 機械式 重力式コンクリートダム 複数 ブレーキ 重力式コンクリートダム等 説明 地盤 内面 ｂ 概略図 重量 面部 相互 回転 構成 Ｕ形状 圧縮荷重 耐用年数 部分 複数箇所 境界部 中心部 継ぎ目部等 各種 所定 操作 課題 タイプ 挿入作業 重量物 駆動モータ 動力伝達機構 減速付モータ ディスク式 ドラム式 先端 ブレーキ等 左方向 効果 保持部分 亀裂等 オイル等 ベルト等 出力軸 経年変化等 既設 新設 虞 周囲 滑り 図面 直径 シース管内 ダミー ジャッキ スペース 小径 図示 重力 技術分野 背景技術 Ｖ形状 自由長部 円板状 ＯＮ－ＯＦＦ ブレーキ付き 停止 停止位置 空圧又 定着具 斜め下方 テンドン 上部 自重 水圧 形式 劣化 地震 性能 概要 目的 手段 下向き 該滑車 特徴 効率 該凹凸 補強 一体 調整 歯車 チェーン 作動 油圧 右 側 １つ 符号\n",
      "\n",
      "===== # 9, Topic : 0, p : 7.6508 %\n",
      "Topic words : 土壌, 角度θ, 汚染土壌, アルカリ性, 定着具, 継手板, 定着構造, 短期間, チャンバー, 締め付け\n",
      "Input : 再生細骨材 骨材 材 再生骨材 回転軸 細骨材 摩砕物 仕切部材 摩砕室 再生骨材Ｈ 特許文献 コンクリート 製造フロー 再生骨材製造 破砕 絶乾密度 ドラム体 原料 処理 機械式すりもみ装置 細骨材製造 品質 製造方法 粒径範囲 回転数 軸長方向 品質再生骨材 吸水率 粒径 投入原料 コンクリート破砕材 解体コンクリート中の原骨材 号公報 製造 磨砕 装填材 粗骨材 骨材製造 図 特開 発明 コンクリート塊 特許 破砕機 仕切り板 処理方法 骨材再生方法 原細骨材 コンクリート用骨材 実験II 該ドラム体 － ｍｍ 製造フローＢ 再生 原骨材 前記特許文献 解体コンクリート中の原細骨材 摩砕室内 再生骨材Ｍ 再生骨材Ｌ コンクリート粒状物 導通孔 解体コンクリート塊 下記特許文献 コンクリート用再生骨材Ｈ ドラム体内 再生骨材ＪＩＳ規格 二つ目 実験 製造フローＡ 実験Ｉ 実験要因 前記摩砕室 ドラム 方法 Ａ 研磨装置 材料 処理速度 Ｂ 回転 前記ドラム体 ～ 複数 該回転軸 前記 摩砕後 摩砕処理 前記仕切部材 導入口 排出口 フロー図 川崎重工株式会社製 外径φ 実験III 上昇落下 商品名 すりもみ装置 装置 環境負荷低減 コンクリートガラ ビット 低減 一端部 他端部 建物Ａ 規格 ケージ型摩砕装置 下記 品質化 解体コンクリート 磨鉱作用 磨砕装置 モータ 部 エネルギー 目 再生砂 研磨 解体 加熱 板 中間処理場 表面 例 上流側 駆動 Ｃ エネルギー負荷 中心軸 ＪＩＳ規格値 実験I 破砕装置 原コンクリート 構造用コンクリート 分級等 発明方法 等 所要数 ケージ ボール 交換作業 金属製 加熱処理 成分 ＪＩＳＡ 前記材料 込み部 一つ目 三つ目 負荷 品質指標 コンクリート廃材 コンクリート構造物 作業効率 油圧駆動 破砕物 粒子同士 ボール等 ケーシング ドラム自体 設備コスト 下記表 所定粒径範囲 技術 向上 課題 水準 関係 値 交換頻度 字状 中心軸端部 密度 比較例 中心軸自身 ＪＩＳ規格 号 外周縁 請求項 鉄球 表 衝撃摩砕作用 上記加熱処理 圧縮破砕機 株式会社セラ・テック製 ＳＫ磨鉱機 ＪＩＳ 製砂機 解体建物 説明 ブロック 使用 影響 面 ｃｍ 確認 増加 所定粒径以下 所定粒径以上 分級機 両端部 中間処理施設 相互 磨砕式 Ｌ 目的 形態 円筒 建物 建築用 加熱設備 実験計画 実験概要 空隙率 左右両端部 駆動力 前記目的 １つ目 四つ目 解体現場 建築物 加熱炉 上下方向 傾斜円板 耐久性 駆動機構 角度 図示 摩擦 間隔 該切 コ 円盤 ローター 粒子相互 微細粒子 部分 コンクートガラ 微粉 傾 状態 衝突 所望 種類 特定 実績 原 ｈ 分 同一 グラフ ジョークラッシャー等 有無等 該ケーシング 実施例 縦断側面図 長期耐久性 打撃衝突作用 同心円状 円筒状 内径φ 正面図 衝撃力 コスト 実施 効率 比較 技術分野 背景技術 規定値 判断指標 インペラ－ブレーカー 経済産業省 段可変速 実施形態 付着モルタル 最大寸法 セラミックピン ピン 籠形 Ｍ 一般用途 基礎梁 乾燥収縮 凍結融解 強度 凍結 強度低下 設置 維持管理 消費電力 設置面積 ショックロード 電気モータ 基本物性 物性 川砂利 現状 事前 熱風 該円盤 セメント 円周 集塵機 残り スーパーサンダー ドラムリクレーマー 開示 リサイクル 促進 平成 付け 日本工業規格 分類 通り 杭 鉄筋 腐食 ビル 通常 同等 程度 簡易 普及 工夫 不都合 手段 要旨 記載 効果 最良 参照 下 ２つ 重量 偏心 幅 重点 性質 コア 事例 砕石 検討 対象 ＪＩＳＡｓ 図面 符号\n",
      "\n",
      "===== # 10, Topic : 5, p : 7.2264 %\n",
      "Topic words : 土壌, 角度θ, 定着具, くさび, 礫, 免震構造, 蓋, 継手板, 汚染土壌, アルカリ性\n",
      "Input : 骨材 再生骨材 再生細骨材 材 品質 再生 製造 回転軸 変数Ｒ コンクリート破砕材 原料投入量 原料 回転数 製造フロー 破砕 骨材品質 コンクリート 仕切部材 機械式すりもみ装置 製造フローＡ 摩砕物 摩砕室 細骨材 コンクリート用再生骨材 仕切り板 製造条件 再生骨材製造 再生骨材Ｈ 特許文献 ドラム体 コンクリート塊 絶乾密度 製造フローＢ 吸水率 実験 処理 軸長方向 フロー 骨材製造 相関 品質制御 製造方法 図 再生骨材品質 細骨材製造 号公報 装填材 発明 品質管理 特開 コンクリート用骨材 骨材再生方法 骨材等 再生骨材Ｂ ｍｍ 前記特許文献 解体コンクリート 付着モルタル 原骨材 再生骨材Ｍ 再生骨材Ｌ 原細骨材 板 粒径 回帰式 天然骨材 骨材表面 実験要因 竪型破砕機 磨砕 導通孔 コンクリート用再生骨材Ｈ 表 二つ目 － 変数 下記 相関係数 品質制御方法 すりもみ もみ 再生骨材ＪＩＳ規格 ～ Ａ 建物Ａ 関係式 関係 特許 品質制御手法 解体コンクリート塊 特定 粒率 投入量 細粒 下記特許文献 モルタル付着量 実験Ｉ 実験III 実験IV 外径φ 外周壁内面 回転 要求品質 品質指標 加熱 加熱処理 ドラム 摩砕処理 ローター 偏心ローター式 該ドラム体 モータ 該変数Ｒ 粗粒率 方法 建物 目 導入口 排出口 制御 解体建物 下記表 比較例 構造用コンクリート 偏心ローター式竪型破砕機 格子筒状 該外周壁 コンクリートガラ 確認 前記 上昇落下 上流側 駆動 破砕機 コンクリート廃材再生処理設備 分級等 実験II 硬化セメントペースト等 中心軸 下記表－ 説明 変数Ｒと絶乾密度 偏心ローター 品質管理方法 複数 水準 一つ目 三つ目 発明方法 竪型偏心ローター装置 竪型破砕機内 上記変数Ｒ 偏心ローター装置 前記摩砕室 一端部 他端部 油圧駆動 モルタル 該回転軸 前記ドラム体 仕切り版回転数 コンクリートコア 前記仕切部材 表面 有無 技術 水 範囲 ビット 説明図 品質情報 上記ローター コンクリート構造物 竪 筒状ローター外面 建築用 外周壁 中心軸端部 最大寸法 安定化 接触機会 偏心 鉄球 管理 摩砕室内 摩砕後 中心軸自身 密度 モルタル分 ドラム体内 ドラム自体 ローター下部 上記加熱処理 使用 解体 課題 部 面 種類 値 グラフ 外筒 粒径範囲 Ｌ 強度 外面 色調 ボール 塩酸処理 両端部 Ｂ 所要数 実験計画 実験概要 実験ｎ 字状 空隙率 中間処理施設 中間処理場 相関性 傾斜円板 例 耐久性 運転条件 最適条件 １つ目 四つ目 左右両端部 セメント水和物 有無等 土壌等 前記目的 解体現場 ボール等 込み部 駆動力 駆動機構 鍔状リング ＪＩＳ 影響 部分 熱風 攪拌体 該外面 間隔 所定 幅 都度 磨き 作用 効果 形態 図面 傾 状態 上下方向 所望 ブロック 円筒 負荷 物性 上記 ｈ III 長期耐久性 交換作業 外周縁 縦断側面図 内径φ 実施例 正面図 建築物 手法 原 II 技術分野 背景技術 作業効率 設備コスト 環境負荷低減 経済産業省 分離兼用通路 段可変速 交換頻度 Ｍ 一般用途 基礎梁 乾燥収縮 凍結融解 凍結 強度低下 含水付着物 所要 コスト ＣＣＤカメラ 色調データ 試験 目的 請求項 限界点 実施 金属製 効率 低減 消費電力 設置面積 ショックロード 電気モータ 一定期間 川砂利 試験項目 ロットあたり リサイクル 促進 平成 付け 日本工業規格 分類 通り 杭 鉄筋 腐食 ビル 通常 同等 事前 未満 不純物 空間 個々 温度 むら 硬度 間隙 下端 隙間 滞留 除去 開示 幾つ 表現 目視 具合 理由 不都合 手段 要旨 記載 程度 簡易 サンプル 選定 判断 最良 角度 相互 参照 衝突 図示 摩擦 向上 該切 コ 下 ２つ 重量 原材料 性質 実績 事例 砕石 まとめ 限度 符号\n",
      "======== Epoch 1  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3823e-25, 4.0998e-24, 5.3646e-16, 1.7430e-15, 3.1048e-06, 5.6338e-19,\n",
      "         8.5602e-23, 9.7677e-01, 2.3221e-02, 2.6836e-19, 3.5886e-21, 2.0545e-19,\n",
      "         1.8556e-10, 7.0327e-26, 3.6096e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0828, 0.1047, 0.0542, 0.0447, 0.0775, 0.0920, 0.0767, 0.0445, 0.0533,\n",
      "         0.0376, 0.1187, 0.0443, 0.0369, 0.0949, 0.0372]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 1 Average loss: 1005.3677\n",
      "Test epoch : 1 Average loss: 964.8451\n",
      "PP(train) = 2020.220, PP(valid) = 2089.818\n",
      "======== Epoch 2  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.7995e-21, 6.4295e-25, 3.9868e-11, 3.3745e-22, 1.0000e+00, 6.0259e-22,\n",
      "         5.3533e-30, 2.2707e-08, 2.4385e-07, 4.8414e-12, 5.7644e-19, 4.7828e-18,\n",
      "         3.5878e-18, 2.4892e-12, 1.7179e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 2 Average loss: 1005.2768\n",
      "Test epoch : 2 Average loss: 964.8022\n",
      "PP(train) = 2017.742, PP(valid) = 2088.989\n",
      "======== Epoch 3  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.5624e-26, 3.1535e-17, 6.3089e-09, 2.1597e-06, 1.6295e-12, 1.8013e-25,\n",
      "         6.3381e-18, 3.0178e-09, 9.9999e-01, 5.3071e-06, 1.1334e-16, 2.3773e-12,\n",
      "         5.4995e-17, 7.1222e-20, 4.4268e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 3 Average loss: 1005.1174\n",
      "Test epoch : 3 Average loss: 964.7541\n",
      "PP(train) = 2014.889, PP(valid) = 2088.175\n",
      "======== Epoch 4  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0746e-16, 3.7893e-09, 6.6022e-10, 9.9989e-01, 2.6911e-05, 1.3417e-15,\n",
      "         5.3898e-15, 6.9169e-06, 1.6481e-06, 1.9064e-10, 4.1214e-12, 4.1435e-08,\n",
      "         1.9330e-14, 5.4265e-07, 7.8108e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0417, 0.0550, 0.0919, 0.0502, 0.0527, 0.0524, 0.0532, 0.0686,\n",
      "         0.1351, 0.1029, 0.0484, 0.0562, 0.0521, 0.0906]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 4 Average loss: 1004.9651\n",
      "Test epoch : 4 Average loss: 964.7049\n",
      "PP(train) = 2011.921, PP(valid) = 2087.440\n",
      "======== Epoch 5  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2418e-16, 7.1549e-16, 8.5652e-01, 2.0202e-10, 1.4344e-01, 1.8660e-10,\n",
      "         6.0442e-12, 3.9268e-05, 9.7062e-09, 2.3618e-10, 3.5119e-11, 1.6038e-14,\n",
      "         1.0921e-08, 2.6743e-08, 1.1662e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0713, 0.0831, 0.0444, 0.0662, 0.0866, 0.0512, 0.0797, 0.0803, 0.0863,\n",
      "         0.0631, 0.0637, 0.0499, 0.0509, 0.0834, 0.0399]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 5 Average loss: 1004.7364\n",
      "Test epoch : 5 Average loss: 964.6503\n",
      "PP(train) = 2008.606, PP(valid) = 2086.424\n",
      "======== Epoch 6  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.3787e-26, 1.2200e-15, 6.2936e-10, 2.9449e-27, 3.8610e-16, 7.3609e-37,\n",
      "         1.3513e-13, 1.0000e+00, 2.8613e-17, 1.1656e-12, 3.3760e-21, 7.2492e-20,\n",
      "         2.3016e-27, 3.3333e-21, 2.0054e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 6 Average loss: 1004.5088\n",
      "Test epoch : 6 Average loss: 964.5979\n",
      "PP(train) = 2005.519, PP(valid) = 2085.670\n",
      "======== Epoch 7  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1843e-18, 4.4976e-16, 6.7376e-17, 9.9997e-01, 6.5153e-18, 3.2438e-25,\n",
      "         6.4229e-21, 3.3859e-05, 3.1314e-08, 8.1057e-25, 4.7336e-20, 3.8694e-15,\n",
      "         1.8045e-14, 6.8013e-16, 1.7258e-13]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0488, 0.0417, 0.0550, 0.0919, 0.0502, 0.0527, 0.0524, 0.0532, 0.0686,\n",
      "         0.1351, 0.1029, 0.0484, 0.0562, 0.0521, 0.0906]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 7 Average loss: 1004.2671\n",
      "Test epoch : 7 Average loss: 964.5439\n",
      "PP(train) = 2002.335, PP(valid) = 2084.781\n",
      "======== Epoch 8  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.5594e-17, 1.1168e-11, 9.7124e-05, 4.3099e-14, 2.5367e-19, 3.8273e-11,\n",
      "         3.8334e-18, 9.9990e-01, 2.1201e-14, 7.2397e-19, 7.5963e-17, 2.4096e-18,\n",
      "         1.8896e-08, 7.3060e-10, 1.6156e-11]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 8 Average loss: 1004.0797\n",
      "Test epoch : 8 Average loss: 964.4885\n",
      "PP(train) = 1999.047, PP(valid) = 2083.740\n",
      "======== Epoch 9  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0932e-32, 2.3637e-24, 1.0000e+00, 5.9891e-21, 2.7869e-21, 3.1300e-33,\n",
      "         7.6492e-23, 7.7090e-11, 5.1046e-18, 7.1893e-29, 2.6969e-14, 2.3898e-20,\n",
      "         5.6460e-15, 9.4429e-20, 2.2764e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 9 Average loss: 1003.9141\n",
      "Test epoch : 9 Average loss: 964.4396\n",
      "PP(train) = 1996.075, PP(valid) = 2083.103\n",
      "======== Epoch 10  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.0222e-18, 1.4140e-20, 3.1225e-06, 2.2295e-14, 2.1463e-01, 4.5683e-14,\n",
      "         3.4674e-03, 1.3666e-03, 4.5134e-13, 5.3296e-14, 6.1248e-14, 9.7335e-07,\n",
      "         1.5638e-03, 9.3404e-14, 7.7897e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0728, 0.0459, 0.0867, 0.0480, 0.0673, 0.0676, 0.1084, 0.0966, 0.0568,\n",
      "         0.0602, 0.0640, 0.0574, 0.0504, 0.0698, 0.0481]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 10 Average loss: 1003.6984\n",
      "Test epoch : 10 Average loss: 964.3869\n",
      "PP(train) = 1992.905, PP(valid) = 2082.177\n",
      "======== Epoch 11  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.3362e-21, 4.5705e-18, 1.7995e-06, 1.0027e-16, 4.1154e-07, 2.5933e-17,\n",
      "         2.9810e-12, 5.9561e-10, 1.0300e-05, 3.4577e-08, 5.9928e-27, 9.6002e-07,\n",
      "         3.0120e-09, 1.1124e-04, 9.9988e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 11 Average loss: 1003.4772\n",
      "Test epoch : 11 Average loss: 964.3329\n",
      "PP(train) = 1989.720, PP(valid) = 2081.215\n",
      "======== Epoch 12  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.0223e-13, 2.5162e-12, 3.5650e-04, 1.1126e-08, 9.5152e-04, 7.1433e-15,\n",
      "         7.8664e-16, 2.5147e-15, 7.6830e-01, 9.8509e-05, 9.7428e-13, 8.8733e-16,\n",
      "         2.3030e-01, 3.4163e-11, 3.0568e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0550, 0.0699, 0.0509, 0.0632, 0.0331, 0.0742, 0.0616, 0.0900, 0.0615,\n",
      "         0.0754, 0.0754, 0.0629, 0.0534, 0.1129, 0.0606]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 12 Average loss: 1003.3337\n",
      "Test epoch : 12 Average loss: 964.2848\n",
      "PP(train) = 1986.764, PP(valid) = 2080.517\n",
      "======== Epoch 13  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1104e-12, 4.1916e-10, 7.6706e-10, 6.7617e-20, 4.7301e-10, 6.6017e-19,\n",
      "         7.0207e-17, 5.2209e-13, 2.1030e-09, 3.9803e-16, 3.2152e-12, 9.1571e-14,\n",
      "         1.2188e-09, 3.5730e-16, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 13 Average loss: 1003.1166\n",
      "Test epoch : 13 Average loss: 964.2356\n",
      "PP(train) = 1983.721, PP(valid) = 2079.666\n",
      "======== Epoch 14  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8036e-18, 2.7109e-22, 7.5018e-07, 7.2478e-10, 1.1383e-07, 1.3462e-25,\n",
      "         6.2684e-15, 8.3316e-06, 9.6281e-01, 2.3765e-08, 5.2037e-25, 1.5612e-16,\n",
      "         3.7168e-02, 4.3602e-13, 1.0554e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0560, 0.0678, 0.0453, 0.0630, 0.0308, 0.0758, 0.0627, 0.0917, 0.0577,\n",
      "         0.0703, 0.0831, 0.0648, 0.0515, 0.1283, 0.0514]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 14 Average loss: 1002.8224\n",
      "Test epoch : 14 Average loss: 964.1851\n",
      "PP(train) = 1980.744, PP(valid) = 2078.835\n",
      "======== Epoch 15  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.7283e-05, 1.6566e-16, 1.3314e-02, 9.5281e-14, 3.8758e-01, 5.9418e-01,\n",
      "         1.1728e-17, 2.5240e-15, 7.8127e-08, 3.3302e-03, 4.4528e-06, 1.5631e-03,\n",
      "         4.3415e-12, 1.9611e-10, 4.9035e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0451, 0.0577, 0.0575, 0.0400, 0.0526, 0.0559, 0.0583, 0.1147, 0.0763,\n",
      "         0.0988, 0.0554, 0.0804, 0.0478, 0.0855, 0.0742]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 15 Average loss: 1002.7239\n",
      "Test epoch : 15 Average loss: 964.1383\n",
      "PP(train) = 1977.840, PP(valid) = 2078.094\n",
      "======== Epoch 16  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2439e-26, 3.0175e-17, 3.3952e-04, 5.0202e-06, 1.3874e-06, 1.6254e-16,\n",
      "         7.4129e-17, 1.5583e-15, 3.2119e-01, 4.7826e-20, 1.0394e-25, 2.2945e-06,\n",
      "         1.7112e-18, 7.9380e-18, 6.7846e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0738, 0.0517, 0.0807, 0.0550, 0.0492, 0.0710, 0.0923, 0.0940, 0.0597,\n",
      "         0.0583, 0.0698, 0.0596, 0.0520, 0.0881, 0.0448]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 16 Average loss: 1002.4851\n",
      "Test epoch : 16 Average loss: 964.0907\n",
      "PP(train) = 1974.924, PP(valid) = 2077.268\n",
      "======== Epoch 17  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1020e-06, 3.9674e-10, 1.0457e-11, 8.2608e-06, 5.9940e-10, 4.3670e-13,\n",
      "         1.5725e-15, 9.9864e-01, 5.4360e-09, 5.4872e-17, 3.9613e-15, 1.2770e-05,\n",
      "         5.4272e-10, 3.6990e-07, 1.3341e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1055, 0.0544, 0.0443, 0.0791, 0.0922, 0.0769, 0.0437, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0366, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 17 Average loss: 1002.2998\n",
      "Test epoch : 17 Average loss: 964.0440\n",
      "PP(train) = 1972.060, PP(valid) = 2076.519\n",
      "======== Epoch 18  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.8164e-11, 3.3428e-10, 1.2559e-04, 6.3191e-12, 1.2207e-08, 3.9502e-25,\n",
      "         7.5835e-12, 1.8039e-07, 5.0978e-05, 1.3816e-15, 4.3494e-15, 3.1300e-22,\n",
      "         9.7506e-01, 5.3661e-12, 2.4766e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0490, 0.0736, 0.0769, 0.0604, 0.0418, 0.0648, 0.0549, 0.0794, 0.0744,\n",
      "         0.0927, 0.0486, 0.0531, 0.0584, 0.0646, 0.1075]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 18 Average loss: 1002.1052\n",
      "Test epoch : 18 Average loss: 964.0000\n",
      "PP(train) = 1969.344, PP(valid) = 2075.866\n",
      "======== Epoch 19  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[8.4542e-11, 5.4940e-21, 9.9091e-01, 1.4449e-15, 1.6521e-08, 8.1537e-15,\n",
      "         1.8853e-19, 9.0897e-03, 4.9603e-10, 1.4792e-10, 4.1501e-14, 1.8518e-08,\n",
      "         2.2142e-07, 2.9053e-16, 3.3567e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0761, 0.0900, 0.0443, 0.0713, 0.0840, 0.0487, 0.0749, 0.0750, 0.0945,\n",
      "         0.0580, 0.0627, 0.0477, 0.0510, 0.0865, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 19 Average loss: 1001.9484\n",
      "Test epoch : 19 Average loss: 963.9561\n",
      "PP(train) = 1966.486, PP(valid) = 2075.070\n",
      "======== Epoch 20  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.2996e-06, 8.2578e-20, 2.2073e-08, 2.7866e-01, 9.6400e-14, 5.4629e-03,\n",
      "         1.4103e-12, 1.7131e-05, 9.3329e-03, 1.1667e-05, 3.1771e-20, 1.2937e-07,\n",
      "         6.8330e-08, 1.4180e-16, 7.0651e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0724, 0.0452, 0.0890, 0.0612, 0.0583, 0.0644, 0.0897, 0.0816, 0.0638,\n",
      "         0.0706, 0.0736, 0.0553, 0.0540, 0.0675, 0.0533]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 20 Average loss: 1001.7591\n",
      "Test epoch : 20 Average loss: 963.9114\n",
      "PP(train) = 1963.621, PP(valid) = 2074.255\n",
      "======== Epoch 21  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1568e-18, 7.0996e-22, 3.0939e-10, 9.5707e-11, 7.7178e-01, 3.6674e-08,\n",
      "         5.3832e-18, 8.6386e-15, 2.2011e-01, 4.4562e-15, 7.4984e-06, 4.1673e-03,\n",
      "         1.4409e-07, 1.0623e-12, 3.9371e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0481, 0.0526, 0.0437, 0.0434, 0.0756, 0.0693, 0.0964, 0.1063, 0.0485,\n",
      "         0.0892, 0.0718, 0.0623, 0.0477, 0.0746, 0.0706]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 21 Average loss: 1001.5888\n",
      "Test epoch : 21 Average loss: 963.8704\n",
      "PP(train) = 1961.002, PP(valid) = 2073.672\n",
      "======== Epoch 22  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.1241e-10, 9.2787e-01, 6.5090e-06, 5.2801e-02, 8.8818e-07, 1.5157e-10,\n",
      "         1.5902e-11, 1.2743e-06, 6.5058e-12, 7.1036e-16, 8.9673e-12, 7.7151e-11,\n",
      "         1.8301e-03, 5.8088e-14, 1.7488e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1050, 0.0479, 0.0415, 0.1044, 0.0457, 0.0475, 0.0728, 0.0421, 0.0764,\n",
      "         0.0790, 0.0402, 0.0466, 0.0599, 0.1101, 0.0808]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 22 Average loss: 1001.4167\n",
      "Test epoch : 22 Average loss: 963.8285\n",
      "PP(train) = 1958.294, PP(valid) = 2072.987\n",
      "======== Epoch 23  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7858e-24, 4.0982e-17, 2.0552e-12, 9.7524e-02, 9.0248e-01, 1.4493e-26,\n",
      "         1.2192e-19, 4.3645e-08, 9.4438e-11, 2.9695e-15, 3.5033e-13, 1.9896e-17,\n",
      "         8.0427e-19, 3.7387e-16, 1.0121e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0455, 0.0477, 0.0437, 0.0419, 0.0909, 0.0649, 0.1004, 0.1021, 0.0473,\n",
      "         0.0983, 0.0707, 0.0594, 0.0469, 0.0616, 0.0787]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 23 Average loss: 1001.1608\n",
      "Test epoch : 23 Average loss: 963.7846\n",
      "PP(train) = 1955.595, PP(valid) = 2072.270\n",
      "======== Epoch 24  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.8280e-16, 1.7073e-06, 2.8679e-08, 2.7274e-19, 1.5946e-20, 6.8143e-18,\n",
      "         1.3640e-15, 3.8616e-08, 1.0761e-10, 2.9048e-22, 1.4849e-05, 3.4151e-20,\n",
      "         1.3485e-04, 2.9613e-16, 9.9985e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 24 Average loss: 1001.0207\n",
      "Test epoch : 24 Average loss: 963.7467\n",
      "PP(train) = 1952.989, PP(valid) = 2071.680\n",
      "======== Epoch 25  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.0243e-20, 6.1054e-17, 1.0000e+00, 7.6799e-17, 2.3632e-07, 3.7801e-09,\n",
      "         4.2471e-13, 4.5445e-09, 2.4118e-14, 4.3171e-13, 7.8371e-17, 2.5332e-22,\n",
      "         7.9501e-09, 1.3275e-06, 5.6897e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 25 Average loss: 1000.8591\n",
      "Test epoch : 25 Average loss: 963.7069\n",
      "PP(train) = 1950.328, PP(valid) = 2070.970\n",
      "======== Epoch 26  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[6.5175e-25, 3.9719e-12, 1.7435e-12, 1.1498e-08, 2.6868e-04, 8.1550e-16,\n",
      "         5.5560e-16, 6.9187e-16, 9.9552e-01, 7.3474e-22, 7.0987e-15, 2.4688e-14,\n",
      "         5.5194e-06, 1.1814e-19, 4.2087e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0562, 0.0673, 0.0444, 0.0628, 0.0305, 0.0760, 0.0631, 0.0920, 0.0570,\n",
      "         0.0692, 0.0845, 0.0651, 0.0511, 0.1311, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 26 Average loss: 1000.5971\n",
      "Test epoch : 26 Average loss: 963.6695\n",
      "PP(train) = 1947.789, PP(valid) = 2070.403\n",
      "======== Epoch 27  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.1934e-13, 1.4826e-13, 4.4226e-05, 3.7996e-18, 9.4109e-04, 1.3561e-19,\n",
      "         9.4389e-11, 1.9896e-07, 9.9901e-01, 2.2942e-10, 1.1142e-11, 2.1517e-10,\n",
      "         2.1521e-08, 1.1244e-20, 6.8162e-12]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0628, 0.0304, 0.0760, 0.0630, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1313, 0.0498]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 27 Average loss: 1000.4660\n",
      "Test epoch : 27 Average loss: 963.6313\n",
      "PP(train) = 1945.225, PP(valid) = 2069.783\n",
      "======== Epoch 28  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.8133e-09, 1.6466e-11, 4.5707e-12, 7.3527e-13, 1.6638e-08, 1.3381e-20,\n",
      "         3.3938e-04, 6.4321e-06, 1.2927e-07, 2.7062e-15, 2.2493e-14, 5.3814e-14,\n",
      "         2.9112e-07, 5.9902e-13, 9.9965e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
      "         0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 28 Average loss: 1000.3361\n",
      "Test epoch : 28 Average loss: 963.5920\n",
      "PP(train) = 1942.694, PP(valid) = 2069.120\n",
      "======== Epoch 29  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[9.1778e-05, 1.6583e-09, 3.0518e-11, 1.0547e-10, 1.0182e-10, 3.9990e-15,\n",
      "         4.2660e-17, 2.8503e-06, 2.0214e-03, 9.4482e-21, 6.8309e-14, 2.3529e-15,\n",
      "         9.9788e-01, 8.2011e-07, 3.6846e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0483, 0.0744, 0.0760, 0.0605, 0.0413, 0.0646, 0.0539, 0.0789, 0.0746,\n",
      "         0.0937, 0.0482, 0.0529, 0.0585, 0.0644, 0.1097]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 29 Average loss: 1000.1570\n",
      "Test epoch : 29 Average loss: 963.5545\n",
      "PP(train) = 1940.188, PP(valid) = 2068.490\n",
      "======== Epoch 30  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.1581e-12, 4.7681e-05, 5.9042e-14, 2.8846e-16, 3.3777e-03, 9.4336e-29,\n",
      "         4.0675e-19, 9.9657e-01, 9.4020e-18, 5.9320e-10, 2.4807e-13, 3.6329e-17,\n",
      "         3.8724e-06, 6.9706e-20, 4.3459e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0833, 0.1054, 0.0543, 0.0442, 0.0792, 0.0922, 0.0770, 0.0438, 0.0531,\n",
      "         0.0371, 0.1192, 0.0439, 0.0366, 0.0938, 0.0370]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 30 Average loss: 999.9303\n",
      "Test epoch : 30 Average loss: 963.5190\n",
      "PP(train) = 1937.722, PP(valid) = 2067.951\n",
      "======== Epoch 31  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.6688e-13, 5.5894e-23, 3.1104e-12, 1.3355e-26, 1.9633e-13, 9.8168e-17,\n",
      "         1.9210e-12, 4.6208e-18, 7.8378e-12, 4.5808e-15, 2.1438e-15, 1.0032e-16,\n",
      "         9.9544e-01, 1.2626e-04, 4.4386e-03]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0484, 0.0743, 0.0763, 0.0605, 0.0414, 0.0646, 0.0541, 0.0790, 0.0746,\n",
      "         0.0936, 0.0483, 0.0529, 0.0585, 0.0644, 0.1095]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 31 Average loss: 999.7391\n",
      "Test epoch : 31 Average loss: 963.4850\n",
      "PP(train) = 1935.244, PP(valid) = 2067.361\n",
      "======== Epoch 32  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3892e-10, 1.4138e-04, 2.2495e-08, 1.1398e-17, 9.9986e-01, 7.8345e-34,\n",
      "         7.4730e-19, 1.6463e-11, 2.0377e-09, 1.8937e-16, 7.2388e-28, 3.8449e-09,\n",
      "         6.3179e-14, 5.8803e-19, 1.6000e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 32 Average loss: 999.6148\n",
      "Test epoch : 32 Average loss: 963.4507\n",
      "PP(train) = 1932.884, PP(valid) = 2066.867\n",
      "======== Epoch 33  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[7.6155e-11, 2.2801e-18, 1.0423e-08, 9.8060e-15, 4.6126e-09, 6.2171e-20,\n",
      "         5.2909e-28, 1.2555e-03, 2.7880e-10, 2.3974e-08, 1.2793e-17, 9.9874e-01,\n",
      "         7.4062e-23, 1.4826e-22, 1.0552e-08]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0615, 0.0622, 0.0816, 0.0550, 0.0547, 0.1200, 0.0458, 0.0785, 0.0983,\n",
      "         0.0684, 0.0441, 0.0663, 0.0594, 0.0664, 0.0377]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 33 Average loss: 999.5125\n",
      "Test epoch : 33 Average loss: 963.4166\n",
      "PP(train) = 1930.428, PP(valid) = 2066.250\n",
      "======== Epoch 34  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.9514e-04, 9.9960e-01, 1.1976e-13, 4.8603e-07, 1.2798e-10, 9.8500e-15,\n",
      "         1.3028e-08, 6.8856e-06, 2.5139e-09, 2.0955e-17, 4.1489e-10, 2.4378e-15,\n",
      "         1.0604e-06, 1.9430e-13, 1.0049e-06]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1095, 0.0479, 0.0398, 0.1059, 0.0449, 0.0465, 0.0731, 0.0405, 0.0766,\n",
      "         0.0766, 0.0374, 0.0459, 0.0599, 0.1150, 0.0806]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 34 Average loss: 999.3394\n",
      "Test epoch : 34 Average loss: 963.3820\n",
      "PP(train) = 1928.004, PP(valid) = 2065.634\n",
      "======== Epoch 35  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0652e-08, 4.5448e-07, 1.0531e-16, 5.0919e-17, 6.2314e-01, 1.0662e-24,\n",
      "         3.6537e-17, 3.4864e-01, 4.3803e-11, 2.8212e-02, 1.8132e-19, 2.9908e-06,\n",
      "         1.0499e-13, 2.5739e-17, 2.5160e-14]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0578, 0.0665, 0.0472, 0.0427, 0.0922, 0.0768, 0.0977, 0.0804, 0.0496,\n",
      "         0.0697, 0.0837, 0.0558, 0.0435, 0.0755, 0.0610]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 35 Average loss: 999.1439\n",
      "Test epoch : 35 Average loss: 963.3477\n",
      "PP(train) = 1925.721, PP(valid) = 2065.164\n",
      "======== Epoch 36  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.7254e-23, 3.4076e-20, 1.0000e+00, 4.2014e-16, 2.6375e-12, 1.3627e-22,\n",
      "         5.1426e-20, 6.3965e-15, 2.8700e-16, 5.2911e-16, 1.7377e-10, 1.6780e-23,\n",
      "         1.9402e-14, 1.1848e-15, 5.2174e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0760, 0.0899, 0.0442, 0.0716, 0.0840, 0.0484, 0.0748, 0.0753, 0.0949,\n",
      "         0.0582, 0.0623, 0.0477, 0.0511, 0.0864, 0.0353]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 36 Average loss: 998.9340\n",
      "Test epoch : 36 Average loss: 963.3147\n",
      "PP(train) = 1923.365, PP(valid) = 2064.610\n",
      "======== Epoch 37  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.7524e-25, 7.2072e-22, 6.0969e-01, 6.5599e-07, 4.9355e-05, 1.7859e-20,\n",
      "         3.3269e-20, 9.5411e-12, 2.6762e-09, 2.6773e-08, 8.4894e-16, 6.5199e-16,\n",
      "         2.4594e-13, 2.2481e-16, 3.9026e-01]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0795, 0.0694, 0.0629, 0.0634, 0.0750, 0.0558, 0.0877, 0.0830, 0.0804,\n",
      "         0.0568, 0.0632, 0.0515, 0.0520, 0.0813, 0.0382]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 37 Average loss: 998.8239\n",
      "Test epoch : 37 Average loss: 963.2835\n",
      "PP(train) = 1921.062, PP(valid) = 2064.095\n",
      "======== Epoch 38  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[3.8632e-24, 2.7684e-23, 1.5181e-21, 8.5439e-20, 9.5689e-18, 9.2660e-22,\n",
      "         1.2257e-16, 1.0000e+00, 6.8086e-24, 1.3294e-16, 2.3722e-21, 1.2806e-13,\n",
      "         1.5150e-19, 3.9481e-28, 3.6983e-17]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 38 Average loss: 998.6393\n",
      "Test epoch : 38 Average loss: 963.2522\n",
      "PP(train) = 1918.757, PP(valid) = 2063.581\n",
      "======== Epoch 39  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4201e-18, 1.2768e-09, 2.3646e-09, 2.4838e-12, 1.0896e-06, 7.0495e-18,\n",
      "         1.3594e-14, 9.4952e-10, 8.7213e-13, 2.2992e-13, 1.0000e+00, 2.8993e-13,\n",
      "         6.5420e-09, 1.2330e-35, 5.6598e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0920, 0.1292, 0.0394, 0.0377, 0.0644, 0.0584, 0.0361, 0.0756, 0.0541,\n",
      "         0.0360, 0.1119, 0.0427, 0.0304, 0.1005, 0.0917]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 39 Average loss: 998.5016\n",
      "Test epoch : 39 Average loss: 963.2212\n",
      "PP(train) = 1916.412, PP(valid) = 2063.027\n",
      "======== Epoch 40  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.4100e-08, 1.3496e-19, 9.3498e-13, 3.7057e-04, 3.4433e-13, 2.5454e-18,\n",
      "         9.9382e-19, 1.1840e-08, 9.2402e-01, 2.1429e-02, 7.1144e-16, 1.6691e-11,\n",
      "         6.7682e-20, 9.0727e-19, 5.4182e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0575, 0.0666, 0.0463, 0.0631, 0.0323, 0.0758, 0.0655, 0.0916, 0.0574,\n",
      "         0.0687, 0.0823, 0.0649, 0.0510, 0.1273, 0.0496]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 40 Average loss: 998.4006\n",
      "Test epoch : 40 Average loss: 963.1948\n",
      "PP(train) = 1914.237, PP(valid) = 2062.644\n",
      "======== Epoch 41  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.2971e-14, 4.8514e-12, 1.5489e-10, 4.9892e-13, 9.9793e-01, 1.9953e-19,\n",
      "         2.0642e-22, 2.6745e-16, 1.3702e-11, 2.0649e-03, 8.5345e-10, 1.3654e-15,\n",
      "         1.4296e-10, 8.8090e-17, 3.7010e-07]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0421, 0.0382, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0671, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 41 Average loss: 998.2177\n",
      "Test epoch : 41 Average loss: 963.1655\n",
      "PP(train) = 1911.984, PP(valid) = 2062.172\n",
      "======== Epoch 42  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[4.6073e-26, 1.2677e-20, 1.7502e-28, 9.5502e-24, 5.9052e-11, 2.8186e-23,\n",
      "         5.9980e-25, 3.4596e-06, 1.0000e+00, 4.9076e-24, 3.2442e-14, 1.0762e-21,\n",
      "         4.9114e-17, 2.6782e-19, 1.3049e-16]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0561, 0.0674, 0.0443, 0.0629, 0.0304, 0.0760, 0.0629, 0.0919, 0.0570,\n",
      "         0.0693, 0.0846, 0.0651, 0.0511, 0.1314, 0.0497]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 42 Average loss: 998.0027\n",
      "Test epoch : 42 Average loss: 963.1337\n",
      "PP(train) = 1909.769, PP(valid) = 2061.658\n",
      "======== Epoch 43  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.9883e-16, 1.5168e-11, 1.7557e-07, 1.3755e-09, 8.2868e-08, 1.0339e-03,\n",
      "         5.3744e-01, 1.5340e-07, 1.1922e-05, 2.5807e-17, 1.3297e-11, 3.0638e-07,\n",
      "         2.6752e-09, 4.6151e-01, 3.8345e-09]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0626, 0.0639, 0.0538, 0.0794, 0.0669, 0.0750, 0.0484, 0.0549, 0.1103,\n",
      "         0.0512, 0.0731, 0.0845, 0.0509, 0.0781, 0.0469]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 43 Average loss: 997.8619\n",
      "Test epoch : 43 Average loss: 963.1054\n",
      "PP(train) = 1907.602, PP(valid) = 2061.205\n",
      "======== Epoch 44  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.6738e-22, 4.9209e-25, 2.0138e-18, 1.1135e-22, 5.2393e-17, 7.1549e-17,\n",
      "         1.9643e-18, 9.5933e-01, 1.7350e-17, 5.3575e-24, 5.4461e-09, 6.3097e-19,\n",
      "         9.7669e-05, 1.0904e-21, 4.0574e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0837, 0.1023, 0.0560, 0.0447, 0.0785, 0.0914, 0.0782, 0.0452, 0.0535,\n",
      "         0.0377, 0.1167, 0.0444, 0.0372, 0.0932, 0.0372]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 44 Average loss: 997.7220\n",
      "Test epoch : 44 Average loss: 963.0771\n",
      "PP(train) = 1905.451, PP(valid) = 2060.744\n",
      "======== Epoch 45  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[5.6092e-12, 2.9100e-19, 4.1207e-01, 1.6984e-20, 1.0314e-05, 1.2009e-11,\n",
      "         4.0664e-30, 2.0555e-08, 1.9206e-06, 4.4079e-21, 3.2369e-19, 5.8792e-01,\n",
      "         1.1850e-07, 5.7197e-09, 2.2130e-15]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0684, 0.0737, 0.0646, 0.0625, 0.0665, 0.0841, 0.0571, 0.0787, 0.0988,\n",
      "         0.0652, 0.0518, 0.0590, 0.0569, 0.0754, 0.0374]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 45 Average loss: 997.5739\n",
      "Test epoch : 45 Average loss: 963.0516\n",
      "PP(train) = 1903.334, PP(valid) = 2060.342\n",
      "======== Epoch 46  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.3515e-16, 3.3997e-06, 5.3421e-03, 5.8830e-04, 9.9588e-05, 3.5911e-12,\n",
      "         1.0826e-09, 1.4137e-06, 1.0472e-02, 9.5263e-01, 1.3981e-05, 4.4781e-06,\n",
      "         4.9288e-04, 5.3062e-17, 3.0353e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0538, 0.0879, 0.0316, 0.1000, 0.0729, 0.0739, 0.0816, 0.0625, 0.0561,\n",
      "         0.0708, 0.0400, 0.0643, 0.0397, 0.1101, 0.0549]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 46 Average loss: 997.4470\n",
      "Test epoch : 46 Average loss: 963.0228\n",
      "PP(train) = 1901.163, PP(valid) = 2059.838\n",
      "======== Epoch 47  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.9654e-16, 5.7886e-14, 4.0671e-10, 1.2105e-20, 9.9906e-01, 1.8894e-18,\n",
      "         4.8405e-15, 9.0998e-04, 8.8048e-08, 8.4390e-20, 4.0234e-13, 5.0773e-20,\n",
      "         6.8484e-07, 4.4580e-19, 2.9142e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0448, 0.0479, 0.0422, 0.0381, 0.0959, 0.0657, 0.1066, 0.1083, 0.0450,\n",
      "         0.0939, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 47 Average loss: 997.2801\n",
      "Test epoch : 47 Average loss: 962.9951\n",
      "PP(train) = 1899.075, PP(valid) = 2059.413\n",
      "======== Epoch 48  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.0400e-09, 8.8109e-01, 4.4831e-07, 7.6568e-08, 3.7992e-02, 1.3552e-21,\n",
      "         1.2863e-17, 1.6658e-05, 2.4232e-14, 4.2960e-03, 6.5839e-13, 1.3154e-10,\n",
      "         6.2853e-02, 9.9257e-07, 1.3753e-02]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.1011, 0.0500, 0.0426, 0.0986, 0.0469, 0.0490, 0.0741, 0.0450, 0.0756,\n",
      "         0.0788, 0.0397, 0.0476, 0.0597, 0.1090, 0.0822]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 48 Average loss: 997.0872\n",
      "Test epoch : 48 Average loss: 962.9696\n",
      "PP(train) = 1896.973, PP(valid) = 2059.013\n",
      "======== Epoch 49  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[2.8823e-13, 6.3277e-26, 1.5281e-17, 2.5323e-18, 6.5709e-18, 3.3754e-24,\n",
      "         2.2922e-22, 1.0000e+00, 7.1432e-11, 6.8567e-27, 1.3948e-20, 1.9947e-24,\n",
      "         3.7686e-15, 2.3403e-18, 1.8638e-10]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
      "         0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 49 Average loss: 996.9455\n",
      "Test epoch : 49 Average loss: 962.9441\n",
      "PP(train) = 1894.889, PP(valid) = 2058.594\n",
      "======== Epoch 50  ========\n",
      "sitaNone-batch0\n",
      "next_sita_hatNone-batch0\n",
      "sitaNone-batch1\n",
      "next_sita_hatNone-batch1\n",
      "sitaNone-batch2\n",
      "next_sita_hatNone-batch2\n",
      "sitaNone-batch3\n",
      "next_sita_hatNone-batch3\n",
      "sitatensor([[1.4649e-22, 1.6587e-14, 2.1577e-06, 4.5814e-09, 9.9998e-01, 5.6730e-13,\n",
      "         1.6481e-20, 1.0496e-07, 5.0721e-16, 2.3209e-16, 1.7346e-20, 7.9254e-08,\n",
      "         3.2743e-19, 9.4335e-19, 1.2609e-05]], grad_fn=<ReshapeAliasBackward0>)-batch4\n",
      "next_sita_hattensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
      "         0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
      "       grad_fn=<SoftmaxBackward0>)-batch4\n",
      "Overall sparsity = 0.995, l1 strength = 0.00000\n",
      "Target sparsity = 0.850\n",
      "Train epoch: 50 Average loss: 996.8254\n",
      "Test epoch : 50 Average loss: 962.9197\n",
      "PP(train) = 1892.844, PP(valid) = 2058.173\n",
      "Writing to ./topicwords/19-topwords_e50.txt\n",
      "Topic 0: 角度θ 汚染物質 汚染土壌 土壌 溶液 蓋 座標 定着構造 タイル くさび\n",
      "Topic 1: 蓋 タイル 角度θ 溶液 汚染物質 部品等 樹脂等 組立作業 排気口 くさび\n",
      "Topic 2: 参照 位置 配置 技術分野 形態 特許文献 構造 手段 背景技術 説明\n",
      "Topic 3: 現地 土壌 目 提供 汚染物質 鋼矢板 基準 室 タイル 計算\n",
      "Topic 4: 上方 ｃ 発明 参照 力 技術分野 形態 特許文献 手段 背景技術\n",
      "Topic 5: 角度θ 蓋 くさび 溶液 汚染土壌 礫 部品等 便宜 仮想線 樹脂等\n",
      "Topic 6: 角度θ 蓋 タイル くさび 座標 汚染物質 便宜 部品等 凸 溶液\n",
      "Topic 7: － Ａ 位置 ２つ 技術分野 形態 特許文献 手段 背景技術 説明\n",
      "Topic 8: 効果 砂 Ａ 側方 ｂ 端部 技術分野 形態 特許文献 手段\n",
      "Topic 9: 汚染物質 蓋 汚染土壌 角度θ 礫 土壌 タイル 溶液 α 有害物質\n",
      "Topic 10: 汚染土壌 土壌 タイル 角度θ 溶液 部品等 汚染物質 座標 計算 樹脂等\n",
      "Topic 11: 角度θ 有害物質 タイル 汚染物質 溶液 土壌 座標 くさび 蓋 部品等\n",
      "Topic 12: 施工 内側 Ａ 図面 技術分野 形態 特許文献 手段 背景技術 説明\n",
      "Topic 13: 汚染物質 座標 角度θ 蓋 タイル 土壌 仮想線 便宜 樹脂等 くさび\n",
      "Topic 14: 所定 他 符号 等 参照 荷重 種 接合構造 特徴 課題\n",
      "\n",
      "===== # 1, Topic : 12, p : 7.8058 %\n",
      "Topic words : 施工, 内側, Ａ, 図面, 技術分野, 形態, 特許文献, 手段, 背景技術, 説明\n",
      "Input : 補強鋼板 補強鋼板貫入装置 打撃装置 地中部 圧入 圧入治具 鋼板 鋼板圧入工法 ＲＣ橋脚 障害物 油圧ジャッキ 油圧圧入工法 油圧圧入装置 橋脚 柱状構造物 油圧圧入 コンクリート片等 地中貫入方法 耐震補強工事 パーカッション装置 衝撃力 地上部 ドロップハンマー方式 鋼板補強 パーカッション方式 補強枠 図 ロッド先端 発明 ウォータージェット装置 特許文献 ロッド先端部 補強構造 打撃台 橋脚等 鋼板補強工法 橋脚耐震補強工事 ハンマー 打撃方式 鋼板補強工事 併用工法 装置 地下枠 鋼板圧入用 補強鋼板上部 圧入方式 油圧 圧入設置 コンクリート片 補強鋼板設置方法 力 ガイドロッド 地中 転石 工法 開削方式 セパレート構造 高圧水 高圧水噴射工法 汎用ウインチ 貫入方式 打撃力 錘 開削 シリンダ本体 施工 上部 ロッド上端 空隙部 施工現場 機械装置 高圧水パイプ 斜視図 補強対策工法 側面 周囲 号公報 請求項 オープンカット等 圧入設置完了 打撃 高圧水噴射装置 四角枠状 ウォータージェット併用工法 高圧水噴射 ウォータージェット併用 ウォータージェット 上端部 橋脚側面 セパレータ構造 例 モルタル充填スペース 正面図 左右 角部 一体化用 特開 開削設置工法 パーカション装置 ホールアンカー 自動溶接 縦リブ モルタル 土留掘削 作業ヤード 上空制限 機械 コンパクト化 中心部 ワイヤロープ ウォオータージェット併用工法 説明 対象 内側 － 条件 特徴 部材 パーカションドリルビット 構成 パーカション方式 角部上面 オープンカット 上端 先行技術文献 摩擦低減補助工法 技術分野 背景技術 各面 下 下向き 分割 状態 内面 スペーサー 土砂 継手 課題 工期 継続施工 大型 重機 小型 組立て 下部 効果 低減 形態 ４つ パーカッションドリル ドリフター 汎用性 土留 筒状 特開平 上部外面 側面均等 新幹線高架橋 状況確認 図示例 工種 平面形状 中央位置 位置 基部 地盤 抵抗 所定 下端 ノズル ～ 両側 砂 開示 経済 判断 手段 現行 設 機構 次 規模 短縮 コスト 事前 選択 最良 実施 ピン 取り付け 図面 ａ ｂ 符号\n",
      "\n",
      "===== # 2, Topic : 1, p : 7.4451 %\n",
      "Topic words : 蓋, タイル, 角度θ, 溶液, 汚染物質, 部品等, 樹脂等, 組立作業, 排気口, くさび\n",
      "Input : 固定プレート 天井材 天井制震装置 天井 野縁 天井スラブ 耐震天井構造 天井側取付部材 上部構造体 プレート 固定プレート側取付部材 弾性材 斜材 図 天井下地 構造 天井構造 ボルト 前記天井材 下端 前記上部構造体 支柱 耐震天井 前記固定プレート 野縁受け 部材 調整 側 天井面 上端 ブレース 固定 上部プレート 前記天井制震装置 レベル調整手段 取り付け 天井部材 前記野縁 取付方法 複数本 ねじボルト 天井等 面材 下部プレート 構造部材 天井端部 下面側 下端部 支柱取付孔 発明 前記天井材側 天井制震装置取付孔 上端部 地震 前記天井側取付部材 テンション調整 固定プレート側取り付けプレート部材 前記固定プレート側取付部材 ねじボルト付き羽子板プレート レベル 下側 レベル調整 前記 平面図 袋ナット付き羽子板プレート ナット 調整用ワッシャ 制震効果 下面 壁面 固定プレート本体 端部 ターンバックル 天井懐 規模天井 ブレース上端取り付け用ブラケット ブレース下端取り付け用ブラケット 工程 形態 前記野縁受け 前記斜材 構造躯体 状態 実施 所定間隔 複数 前記実施 段差天井等 変位 ホールインアンカー 天井裏空間 一端側 両端側 支柱本体 減衰ゴム等 分解斜視図 減衰材 取り付けナット 前記固定プレ－ト 前記支柱 衝突 構成 固定プレ－ト ボルト上端部分 アングル材等 特許文献 断面図 説明 落下 斜め － 周囲 貫通孔付き 手段 アンカー 要部拡大断面図 前記目的 前記アンカー 躯体補強 外周部 縦断面図 耐震性 弾性ダンパー 耐震設計基準 ～図 側面図 ビス等 鋼材等 被害 設計 観点 種々 クリアランス 課題 下方 特徴 テンション 図面 隙間 位置 上下 設置位置 所定面積 設置空間 空間 部分 設置 技術分野 背景技術 位置関係 滑動幅 空調機器 設備機器 衝突部分 基準 材料 程度 内外程度 特開 号公報 ビス 目的 構成材料 自由度 パイプ状 垂下状態 接着剤 加硫時 地震エネルギー 現行 建物 中心 通常 参照 破損 開示 前述 動き クリップ 軽量 適用 揺れ 最良 回り縁 枚数 中央 上側 上面 要旨 範囲 例 符号\n",
      "\n",
      "===== # 3, Topic : 14, p : 8.2194 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 処分坑道 坑道 処分孔 図 掘削坑道 路盤 セグメント 底部 路盤構造 コンクリートブロック 廃棄体 周面 処分トンネル 地層処分施設 － 取手部 断面図 幅方向 孔 トンネル掘削機 掘削作業 周方向 ブロック 取手収容孔 前記坑道 建設 軸方向 処分孔掘削位置 重機 地層処分 作業 発明 凸部 路盤コンクリート 請求項 地山 地下施設部 実施 規制手段 締結手段等 掘削 底面 複数 断面形状 円筒面状 手段 平面状 前記ブロック コンクリート 上記 設置 方法 掘削位置 形態 側方 斜視図 掘削面 撤去作業 上面 上記請求項 坑道建設 セグメント内周面 上記構成 重機等 処分作業 搬入台車 坑道底部 放射性廃棄物 孔あけ 地下施設 レベル放射性廃棄物 処分方法 地下 位置 鋼製枠体 設置作業 形状 トンネル 路盤材料 切羽 移動 特許文献 同一 側面 核燃料サイクル 円形 近傍 往来 円形坑道 燃料 廃棄物 前記複数 進区間 上記実施 掘削残土 円筒面 構成 セメント ベントナイト 底部上面 曲率半径 参照 部分 態様 特徴 内部 円弧 セグメント等 ブロック状 前記セグメント コンクリートセグメント セグメント設置作業 円周面 外周面 処理工場 取手 搬入台車等 両端部 鋼製 概略断面図 板状体 トンネル群 主要トンネル 説明 ずり 強度 手間 プレキャストコンクリート 地層 所定 岩石 同士 個々 鉄筋 凹部 材質 材 設置位置 路盤表面 路盤コンクリートブロック 方法等 地上受入施設 使用済み核燃料 使用済燃料 地 概略図 矩形状 長期安定性 立坑 セグメント分 蓋体 下側 反対側 ベントナイト等 グラウト等 鋼材等 キャニスター搬入立坑 撤去 凸 平面 型枠 リサイクル燃料備蓄センター 特許 課題 劣化 カルシウム 膨潤性 一つ 支保工 工程 効率 他 進区間ごと 緊急用立坑 各種立坑 排気立坑 資材立坑 使用量 揚重機器 上下鋼板の間 工場 技術分野 背景技術 原子力発電所 含有量 馬蹄形 楕円形 封入材 号公報 岩種 岩 堆積軟岩 バリア性能 添付図面 セメントモルタル 最小限度 厚み寸法 維持管理 鋼材 図面 中心 ウラン プルトニウム 利用 法令 人員 一般 開示 工期 機能 影響 施工 目的 効果 最良 直径 程度 変形 最小限 花崗岩 隙間 通常 悪影響 下部 荷重 上部 手順 フック 取り外し 凹凸 支障 状態 図示 鋼板 埋設 引き上げ 破壊 リブ 符号\n",
      "\n",
      "===== # 4, Topic : 14, p : 8.7895 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 掘削土 処理 固化処理 鉄塩 珪酸アルカリ金属塩 鉄 砒素 不溶化処理 試料土 発明 処理方法 固化材 技術 基準値 原土 溶出量 コーン指数 発明者 中性 環境基準値 セメント系固化材 石灰系固化材 マグネシア系固化材 確認試験 中性固化材 中和処理 アルカリ性 鉄イオン 石膏系 珪酸ソーダ ｋＮ ｍ 珪酸 否 特許文献 当該掘削土 上記関連技術 課題 前記掘削土 Ｌ ｐＨ 塩化 下記特許文献 不溶化剤 強度 砒素含有土砂 鉄塩由来 双方 工程 硫酸 環境基準 上記課題 当該試料土 号公報 環境庁告示 一般残土 酸化マグネシウム 特開 商品名 鉄液 砒酸鉄 シリカ鉄 所定 薬剤 ＪＩＳ 表 ケース 参照 粉末珪酸ソーダ 粉末珪酸カリウム 溶出試験 試験 コーン貫入試験 技術分野 背景技術 号 未満 － 該工程 硝酸 効果 Ｋ 環境汚染 カリウム水ガラス 水素イオン 中性付近 号参照 強度発現性 吸光光度法 検液 リチウム水ガラス 原因物質 汚染 ｐＨ ストックヤード ゲル化 ポリテツ 混合物 比較対照 説明 土壌 平成 トラック 状態 段落 開示 利用 コスト 増加 目的 手段 特徴 溶性 酸性 シリカゾル 最良 形態 研究 次 含水比 Ａ ～ 通り\n",
      "\n",
      "===== # 5, Topic : 5, p : 7.2219 %\n",
      "Topic words : 角度θ, 蓋, くさび, 溶液, 汚染土壌, 礫, 部品等, 便宜, 仮想線, 樹脂等\n",
      "Input : 補強鋼板 摺動刃 油圧シリンダ 補強鋼板貫入装置 橋脚 ＲＣ橋脚 摺動刃駆動装置 図 地中部 鋼板 圧入 摺動板 水平方向 圧入用 クランプ部材 鋼板圧入工法 摺動用 橋脚等 油圧圧入装置 コーナー部 圧入治具 柱状構造物 拡縮部材 地中貫入方法 水平断面 水平断面図 クランプ用 取付板 鉛直断面図 部材 圧入工法 鋼板ブロック ２つ 鋼板補強 圧入装置 ピストンロッド 耐震補強工事 下端部 装置 鋸歯状 断面図 油圧圧入 特許文献 水平配置 ロッド 補強枠 正面図 油圧ジャッキ 発明 ＲＣ橋脚 方向 貫入 地上部 上端部 補強構造 鋼板圧入用 橋脚耐震補強工事 補強鋼材キャップ 請求項 鋼板補強工法 地中部鋼板補強 左右 既設橋脚等 ウォータージェット装置 取付部材 ピストンロッド先端 鋼板補強工事 平面図 地中 分割鋼板部材 障害物 コンクリート片 角鋼管 油圧 隙間 地下枠 構造 ケーシング 下端 前記隙間 コンクリート片等 橋脚側 補強鋼材 補強鋼板設置方法 楔 水平往復ストローク 四角枠 開削 ４つ アングル材 平面視 自動溶接 板状刃 状態 空隙部 セパレート構造 部分拡大鉛直断面図 油圧圧入工法 溶接 シリンダ本体 傾斜面 縦配置 内面 号公報 鋸断 盛替え 四角枠状 ジャッキ 圧入用ジャッキ 分割 刃先 油圧シリンダ等 端部同士 水平状態 滑り板 既設橋脚 側面 上部 転石 下部 高圧水 縦リブ 機械装置 油圧ポンプユニット 開削方式 補強対策工法 オープンカット等 特開 溝形鋼 外面 施工現場 ａ ｂ 取付部品 形状 高圧水パイプ 圧入設置完了 端部 所定 周囲 間隔 対向配置 先端 ロッド先端 継手部 前記拡縮部材 上端 制御盤付き 上下動 ２つクランプ部材 モルタル充填スペース 力 － 特徴 破砕 外側 押し出し 矩形 孔 ピン ケーシング部品 溶接等 手動切換え 高圧水噴射装置 モルタル 土留掘削 作業ヤード 上空制限 機械 左右両隣 工程順 荷重点 水平移動 ピストンロッド収縮状態 開削設置工法 板厚 ウォータージェット併用 一体化用 楔等 説明 対象 下 組立て 条件 山形 突起 鋸歯 継手 前記 形態 輪 斜視図 筒状 直線状 正面視 アングル材形状 角 オープンカット 周方向 寸法等 Ｈ形鋼 ウォオータージェット併用工法 先行技術文献 前記２つ 既設 傾斜 摩擦低減補助工法 楔方式 技術分野 背景技術 押下げ ゴムパッド ゴムパッド付き 下向き 土砂 複数 両側 課題 工期 上方 現行 大型 重機 小型 中央 記載 楔状 下降 差 効果 構成 次 施工 低減 下面 ピンヒンジ 一対 鋼棒 ｃ 法 挿入溝 内側面 押下げ動作 特開平 上下動ストローク 土留 荷重 工程 分割位置 楔空間 タイマー切換え 新幹線高架橋 上下移動 ホールアンカー 内側 状況確認 工種 実施 例 実施形態 図示例 昇降操作 止め輪 各面 基部 スペーサー 地盤 抵抗 ノズル ～ 当て 繰り返し 砂 開示 経済 判断 手段 解放 規模 短縮 コスト 簡易 最良 正方形 外周 ボルト 他方 ナット 裏面 平板 損傷 ３つ ガイド 三角形 ピッチ ノコギリ 直角 引き上げ 締め 緩め 手順 参照 円形 図面 ｄ 符号\n",
      "\n",
      "===== # 6, Topic : 4, p : 7.2058 %\n",
      "Topic words : 上方, ｃ, 発明, 参照, 力, 技術分野, 形態, 特許文献, 手段, 背景技術\n",
      "Input : 端部 気流 中央部 テーブル 天板 喫煙者 端部近傍 空気 吐出口 上昇気流 煙 装置 テーブル中央部 前記天板 縁部 空気吐出口 下降気流 中央部近傍 中央 図 利用者 局所気流 発明 タバコ 近傍 号公報 喫煙 上方 誘導気流発生部 天板中央部 口 吸込口 室内換気構造 汚染空気 吐出気流 曲面状 実施例 誘導気流 ファン 前記 テーブル天板 室内 空間 特開平 特開 喫煙者用 端部天板付近 換気構造 概念図 前記テーブル 例 上記 公報 置換換気 特許文献 端部上部 照明装置 床面 面 前記テーブル天板 開口部 － 前記面 発熱 喫煙スペース プレート 上面 効果 気流発生装置 天板下 吐出煙 端部上方位置 方向 形状 白熱電球 上昇気流発生装置 実験 境界面 吐出方向 発生 煙排気機構 表面 楕円形 該誘導気流 気流性状 気流分布 概要図 端方向 説明 熱源 下方 部屋 室 嫌煙者 駆動力 平面形状 円形テーブル 喫煙空間 反対側 有効性 受動喫煙 除去装置 面状発熱体 テーブル上面 テーブル寸法 置換換気方式 置換換気システム 物体 前提 技術 内部 開口 凸 天井 下部 構造 室形状 室内構造 浮遊物質除去装置 前記コアンダ効果 皿状 当該面 ドーナツ状 縁 凸状 会議室 談話室 楕円形等 執務室等 流煙 発熱体表面温度 曲面 一般空調室 技術分野 背景技術 コアンダ効果 先行技術 技術思想 浮遊物質 実験装置周辺 影響 分煙 レストラン 方法 有無 不便 全員 課題 部分 所定 現象 表面張力 力 形態 中心 目的 長方形 周囲 受熱 凹 構成 相性 照明 排出 条件 拡大図 室内壁 固体表面 フィルタ等 ファン等 側 位置 上部 乾球温度 流体力 特定方向 略円形 健康増進法 努力義務規定 分子間引力 隔離空間 周壁上部 天井位置 流路 オフィスビル 明細書 流体 ホットプレート 吸着剤 思想 システム 問題点 付近 つまり人 自然対流 導びかれる 本明細書 特定 対流 コンピュータ °Ｃ 側面下部 コンピュータシミュレーション 社会的状況 月 建物 意味 同僚 友人 家族 グループ 選択 開示 周り 性質 元 ベルヌーイ 定理 表れ １つ 真空 球状 調理 程度 格別 配慮 所期 流速 一定 処理 机 環境 意 現状 手段 カウンター 片側 丸 円 穴 上向き 機器 前述 逆 正方形 真上 効率 記載 隙間 該隙間 思い ストレス エネルギー 節約 食堂 導入 騒音 状態 最良 態様 理解 下面 流量 ルーバー 擾乱 現実 スケール 風速 ～ m s ℃ 様子 外部 図面 符号\n",
      "\n",
      "===== # 7, Topic : 12, p : 7.5498 %\n",
      "Topic words : 施工, 内側, Ａ, 図面, 技術分野, 形態, 特許文献, 手段, 背景技術, 説明\n",
      "Input : アングル材 Ｖ型溝 Ｖ型突起 セグメント Ｖ字状 図 シール材 トンネル 隅 鋼製 Ｖ字状断面 スペーサー 説明図 発明 継手構造 尺材 構造 実施例 リブ 並行 断面 隅ｂ 円弧状 出隅ａ 端面 継手部分 状態 形状 立坑 出隅 長手方向 接合部分 表面 説明 ａ 直角部分 隅ｄ 一端面 他端面 リブａ 特許文献 組み合わせ 市販 取り付け 設計図通り 連結部 実施 シールドトンネル コンクリート製 曲線部分 安価アングル材 溝 突起 点状 帯板 外側 内側 円弧 隅ｃ 号公報 止水性 出隅ｃ トンネル工事 構成 課題 他 上記 構築 薄板 継手部 セグメント連結部 端部 接合部 トンネル掘削機 円形トンネル トンネル形状 リブｂ ｂ セグメント費 工事費 並行移動 特許 次 曲面 設計通り 特徴 効果 形態 図面 複数 流入 阻止 帯状 鋼板 立坑工事 合成セグメント 板体 並行移動図形 技術分野 背景技術 問題点 コンクリート内部 組み立て状態 直線状態 組み合わせ状態 進機 圧縮力 せん断力 溶接作業 取り付け作業 開昭 アンカー棒 寸法違い 市販品 溶接 地下水 づった程度 キン理論 ガタつき － 開示 部材 変形 円 手段 比率 価格 最良 参照 対象 本体 両者 壁面 合同 裏側 桁 製品 市場 外力 間隔 目的 一定 双方 存在 辺 材料 外部 モルタル 役目 破損 密封 原理 角度 配置 符号\n",
      "\n",
      "===== # 8, Topic : 1, p : 7.4121 %\n",
      "Topic words : 蓋, タイル, 角度θ, 溶液, 汚染物質, 部品等, 樹脂等, 組立作業, 排気口, くさび\n",
      "Input : 熱回収 熱回収装置 熱回収媒体 熱回収システム 熱回収ヘッダー 熱回収管 伝熱容器 熱交換器 循環管路網 実施形態 密封容器 使用済燃料 冷却水 崩壊熱 熱交換 熱回収水 前記熱回収装置 図 熱回収効果 供給分岐管 返送分岐管 キャスク 冷却システム 前記密封容器 冷却管 供給主管 前記循環管路網 発明 ｂ 熱媒体 返送主管 前記熱回収ヘッダー 安全性 放熱部 該密封容器 上記実施形態 ヒートパイプ 螺旋状 伝熱装置 伝熱媒体 循環ポンプ 管路網 冷却 系統 ～図 前記伝熱容器 冷却流体 構成 外周面 熱エネルギ 廃熱 熱エネルギー 熱特性 貯蔵施設 請求項 強制循環 外側 系統図 筒状 周面 該伝熱容器 通常 概略系統図 管体内 特許文献 上部 一体 リバースリターン方式 有蓋筒状 施設 形態 放熱 周囲 吸熱部 状態 対象 温水 システム 斜視図 断面図 円筒状 固体粒子群 温度管理 記載 ヘッダー 概略構成 キャニスタ等 上記システム キャニスタ 特殊構造 簡略化 コスト軽減 所定水温 水温 通風式 説明 課題 高温 サプライヘッダー リターンヘッダー 冷却管内 効果 外気 高密度貯蔵システム 良伝熱性 上記 海塩粒子 下端部 上端部 内部 熱源 前提 安全対策 特徴 既設 冷水 腐食 信頼性 施設増築 上記事情 表面温度 温室効果ガス削減 技術分野 背景技術 冷暖房等 キャスク群 タービン発電用 自然通風力 ℃程度 同等程度 特開 号公報 図示例 金属薄板 必要最小限 作動液 上下方向 臨海域 対策 取り入れ外気 台数 収納台数 変更 周回数 設計的変更 種 除去 － 開示 開発 放射線 漏洩 検証 目的 手段 最良 素材 密着 隙間 径 真空 周知 等間隔 内側 放射状 両側 吸熱 基部 矢印 給湯 大気 省エネルギー 観点 換気 一般 ごと ａ 規模 条件 増設 増加 上方 要旨 範囲 応用 形状 寸法 本数 容量 元 性能 物性 液体 他 図面 符号\n",
      "\n",
      "===== # 9, Topic : 14, p : 7.6387 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 被覆材層 劣化度 耐火被覆材層 引抜強度 劣化評価方法 耐火被覆材 棒状体 湿式耐火被覆材層 請求項 圧縮強度 被覆材 劣化 先端部 孔 前記被覆材層 耐火被覆材層等 フック 小規模火災 発明 貫入抵抗値 火災温度 径 引き抜き抵抗 特許文献 評価 骨材 トンネル 上記被覆材 層 強度 トンネル内壁面 被覆材層表層部 上記耐火被覆材 測定治具 被覆材交換 前記引抜強度 表層部 頭部 判定 方法 試験 ℃ 耐火性 温度 健全度 表 トンネル内壁面等 前記棒状体 ｂ 結合材 材齢 試験体 荷重計 現場 簡易 貫入抵抗値比 火災 推定値 雄ねじ 微粉骨材 強度維持 加熱温度 引き抜き荷重 有機骨材等 関係 変形 程度 差 所定 特徴 コンクリート表層部 劣化度推定方法 トンネル火災 被災 アルミナ系骨材 シリカ系骨材 健全部 凍害劣化度 特開 号公報 外径 引き抜き 値 引抜荷 交換 パネル 試験機 トンネル内等 測定装置 引抜き強度 コンクリート 図 上記 トンネル等 有無 記載 劣化状況 湿式 強度判定方法 小規模火災等 タッピングねじ方式 試験方法 判定作業 貫入抵抗測定具 ねじ 測定 鉛直断面図 表面 参照 工具 １つ 脚長 ℃、 圧縮強度比較 平均圧縮強度 ビス引抜き試験方法 被災状況 健全度診断 層厚ｔ 引き抜き治具 先端 構造物 炭酸化度 該ビス引抜き強度 山岳トンネル等 耐凍害性評価手法 先端軸部 前記 型枠 引き抜き荷重計 説明 ビス － 課題 度合い 補修 】( 目安 ドリル等 基準値 計測値 シールドトンネル トンネル内面 ＡＬＣパネル 計測 形態 ねじ式 ねじ孔 JIS型枠 円柱状等 経時的劣化 確認試験 程度等 デジタルフォースゲージ等 実験等 ステンレス鋼等 測定端子 ねじ長 JIS型枠品 当該ＡＬＣパネル 先行技術文献 電動工具等 炭酸カルシュウム等 吸熱効果等 当該ビス ＡＬＣパネル自体 前記下孔 鋼製ドリル 有機系原料 JIS成型供試体 貫入針 前記頭部 前記フック 技術分野 背景技術 円柱状 あまり 乾式 真鍮 材料 損傷 市販 効果 軸 加熱 φ 切断加工供試体 品 建研式 絶乾品 相関関係 診断 手法 次 実施 実施形態 内面 変形利用 引抜荷重 図示例 例 水酸化アルミニウム 電気炉 次表 ℃近辺 引抜試験 プッシュプルゲージ a b コア 任意 開示 手段 冶具 コスト 構成 即座 最良 人力 前者 後者 隙間 ポルトランドセメント・アルミナセメント RH 環境 養生 吸熱材 変性 参考 図面 符号\n",
      "\n",
      "===== # 10, Topic : 14, p : 8.1607 %\n",
      "Topic words : 所定, 他, 符号, 等, 参照, 荷重, 種, 接合構造, 特徴, 課題\n",
      "Input : 継手 止水板 Ｔ型継手 継手構造 水 ゴム板 Ｃ型継手管 継手挿入スリット 鋼矢板 図 鋼管矢板 止水材 Ｃ型継手管内 合せ部 止水構造 雌継手 左右 ゴム 構造 帯板 水平断面図 止水効果 Ｐ－Ｔ継手 止水性能 止水 請求項 漏洩防止ゴム 水板 鋼管軸方向 弾性体 板厚方向 ウェブ板部 フランジ板部 端部 スリット 止水性 ｂ Ｔ型継手等 発明 長手方向 Ｌ－Ｔ継手 Ｐ－Ｐ継手 雄継手 幅方向 挿入 Ｐ－Ｔ型継手 拡大断面図 継手等 弾性止水材 板幅方向 内部空間 不連続スリット 鋼管セル等 刃物 － 両端部 ｃ 鋼管矢板等 モルタル等 継手先端刃物 Ｃ型継手管等 止水板幅方向 特許文献 例 左右両側 断面 不連続孔 文献図 間隔 方向 厚板 板厚 継手同士 幅 側面図 正面図 水部材 水膨潤ゴム 隙間 部 鋼管本体 間隔保持部材 改良案 充填空間 孔 合成樹脂等 当該継手 雌継手等 ＣＴ 先端部同士 ａ 先端 鋼板セル等 水手段 平面図 幅止め鋼板 端面 皿ねじ 弾性変形 先端フランジ板 先端部 小径鋼管 鋼矢板長手方向 ＣＴ形鋼 鋼管矢板同士 構造等 施工方法 外面 参照 外側 特徴 刃 泥水 作業 洗浄作業等 板状部材 フランジ 充填材 ずれ等 スタッドボルト・ナット・帯板 記載 発生 実施形態 押え布 ゴムシート等 内部 傾斜 効果 部材 下端 下端部等 周面 幅止め鋼板等 形鋼 土砂 掘削 周囲 課題 スタッドボルト 左右両端部 隙間発生側 側面 下端部 先端部どうし 雌ねじ孔 反対側 弾性 取付方法 表面 コスト 設時 本体 布 勾配 上記 洗浄 フランジナット 先端側 端面同士 特開 号公報 一体性 排土 座ぐり 開口幅 ボルト等 傾斜端面 説明 外部 板材 中央 一つ 所定 奥 次 両面 状態 構成 支障 護岸等 泥土 ～ 泥土等 ～図 基端側 円周方向 両側 フラットバー等 合成樹脂製 ボルト孔 施工 ウェブ 作業能率 複数層 技術 嵌入 通常 補強 内側 破損 侵入 】　( 形態 種々 溶接固定側 薄板状 材質 Ｖ字状 刃先 ｄ モルタルジャケット方式 作用効果 側面視 内外複数層 施工終了 ボルト 性 技術分野 背景技術 洗浄工程 アスファルト混合物 護岸 工程 薄肉円筒 手段 遊嵌状態 図示例 座金付き 溶接 切り込み 図示 内外 ～（ｃ 防波堤 築造 相互 流入 開示 前述 拡開 各層 他方 外 向上 低減 最良 カットティー 要件 下部 尖鋭 斜め 内面 手順 設前 図面 符号\n"
     ]
    }
   ],
   "source": [
    "def splitdata(dicts):\n",
    "    test_valid_size = int(len(dicts) * 0.1)\n",
    "    test_data  = dicts[:test_valid_size]\n",
    "    valid_data = dicts[test_valid_size : test_valid_size*2]\n",
    "    train_data = dicts[test_valid_size*2 :]\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "sitas = []\n",
    "sita_hats = []\n",
    "files = glob.glob('./Dataset/*')\n",
    "for period, file in enumerate(files):\n",
    "    contents = open(file, 'rb')\n",
    "    dicts = pickle.load(contents)\n",
    "    dicts = dicts[:200] # 無くしたい\n",
    "    if period==0:\n",
    "        bow_vocab = gensim.corpora.Dictionary(dicts)\n",
    "        bow_vocab_size = len(bow_vocab)\n",
    "        hidden_dim = 500\n",
    "        topic_num = 15\n",
    "        batch_size = 32\n",
    "        sita_hat = None\n",
    "        model = trainer.Estimator(input_dim = bow_vocab_size, hidden_dim = hidden_dim, topic_num = topic_num)\n",
    "    train_data, valid_data, test_data = splitdata(dicts)\n",
    "    test_valid_size = int(len(dicts) * 0.1)\n",
    "    last_batch_idx = len(dicts[test_valid_size*2 :])//batch_size-1\n",
    "    # ntm_model, pred_sita, train, z_valid = model.fit(train_data, valid_data, bow_vocab, batch_size, period, n_epoch=1)\n",
    "    # sita_hat = None\n",
    "    # model, z_train, z_valid = model.fit(train_data, valid_data, bow_vocab, batch_size, last_batch_idx, sita_hat, period, n_epoch=1)\n",
    "    sita, next_sita_hat = model.fit(train_data, valid_data, bow_vocab, batch_size, last_batch_idx, sita_hat, period, n_epoch=50)\n",
    "    sitas.append(sita)\n",
    "    sita_hats.append(next_sita_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.1900e-36, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          6.0856e-14, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "        grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 2.5036e-31, 0.0000e+00, 1.8754e-19, 0.0000e+00,\n",
       "          0.0000e+00, 9.2792e-38, 3.1118e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.4930e-19, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 2.8026e-44, 0.0000e+00, 6.5096e-32, 0.0000e+00,\n",
       "          0.0000e+00, 5.6052e-44, 1.2170e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.9062e-34, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 8.6841e-34, 0.0000e+00, 2.9853e-35, 0.0000e+00,\n",
       "          0.0000e+00, 2.2055e-27, 1.4237e-23, 8.5186e-41, 0.0000e+00, 0.0000e+00,\n",
       "          7.1978e-25, 0.0000e+00, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 4.7386e-38, 1.7065e-18, 2.5047e-26, 1.9055e-27, 0.0000e+00,\n",
       "          2.0739e-43, 4.4991e-08, 3.6034e-36, 1.3263e-40, 0.0000e+00, 9.8846e-39,\n",
       "          1.3649e-24, 1.7849e-40, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 1.6949e-40, 5.2188e-31, 2.0354e-38, 2.4003e-33, 0.0000e+00,\n",
       "          0.0000e+00, 1.2656e-23, 1.3927e-33, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          4.7070e-34, 1.8121e-39, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 1.7656e-43, 9.6343e-27, 4.1303e-27, 1.9957e-19, 0.0000e+00,\n",
       "          0.0000e+00, 2.6579e-11, 6.9905e-16, 3.1566e-26, 5.0513e-41, 5.9292e-37,\n",
       "          8.6015e-21, 4.9633e-38, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 1.7555e-37, 2.7725e-14, 1.0452e-24, 6.4228e-33, 0.0000e+00,\n",
       "          3.7835e-44, 3.1937e-19, 1.8160e-18, 1.1909e-30, 1.4013e-45, 6.1056e-33,\n",
       "          4.7109e-24, 3.0106e-28, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[2.8026e-45, 6.9962e-35, 2.4829e-16, 5.2005e-15, 1.5695e-22, 0.0000e+00,\n",
       "          0.0000e+00, 9.9994e-01, 5.5539e-05, 8.5356e-23, 0.0000e+00, 1.6389e-35,\n",
       "          3.2177e-20, 1.5403e-23, 4.4986e-06]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 4.3215e-37, 1.2244e-23, 1.0669e-36, 1.6827e-21, 3.1556e-39,\n",
       "          0.0000e+00, 1.5030e-05, 3.5823e-11, 2.2794e-25, 0.0000e+00, 5.7827e-35,\n",
       "          5.4955e-26, 5.2309e-33, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[0.0000e+00, 2.9292e-35, 6.4890e-20, 1.0270e-31, 8.6857e-23, 2.4999e-42,\n",
       "          1.4588e-42, 1.8876e-16, 4.0558e-12, 2.6830e-32, 0.0000e+00, 1.2823e-40,\n",
       "          9.0587e-18, 9.2692e-24, 1.0000e+00]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[4.5015e-32, 3.7694e-31, 1.5457e-21, 1.7732e-11, 9.9996e-01, 6.8539e-38,\n",
       "          1.4013e-45, 6.9012e-12, 3.4046e-11, 1.2948e-26, 2.6352e-32, 3.0388e-33,\n",
       "          1.9642e-19, 1.5715e-17, 4.2526e-05]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[8.0110e-32, 1.8172e-28, 7.3960e-10, 1.6558e-05, 4.6540e-14, 8.9020e-26,\n",
       "          1.6881e-24, 1.3449e-15, 7.6316e-01, 5.7563e-15, 2.4960e-16, 1.5138e-20,\n",
       "          6.2718e-08, 9.5922e-19, 2.3682e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[3.9765e-34, 1.0042e-28, 1.4982e-15, 3.3064e-18, 3.0029e-06, 5.5577e-26,\n",
       "          7.0077e-22, 5.3496e-10, 6.5252e-09, 3.9846e-09, 1.0841e-20, 4.0437e-12,\n",
       "          1.7620e-05, 6.1754e-11, 9.9998e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[4.2298e-30, 1.4341e-22, 8.9634e-23, 4.5155e-22, 4.0341e-13, 6.6782e-32,\n",
       "          1.1770e-31, 5.2071e-12, 6.9789e-04, 7.3368e-21, 5.9052e-35, 1.8135e-30,\n",
       "          5.7119e-14, 6.7297e-19, 9.9930e-01]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[8.9795e-24, 1.2007e-13, 8.1134e-10, 5.7046e-18, 2.6594e-10, 4.2785e-19,\n",
       "          2.2196e-25, 7.0971e-01, 2.6852e-18, 2.1213e-17, 3.8565e-23, 1.0464e-21,\n",
       "          2.9029e-01, 5.2560e-09, 2.1471e-08]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[1.1472e-27, 4.9535e-23, 3.6496e-14, 2.5925e-15, 2.4342e-17, 3.2578e-30,\n",
       "          9.9976e-39, 8.2561e-01, 1.5754e-01, 1.6820e-16, 1.4965e-31, 7.6242e-27,\n",
       "          1.6713e-02, 3.0090e-25, 1.2997e-04]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[3.3642e-20, 4.7051e-10, 4.2316e-26, 3.7470e-15, 9.7358e-01, 3.9791e-08,\n",
       "          1.1962e-19, 1.0066e-09, 2.6418e-02, 3.3202e-08, 3.2260e-19, 4.3761e-15,\n",
       "          6.7124e-22, 2.5879e-21, 2.3883e-08]], grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[1.4649e-22, 1.6587e-14, 2.1577e-06, 4.5814e-09, 9.9998e-01, 5.6730e-13,\n",
       "          1.6481e-20, 1.0496e-07, 5.0721e-16, 2.3209e-16, 1.7346e-20, 7.9254e-08,\n",
       "          3.2743e-19, 9.4335e-19, 1.2609e-05]], grad_fn=<ReshapeAliasBackward0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0834, 0.1056, 0.0543, 0.0442, 0.0791, 0.0923, 0.0769, 0.0436, 0.0531,\n",
       "          0.0370, 0.1193, 0.0438, 0.0365, 0.0939, 0.0369]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0818, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
       "          0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0623, 0.0620, 0.0551, 0.0605, 0.0363, 0.0749, 0.0725, 0.0934, 0.0584,\n",
       "          0.0658, 0.0798, 0.0637, 0.0518, 0.1152, 0.0484]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0708, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0817, 0.0444, 0.1043, 0.0502, 0.0601, 0.0669, 0.1075, 0.0923, 0.0593,\n",
       "          0.0523, 0.0620, 0.0556, 0.0510, 0.0709, 0.0414]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0736, 0.0987, 0.0620, 0.0501, 0.0678, 0.0861, 0.0717, 0.0536, 0.0606,\n",
       "          0.0501, 0.0949, 0.0479, 0.0433, 0.0871, 0.0524]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0788, 0.0992, 0.0537, 0.0477, 0.0683, 0.0903, 0.0751, 0.0503, 0.0548,\n",
       "          0.0421, 0.1130, 0.0475, 0.0394, 0.0999, 0.0400]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0451, 0.0484, 0.0423, 0.0387, 0.0933, 0.0661, 0.1054, 0.1082, 0.0454,\n",
       "          0.0935, 0.0678, 0.0604, 0.0458, 0.0635, 0.0760]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0447, 0.0479, 0.0421, 0.0381, 0.0959, 0.0657, 0.1066, 0.1084, 0.0450,\n",
       "          0.0940, 0.0672, 0.0601, 0.0456, 0.0621, 0.0767]],\n",
       "        grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sita_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9ElEQVR4nO3df6zd9V3H8efLdsSxHzLtZW79YaupbM0CG14Zuqg4RFtcqCb+UTYH4pamCUw0GmFZMv9YYmbwx1xgqw3WbpFADENXlw5GNpU/NkwL8qsg7KYovRSliE4df9SOt3+cU3M4Pfeeb8tpz+3H5yO5uff7/X44553b3me/93vv95CqQpJ05vuuaQ8gSZoMgy5JjTDoktQIgy5JjTDoktSI5dN64hUrVtTatWun9fSSdEZ64IEHXqiqmVHHphb0tWvXsm/fvmk9vSSdkZL880LHvOQiSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiLFBT7IzyfNJHlvgeJJ8OslckkeSXDj5MSVJ43Q5Q98FbFzk+CZgff9tK/DZVz+WJOlEjQ16Vd0HvLjIks3A56vnfuCcJG+Z1ICSpG4mcafoSuDgwPZ8f99zwwuTbKV3Fs+aNWsm8NSSdHrdsu1rr/oxrt3+3glMcrxJ/FA0I/aN/N8gVdWOqpqtqtmZmZEvRSBJOkmTCPo8sHpgexVwaAKPK0k6AZMI+m7gqv5vu1wMfKuqjrvcIkk6tcZeQ09yO3AJsCLJPPA7wGsAqmo7sAe4HJgDXgKuOVXDSpIWNjboVXXlmOMFXDuxiSRJJ8U7RSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHP+eJH+d5OEk+5NcM/lRJUmLGRv0JMuAW4BNwAbgyiQbhpZdCzxeVRcAlwB/kOSsCc8qSVpElzP0i4C5qjpQVUeAO4DNQ2sKeEOSAK8HXgSOTnRSSdKiugR9JXBwYHu+v2/QzcDbgUPAo8D1VfXy8AMl2ZpkX5J9hw8fPsmRJUmjdAl6Ruyroe2fAx4C3gq8E7g5yRuP+4+qdlTVbFXNzszMnOCokqTFdAn6PLB6YHsVvTPxQdcAd1XPHPA08LbJjChJ6qJL0PcC65Os6/+gcwuwe2jNM8ClAEneDJwHHJjkoJKkxS0ft6Cqjia5DrgHWAbsrKr9Sbb1j28HPgHsSvIovUs0N1TVC6dwbknSkLFBB6iqPcCeoX3bBz4+BPzsZEeTJJ0I7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mY5Mkkc0luXGDNJUkeSrI/yd9NdkxJ0jjLxy1Isgy4BbgMmAf2JtldVY8PrDkH+AywsaqeSXLuKZpXkrSALmfoFwFzVXWgqo4AdwCbh9a8H7irqp4BqKrnJzumJGmcLkFfCRwc2J7v7xv0w8CbkvxtkgeSXDWpASVJ3Yy95AJkxL4a8Tg/AlwKvBb4RpL7q+qpVzxQshXYCrBmzZoTn1aStKAuZ+jzwOqB7VXAoRFr7q6qb1fVC8B9wAXDD1RVO6pqtqpmZ2ZmTnZmSdIIXYK+F1ifZF2Ss4AtwO6hNV8EfiLJ8iRnA+8GnpjsqJKkxYy95FJVR5NcB9wDLAN2VtX+JNv6x7dX1RNJ7gYeAV4Gbq2qx07l4JKkV+pyDZ2q2gPsGdq3fWj7JuCmyY0mSToR3ikqSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY3oFPQkG5M8mWQuyY2LrPvRJN9J8kuTG1GS1MXYoCdZBtwCbAI2AFcm2bDAut8D7pn0kJKk8bqcoV8EzFXVgao6AtwBbB6x7iPAF4DnJzifJKmjLkFfCRwc2J7v7/s/SVYCvwhsX+yBkmxNsi/JvsOHD5/orJKkRXQJekbsq6HtTwE3VNV3FnugqtpRVbNVNTszM9NxRElSF8s7rJkHVg9srwIODa2ZBe5IArACuDzJ0ar6q0kMKUkar0vQ9wLrk6wDngW2AO8fXFBV6459nGQX8CVjLkmn19igV9XRJNfR++2VZcDOqtqfZFv/+KLXzSVJp0eXM3Sqag+wZ2jfyJBX1a+8+rEkSSfKO0UlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa0SnoSTYmeTLJXJIbRxz/QJJH+m9fT3LB5EeVJC1mbNCTLANuATYBG4Ark2wYWvY08FNVdT7wCWDHpAeVJC2uyxn6RcBcVR2oqiPAHcDmwQVV9fWq+vf+5v3AqsmOKUkap0vQVwIHB7bn+/sW8iHgy6MOJNmaZF+SfYcPH+4+pSRprC5Bz4h9NXJh8tP0gn7DqONVtaOqZqtqdmZmpvuUkqSxlndYMw+sHtheBRwaXpTkfOBWYFNV/dtkxpMkddXlDH0vsD7JuiRnAVuA3YMLkqwB7gI+WFVPTX5MSdI4Y8/Qq+pokuuAe4BlwM6q2p9kW//4duDjwPcBn0kCcLSqZk/d2JKkYV0uuVBVe4A9Q/u2D3z8YeDDkx1NknQivFNUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHE+ST/ePP5LkwsmPKklazNigJ1kG3AJsAjYAVybZMLRsE7C+/7YV+OyE55QkjdHlDP0iYK6qDlTVEeAOYPPQms3A56vnfuCcJG+Z8KySpEUs77BmJXBwYHseeHeHNSuB5wYXJdlK7wwe4L+TPHlC056YFcALp/DxJ8EZJ8MZJ8MZJ2fROa/7k1f12D+w0IEuQc+IfXUSa6iqHcCODs/5qiXZV1Wzp+O5TpYzToYzToYzTs605uxyyWUeWD2wvQo4dBJrJEmnUJeg7wXWJ1mX5CxgC7B7aM1u4Kr+b7tcDHyrqp4bfiBJ0qkz9pJLVR1Nch1wD7AM2FlV+5Ns6x/fDuwBLgfmgJeAa07dyJ2dlks7r5IzToYzToYzTs5U5kzVcZe6JUlnIO8UlaRGGHRJakRzQR/3MgXTlmR1kr9J8kSS/Umun/ZMC0myLMk/JPnStGdZSJJzktyZ5B/7n9Mfm/ZMw5L8Rv/P+rEktyf57iUw084kzyd5bGDf9ya5N8k3++/ftARnvKn/Z/1Ikr9Mcs4URxw548Cx30pSSVacrnmaCnrHlymYtqPAb1bV24GLgWuX4IzHXA88Me0hxvhj4O6qehtwAUts3iQrgV8DZqvqHfR+sWDLdKcCYBewcWjfjcBXq2o98NX+9jTt4vgZ7wXeUVXnA08BHz3dQw3ZxfEzkmQ1cBnwzOkcpqmg0+1lCqaqqp6rqgf7H/8XvQCtnO5Ux0uyCvh54NZpz7KQJG8EfhL4U4CqOlJV/zHVoUZbDrw2yXLgbJbAPRpVdR/w4tDuzcDn+h9/DviF0znTsFEzVtVXqupof/N+eve8TM0Cn0eAPwJ+mxE3WJ5KrQV9oZcgWJKSrAXeBfz9lEcZ5VP0/kK+POU5FvODwGHgz/qXhm5N8rppDzWoqp4Ffp/emdpz9O7R+Mp0p1rQm4/dP9J/f+6U5xnnV4EvT3uIYUmuAJ6tqodP93O3FvROL0GwFCR5PfAF4Ner6j+nPc+gJO8Dnq+qB6Y9yxjLgQuBz1bVu4BvM/3LBK/Qvw69GVgHvBV4XZJfnu5UZ74kH6N3+fK2ac8yKMnZwMeAj0/j+VsL+hnxEgRJXkMv5rdV1V3TnmeE9wBXJPknepet3pvkz6c70kjzwHxVHfsO5056gV9KfgZ4uqoOV9X/AHcBPz7lmRbyr8deJbX//vkpzzNSkquB9wEfqKV3I80P0fvH++H+188q4MEk3386nry1oHd5mYKpShJ613yfqKo/nPY8o1TVR6tqVVWtpfc5/FpVLbmzyqr6F+BgkvP6uy4FHp/iSKM8A1yc5Oz+n/2lLLEf3A7YDVzd//hq4ItTnGWkJBuBG4Arquqlac8zrKoerapzq2pt/+tnHriw/3f1lGsq6P0flhx7mYIngL+oqv3Tneo47wE+SO+s96H+2+XTHuoM9hHgtiSPAO8Efne647xS/7uHO4EHgUfpfc1N/fb1JLcD3wDOSzKf5EPAJ4HLknyT3m9ofHIJzngz8Abg3v7XzvYlOOP05ll637FIkk5GU2fokvT/mUGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxP8CTqqCUBqHI38AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9ElEQVR4nO3df6zd9V3H8efLdsSxHzLtZW79YaupbM0CG14Zuqg4RFtcqCb+UTYH4pamCUw0GmFZMv9YYmbwx1xgqw3WbpFADENXlw5GNpU/NkwL8qsg7KYovRSliE4df9SOt3+cU3M4Pfeeb8tpz+3H5yO5uff7/X44553b3me/93vv95CqQpJ05vuuaQ8gSZoMgy5JjTDoktQIgy5JjTDoktSI5dN64hUrVtTatWun9fSSdEZ64IEHXqiqmVHHphb0tWvXsm/fvmk9vSSdkZL880LHvOQiSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiLFBT7IzyfNJHlvgeJJ8OslckkeSXDj5MSVJ43Q5Q98FbFzk+CZgff9tK/DZVz+WJOlEjQ16Vd0HvLjIks3A56vnfuCcJG+Z1ICSpG4mcafoSuDgwPZ8f99zwwuTbKV3Fs+aNWsm8NSSdHrdsu1rr/oxrt3+3glMcrxJ/FA0I/aN/N8gVdWOqpqtqtmZmZEvRSBJOkmTCPo8sHpgexVwaAKPK0k6AZMI+m7gqv5vu1wMfKuqjrvcIkk6tcZeQ09yO3AJsCLJPPA7wGsAqmo7sAe4HJgDXgKuOVXDSpIWNjboVXXlmOMFXDuxiSRJJ8U7RSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHP+eJH+d5OEk+5NcM/lRJUmLGRv0JMuAW4BNwAbgyiQbhpZdCzxeVRcAlwB/kOSsCc8qSVpElzP0i4C5qjpQVUeAO4DNQ2sKeEOSAK8HXgSOTnRSSdKiugR9JXBwYHu+v2/QzcDbgUPAo8D1VfXy8AMl2ZpkX5J9hw8fPsmRJUmjdAl6Ruyroe2fAx4C3gq8E7g5yRuP+4+qdlTVbFXNzszMnOCokqTFdAn6PLB6YHsVvTPxQdcAd1XPHPA08LbJjChJ6qJL0PcC65Os6/+gcwuwe2jNM8ClAEneDJwHHJjkoJKkxS0ft6Cqjia5DrgHWAbsrKr9Sbb1j28HPgHsSvIovUs0N1TVC6dwbknSkLFBB6iqPcCeoX3bBz4+BPzsZEeTJJ0I7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mY5Mkkc0luXGDNJUkeSrI/yd9NdkxJ0jjLxy1Isgy4BbgMmAf2JtldVY8PrDkH+AywsaqeSXLuKZpXkrSALmfoFwFzVXWgqo4AdwCbh9a8H7irqp4BqKrnJzumJGmcLkFfCRwc2J7v7xv0w8CbkvxtkgeSXDWpASVJ3Yy95AJkxL4a8Tg/AlwKvBb4RpL7q+qpVzxQshXYCrBmzZoTn1aStKAuZ+jzwOqB7VXAoRFr7q6qb1fVC8B9wAXDD1RVO6pqtqpmZ2ZmTnZmSdIIXYK+F1ifZF2Ss4AtwO6hNV8EfiLJ8iRnA+8GnpjsqJKkxYy95FJVR5NcB9wDLAN2VtX+JNv6x7dX1RNJ7gYeAV4Gbq2qx07l4JKkV+pyDZ2q2gPsGdq3fWj7JuCmyY0mSToR3ikqSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY3oFPQkG5M8mWQuyY2LrPvRJN9J8kuTG1GS1MXYoCdZBtwCbAI2AFcm2bDAut8D7pn0kJKk8bqcoV8EzFXVgao6AtwBbB6x7iPAF4DnJzifJKmjLkFfCRwc2J7v7/s/SVYCvwhsX+yBkmxNsi/JvsOHD5/orJKkRXQJekbsq6HtTwE3VNV3FnugqtpRVbNVNTszM9NxRElSF8s7rJkHVg9srwIODa2ZBe5IArACuDzJ0ar6q0kMKUkar0vQ9wLrk6wDngW2AO8fXFBV6459nGQX8CVjLkmn19igV9XRJNfR++2VZcDOqtqfZFv/+KLXzSVJp0eXM3Sqag+wZ2jfyJBX1a+8+rEkSSfKO0UlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa0SnoSTYmeTLJXJIbRxz/QJJH+m9fT3LB5EeVJC1mbNCTLANuATYBG4Ark2wYWvY08FNVdT7wCWDHpAeVJC2uyxn6RcBcVR2oqiPAHcDmwQVV9fWq+vf+5v3AqsmOKUkap0vQVwIHB7bn+/sW8iHgy6MOJNmaZF+SfYcPH+4+pSRprC5Bz4h9NXJh8tP0gn7DqONVtaOqZqtqdmZmpvuUkqSxlndYMw+sHtheBRwaXpTkfOBWYFNV/dtkxpMkddXlDH0vsD7JuiRnAVuA3YMLkqwB7gI+WFVPTX5MSdI4Y8/Qq+pokuuAe4BlwM6q2p9kW//4duDjwPcBn0kCcLSqZk/d2JKkYV0uuVBVe4A9Q/u2D3z8YeDDkx1NknQivFNUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJNiZ5MslckhtHHE+ST/ePP5LkwsmPKklazNigJ1kG3AJsAjYAVybZMLRsE7C+/7YV+OyE55QkjdHlDP0iYK6qDlTVEeAOYPPQms3A56vnfuCcJG+Z8KySpEUs77BmJXBwYHseeHeHNSuB5wYXJdlK7wwe4L+TPHlC056YFcALp/DxJ8EZJ8MZJ8MZJ2fROa/7k1f12D+w0IEuQc+IfXUSa6iqHcCODs/5qiXZV1Wzp+O5TpYzToYzToYzTs605uxyyWUeWD2wvQo4dBJrJEmnUJeg7wXWJ1mX5CxgC7B7aM1u4Kr+b7tcDHyrqp4bfiBJ0qkz9pJLVR1Nch1wD7AM2FlV+5Ns6x/fDuwBLgfmgJeAa07dyJ2dlks7r5IzToYzToYzTs5U5kzVcZe6JUlnIO8UlaRGGHRJakRzQR/3MgXTlmR1kr9J8kSS/Umun/ZMC0myLMk/JPnStGdZSJJzktyZ5B/7n9Mfm/ZMw5L8Rv/P+rEktyf57iUw084kzyd5bGDf9ya5N8k3++/ftARnvKn/Z/1Ikr9Mcs4URxw548Cx30pSSVacrnmaCnrHlymYtqPAb1bV24GLgWuX4IzHXA88Me0hxvhj4O6qehtwAUts3iQrgV8DZqvqHfR+sWDLdKcCYBewcWjfjcBXq2o98NX+9jTt4vgZ7wXeUVXnA08BHz3dQw3ZxfEzkmQ1cBnwzOkcpqmg0+1lCqaqqp6rqgf7H/8XvQCtnO5Ux0uyCvh54NZpz7KQJG8EfhL4U4CqOlJV/zHVoUZbDrw2yXLgbJbAPRpVdR/w4tDuzcDn+h9/DviF0znTsFEzVtVXqupof/N+eve8TM0Cn0eAPwJ+mxE3WJ5KrQV9oZcgWJKSrAXeBfz9lEcZ5VP0/kK+POU5FvODwGHgz/qXhm5N8rppDzWoqp4Ffp/emdpz9O7R+Mp0p1rQm4/dP9J/f+6U5xnnV4EvT3uIYUmuAJ6tqodP93O3FvROL0GwFCR5PfAF4Ner6j+nPc+gJO8Dnq+qB6Y9yxjLgQuBz1bVu4BvM/3LBK/Qvw69GVgHvBV4XZJfnu5UZ74kH6N3+fK2ac8yKMnZwMeAj0/j+VsL+hnxEgRJXkMv5rdV1V3TnmeE9wBXJPknepet3pvkz6c70kjzwHxVHfsO5056gV9KfgZ4uqoOV9X/AHcBPz7lmRbyr8deJbX//vkpzzNSkquB9wEfqKV3I80P0fvH++H+188q4MEk3386nry1oHd5mYKpShJ613yfqKo/nPY8o1TVR6tqVVWtpfc5/FpVLbmzyqr6F+BgkvP6uy4FHp/iSKM8A1yc5Oz+n/2lLLEf3A7YDVzd//hq4ItTnGWkJBuBG4Arquqlac8zrKoerapzq2pt/+tnHriw/3f1lGsq6P0flhx7mYIngL+oqv3Tneo47wE+SO+s96H+2+XTHuoM9hHgtiSPAO8Efne647xS/7uHO4EHgUfpfc1N/fb1JLcD3wDOSzKf5EPAJ4HLknyT3m9ofHIJzngz8Abg3v7XzvYlOOP05ll637FIkk5GU2fokvT/mUGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqxP8CTqqCUBqHI38AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAP+0lEQVR4nO3df6zdd13H8efLWxYdigN7EW0rrViYEzaZ1wKiguJix69CJLEDYQikqbH8iiglRAIhMZAZRaXQNLMOI6EhMKDBQiH4AyNgejfHtm4UrkXXuw53AQUZxFJ4+8c9I2en597zvd25Pd3H5yM5uefz+X76Pa+c2/vq937P+Z6mqpAkPfB936QDSJLGw0KXpEZY6JLUCAtdkhphoUtSI9ZM6oHXrl1bGzdunNTDS9ID0g033PDlqpoetm1ihb5x40ZmZ2cn9fCS9ICU5D+W2uYpF0lqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjOhV6kq1JjiWZS7J7yPbfT3JT73Zrku8kedj440qSljLyStEkU8Ae4ApgHjiS5GBV3Xbvmqq6Brimt/5ZwKur6qurE1m6f974xjeeF/uQxq3LEfoWYK6qjlfVKeAAsG2Z9VcB7xlHOElSd10KfR1wom8835s7Q5ILga3A++9/NEnSSnQp9AyZW+o/In0W8M9LnW5JsiPJbJLZhYWFrhklSR10KfR5YEPfeD1wcom121nmdEtV7auqmaqamZ4e+umPkqSz1KXQjwCbk2xKcgGLpX1wcFGSHwaeAnxovBElSV2MfJdLVZ1Osgs4DEwB+6vqaJKdve17e0ufC3ysqu5ZtbSSpCV1+g8uquoQcGhgbu/A+DrgunEFkyStjFeKSlIjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDWiU6En2ZrkWJK5JLuXWPPUJDclOZrkH8cbU5I0yppRC5JMAXuAK4B54EiSg1V1W9+ai4B3AFur6o4kD1+lvJKkJXQ5Qt8CzFXV8ao6BRwAtg2seT5wfVXdAVBVd483piRplC6Fvg440Tee7831ezTw0CT/kOSGJC8atqMkO5LMJpldWFg4u8SSpKG6FHqGzNXAeA3wc8AzgF8H/jDJo8/4Q1X7qmqmqmamp6dXHFaStLSR59BZPCLf0DdeD5wcsubLVXUPcE+STwKXAZ8fS0pJ0khdjtCPAJuTbEpyAbAdODiw5kPALyVZk+RC4AnA7eONKklazsgj9Ko6nWQXcBiYAvZX1dEkO3vb91bV7Uk+CtwMfBe4tqpuXc3gkqT76nLKhao6BBwamNs7ML4GuGZ80SRJK+GVopLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJakSnQk+yNcmxJHNJdg/Z/tQkX0tyU+/2hvFHlSQtZ82oBUmmgD3AFcA8cCTJwaq6bWDpP1XVM1choySpgy5H6FuAuao6XlWngAPAttWNJUlaqS6Fvg440Tee780NelKSzyb5SJKfGbajJDuSzCaZXVhYOIu4kqSldCn0DJmrgfGNwCOr6jLgL4APDttRVe2rqpmqmpmenl5RUEnS8roU+jywoW+8HjjZv6Cqvl5V3+jdPwQ8KMnasaWUJI3UpdCPAJuTbEpyAbAdONi/IMkjkqR3f0tvv18Zd1hJ0tJGvsulqk4n2QUcBqaA/VV1NMnO3va9wPOA30lyGvgWsL2qBk/LSJJW0chCh++dRjk0MLe37/7bgbePN5okaSW8UlSSGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY3oVOhJtiY5lmQuye5l1v18ku8ked74IkqSuhhZ6EmmgD3AlcAlwFVJLlli3VuBw+MOKUkarcsR+hZgrqqOV9Up4ACwbci6lwPvB+4eYz5JUkddCn0dcKJvPN+b+54k64DnAnuX21GSHUlmk8wuLCysNKskaRldCj1D5mpg/DbgtVX1neV2VFX7qmqmqmamp6c7RpQkdbGmw5p5YEPfeD1wcmDNDHAgCcBa4OlJTlfVB8cRUpI0WpdCPwJsTrIJuBPYDjy/f0FVbbr3fpLrgA9b5pJ0bo0s9Ko6nWQXi+9emQL2V9XRJDt725c9by5JOje6HKFTVYeAQwNzQ4u8ql58/2NJklbKK0UlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktSIToWeZGuSY0nmkuwesn1bkpuT3JRkNskvjj+qJGk5a0YtSDIF7AGuAOaBI0kOVtVtfcs+ARysqkpyKfBe4OLVCCxJGq7LEfoWYK6qjlfVKeAAsK1/QVV9o6qqN3wwUEiSzqkuhb4OONE3nu/N3UeS5yb5HPC3wEuG7SjJjt4pmdmFhYWzyStJWkKXQs+QuTOOwKvqA1V1MfAc4M3DdlRV+6pqpqpmpqenVxRUkrS8LoU+D2zoG68HTi61uKo+CTwqydr7mU2StAIjXxQFjgCbk2wC7gS2A8/vX5Dkp4B/670oejlwAfCVcYeV1LbHvetx93sft1x9yxiSPDCNLPSqOp1kF3AYmAL2V9XRJDt72/cCvwG8KMm3gW8Bv9n3Iqkk6RzocoROVR0CDg3M7e27/1bgreONJklaCa8UlaRGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIzoVepKtSY4lmUuye8j2FyS5uXf7VJLLxh9VkrSckYWeZArYA1wJXAJcleSSgWVfBJ5SVZcCbwb2jTuoJGl5XY7QtwBzVXW8qk4BB4Bt/Quq6lNV9V+94WeA9eONKUkapUuhrwNO9I3ne3NLeSnwkWEbkuxIMptkdmFhoXtKSdJIXQo9Q+Zq6MLkV1gs9NcO215V+6pqpqpmpqenu6eUJI20psOaeWBD33g9cHJwUZJLgWuBK6vqK+OJJ0nqqssR+hFgc5JNSS4AtgMH+xck+QngeuCFVfX58ceUJI0y8gi9qk4n2QUcBqaA/VV1NMnO3va9wBuAHwHekQTgdFXNrF5sSdKgLqdcqKpDwKGBub19918GvGy80SRJK+GVopLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJakSnQk+yNcmxJHNJdg/ZfnGSTyf53ySvGX9MSdIoa0YtSDIF7AGuAOaBI0kOVtVtfcu+CrwCeM5qhJQkjdblCH0LMFdVx6vqFHAA2Na/oKrurqojwLdXIaMkqYMuhb4OONE3nu/NSZLOI10KPUPm6mweLMmOJLNJZhcWFs5mF5KkJXQp9HlgQ994PXDybB6sqvZV1UxVzUxPT5/NLiRJS+hS6EeAzUk2JbkA2A4cXN1YkqSVGvkul6o6nWQXcBiYAvZX1dEkO3vb9yZ5BDALPAT4bpJXAZdU1ddXL7okqd/IQgeoqkPAoYG5vX33v8TiqRhJ0oR4pagkNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIzoVepKtSY4lmUuye8j2JPnz3vabk1w+/qiSpOWMLPQkU8Ae4ErgEuCqJJcMLLsS2Ny77QDeOeackqQRuhyhbwHmqup4VZ0CDgDbBtZsA/66Fn0GuCjJj405qyRpGWs6rFkHnOgbzwNP6LBmHXBX/6IkO1g8ggf4RpJjK0q7MmuBL6/i/sfBjONxzjO+6U1vWukf8Xkcj5EZ8+KcoyjLWs3n8pFLbehS6MOenTqLNVTVPmBfh8e835LMVtXMuXiss2XG8TDjeJhxfCaVs8spl3lgQ994PXDyLNZIklZRl0I/AmxOsinJBcB24ODAmoPAi3rvdnki8LWqumtwR5Kk1TPylEtVnU6yCzgMTAH7q+pokp297XuBQ8DTgTngm8Bvr17kzs7JqZ37yYzjYcbxMOP4TCRnqs441S1JegDySlFJaoSFLkmNaK7QR31MwaQl2ZDk75PcnuRokldOOtNSkkwl+dckH550lqUkuSjJ+5J8rvecPmnSmQYleXXve31rkvck+f7zINP+JHcnubVv7mFJPp7kC72vDz0PM17T+17fnOQDSS6aYMShGfu2vSZJJVl7rvI0VegdP6Zg0k4Dv1dVPw08Efjd8zDjvV4J3D7pECP8GfDRqroYuIzzLG+SdcArgJmqeiyLbyzYPtlUAFwHbB2Y2w18oqo2A5/ojSfpOs7M+HHgsVV1KfB54HXnOtSA6zgzI0k2AFcAd5zLME0VOt0+pmCiququqrqxd/9/WCygdZNNdaYk64FnANdOOstSkjwE+GXgLwGq6lRV/fdEQw23BviBJGuACzkPrtGoqk8CXx2Y3ga8q3f/XcBzzmWmQcMyVtXHqup0b/gZFq95mZglnkeAPwX+gCEXWK6m1gp9qY8gOC8l2Qg8HviXCUcZ5m0s/oX87oRzLOcngQXgr3qnhq5N8uBJh+pXVXcCf8zikdpdLF6j8bHJplrSj957/Ujv68MnnGeUlwAfmXSIQUmeDdxZVZ8914/dWqF3+giC80GSHwTeD7yqqr4+6Tz9kjwTuLuqbph0lhHWAJcD76yqxwP3MPnTBPfROw+9DdgE/Djw4CS/NdlUD3xJXs/i6ct3TzpLvyQXAq8H3jCJx2+t0B8QH0GQ5EEslvm7q+r6SecZ4snAs5P8O4unrX41yd9MNtJQ88B8Vd37G877WCz488mvAV+sqoWq+jZwPfALE860lP+891NSe1/vnnCeoZJcDTwTeEGdfxfSPIrFf7w/2/v5WQ/cmOQR5+LBWyv0Lh9TMFFJwuI539ur6k8mnWeYqnpdVa2vqo0sPod/V1Xn3VFlVX0JOJHkMb2ppwG3TTDSMHcAT0xyYe97/zTOsxdu+xwEru7dvxr40ASzDJVkK/Ba4NlV9c1J5xlUVbdU1cOramPv52ceuLz3d3XVNVXovRdL7v2YgtuB91bV0cmmOsOTgReyeNR7U+/29EmHegB7OfDuJDcDPwv80WTj3Ffvt4f3ATcCt7D4Mzfxy9eTvAf4NPCYJPNJXgq8BbgiyRdYfIfGW87DjG8Hfgj4eO9nZ+95mHFyec6/31gkSWejqSN0Sfr/zEKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5Jjfg/CShtuu0HzdsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOFElEQVR4nO3cf6zdd13H8efLlkXHD4f2DqGttpLyoyEbzOuYEhWZk3YQqol/bCDDCWmWbIhG40aI+AeJweAPJAxqM+sgLlsMTKmkMAhT9wfO7G6yH93cuG66XjrdnSgq/DHL3v5xTs3Z6bn3fNue9nv74flIbnq/3++n57xz2/vst997vidVhSTpzPddfQ8gSZoNgy5JjTDoktQIgy5JjTDoktSI9X098YYNG2rLli19Pb0knZHuvvvup6pqbtKx3oK+ZcsWFhYW+np6STojJfmXlY5NveSSZF+SJ5M8sMLxJPlIksUk9yW54GSGlSSdmC7X0G8EdqxyfCewbfixG/j4yY8lSTpeU4NeVXcAX19lyS7gkzVwJ3BOkhfPakBJUjezeJXLRuDQyPbScN8xkuxOspBkYXl5eQZPLUk6ahZBz4R9E98gpqr2VtV8Vc3PzU38Ia0k6QTNIuhLwOaR7U3A4Rk8riTpOMwi6PuBK4avdrkI+EZVPTGDx5UkHYepr0NPcjPwemBDkiXgt4HnAFTVHuAAcCmwCHwLuPJUDStJWtnUoFfV5VOOF3D1zCaSJJ2Q3u4U1dpw/VW3n9Tvv3rPG2Y0iaST5ZtzSVIjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNcKgS1IjDLokNaJT0JPsSPJwksUk1004/r1J/irJvUkOJrly9qNKklYzNehJ1gHXAzuB7cDlSbaPLbsaeLCqzgdeD/x+krNmPKskaRVdztAvBBar6tGqehq4Bdg1tqaA5ycJ8Dzg68CRmU4qSVpVl6BvBA6NbC8N9436KPBK4DBwP/CeqnpmJhNKkjrpEvRM2Fdj228EvgK8BHg18NEkLzjmgZLdSRaSLCwvLx/nqJKk1XQJ+hKweWR7E4Mz8VFXArfWwCLwGPCK8Qeqqr1VNV9V83Nzcyc6syRpgi5BvwvYlmTr8AedlwH7x9Y8DlwMkORFwMuBR2c5qCRpdeunLaiqI0muAW4D1gH7qupgkquGx/cAHwBuTHI/g0s011bVU6dwbknSmKlBB6iqA8CBsX17Rj4/DPzsbEeTJB0P7xSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mR5OEki0muW2HN65N8JcnBJH872zElSdOsn7YgyTrgeuASYAm4K8n+qnpwZM05wMeAHVX1eJJzT9G8kqQVdDlDvxBYrKpHq+pp4BZg19iatwK3VtXjAFX15GzHlCRN0yXoG4FDI9tLw32jXga8MMnfJLk7yRWTHijJ7iQLSRaWl5dPbGJJ0kRdgp4J+2psez3wI8CbgDcCv5XkZcf8pqq9VTVfVfNzc3PHPawkaWVTr6EzOCPfPLK9CTg8Yc1TVfVN4JtJ7gDOBx6ZyZSSpKm6nKHfBWxLsjXJWcBlwP6xNZ8BfiLJ+iRnA68FHprtqJKk1Uw9Q6+qI0muAW4D1gH7qupgkquGx/dU1UNJPg/cBzwD3FBVD5zKwSVJz9blkgtVdQA4MLZvz9j2h4APzW40SdLx8E5RSWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRnQKepIdSR5OspjkulXW/WiSbyf5hdmNKEnqYmrQk6wDrgd2AtuBy5NsX2Hd7wK3zXpISdJ0Xc7QLwQWq+rRqnoauAXYNWHdu4FPA0/OcD5JUkddgr4RODSyvTTc9/+SbAR+Htiz2gMl2Z1kIcnC8vLy8c4qSVpFl6Bnwr4a2/4wcG1VfXu1B6qqvVU1X1Xzc3NzHUeUJHWxvsOaJWDzyPYm4PDYmnngliQAG4BLkxypqr+cxZCSpOm6BP0uYFuSrcDXgMuAt44uqKqtRz9PciPwWWMuSafX1KBX1ZEk1zB49co6YF9VHUxy1fD4qtfNJUmnR5czdKrqAHBgbN/EkFfVL538WJKk4+WdopLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiE5BT7IjycNJFpNcN+H425LcN/z4cpLzZz+qJGk1U4OeZB1wPbAT2A5cnmT72LLHgJ+qqvOADwB7Zz2oJGl1Xc7QLwQWq+rRqnoauAXYNbqgqr5cVf8x3LwT2DTbMSVJ03QJ+kbg0Mj20nDfSt4JfG7SgSS7kywkWVheXu4+pSRpqi5Bz4R9NXFh8tMMgn7tpONVtbeq5qtqfm5urvuUkqSp1ndYswRsHtneBBweX5TkPOAGYGdV/ftsxpMkddXlDP0uYFuSrUnOAi4D9o8uSPKDwK3A26vqkdmPKUmaZuoZelUdSXINcBuwDthXVQeTXDU8vgd4P/D9wMeSABypqvlTN7YkaVyXSy5U1QHgwNi+PSOfvwt412xHkyQdD+8UlaRGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGrO97AKkFX7r9pSf1+y9+wz/NaBJ9J/MMXZIaYdAlqRGdgp5kR5KHkywmuW7C8ST5yPD4fUkumP2okqTVTA16knXA9cBOYDtweZLtY8t2AtuGH7uBj894TknSFF3O0C8EFqvq0ap6GrgF2DW2ZhfwyRq4EzgnyYtnPKskaRVdXuWyETg0sr0EvLbDmo3AE6OLkuxmcAYP8D9JHj6uaY/PBuCpU/j4s3DGz3jNH5/GSVZ2xn8dIadtkFU08HVcM07lnD+00oEuQZ/0N61OYA1VtRfY2+E5T1qShaqaPx3PdaKccTaccTaccXb6mrPLJZclYPPI9ibg8AmskSSdQl2CfhewLcnWJGcBlwH7x9bsB64YvtrlIuAbVfXE+ANJkk6dqZdcqupIkmuA24B1wL6qOpjkquHxPcAB4FJgEfgWcOWpG7mz03Jp5yQ542w442w44+z0MmeqjrnULUk6A3mnqCQ1wqBLUiOaC/q0tynoW5LNSf46yUNJDiZ5T98zrSTJuiT/kOSzfc+ykiTnJPlUkn8cfk1/rO+ZxiX5teGf9QNJbk7y3Wtgpn1JnkzywMi+70vyxSRfHf76wjU444eGf9b3JfmLJOf0OOLEGUeO/UaSSrLhdM3TVNA7vk1B344Av15VrwQuAq5egzMe9R7gob6HmOKPgM9X1SuA81lj8ybZCPwKMF9Vr2LwwoLL+p0KgBuBHWP7rgO+VFXbgC8Nt/t0I8fO+EXgVVV1HvAI8N7TPdSYGzl2RpJsBi4BHj+dwzQVdLq9TUGvquqJqrpn+Pl/MwjQxn6nOlaSTcCbgBv6nmUlSV4A/CTwJwBV9XRV/WevQ022HvieJOuBs1kD92hU1R3A18d27wI+Mfz8E8DPnc6Zxk2asaq+UFVHhpt3MrjnpTcrfB0B/hD4TSbcYHkqtRb0ld6CYE1KsgV4DfD3PY8yyYcZ/IV8puc5VvPDwDLwp8NLQzckeW7fQ42qqq8Bv8fgTO0JBvdofKHfqVb0oqP3jwx/Pbfneab5ZeBzfQ8xLslbgK9V1b2n+7lbC3qntyBYC5I8D/g08KtV9V99zzMqyZuBJ6vq7r5nmWI9cAHw8ap6DfBN+r9M8CzD69C7gK3AS4DnJvnFfqc68yV5H4PLlzf1PcuoJGcD7wPe38fztxb0M+ItCJI8h0HMb6qqW/ueZ4LXAW9J8s8MLlu9Icmf9TvSREvAUlUd/R/OpxgEfi35GeCxqlquqv8FbgV+vOeZVvJvR98ldfjrkz3PM1GSdwBvBt5Wa+9Gmpcy+Mf73uH3zybgniQ/cDqevLWgd3mbgl4lCYNrvg9V1R/0Pc8kVfXeqtpUVVsYfA1vr6o1d1ZZVf8KHEry8uGui4EHexxpkseBi5KcPfyzv5g19oPbEfuBdww/fwfwmR5nmSjJDuBa4C1V9a2+5xlXVfdX1blVtWX4/bMEXDD8u3rKNRX04Q9Ljr5NwUPAn1fVwX6nOsbrgLczOOv9yvDj0r6HOoO9G7gpyX3Aq4Hf6XecZxv+7+FTwD3A/Qy+53q/fT3JzcDfAS9PspTkncAHgUuSfJXBKzQ+uAZn/CjwfOCLw++dPWtwxv7mWXv/Y5EknYimztAl6TuZQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWrE/wEuMoVKZ8hHBAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# fig,axes = plt.subplots(nrows=5,ncols=4,figsize=(20,10))\n",
    "for j in range(20):\n",
    "    for i, item in enumerate(sitas[j][0]):\n",
    "        # print(i, item.item())\n",
    "        plt.bar(i, item.item())\n",
    "\n",
    "    # axes[i//4, i//5].show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQKElEQVR4nO3dcayddX3H8fdnrUTBsbpQJ7Z1raYBGplCbljVzGygSQuE7g+TQaYwtqVpUhSNxtWZTP5aSOackjU0DaIQiWxBljXaiQY0m4kQCiJYC/MOnb1QxjVGNJIMG7/74zwux8tp70PvuT33/vp+JTe9z/P7/Z7zOW3v5z736XlOU1VIktr1G5MOIElaXBa9JDXOopekxln0ktQ4i16SGrdy0gFGOeuss2r9+vWTjiFJy8ZDDz30o6paPWpsSRb9+vXrOXDgwKRjSNKykeS/jzXmpRtJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWrckrwzVtLJs37XlxZ8jB/ceNkYkmixeEYvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNa5X0SfZkuSJJNNJdo0YPzfJN5P8b5IPvZS1kqTFNW/RJ1kB7Aa2ApuAq5JsmjPtx8D7gI+fwFpJ0iLqc0Z/ETBdVU9W1QvAncC24QlV9WxVPQj84qWulSQtrj5FvwY4PLQ90+3rYyFrJUljsLLHnIzYVz2P33ttku3AdoDXve51PQ8vSSfm0LnnLfgY5z1+aAxJFl+fop8B1g1trwWe7nn83murai+wF2BqaqrvN5LFd8NvjeEYzy38GJJ0gvpcunkQ2JhkQ5LTgCuBfT2Pv5C1kqQxmPeMvqqOJrkOuAdYAdxaVQeT7OjG9yR5DXAAOBP4ZZL3A5uq6qej1i7Sc5EkjdDn0g1VtR/YP2ffnqHPn2FwWabXWknSyeOdsZLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqXK83NZPG4e//5PIFrf/gP31xTEmkU4tn9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDWuV9En2ZLkiSTTSXaNGE+Sm7rxR5NcODT2gSQHk3wnyeeTvHycT0CSdHzzFn2SFcBuYCuwCbgqyaY507YCG7uP7cDN3do1wPuAqap6I7ACuHJs6SVJ8+pzRn8RMF1VT1bVC8CdwLY5c7YBt9fA/cCqJGd3YyuBVyRZCZwOPD2m7JKkHvoU/Rrg8ND2TLdv3jlV9RTwceCHwBHguar6yqgHSbI9yYEkB2ZnZ/vmlyTNo0/RZ8S+6jMnyasYnO1vAF4LnJHk3aMepKr2VtVUVU2tXr26RyxJUh99/nPwGWDd0PZaXnz55Vhz3gF8v6pmAZLcDbwV+NyJBp7P+l1fWtD6H9x42ZiSSNLS0OeM/kFgY5INSU5j8I+p++bM2Qdc3b36ZjODSzRHGFyy2Zzk9CQBLgEOjTG/JGke857RV9XRJNcB9zB41cytVXUwyY5ufA+wH7gUmAaeB67txh5IchfwMHAU+BawdzGeiCRptD6Xbqiq/QzKfHjfnqHPC9h5jLUfAz62gIySpAXwzlhJapxFL0mNs+glqXG9rtFLS9HMrv9Y8DHW3vgHY0giLW2e0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcb6pmUbaveO+BR9j556Lx5BE0kJ5Ri9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDWuV9En2ZLkiSTTSXaNGE+Sm7rxR5NcODS2KsldSR5PcijJW8b5BCRJxzdv0SdZAewGtgKbgKuSbJozbSuwsfvYDtw8NPYp4MtVdS7wJuDQGHJLknrqc0Z/ETBdVU9W1QvAncC2OXO2AbfXwP3AqiRnJzkTeDvwaYCqeqGqfjK++JKk+fQp+jXA4aHtmW5fnzmvB2aBzyT5VpJbkpwx6kGSbE9yIMmB2dnZ3k9AknR8fYo+I/ZVzzkrgQuBm6vqAuDnwIuu8QNU1d6qmqqqqdWrV/eIJUnqo0/RzwDrhrbXAk/3nDMDzFTVA93+uxgUvyTpJOlT9A8CG5NsSHIacCWwb86cfcDV3atvNgPPVdWRqnoGOJzknG7eJcB3xxVekjS/ef8rwao6muQ64B5gBXBrVR1MsqMb3wPsBy4FpoHngWuHDvFe4I7um8STc8YkSYus1/8ZW1X7GZT58L49Q58XsPMYax8Bpk48oiRpIbwzVpIaZ9FLUuMseklqXK9r9JJOzL33vWHBx7jk4v8aQxKdyjyjl6TGeUY/Aeffdv6C1j92zWNjSiLpVOAZvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcL6+Uhtxwww0TXS8tBs/oJalxFr0kNc6il6TGeY2+AYfOPW/Bxzjv8UNjSCJpKbLoJY3fDb+1wPXPjSeHAC/dSFLzLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS43rdGZtkC/ApYAVwS1XdOGc83filwPPAn1XVw0PjK4ADwFNVdfmYskunpNd87ZEFrX/mj948lhxaPuY9o+9KejewFdgEXJVk05xpW4GN3cd24OY549cDvpmKJE1An0s3FwHTVfVkVb0A3AlsmzNnG3B7DdwPrEpyNkCStcBlwC1jzC1J6qlP0a8BDg9tz3T7+s75JPBh4JfHe5Ak25McSHJgdna2RyxJUh99rtFnxL7qMyfJ5cCzVfVQkj883oNU1V5gL8DU1NTc40vSkrd7x30LWr9zz8VjSvLr+hT9DLBuaHst8HTPOe8CrkhyKfBy4Mwkn6uqd594ZEmnovNvO39B6x+75rExJVl++ly6eRDYmGRDktOAK4F9c+bsA67OwGbguao6UlUfqaq1VbW+W3efJS9JJ9e8Z/RVdTTJdcA9DF5eeWtVHUyyoxvfA+xn8NLKaQYvr7x28SJLkl6KXq+jr6r9DMp8eN+eoc8L2DnPMb4OfP0lJ5QkLYh3xkpS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxvYo+yZYkTySZTrJrxHiS3NSNP5rkwm7/uiRfS3IoycEk14/7CUiSjm/eok+yAtgNbAU2AVcl2TRn2lZgY/exHbi5238U+GBVnQdsBnaOWCtJWkR9zugvAqar6smqegG4E9g2Z8424PYauB9YleTsqjpSVQ8DVNXPgEPAmjHmlyTNo0/RrwEOD23P8OKynndOkvXABcADox4kyfYkB5IcmJ2d7RFLktRHn6LPiH31UuYkeSXwBeD9VfXTUQ9SVXuraqqqplavXt0jliSpjz5FPwOsG9peCzzdd06SlzEo+Tuq6u4TjypJOhF9iv5BYGOSDUlOA64E9s2Zsw+4unv1zWbguao6kiTAp4FDVfWJsSaXJPWycr4JVXU0yXXAPcAK4NaqOphkRze+B9gPXApMA88D13bL3wa8B3gsySPdvr+uqv1jfRaSpGOat+gBumLeP2ffnqHPC9g5Yt03GH39XpJ0knhnrCQ1zqKXpMZZ9JLUOItekhpn0UtS4yx6SWqcRS9JjbPoJalxFr0kNc6il6TGWfSS1DiLXpIaZ9FLUuMseklqnEUvSY2z6CWpcRa9JDXOopekxln0ktQ4i16SGmfRS1LjLHpJapxFL0mNs+glqXEWvSQ1zqKXpMZZ9JLUOItekhrXq+iTbEnyRJLpJLtGjCfJTd34o0ku7LtWkrS45i36JCuA3cBWYBNwVZJNc6ZtBTZ2H9uBm1/CWknSIupzRn8RMF1VT1bVC8CdwLY5c7YBt9fA/cCqJGf3XCtJWkSpquNPSN4FbKmqv+y23wP8flVdNzTni8CNVfWNbvte4K+A9fOtHTrGdgY/DQCcAzyxsKd2TGcBP1qkY4+LGcdjOWSE5ZHTjOOxmBl/t6pWjxpY2WNxRuyb+93hWHP6rB3srNoL7O2RZ0GSHKiqqcV+nIUw43gsh4ywPHKacTwmlbFP0c8A64a21wJP95xzWo+1kqRF1Oca/YPAxiQbkpwGXAnsmzNnH3B19+qbzcBzVXWk51pJ0iKa94y+qo4muQ64B1gB3FpVB5Ps6Mb3APuBS4Fp4Hng2uOtXZRn0t+iXx4aAzOOx3LICMsjpxnHYyIZ5/3HWEnS8uadsZLUOItekhp3yhT9cngrhiTrknwtyaEkB5NcP+lMx5JkRZJvdfdQLDlJViW5K8nj3e/nWyadaa4kH+j+nL+T5PNJXr4EMt2a5Nkk3xna99tJvprke92vr5pkxi7TqJx/1/15P5rkX5KsmmDEkRmHxj6UpJKcdTKynBJFv4zeiuEo8MGqOg/YDOxcojkBrgcOTTrEcXwK+HJVnQu8iSWWNcka4H3AVFW9kcGLFa6cbCoAPgtsmbNvF3BvVW0E7u22J+2zvDjnV4E3VtXvAf8JfORkh5rjs7w4I0nWAe8EfniygpwSRc8yeSuGqjpSVQ93n/+MQTmtmWyqF0uyFrgMuGXSWUZJcibwduDTAFX1QlX9ZKKhRlsJvCLJSuB0lsA9JlX178CP5+zeBtzWfX4b8McnM9Moo3JW1Veq6mi3eT+D+3Ym5hi/lwD/AHyYY9w8uhhOlaJfAxwe2p5hCRbosCTrgQuAByYcZZRPMviL+ssJ5ziW1wOzwGe6y0u3JDlj0qGGVdVTwMcZnNUdYXDvyVcmm+qYfqe7L4bu11dPOE8ffw7826RDzJXkCuCpqvr2yXzcU6Xoe78Vw1KQ5JXAF4D3V9VPJ51nWJLLgWer6qFJZzmOlcCFwM1VdQHwc5bG5Yb/113n3gZsAF4LnJHk3ZNN1YYkH2VwGfSOSWcZluR04KPA35zsxz5Vir7P2zgsCUlexqDk76iquyedZ4S3AVck+QGDS2AXJ/ncZCO9yAwwU1W/+mnoLgbFv5S8A/h+Vc1W1S+Au4G3TjjTsfxP9260dL8+O+E8x5TkGuBy4E9r6d0k9AYG39i/3X39rAUeTvKaxX7gU6Xol8VbMSQJg+vKh6rqE5POM0pVfaSq1lbVega/j/dV1ZI6E62qZ4DDSc7pdl0CfHeCkUb5IbA5yendn/slLLF/MB6yD7im+/wa4F8nmOWYkmxh8K65V1TV85POM1dVPVZVr66q9d3XzwxwYff3dVGdEkXf/QPNr96K4RDwz0vgrRhGeRvwHgZnyY90H5dOOtQy9V7gjiSPAm8G/naycX5d99PGXcDDwGMMvhYnfgt/ks8D3wTOSTKT5C+AG4F3Jvkeg1eL3DjJjHDMnP8I/Cbw1e5rZ88SzDiZLEvvpxtJ0jidEmf0knQqs+glqXEWvSQ1zqKXpMZZ9JLUOItekhpn0UtS4/4PStaNoI43w/kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(20):\n",
    "    for i, item in enumerate(sita_hats[j][0]):\n",
    "        # print(i, item.item())\n",
    "        plt.bar(i, item.item())\n",
    "        # plt.savefig('' + str(i) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1396, 0.0239, 0.0263,  ..., 0.0221, 0.1890, 0.0608],\n",
       "        [0.0582, 0.0133, 0.0250,  ..., 0.0225, 0.2440, 0.0666],\n",
       "        [0.0729, 0.0186, 0.0256,  ..., 0.0234, 0.2535, 0.0801],\n",
       "        ...,\n",
       "        [0.0858, 0.0352, 0.0254,  ..., 0.0125, 0.2291, 0.0738],\n",
       "        [0.1421, 0.0133, 0.0249,  ..., 0.0042, 0.3370, 0.0403],\n",
       "        [0.1147, 0.0223, 0.0327,  ..., 0.0232, 0.1612, 0.0616]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "z = torch.softmax(z_train, dim=1).detach()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "loss = nn.MSELoss()\n",
    "# input = torch.randn(1, 5, requires_grad=True)\n",
    "input = torch.tensor([1.0, 2.0, 3.0])\n",
    "target = torch.tensor([1.0, 2.0, 3.0])\n",
    "# target = torch.tensor([2, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d5a93703cc22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "target = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "input = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "loss = nn.MSELoss()\n",
    "loss(input, target)\n",
    "# a = torch.cat((target, input), 0)\n",
    "# torch.softmax(a, dim=1)\n",
    "# target.add_(input)\n",
    "# target + input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/08dvs7wn6t5_jn5sxxsly14c0000gn/T/ipykernel_6248/503755249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "target = torch.randn(1,1000)\n",
    "nn.MSELoss(target, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10635299, 0.12664911, 0.04766772, 0.02757354, 0.05697789,\n",
       "       0.05699783, 0.03527545, 0.11439677, 0.07075627, 0.04713076,\n",
       "       0.05768883, 0.12192362, 0.02987717, 0.03248456, 0.06824748])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_z = []\n",
    "for i in range(topic_num):\n",
    "    term_z.append(sum(z[:,i]))\n",
    "term_z = np.array(term_z)\n",
    "# z = torch.softmax(z, dim=1).detach().cpu().numpy()\n",
    "# z_a\n",
    "term_z/sum(term_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11836473, 0.13411619, 0.0479399 , 0.        , 0.06292735,\n",
       "       0.06392176, 0.02147614, 0.12511938, 0.0829907 , 0.04677167,\n",
       "       0.06448372, 0.13043394, 0.00752335, 0.01465324, 0.07927793])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#正規化のクラスを準備\n",
    "ms = MinMaxScaler()\n",
    " \n",
    "#特徴量の最大値と最小値を計算し変換\n",
    "ms.fit_transform(term_z.T).T[0]/sum(ms.fit_transform(term_z.T).T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\err09\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\err09\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:162: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n",
      "posx and posy should be finite values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_graph.grid_graph(z_train, topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\err09\\OneDrive\\Project\\Study\\01_topicmodel\\FLmodel_NTM\\trainer.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.data = np.array(data)\n",
      "c:\\Users\\err09\\OneDrive\\Project\\Study\\01_topicmodel\\FLmodel_NTM\\trainer.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.bow_data = np.array([bow_vocab.doc2bow(s) for s in data])\n",
      "c:\\Users\\err09\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP(test) = 16427.260\n"
     ]
    }
   ],
   "source": [
    "dataloader_test  = trainer.DataLoader(test_data, bow_vocab, batch_size, shuffle=False)\n",
    "pp_test = trainer.compute_perplexity(ntm_model, dataloader_test)\n",
    "print(\"PP(test) = %.3f\" % (pp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ./topwords_e9999.txt\n",
      "Topic 0: テーブル ワイヤ スパン 駆動装置 シリンダ 走行レール 検出精度 ウインチ 位置情報 右\n",
      "Topic 1: 型枠 設 配筋 鉄筋 コンクリート 施工 工期 現場 Ｃ 径\n",
      "Topic 2: 他端 移動 一端 先端 型枠 後方 コンクリート 両端 セメント 隔壁\n",
      "Topic 3: 建物 スパン 一般 電波 コスト 平面図 付加機能 上記課題 上記実施 履歴\n",
      "Topic 4: 内面 － 特開平 下部 複数 材 両端 ～図 隔壁 型枠\n",
      "Topic 5: 上記 説明 図面 形態 技術分野 図 実施形態 課題 効果 符号\n",
      "Topic 6: 上部 ｄ 実施形態 ｃ 作業 側面図 ｂ 号公報 両側 上面\n",
      "Topic 7: 地震 架構 降伏点鋼 剛性 モーメント 溶接等 鋼材 鉛直荷重 柱 記載\n",
      "Topic 8: 同上 水底 一定間隔 ドライ空間 斜路 水上 目成形ケーソン ケーソン側壁 降下状態 ケーソン成形型枠装置\n",
      "Topic 9: － 特開平 号公報 トンネル 記載 後方 外側 地山 掘削 左右\n",
      "Topic 10: 斜視図 内面 図示 径 後方 ｄ 中央 左右方向 円筒状 前方\n",
      "Topic 11: 地震 部分 変位 ダンパー 下部 水平力 上面 揺れ 架構 構造\n",
      "Topic 12: 両端 断面図 周面 参照 他方 端部 ｃ ｂ 増強 端面\n",
      "Topic 13: 部材 力 形状 方向 一端 移動 間隔 斜視図 上下 内側\n",
      "Topic 14: 効率 程度 方向 装置 前記 範囲 面 作業 問題点 他\n",
      "[[ 7.1993372e-06 -1.6985834e-06  1.4108712e-06 ... -2.9334951e-05\n",
      "  -4.0255563e-05 -2.6012287e-05]\n",
      " [ 8.6775499e-06  9.9357158e-07 -7.0794440e-06 ...  1.0917112e-05\n",
      "  -6.3899970e-06  3.1294971e-05]\n",
      " [ 1.8935904e-06 -9.3974234e-07 -4.7839849e-06 ... -2.4753861e-05\n",
      "  -2.1822005e-05 -3.7616854e-05]\n",
      " ...\n",
      " [ 1.6902081e-06 -2.5305806e-06  3.5194039e-06 ... -6.1711791e-05\n",
      "  -3.5940255e-05 -2.2357799e-05]\n",
      " [-8.5179781e-06 -4.4525768e-06  6.6510302e-06 ...  3.5564182e-05\n",
      "   4.3075372e-05  4.3023956e-05]\n",
      " [-9.3602875e-07 -1.3951710e-05  3.7516397e-06 ... -5.1770667e-06\n",
      "   2.0776852e-05 -1.0609367e-05]]\n"
     ]
    }
   ],
   "source": [
    "logdir = \"./\"\n",
    "ntm_model.print_topic_words(bow_vocab, os.path.join(logdir, 'topwords_e%d.txt' % 9999))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== # 1, Topic : 13, p : 14.7881 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 破壊 孔 破壊電極 進行方向 衝撃波 内部 図 破壊対象 挿入物 電解液 内部形状 プラズマ破壊方法 プラズマ破壊装置 上記発明 発明 形状 反射衝撃波 すり鉢状 回転楕円体 軸方向 上記孔 断面図 上記衝撃波 実施 パルスパワー源 上記内部形状 対数螺旋形状 形態 側方 上記進行方向制御手段 孔加工手段 孔内部 構成 所望 破壊力 焦点 挿入物配置手段 先端 コンデンサバンク 同軸ケーブル エネルギー 方向 楕円体 境目 液 効率 破壊方法 ～（ｃ 電極 反射 岩石 手段 法線方向 側面 すり鉢形状 平面衝撃波 壁面 部分 ラプチャーディスク すり鉢状部分 ａ すり鉢 上記挿入物 破壊対象自体 孔底面 進行方向制御手段 上記破壊対象自体 底部 容器状 上記電解液 上記境目 上記回転楕円体 接地電極 箇所 向き 奥 層状 圧力 技術 角度 導電体 絶縁体 制御 加工 中央部 回転楕円形 特定方向 略回転楕円体 導線部分 外周部 構造 日本規格協会 説明 スイッチ 電荷 破損 他方 凸 音速 底 配置 方位 範囲 先端付近 電解液層状注入手段 装置 動作原理 図示省略 遮蔽層 放電 放電エネルギー 寿命化 円錐状 原理 干渉縞 高山和喜 破裂弁 特許請求 孔あけ 先端形状 円錐形状 上記目的 上記ラプチャーディスク 上記実施 回路 電源 ｂ ロス 上述 曲面 転換 屈折 特性 所望形状 円柱形状 特定 一定 対数螺旋流路 ｃ 筐体 中央部分 遷移図 概念図 断面構造 中心部分 周囲 ドリル 水 課題 出口 斜め 中空部分 既知 参照 フォーカス 内側 渦 文献 はなし ｐ ～ 意味 一方向 矢印 近傍 圧力増幅率 干渉 自体 円錐 衝撃圧力 問題点 発生点 技術分野 特開平 号公報 形 衝撃 周辺機器 目的 一定角度 － 概要 車体 片側 極 電位差 内面 固体 穴 例 上方 挙動 順 下方 曲線 下端 一般 集中 抑制 任意 意義 母線 種類 比重 差 角 rupture disc 内壁 空間 媒体 例示 均等 変更 効果 図面 様子 符号\n",
      "\n",
      "===== # 2, Topic : 5, p : 13.1272 %\n",
      "Topic words : 上記, 説明, 図面, 形態, 技術分野, 図, 実施形態, 課題, 効果, 符号\n",
      "Input : 吸着材 シート状捕捉吸着体 捕捉吸着体 吸着性基材 アニオン交換基 カチオン交換基 シート イオン交換基 内装材 有害ガス シート状 吸着基材 交換基 捕捉吸着 塩基性臭気 吸着性 収蔵庫 上記吸着性基材 ガス捕捉シート 展示室 捕捉性 床材 展示ケース ホルムアルデヒド等 微粒子状 吸着ガス 吸着特性 アンモニア 空調機 コンクリート ホルムアルデヒド 通気性シート 備品等 上記シート状捕捉吸着体 酸性臭気 図 ガス 発明 μｇ 有機酸等 接着等 作業性 コンクリート建材 吸着基材自体 材 上記吸着材 請求項 布シート カチオン交換基スルホン酸基 アンモニア等 美術館 コンクリート壁 塩基性 前記吸着材 シート状物 上記カチオン交換基 吸着性能 吸着能 アニオン交換基強塩基性 有機酸 例 混合物 ポリエステル繊維製 通気性 活性炭 内装材等 方法 リン酸基等 カルボキシル基 アンモニウム基 無機質多孔性体 実施例 断面図 汎用性 可能性 美術品 手段 m 接着材 有機質多孔性体等 化学吸着特性 構造 種類 上記 棚 コンクリート面 重合性単量体 ゼオライト 形態 物理的吸着 高分子多孔体等 館 粒状 シリカゲル パネル 給気口 室内 空気 実施 布 固着手段 合成樹脂シート ガス物質 特定ガス 取り付けスペース 下げ状態 スチレンスルホン酸 布等 陳列室等 展示空間 空調 説明 博物館 絵画 一対 単独 種々 問題等 アミン等 場所等 シリカゲル等 棚等 美術館等 課題 複数 箇所 アクリル酸グリシジル 空間 床 接着 備品 内装パネル アンモニア発生量 収納庫 上記方法 混合系 ホルムアルデヒド用 室内設置型空気清浄機 空調装置 空調運転 ボード面 アンモニア用 変質 号 新建材 コスト 上記課題 記載 図面 他方 e ｈ 発生 特性 室内空間側 所定混合比 分解斜視図 内側面 状態 下げ 取り付け グラフト重合法 室内換気 コスト高 吸湿ボード 生産コスト 固定手段 作業工程 劣化防止対策 装着方法 具体的例 技術分野 技術 館建物 ケミカルフィルター 除去 除去機能 休館日 建物 使用開始 相当日数 大型化 対策 特開平 ぶん構造 生産 物質 アミン 電離線 所定 融着 任意箇所 流路 漏れ出 複数種類 粒状活性炭 ブレンド割合 必要サイズ 場所 保護 館内 通常 夜間 竣工 音 特許 － 前者 後者 効率 目的 マイナス 原因 施工 特徴 パルプ 公知 紫外線 プラズマ ラジカル 材質 紙 形状 二つ 別 内面 a 埋設 b c 内部 d 材料 一端 f g 浄化 敷設 かなり 低減 片面 mm 厚み 層 ℃ ローラ 上下 処理 酢酸 効果 形成 分離 能力 符号\n",
      "\n",
      "===== # 3, Topic : 13, p : 36.0295 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : パイプ 敷設方法 方法 カウンター 水底 材 水底パイプ 敷設船 弾性材料 パイプa 図 敷設 発明 鋼棒 重量 敷設船上 材料 可能性 クランプｂ 敷設パイプ 説明図 陸上用 状態 外周 敷設船ｄ 比重 軸方向 溝 陸上 水底敷設パイプ 周囲 接着剤 相対移動 円弧板 合成樹脂製 軽量 力 クランプｂ群 構造 水 作業 市販パイプ 樹脂系パイプ ポリエチレンパイプ パイプドラム 説明 延長方向 円周方向 信頼性 一般 環状 テンションワイヤｃ ヒンジ部 管状体 内側 帯 外力 敷設方法特徴 繊維 連結構造 重量不足 弾性 軽量パイプa 実施例 ヒンジ 取り付け 球体継手 取付け方法 敷設場所 敷設状態 合成樹脂繊維 繊維製 鋼製 直径 次 イ ロ ハ 目的 上記 間隔 相互 ピッチ バンド 方向 工場 効果 帯鋼 弾性材 繊維バンド 天然繊維 化学繊維 炭素繊維 弾性部材 用 塑性材料 水底パイプライン ポリエチレン製 歯部 装備 課題 波浪 影響 図面 反対 程度 耐力 流れ ～ ニ 移動 位置 連結 市販 取付け 櫛 機能 変位 扱い 各種 破損 塩化ビニール製 環状体 筒体 端部 帯状体 底部 材ドラム 断面図 実施 歯状 歯 延長 不足 カウンター群 一般市販 重量物 取付けピッチ 連結楔 鉄帯 取付け順序 取り付けかた 溝状 取り付け数 技術分野 技術 特徴 問題点 手段 複数 鉄 部材 ボルトナット ボルト ナット 外周面 重り 分散重り 帯状 凹凸模様 固定手段 複数本 外向き 挟持 固定 案内ローラー 経済効果 抵抗力 工場生産 水深 海底 大型 取り扱い 現場 距離 陸 構成 状況 設備 複数個 態様 需要 種類 自重 通常 使用 鉛 一定 個 波 剛性 他 両側 寸法 形状 介在 ずれ ゴム 関係 両者 意味 ホ ロープ 製品 ポリエステル ナイロン ヘ 所定 結合 嵌め込み リベット 公知 等間隔 閉合部 カシメ 応力 ト 外側 外部 甲板 チ 強度 姿勢 条件\n",
      "\n",
      "===== # 4, Topic : 13, p : 30.4343 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 床面 空気 人工芝 支持体 空気通路 供給 敷設構造 供給口 図 床 空気溜り 説明図 実施例 空気供給部 構造 発明 空気供給口 空気供給路 競技場 蓋 単位面積当たり重量 状態 溝 コンクリート製 が床面 空気供給構造 装置 説明 供給路 取りドラム ロープ 敷設方法 重量 供給パイプ 供給位置 目的 ドラム 敷設 方法 地下室 方向 水平方向 木製 浮上 次 イ ロ しわ 上向き 全面 力 空気溜りａ 空気流路 平面 直方体 半球体 実施 例 板体 点状 敷設状態 競技 半球状 取り作業 巻き取りドラム 展示場 送風機 サッカー 芝生 下 排気口 課題 山状 労力 空気圧 図面 上面 ハ 線状 他端 縁 枠 角柱状 突起 範囲 圧力 下面 茶碗状 目状 効果 断面図 合成樹脂製 送風機程度 人工芝重量 端部 地下 問題点 展示会場 技術分野 技術 作業 複数本 平面性 パイプ 収納装置 茶碗 群 直方体群 摩擦抵抗 位置 コンサートホール 床下 展開 多量 付近 前面 構成 ローラー 手数 あと 整然 手段 上記 特徴 態様 各種 球技 コンサート 客席 アメリカンフットボール 上部 取り付け 間隔 幅 影響 利用 障害 ニ 一端 外部 コンプレッサー 圧 ホ 回転 ヘ ト 介在 形状 チ 基礎 シート 芝 球体 リ 碁盤 ヌ 桟 ル 荷重 一定 微量\n",
      "\n",
      "===== # 5, Topic : 5, p : 17.2528 %\n",
      "Topic words : 上記, 説明, 図面, 形態, 技術分野, 図, 実施形態, 課題, 効果, 符号\n",
      "Input : 監視領域面 変動量 上記監視領域面 のり面 変動 デジタル画像 領域 座標データ 図 投影画像データ 上記 状態 画像 監視範囲 領域面 碁盤目 崩壊兆候 Ａ デジタルカメラ 上記変動量 碁盤目状 変動量出力装置 投影画像 撮影デジタル画像 面 監視対象 対象斜面画像 形態 コンター図 データ 監視業務 実施 立体画像 等高線 ステレオ撮影 計測器 位置 写真撮影 画像データ 兆候 面的単位 的メッシュ面 線 のり面崩壊 目 撮影範囲 発明 装置 標点 パソコン 表層崩壊 ｘ 不動点 挙動監視 挙動 ｙ 変動図 ステレオデジタル画像 写真地形図 崩壊評価 座標 測量 基礎地盤崩壊 各々 判断材料 立体写真測量 上記パソコン 連続面 出力 変動挙動 格子図 部分 縦横 ｚ 隆起変動 陥没変動 代表変動 写真 説明 表層情報 上記コンター図 交点 ｃ 地点 実線 固定条件 撮影条件 撮影位置 細分化 テクスチャーマッピング処理 層崩壊 撮影箇所 撮影台 撮影枚数 杭 ワイヤ 所定 経過 色 基準 所 事前 日付データ 技術 箇所 ソフトウエア 陥没 隆起 出力形態 指示点 断面図 線図 等高線図 説明図 固定点 概略構成図 偏位修正座標系 不動点用標点 測量等 方法 上述 伸縮 課題 局所局所 関係 直線状 ２つ 鎖線 点線 視覚 条件 コスト 位置情報 状況 リアル 間隔指示等 ＸＹＺ目盛り Ｚ目盛り 縦方向 横方向 技術分野 可能性 手段 初期状態 制御手段 解析ソフトウエア ）－（ｘ 設置工事 設置 自由度 目視 踏査 黒丸 印 個々 当該 図上 任意 概要 パーソナルコンピュータ 周知 被写体 人 空間 原理 複数 役割 種類 例 ｂ ａ 値 システム 次 三脚 仮 動き 入力 手間 メンテナンス 場所 設定 変更 数値 Ｘ 効果 図面 イメージ図 符号\n",
      "\n",
      "===== # 6, Topic : 5, p : 14.8859 %\n",
      "Topic words : 上記, 説明, 図面, 形態, 技術分野, 図, 実施形態, 課題, 効果, 符号\n",
      "Input : 作動油 油圧室 油路 リリーフ弁 油圧室Ｒ 油路Ｐ 制震用油圧ダンパ チェック弁 制御弁 作動状態 開閉制御弁 油圧 前記油路 減衰力 作動 該制震用油圧ダンパ 弁体 Ｐ オイルタンク アキュムレータ 設定値 ピストン 通路Ｐ 主弁 ダンパ性能 実施形態 圧力 通路 開閉作動 前記 Ｒ 実施 パイロット弁 発明 作動領域 上記実施形態 図 請求項 ピストンロッド リリーフ作動 周囲温度 流れ 油圧上昇 油圧回路 一対 領域 ダンパ特性 油圧低下 制震対象物 前記リリーフ弁 シリンダ 往復動 ピストン速度 油温 状態 前記チェック弁 揺れ 圧 ｃ 電気制御装置 建物 風等 圧力変動 移動速度 弁座 振動外力 要因 図示省略 油圧回路図 減衰作用 減衰効果 減衰係数 体積 速度 上昇等 流動 ｂ 構成 前記アキュムレータ 圧力低下 上記設定値 直列 並列 他方 振動 流量 開閉 特開平 号公報 公報 程度 形態 ＭＰａ程度 接続部位Ｐ 説明 地震 振幅 周期 絞り ピストン移動量 左上接続部位Ｐ 移動量 軸方向移動 上記 最大値Ｃｍａｘ 最小値Ｃｍｉｎ 変化等 断面図 軸方向 両側 － 変化 課題 機能 全量 所期 図面 基盤 向き ｄ 通常 内部 圧力センサ 所定速度 拡大断面図 作用 効果 体積変化 下流側 上流側 技術分野 技術 連通 最大 最小 検出信号 略一定 調圧弁 種 一つ 悪影響 影響 手段 特徴 右 遮断 閉型 方法 ストロークセンサ ～ よう スプリング 使用 個数 下方 符号\n",
      "\n",
      "===== # 7, Topic : 13, p : 32.8914 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 免震装置 上部荷重支持支柱 既設柱 上部荷重 増打コンクリート 柱 既存柱 免震化工法 図 コンクリート 免震構造化 荷重 既存建物 免震化 梁 免震構造化工法 増打 支持支柱 柱巻き鋼板 スラブ ジャッキ フラットジャッキ 増打部 切断 配筋 上記上部荷重支持支柱 構造 支持杭 上部 部分 軸力 上部構造体 工程 免震装置付き既設柱 既設 施工 実施 形態 フープ筋 免震装置取り付け 上部ベースプレート 免震構造化技術 柱巻き補強鋼板 架台 コンクリート増打部 支持体 増打部分 発明 中間部 鉄骨柱 せん断補強筋 隅部 軸径 免震装置上部架台 上部構造 型枠 状態 上部架台 下部 収縮モルタル 切断個所 座屈 支柱 下部ベースプレート フラットジャッキ等 作業 建物 コンクリート構造 下端部 地中梁 方式 免震装置設置用 鋼鈑 コア抜き 出隅部 免震装置上部架台配筋用 下端 コスト 耐火被覆 参照)、免震装置 切断面 補強用 補強 壁 補強筋 梁下端 梁接合部 切断位置 外周面 盛り替え 上下部分 下方 形態図 柱切断位置 施工効率 コスト低減 免震化工法 所定 ａ 面 工程図 上記実施 Ａ 切断片 長期軸力 設置 基礎部 上下 独立柱 柱外面 当該柱 切断部 上階 割フープ ジャッキ等 ｂ 軽量化 耐火被覆材 強度 位置 隅 圧入工法 既設スラブ 露出部分 断面図 アンカー 耐火処理 フープ筋用 サポート 段階 上方 外壁等 作業性 矢視 耐火処置 切断作業 切断面等 ジャッキレス工法 モルタル 杭 鋼板 取付位置 切り取り位置 範囲 撤去 ダイヤモンドワイヤソー 鉄筋 低減 施工工程図 架台施工図 ジャッキアップ 施工コスト 縦筋 下端部分 上記ジャッキ方式 水平切断面 設置部分 モルタル注入図 外周部 収縮モルタル等 特開平 号公報 等 問題点 回転スペース 樹脂アンカー 適用 外壁 強度スパイラルフープ筋 露出部 基礎梁 中間部分 切断器具 取り付け図 下方部 設置完了図 説明 工期 角柱 一体 図示 プレート 設 外部 向上 矢視図 準備 グラウトモルタル 狭隘部 表面 手段 鋼管 上記 準備工程図 耐震補強対策 下方部分 切除部分 鋼板厚 技術 取り付け 注入 溶接 配置図 設定図 施工手順 コスト上昇 上下スラブ 製造コスト 開口部分 目溶接等 外周 キャンバー設置 手段等 上方向 手押し水圧ポンプ等 現場作業 挿入 周囲 アンカージベル スタッドジベル 機能 重量 運搬 組み立て 人手 課題 距離 取り扱い 図面 本数 支障 階 上述 押圧力 かご 通常 他 効果 調整用 技術分野 溶接付け グラウトモルタル注入 水平方向 スラブ下 機械式継手方式 処置 長期 板 水平 切り取り グラウト仕切り板 排水処理 参照)。 参照)。【 必要性 鉛直方向 せき板 切除量 縦主筋 適用例 適用範囲 山止め壁 除去費用 例 表面仕上げ ＣＵＴ線 主筋 目 費用 仕上げ 角形鋼管 天端 開放状態 分割撤去 －（Ａ 提案 最初 地盤 構築 手間 相応 耐力 スタットジベル ダイヤモンドチェーンカッター 硬化 役目 設開口 防火 状況 改善 全長 目的 後述 ケース 間隔 内部 同数 所望 移動 上側 順 方法 単独 間隙 鉄筋コンクリート ースプレート レベル 空隙 結合 一体化 手法 操作 部材 種類 趣旨 種々 変更 心配 削減 符号\n",
      "\n",
      "===== # 8, Topic : 13, p : 26.7727 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 免震装置 上部荷重支持支柱 上部荷重 増打コンクリート 柱 既設柱 既存柱 荷重 免震構造化 既存建物 ジャッキ 免震化工法 図 上部 コンクリート 免震構造化工法 構造 免震化 梁 配筋 柱巻き鋼板 支持支柱 架台 上部構造体 工程 支持杭 免震装置取り付け 軸力 施工 免震構造化技術 免震装置付き既設柱 上部ベースプレート 支保用ジャッキ 部分 既設 柱巻き補強鋼板 隅部 コンクリート増打部 支持体 増打部 ＰＣ鋼線 スラブ 発明 鋼鈑 増打部分 せん断補強筋 上部構造 切断 増打 軸径 鉄骨柱 支柱 作業 免震装置上部架台 フープ筋 コンクリート構造 方式 地中梁 上記上部荷重支持支柱 状態 座屈 下部ベースプレート 免震装置設置用 免震機構 実施 形態 ジャッキ上部 下部 免震装置上部架台配筋用 免震装置取り付け部位 出隅部 免震装置取付位置 型枠 長期軸力 コスト 部材 上記免震装置上部架台配筋用 上部架台 建物 補強用 外周面 補強 上記既存柱 補強筋 荷重計 壁 梁接合部 収縮モルタル 工法 ｂ 耐火被覆 架台コンクリート 取り付け 盛り替え 設置完了図 基礎部 当該既存柱 切断位置 免震化工法 上下部分 ａ 所定 面 柱切断位置 プレストレス 矢視 ジャッキアップ 既設建物 基礎部材 鉄筋 切断個所 コンクリート用 上下 独立柱 柱外面 切断片 外周 切断部 収縮コンクリート 軽量化 特開平 問題点 段階 コア抜き 上階 隅 取り付け部位 割フープ 強度 下端 圧入工法 手押し水圧ポンプ等 平面図 形態図 施工効率 アンカー コスト低減 フープ筋用 外壁等 サポート 上方 切断面 設置 梁等 既設スラブ ＰＣ鋼線等 外周部 切断面等 杭 鋼板 耐火処置 作業性 切り取り位置 架台施工図 取り付け図 上記実施 準備 Ａ Ｂ ジャッキレス工法 縦筋 工程図 施工図 施工工程図 設置部分 最初 長期 参照 表面 ダイヤモンドワイヤソー 上述 基礎梁 円周方向 施工コスト モルタル 等 水平切断面 作業完了図 負担軸力 上下部外周 モルタル注入図 コンクリート充填鋼管 号公報 切り取り 外壁 低減 強度スパイラルフープ筋 ジャッキダウン 上記ジャッキ方式 キャンバー設置等 矢視図 中間部 狭隘部 説明 工期 発生 一体 メンテナンス 撤去 プレート 設 収縮モルタル注入 キャンバー設置 梁下端 配置工程図 グラウトモルタル 鋼管 端面 範囲 注入 位置 適用 コストアップ 耐震補強対策 取付位置 鋼板厚 技術 ＣＵＴ線 溶接 断面図 設定図 露出部分 施工手順 準備工程 目溶接等 コスト上昇 製造コスト 耐火被覆材 プレストレス機構 ひび割れ等 鉄筋等 手段等 圧縮力 躯体表面 耐火処理 調整ピース等 開口部分 間隙部分 切除部分 水平方向 挿入 周囲 例 構築 アンカージベル スタッドジベル グラウトモルタル注入 重量 運搬 組み立て 人手 角柱 部位 課題 取り扱い 図面 図示 本数 水平 かご 通常 向上 点検 現場作業 効果 技術分野 溶接付け 露出個所 鉛直方向 維持部材 機械式継手方式 処置 板 号 処理 グラウト仕切り板 表面仕上げ 水平度 天端 スラブ下 排水処理 せき板 ケミカルアンカー 縦主筋 樹脂アンカー －（Ａ －（Ｂ 山止め壁 除去費用 主筋 目 費用 手段 仕上げ 回転スペース 負担 適用範囲 提案 地盤 手間 相応 耐力 スタットジベル ダイヤモンドチェーンカッター 硬化 機能 役目 設開口 防火 恐れ － 概要 施 特性 所望 状況 改善 最小限 距離 最小 全長 目的 後述 間隔 体勢 同数 上側 順 過程 稼動 押圧力 単独 ースプレート レベル 空隙 結合 一体化 手法 余分 外部 新規 交換 種類 趣旨 種々 変更 心配 削減 符号\n",
      "\n",
      "===== # 9, Topic : 9, p : 14.9461 %\n",
      "Topic words : －, 特開平, 号公報, トンネル, 記載, 後方, 外側, 地山, 掘削, 左右\n",
      "Input : グラウト材 セメント 隙間部 ～ 流動性 高性能ＡＥ減水剤 水 コンクリート隙間部 膨張剤 砂セメント比 砂 減水剤 コンクリート 水セメント比 発明 混和材 膨張率 圧縮強度 試験 Ｃ サンプルＡ～Ｃ 強度 ペースト状 セメント比 収縮グラウト材 Ａ 収縮性 混和剤 隙間 充填 幅 強度増進混和材等 ＡＥ減水剤 性能 混練物 高速 材料 膨張剤添加量 程度 サンプル 試験体 構造物 最大粒径 方法 充填性 施工性 継ぎ部 下部コンクリート 亀裂部等 混和材等 Ｎ 表 目的 添加 強セメント 上限値 混和材料 製造コスト 強度増進剤 Ｓ 割合 珪砂 混練工 ミキサ 重量比 前記水セメント比 所定 セメント水和物 収縮 セメントベース 減水性能 材齢 市販 地震等 砂結合材比 サンプルＢ ペースト温度 ガス発生性膨張剤 下限値 低速 製造方法 前記 範囲 次 硬化 ｍｍ コンクリート構造物 作業性 経済性 ＡＥ剤 発泡タイプ 高速回転 膨張反応 空隙部 発明者 構造耐力上一体性 膨張率損失 粒径 膨張効果 品質安定性 空気連行性 Ｗ 効率 上部コンクリート 強度低下 性能試験 前記試験 モルタル等 添加量 配合比 補修用注入剤 収縮モルタル等 低下 品質 表面水 効果 ガス サンプルＣ フロー試験 試験室 分離抵抗性確保 補修等 量 粒度分布 早強ポルトランドセメント等 悪化等 特殊アルミ粉末 フロー値 配合量 Ｊロート 下方 工法 上下 所要 一般 課題 特徴 実施 形態 反応遅延タイプ ３つ 通り 当たり 性状 mm 特殊アルミ粉 機械基礎下部 所定量 普通ポルトランドセメント 付着物 Ｊロート流下 最大荷重 配合 温度 既存材料 粉末状態 ＵＢ 低速回転 前記表 プレミックス製品 スランプ保持特性 ポリカルボン酸系 サンフロー株式会社製 技術分野 技術 上部 橋脚支承部 特開平 号公報 ブリージング防止 商品セルメック 号 セルメック ｋＮ 説明 沈下 一体化 各種 現場 所望 － 出願 相当 事情 手段 構成 工程 面 良質 両方 ＪＩＳ 参照 主成分 アルカリ性 アルミニウム マグネシウム 亜鉛 早期 ｍ 混練 エージング ％～ 悪影響 混練後 壁面 合計 サイズ 耐荷重\n",
      "\n",
      "===== # 10, Topic : 13, p : 28.9964 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 分別土材 粘性土 枠 ワイヤー 図 分級用スクリーン バー材 径 土 粒径 機 上部 先端側 分別 弦振動 開口率 発明 間隔 小粒径 土材 傾斜 塊 スクリーン 先端 枠上部 mm 落下衝撃 振動 枠下部 バイブレータ 傾斜角度α 用 土塊 斜視図 上部構造 mmφ マサ土 土建現場 目 請求項 接触面積 分別用 要部構造 下部 作業員 形態 粘性土土塊 上部側 分級粒径 小粒径土砂 側面図 丸棒 一定間隔 配置間隔 説明 課題 固着 分別用土材 下部側 縦長長方形 付着性 実施 張設構造 実施形態 偏心カム せん断力 作業能率 要部 上部側下面 断面径 種 外周 前記 テンション ～ ｂ 鋼材 要部断面図 板 分別対象 上端側 丸鋼棒 粒径別 構造 投入作業 落し作業 作業中断 mm間隔 作業能率低下要因 大粒径 棒 角鋼棒 機先端 Ｈ型鋼 資源化 バックホウ 隙間 構成 直径 目安 手作業 手段 目的 所定 プレート 数値 繰返し振動 傾斜方向 資源化材料 改良構造 材料 一定 長手方向 張力材料 モータ軸 モータ回転 技術分野 技術 原石山 堆積表土 建設残土 架橋現象 添付図面 受け板 下面 横設鋼材 突出端 想像線 図面 花崗岩 砂利 コンクリートガラ 土砂 機械 同士 周囲 逆 剛体 特徴 自体 柱 ショベル 下端 背面 ナット 上下 ワーヤー 各種 強度 実機 バウンド 前述 形状 通常 懸念 効果 クリーン 符号\n",
      "\n",
      "===== # 11, Topic : 13, p : 16.5901 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 孔 パイロット孔 薬液注入管 地盤 薬液 ドリルヘッド 外管 ボーリング機 改良対象地盤域 バックリーマー パッカー装着薬液注入管 大型建造物 上記 性 薬液注入孔 孔径 建造物 細孔 該薬液注入管 該パイロット孔 注入 滑剤 パッカー 工法 パッカー付き薬液注入管 ボーリング機側 改良工法 上記ドリルヘッド 孔距離 薬液注入管収容外管 薬液注入管収納外管 上記ボーリング機 薬液注入操作 改良対象地盤 対象建造物 障害物 ロッド 発明 改良 上記パイロット孔 図 先端 該ドリルヘッド 管 性薬液注入管 孔操作 孔ボーリング機 上記外管 立坑 対象 上記バックリーマー 上記薬液注入管 水平ボーリング機 該バックリーマー 操作 孔腔内 孔態様 ボーリング機本体側 直下 外管内 外管引き抜き操作 傾斜注入工法 開削式ボーリング機 地盤改良 開削工法 向き 該障害物 水平ボーリング 斜め 該パッカー装着薬液注入管 排土形式 先端部 アクリル酸系 水平注入 上記構造物 形成 吸水性ポリマー 無水マレイン酸系ポリマー 安全性 開削改良工法 上記建造物 薬液注入管配設用 塩化ビニル樹脂製 該建造物 合成高分子ポリマー 上記ボーリング機本体 既述 連結用 ドリルヘッド先端 最小曲率半径 出口側 天然高分子ポリマー 推進 孔開始用 合成樹脂 軸線 孔腔 孔ピッチ 孔工程 孔効率 孔道 孔精度 上記ボーリング機本体側 該改良対象地盤域 孔形成操作 斜めボーリング 位置 斜め下方 ドリルヘッド位置 該薬液 出口 既存建造物 土圧 自由度 吸水性ポリマー系滑剤 形成操作 構造物 浸透 目的 薬液圧入 上記立坑 該改良対象地盤域内 滑剤含有泥水 滑剤供給車 エステル系ポリマー ベントナイト系 アクリル系 吸水性ポリマー系等 先端部分 ヒアルロン酸系 施工 m ベントナイト系滑剤 澱粉系 セルロース系 糖類系 ポリビニルアルコール系 性ロッド 無水マレイン酸系ポリマー)、ポリエーテル系 孔予定軸線上 斜め上方 周囲 側壁 地表 入口 縮合系ポリマー 薬液調製装置 課題 通常 面 該外管 遊端 参照 概略図 パッカー保護用 該構造物 上記連結用ロッド 取り付け用 必要性 可能性 連結用ロッド 大型 発信器 実施例 作業員 態様 蛋白質系 該建造物直下 付加ポリマー 合成樹脂製 ベントナイト系滑材 説明 該立坑 時点 該本孔 水 mm ケーブル 引き抜き 破砕物状 取り付け用リング 該ロッド 精度 構築 近傍 周壁 状態 使用安全性等 周辺部 周壁部 入口部 該掘削部 掘削部 処理工法 発明工法 形式 中心部直下 切断操作 出口部分 所期 コスト 部位 個々 土砂 量 部分 箇所 布製 図面 固化 下方 後端 取り付け位置 取り付けリング 吸水ゲル 左右方向 上下左右 圧壊強度 斜視図 平面図 実施 等 上下方向 リング 構築コスト コスト高 一軸圧縮強度 コービー在 ケント在 粉末状 技術分野 既存 技術 周囲近傍 相当程度 効率 内部側壁 英国ノザンプトンシャー 米国ワシントン 泥膜 より目詰まり 工程 一端付近 回転 閉鎖状態 程度 回転駆動 微粒シリカゾルグラウト 圧入 付近 掘削 地表下 小型 垂直 幾つ 限界 手段 所定 特徴 形態 FlowMole Ltd サウス UTILX Corporation 外側 検知 倍 該泥膜 考慮 圧密式 役目 概要 該図 他端 cm 信号 道程 球状 後述 均斉 末端 バックリーマ 一緒 汎用 グラウト q kPa 移動 効果 規模 先 基本 高まり 符号\n",
      "\n",
      "===== # 12, Topic : 6, p : 11.7688 %\n",
      "Topic words : 上部, ｄ, 実施形態, ｃ, 作業, 側面図, ｂ, 号公報, 両側, 上面\n",
      "Input : 泥液 変形追随性 流動性 材料 吸水性ポリマー 廃棄物処分場 土質材料 浚渫底泥 廃棄物水面処分場 透水性土質材料 初期支持力 水土質材料 ゲル化 発明 透水土質材料 混合物 粒径 荷重 透水係数 偏荷重 水分量 添加量 透水地盤 透水性 透水性能 ～ 製品 立地 μm 硬化剤 m 含水微粒子 セメント 初期 粘着力 土粒子 処分場 水面処分場 廃棄物 吸水性ポリマー添加 吸水性ポリマー量 沖積粘性土層 ↑－↑ sec程度 含水比 発現 グラスファイバー 発明者 程度 方向性 可能性 細粒分 地盤 強度 樹脂 状態 所要量 水面埋立処分場 砂質地盤 水面立地 水中 繊維状 繊維状物 目的 原料 性質 性状 kg 軟弱層 泥状含水微粒子 添加材料 化学的技術 技術 不同沈下 製造方法 鉄鋼スラグ 湿潤密度 含水状態 団粒化 水材料 cm 固化 課題 木材 パルプ 綿 範囲 造粒 糖類ポリマー 条件 立地地盤 資源化技術 偏荷重せん断力 良質土混合 水 使用量 水性能 技術分野 製法技術 添加資材 沖積粘性土地盤上 山間立地 立地現地盤 主体 剛性 脱水 地点 適地 攪拌 特徴 処理 鉱滓 微粉 樹皮 自体 現地 発生 発生地 物理的技術 発生起源ごと 候補地 使用条件 場所 製法 物理 周辺住民 マンナン 天然系 セメント等 水中施工 存在場所 現場試験 説明 面積 クラック 劣化 ゼリー 粘土 乾燥 石灰 粒状 反対 状況 １つ ニーズ 遠隔 手段 該泥液 ガラクトース 実施 形態 水性 浮力 気 未満 だま 土壌 同等 各々 工費 選定 コスト 難点 増大 削減 最大 配合 効果 上記 利益\n",
      "\n",
      "===== # 13, Topic : 13, p : 45.0019 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 工部材 ガイド管 実施形態 スライド管 工体 ネジ ナット部 図 伸縮継手 周方向 トンネル 継手部 ターンバックル ロック穴 端部 組立構造 前記ガイド管 部材 ロックピン ネジ穴 発明 管 Ｖ型溝 ロックネジ 周方向端部 作業 前記伸縮継手 トンネル壁面 前記 構造 先進導坑 突出 継手部分 調整 実施 天端 請求項 状態 周面 掘削作業 調整穴 軸方向 長手方向 Ｖ型突片 ガイド部 管端 伸縮継手手段 工部材継手部 形態 周方向継手部 方向 弧状 ピン 解体作業 正面図 接合端面 管端側 設置作業 トンネル内周面 伸縮継手構造 ピース部材 工 リング状 前記スライド管 端部Ｖ型溝 周方向継手手段 伸縮 前記スライド管側 組立 該ガイド管 断面 ＴＢＭ 他端側 手順 トンネル本体部分 手段 ロック 該スライド管 接合端側 端部内側 複数 セグメント 特徴 記載 角筒 側 天端側接合端 継手手段 前記実施形態 挫屈防止 説明 ～ ヒンジピン 組立手順 周方向両端面 トンネル内側 アーチ状 要部拡大図 調整作業 底部 シャフト部 掘削 要部拡大視図 組立作業 Ａ部拡大図 中空管 孤状 Ｂ部拡大視図 Ｃ部拡大視図 ロック穴間隔 他端側対向端 Ｖ型突起 左右方向 側面方向 同一部材 一つ 課題 ｂ 符号 前記先進導坑 進機 ボルト等 撤去作業 アーチ構造 周長 進作業 該ターンバックル 推進ジャッキ 構造物 永久構造 拡径状態 変形例 ネジ止め 作業性 拡幅掘削作業 先端部内側 解体撤去作業 構造そのもの アンロック 目的 他方 短縮 ａ 一端側 組立状態 左右 一端 シャフト 同一 効果 永久構造物 部分 壁面 道路トンネル 回転調整 掘削形成 組立完成状態 止係 高速掘削性 調整用 調整孔 解体 突出状態 切羽面 自由面 内壁面 接続部分 挫屈防止効果 突出先端 進性 手間 効率 両端 一対 他端 概略 アーチ 固定 内側 ）～（ 挿通 先端 突出長 伸張状態 リング分 止め係 リング形状 突出量 鋼製 係 回転 確保等 所定間隔 ピッチ間隔 接続一体化 鉄筋コンクリート製 鋼棒 回転数 伸張動作 技術分野 技術 シールド工法 形状 所定 力壁 地質調査 力 確保 縮径 添付図面 断面弧状 側面 拡径作業 円周率 拡径調整 図面 トンネルボーリングマシーン 支保工 一般 セクメントピース 領域 機能 当該 防護 拡張 拡径 操作 ターンバックル 一方向 構成 最初 上部 最後 逆 落石 裏側 鋼板 箇所 工程 実 図示 円筒 ズレ 支保 自体 任意 位置 施工 進行 工期\n",
      "\n",
      "===== # 14, Topic : 13, p : 50.8215 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 型枠 前記型枠 土留め壁 コンクリート打設用型枠 補強フレーム 接続部材 内型枠 前記補強フレーム 横アングル材 セパレータ 基端部 係合部 コンクリート 縦アングル材 前記接続部材 メッシュ型枠 前記土留め壁 先端部 請求項 連結部 管パイプ 図 フォームタイ 前記係合部 前記セパレータ ネジ 土留め 前記 前記コンクリート打設用型枠 補強フレーム固定用アングル材 型枠どうし アングル材 ナット 縦断面図 内側 接続構造 前記横アングル材 棒状部材 両端部 ナット部 前記連結部 雌ネジ 作業 地下構造物 端部 雄ネジ 発明 前記型枠どうし 該型枠 一片部 片部 軸部 前記網目 管パイプ枠体 ｂ 該接続部材 記載 土留め壁側 土留め床部 該補強フレーム 型枠支持柱 構造物 該セパレータ 側端部 ｃ 一端部 該土留め壁 支持具 係合部分 方法 鉄筋 部材 該縦アングル材 挿入孔 該内型枠 サポート 角材 側 基端面 外側 鉄筋コンクリート 実施 形態 床部 前記雄ネジ ～図 壁体 所定 縦横 尺 課題 態様 部分 前記間隔 床や壁 外側面 鋼製部材 底部 該基端部 他端部 該先端部 間隔 鉛直下方 前記鉄筋 該フォームタイ 板状部材 筒状部材 該単管パイプ 説明 設 特徴 無数 網目 フック 複数 針金 表面 裏面 位置 圧接部 効果 上記 鋼製 棒状 両端 接部分 側方 側道 手間 山形鋼 鋼矢板等 開口 該当接部分 該挿入孔 下方 該型枠内部 所定間隔 掘削内部 技術分野 技術 構築方法 作業者 上記事情 掘削 貫通穴 一つ 骨組み 圧力 状態 他方 土 通り 場所 手段 地盤 壁面 構成 現場 側面 該鉄筋 方向 ２つ ａ 図面 符号\n",
      "\n",
      "===== # 15, Topic : 13, p : 23.4859 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 補強部材 開口 強度連続繊維 鉄筋コンクリート造梁 鉄筋コンクリート造梁部材 マトリックス樹脂 部材 補強 開口補強構造 強度 梁部材 小型補強部材 図 梁主筋 大型補強部材 前記補強部材 前記高強度連続繊維 開口補強 エポキシ樹脂 前記開口 前記マトリックス樹脂 開口補強筋 補強部分 コンクリート 同心多角形 鉄筋コンクリート造 鉄筋 スターラップ ｂ 炭素繊維 リング状 °Ｃ ひび割れ発生 発明 断面形状 補強部材表面 補強部材自体 板状 人工軽量砂 横断方向 せん断強度 補強鉄筋 ひび割れ 軽量 実施 形態 断面図 せん断補強筋 ドーナツ形 終局せん断強度 同心状 性能 カブリ厚 セラミック球 アラミド繊維 砂 剛性 渦巻き状 程度 強度低下 付着性能 同心楕円 周囲 同心円 鉄筋コンクリート部材 外形図 内部 ａ 部分断面図 開口周囲 補強材 施工性 鉄筋コンクリート柱部分 エポキシ樹脂等 別 型枠 凹凸処理 形状 カブリ厚み 段状 複数 表面 終局強度 性能低下 °Ｃ程度 布状 同心円状 耐熱性能 ～ 円形 効果 状態 温度 長手方向 構成 奥行き方向 開口部 外形 径 発生 樹脂成形 空調設備ダクト 熱伝導率 同心 構成要素 四角形 棒状 空隙 穴 スリーブ 柱部分 ドーナツ状 前記砂 構造 課題 コンクリート側 熱 Ａ－Ａ断面図 前記課題 説明 量 歪み 応力 拡大 比重 鋼材 運搬 取付け 一体 発生防止 概略図 低下 耐熱性 各階 常法 付近 比 手段 作用 代わり 所定形状 軟化 改善 フック 配筋 孔 参照 内径 重ね コンクリートカブリ厚 複数段 奥行き 火災等 上記課題 底部 付加的構成 技術分野 技術 高分子材料 火災 外径 取付用 上下 建物 変化 要因 特徴 一般 近傍 上部 側面 位置 力 モーメント 負荷 図面 符号\n",
      "\n",
      "===== # 16, Topic : 5, p : 11.8566 %\n",
      "Topic words : 上記, 説明, 図面, 形態, 技術分野, 図, 実施形態, 課題, 効果, 符号\n",
      "Input : 繊維 軽量コンクリート 軽量骨材 骨材 軽量 ポリプロピレン 波形ポリプロピレン 軽量性 繊維混入軽量コンクリート 鋼繊維 Ｄ 強度 波形 ストレート形 ビニロン繊維 コンクリート 材料分離指数 軽量骨材コンクリート ベースコンクリート ポリプロピレン繊維 Ｄ系 ～ Ｄ繊維 分散性 図 容積 Ａ室 ストレート形ポリプロピレン 硬化性状 化学繊維 繊維径 フレッシュ性状 発明 強度特性 形状 高性能ＡＥ減水剤 表 耐摩耗性 配合 ピッチ幅 骨材絶乾重量 Ｄ系統 振動締固め 理想形 軽量骨材配合 天然軽量骨材 クラック 軽量骨材自身 人工軽量骨材 材料分離抵抗性試験 産軽量骨材 Ｃ 材齢 繊維混入コンクリート 細骨材 Ａ系統 耐疲労性 天然骨材 谷 ポリカルボン酸系 靭性 耐衝撃性 Ａ 山 Ｂ系統 混入 落下回数 Ｃ系 Ｂ室 親水性処理 ベース軽量コンクリート 自己支持性形状 締固め 混入率 繊維Ａ～Ｄ 繊維Ｃ Ａ系コンクリート 破壊 材料分離抵抗性 性状 材料 繊維補強コンクリート 天然細骨材 θ 幅クラック 特性 スランプ 繊維入り ストレート形ポリプロピレン繊維入り 材料分離 実施例 人工軽量 強度コンクリート 長手方向 ベース 材料分離抵抗 状態 波 セメント 親水性 Ａ～Ｄ 形状例 鋼繊維入り 硬化品 ストレート系 ビニロン繊維入り 鋼繊維自身 硬化コンクリート 貫通クラック 分散 混入量 耐久性 セメントペースト Ｂ系 アミノスルホン酸系 メラミンスルホン酸系 比重 成形 直線状 ポリカルボン酸エーテル系標準形 ℃水中養生 破壊形態 特徴 通常 課題 方向 Ｋｇ Ｌ 直線距離 加工品 平面図 円筒形 ストレート 該繊維混入 ナフタリンスルホン酸塩系 ポリカルボン酸エーテル タフネス 傾斜角 平面 外数 高性能減水剤 構造用コンクリート ≒Ｄ ピッチ 流動コンクリート 絶乾重量 モルタル 現象 前記 種類 範囲 傾向 Ｃ系統 各種強度 付着強度 圧縮強度 裂引張強度 せん断強度 一端 効果 低下 モールド 室 指数 特性値 耐疲労特性 波形状態 波形成形 単位容積重量 鋼製モールド 量 重量 親水処理 一端支持 太平洋セメント株式会社製 配合量 一定ピッチ クラック発生状況 単位容積質量 説明 次 ＪＩＳＡ 投入 実施 他端 向性 発生 径 川砂 ひび割れ 構造用部材 混和剤 補強効果 スランプ値 株式会社エヌエムビー 試験体内 コルゲート板 膨張けつ岩 膨張粘土 膨張スレート 膨張スラグ 添加量 空気量 表面処理 ペースト 加工 形態 単位水量 波板 線状 計算値 強制パン型ミキサ 商品名ポゾリスＳＰ 方形板 斜視図 普通ポルトランドセメントＳ 静弾性係数 比重差 品質等 ≒θ 他部 高低差 中心部 スランプ低下 品質改善 脆性的破壊 技術分野 技術 流動 せん断 発明者 該短繊維 他 焼成フライアッシュ 火山れき 各種 高低 Ｓ 表面 棒状バイブレータ 下式 接着力 鉄球 対数軸 浮き上がり 相違 界面 ～( 性質 面 現状 程度 さいし 手間 手段 影響 交互 外部 応力 波線 一方向 o 理由 進展 副 高め ｍ 目標 容量 等間隔 新潟 Ｇ アサノスーパライト 前者 フアイバーボール マトリックス φ 仕切 内径 あと 上層 下層 両者 比 一般 殆 参照 他方 位置 増加 向上 最大 図面 外形 関係\n",
      "\n",
      "===== # 17, Topic : 13, p : 22.4723 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 推進管 注入 薬液注入範囲 図 注入孔 薬液注入 地山 計画トンネル 薬液 トンネル 薬液注入方法 注入管 推進 セミシールド機 可動発進架台 注入方法 注入効果 発進基地 切羽 薬液注入等 注入完了 防護屋根 注入ホース 水 薬液注入工程 説明図 構築方法 孔機 範囲 作業員 孔 裏込注入 計画断面 推進施工 掘削 発明 断面図 圧入 崩落 薬液注入効果 Ａ－Ａ断面図 Ｂ－Ｂ断面図 アーチ状 注入材 浸透注入 補助工法 斜視図 説明 作業台 管復相注入 発信基地 隙間 天井部等 隣接推進管 外周部 ジャッキ 周囲 盲キャップ 概略斜視図 先端 注入不良箇所 圧入方法 内部 止水 計画範囲 推進完了 天井部分 管径 地盤 部分 位置 崩落防止 バルブ 掘削装置 地山内部 周辺地盤 天井部 推進施工終了 強度低下 請求項 濃度泥水 水等 等 鋼管 目的 実施 形態 外側 下 コンクリート ステージ 作業 防護 同士 施工 確認 箇所 効果 パイプルーフ工法等 地山中 床部 先端部 水層 パイプルーフ工法 崩落等 地下水 地滑り等 掘削部分 シラス等 天井部外側 影響 課題 地上 かなり 距離 複数 特徴 図面 手前 押輪 カッタ 変形 恐れ スライム 効果確認 掘削面 切羽側 濃度 泥水 強度 到達回収坑 上部層 軟弱地盤 無水層 逸脱防止 切羽前方 到達地点 技術分野 技術 土圧 空洞化 地表沈下 対応策 一定間隔 スプリングライン 上部 水平方向 坑口リング 緩み 間隔 千鳥配置 同士隙間 緩み域 確実性 m程度 構造物 一般 空間 進行 間隙 含水分 増大 土砂 流出 背面 破壊 陥没 拡大 手段 落下 出水 ＳＬ 吹付 前面 支圧板 役割 本体 利用 蛇行 最小限 剛性 一緒 伴 側面 不連続 ピッチ 支障 他 進 変化 人力 各々 通常 近傍 低圧 ボーリング ドリル 簡易 機械 設置 符号\n",
      "\n",
      "===== # 18, Topic : 13, p : 21.6669 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 建築用合成材 集成材 構造用 図 ドリフトピン 構造部材 合成材 木材板 鋼板 建築物 接合部位 形態 造作用 発明 部位 構造安全性 長手方向 該建築用合成材 実施 構造用部材 実施形態 断面図 上記 安全性 説明 部材 個所数 板厚 建築 垂直方向 両面 同等 当該集成材 必要個所数 繊維方向 等 水平方向 接着剤等 屋内工事仕上げ用 説明図 特徴 補剛効果 鉄骨等 端部 中央部 断面寸法 態様 木材 上記構造 上記木材板 樹脂製 請求項 ドリフトピン等 座屈耐力 該集成材 基体 符号 両側 耐力 構成 接合 方法 課題 剛性 接合部 上記実施形態 屋内仕上げ工事 鉛直方向 当該図 適用可能性 環境 風情 目的 手段 記載 面 所 上記事情 上記課題 垂直関係 適用 効果 ガセットプレート 配置関係 適用範囲 技術分野 技術 特徴点 固定方法 ＳＳ－ 規格品 構成要素 両端 固定 前記両端 中間地点 剛性低下 角材 強度 棒状 ボルト 表面 悪影響 荷重 対応 現状 要求 主体 ろ 内部 影響 下記 詳述 対象 同一 複数 材料 すじ 斜め 所定 位置 二つ 都合 サンドイッチ 全長 任意 図面\n",
      "\n",
      "===== # 19, Topic : 9, p : 12.3452 %\n",
      "Topic words : －, 特開平, 号公報, トンネル, 記載, 後方, 外側, 地山, 掘削, 左右\n",
      "Input : 繊維 流動コンクリート 流動性 強度 ポリプロピレン コンクリート 波形ポリプロピレン 靭性 鋼繊維 ストレート形 波形 スランプフロー値 Ｄ ビニロン繊維 スランプフロー 高性能ＡＥ減水剤 ベースコンクリート ～ 発明 硬化性状 ストレート形ポリプロピレン 繊維混入高流動コンクリート 繊維径 Ｄ繊維 フレッシュ性状 剤 表 流動 山 理想形 形状 流動特性 ポリプロピレン繊維 ピッチ幅 骨材 繊維混入コンクリート 谷 Ａ系統 Ａ 繊維混入 分散性 図 混和剤 調整目地コンクリート 自己支持性形状 混入量 ポリカルボン酸系 繊維補強コンクリート 充填 セメント 配合 Ａ系高流動コンクリート 無配筋部 添加量 親水性処理 ベース θ 繊維Ｃ 単位水量 量 隙部 せん断強度 繊維Ａ～Ｄ 混和材 容積 長手方向 ストレート形ポリプロピレン繊維入り 流動コンクリート自身 強度化 繊維配合 状態 Ｃ 補修コンクリート 混入 形状例 自己充填性能 親水性 自己充填性 Ａ－ Ｄ系 測定値 高性能減水剤 直線状 Ｂ－ 砕石高性能ＡＥ減水剤 材料分離抵抗 鋼繊維入り 硬化性状 ビニロン繊維入り 耐久性 成形 波 効果 ポリカルボン酸系標準形 高炉スラグ微粉 補修コンクリート等 課題 増 方向 クラック 材料 直線距離 Ｃ系 断面修復 vol.％，( アミノスルホン酸系 無配筋 Ｂ系 傾斜角 平面 微粉 ストレート 平面図 Ｄ系統 Ｄ－ ナフタリンスルホン酸塩系 ピッチ メラミンスルホン酸 橋梁 程度 石粉 範囲 重量 フアイバーボール 高密度配筋部 特性値 シリカフューム 一端 硬化 断面補修部等 隙部等 ≒Ｄ 調整目地 付着強度 材料分離 圧縮強度 Ｃ系統 Ｃ－ 無配筋箇所 せん断強度試験 細骨材 配合量 特性 単位セメント量 波形状態 波形成形 Ｂ系統 材齢 Ｂ 一端支持 一定ピッチ 空気量 粉体量 株式会社エヌエムビー社製 説明 手段 補強法 補強効果 通常 特徴 フライアッシュ 一種 モルタル 他端 向性 種類 ウエランガム 添加 径 普通ポルトランドセメント砂 線状 団子状 晶株式会社製 適用例 シリカフューム等 実施例 分散 コルゲート板 山砂砂利 タフネス試験 vol.％ 表面処理 商品名ＳＰ 波板 ≒θ 技術分野 技術 温度ひび割れ 発明者 実施 バイオガム 軸ミキサー 高低差 高低 弾性係数 鉄筋 コスト 上昇 懸念 種々 犠牲 加工 影響 前記 水 他 形態 交互 外部 応力 波線 一方向 o 理由 発生 進展 高め 組合せ 各種 粘性 容量 Ｌ 強制 比重 等間隔 次 Ｓ 低下 形成 他方 物性 隙間 空隙 図面\n",
      "\n",
      "===== # 20, Topic : 13, p : 13.0970 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : スタンド検出台 スタンドアーム 目的自転車 車輪検出部 自転車駐車システム スタンド 図 案内溝 先端部 自転車 車輪 出し入れ機構 ゲート装置 傾斜部 検出範囲 スタンド使用状態 検出状態 上記スタンド検出台 使用状態 状態 車輪検出 側面図 駐車 発明 スタンド検出動作 起立保持用 前記スタンド検出台 上記車輪検出部 スタンド検出台高 異状 位置 輪 検出信号 頂部 要部側面図 輪検出 車輪検出信号 ゲート Ａ矢視図 Ｂ矢視図 所定 案内 Ｃ－Ｃ線拡大断面図 ゲート部 範囲 フロア 目的 上記構成 上記案内溝 受け渡し 入庫 出庫 通過 前輪通過検出タイミング システム 先端部Ｈ 異状対応 取扱い者 前記案内溝 移送機器 デッドロック状 構成 使用位置 説明 ロック 前輪 課題 異状状態 正常状態 自立出し入れ リフトアップ動作 動作タイミング リフトアップ 上記課題 上記フロア 上記発明 事前 処置 実施 形態 図面 基準 取扱い 元 等 別 作用 受け渡し動作 効果 重量センサ等 位置決め用 光学センサ 音声ガイダンス等 移送 タイミング 対応 取扱い工程 転倒等 正常入庫 正常確認 リフト量 技術分野 技術 側方 信頼性 走行クランプ Ｈ 障害部材 側 人手 支障 周囲 干渉 損傷 原因 手段 車体 後方 前方 底面 事故 未然 解除 前側 符号\n",
      "\n",
      "===== # 21, Topic : 13, p : 38.5442 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 外側足場 内側足場 足場 躯体 躯体Ｋ 昇降装置 型枠足場 構造物 内部空間 外側 内側 型枠 図 クライミング足場 コンクリート 外側型枠 連結部材 クライミングジャッキ クライミングロッド 発明 スリップフォーム工法 クライミング 躯 クライミング操作 施工法 枠 Ｋ 接部材 外側用 水平力 内側型枠 ｂ 車輪 内側用 特開平 号公報 鉛直荷重 状態 側面図 躯体表面 外側足場共通 外側部材 工法 発明クライミング足場 連結ビーム コストアップ 内側部材 装置 側 ｃ 外足場 内外側型枠 左側部分 右側部分 外側面 コンクリート壁 部材 内側面 外側共 説明 ー 相互 鉄筋コンクリート構造物 橋脚 共通 所定 位置 センターコア構造 施工 施工コスト 進退装置 橋脚Ｋ 課題 台数 力受け 操作 接 ブラケット 内外面 内外 コアー等 煙突等 円形等 前記問題点 技術分野 煙突 技術 所定位置 前記 断面形状 形状 建物 鉄筋 分割 手段 実施 形態 先端 外面 内面 態様 交互 四角形 効果 数 図面 符号\n",
      "\n",
      "===== # 22, Topic : 13, p : 24.9896 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : パイロット弁 開閉弁 パイロット流路 止め弁 作動油圧 作動室 作用室 弁座 導入流路 作動油 主弁体 主流路 弁体 弁本体 接続流路 止め弁体 実施形態 閉弁用流路 前記作動室 前記作用室 弁 用油圧ダンパ 前記パイロット流路 孔 連通流路 小径孔 図 流路 前記開閉弁 作用力 貫通孔 作用 環状溝 油圧回路図 前記閉弁用流路 摺動孔 底孔 圧力 作動 形電磁弁 応答性 前記導入流路 側 作用方向 前記パイロット弁 コイルばね 路 環状室 連通 接続孔 着座面 栓部材 開弁方向 径孔 励磁信号 方向 前記 開弁 制御回路 応答 シート部材 該パイロット弁 閉弁状態 圧力センサ 作動油圧力 付勢力 状態 前記主弁体 部材 リリーフ弁 発明 閉弁方向 該導入流路 実施 形態 低圧側 作動油圧力 他方 前記作動室側 拡大断面図 規制部材 ケース部材 振動外力 パイロット形 シリンダ 開弁状態 ばね付勢力 アキュムレータ スプール 挿入孔 技術 押さえ部材 摺動 ストロークセンサ 課題 ｂ 軸方向 頭部側 該作用室 振動 外力 対象物 開位置 説明 変位 流出 構成 一端 受圧面 断面図 貫通溝 円錐形状 有底孔 離座方向 着座方向 出力 背部 圧力均衡等 シリンダ本体 開状態 上昇等 構造物等 背部側 外周面 吐出 吸入 一対 向き 逆 ピストン 温度上昇 分 瞬時 上昇 閉作動 手段 両側 図面 閉位置 ソレノイド アーマチュア 高圧 効果 破損等 周囲温度 技術分野 基台 減衰 周囲 減衰特性 構造 外周 地震 風 気温 環境 低下 次 遮断 特徴 該弁座 流入 番号 中心 螺 圧入 移動 ２つ 径方向 鉄心 他 所定 値 閉弁 かかわり 例 本題 要旨 範囲 態様 符号\n",
      "\n",
      "===== # 23, Topic : 13, p : 26.6438 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 硬化材 先端部材 硬化材混入 硬化材混入工法 硬化材混入層 注入管 硬化材混入装置 四角柱状硬化材混入層 硬化材注入 四角柱状硬化材混入工法 噴射孔 円柱状硬化材注入工法 図 回動 四角柱状硬化材混入層Ｖ 下側 噴射 発明 円柱状硬化材注入層 流体 土砂 硬化材注入層 先端 孔 地盤 縦断面図 硬化材注入工法 制御板 行い硬化材混入 ノズル 行い硬化材注入 高圧 円柱状硬化材混入工法 圧縮空気 四角柱状硬化材注入工法 高圧水 先端両側 工法 垂下状 効率 壁体 部分 流量 状態 貯留槽 上側 側面 圧力 例 揺動 連壁 油圧シリンダ 円柱状混入工法 一定 地中 壁 噴出 下端 上方 入管 影響 前記先端部材 左右 前記注入管 手段 変動 水平状 斜視図 施工 長方形状 斜め状 上記 両側 土 軟弱地盤 対象地盤 切削跡 実施例 床版 セメントミルク 切削 水平壁体 止水壁 壁厚 地中壁 垂直壁体 説明 下方 アスファルト ｃｍ 程度 複数個 効果 距離 噴射効率 ＣＣＰ工法 ＪＳＧ工法 ＣＪＧ工法 上下側 方向 噴射順序 ノズル孔 噴出孔 移動方向 廃泥 一定方向 制御盤 場所 課題 図面 ～ 所要 ステップ Ｖ 方法 チェーンソー ワイヤー 数 筒体 上下面 接線方向 左右方向 軸方向 廃泥液 地下駐車場 実施 前記 地下室 地下ダム 水平状態 図示例 外周面 周面 移動速度 移動区間 切断跡 施工効率 一定流量 技術分野 構築物 基礎 地山 技術 円弧部分 基礎工事 隣接地域 基端 ボーリングマシン 到達距離 平面視 箇所 遠隔操作 切断 特定箇所 流動速 付近土砂 連結部 ユニバーサルジョイント 支保 目的 内外 中心 放射状 住宅 態様 アースオーガー 深度 軟岩 終了 単独 角度 エスカレータ 片側 ソイルカッター アースカッター 等 地上 位置 機構 関係 適用 加工 間隔 増加 複数 面積 差 排出 形成 上述 開口 符号\n",
      "\n",
      "===== # 24, Topic : 9, p : 16.9661 %\n",
      "Topic words : －, 特開平, 号公報, トンネル, 記載, 後方, 外側, 地山, 掘削, 左右\n",
      "Input : 掘削土砂 掘削土砂搬出施設 スラリー処理施設 泥量 泥管 土砂 排泥管 排泥量 掘削 掘削土砂量 スラリー 泥ポンプ 排出管 分岐排出管 排泥ポンプ 泥水 量 搬送掘削土砂量 搬出量 スラリー管理 切替装置 掘削土砂管理システム 搬出 泥水量 前記 搬送土砂量制御手段 処理 スラリー量 スラリー管理システム 計量掘削土砂量 掘削土砂管理方法 スラリー返送量制御装置 処理施設 搬送土砂量制御装置 泥水供給量制御装置 残存処理能力 運行管理装置 排泥量ＱＤ スラリー返送量制御手段 処理量 運転制御施設 泥水供給量制御手段 前記搬送掘削土砂量 掘削土砂管理 前記計量掘削土砂量 運搬装置 圧送ポンプ 切羽Ｈ 掘削土砂処理施設 処理能力 運行管理 土砂ホッパ 切替制御装置 泥水式シールド工法 スラリー管理方法 請求項 切羽 走行位置 加泥材 スラリー返送手段 排出量 前記スラリー 施設 制御 当該運搬装置 発進立坑Ｋ 泥水供給手段 実施形態 排泥 スラリー処理施設スラリー処理施設 装置 前記掘削土砂 掘削土砂搬送施設 供給量 前記掘削土砂搬出施設 前記運搬装置 返送 返送量 掘削土砂搬出施設等 前記泥水 スラリー返送量 位置検出手段 シールド工事 発明 当該掘削土砂 ＱＤ 管理システム 圧力 複数 ダンプトラック 搬送量 進機 泥土式シールド工法 搬送手段 当該計量掘削土砂量 進機Ｓ 泥水供給量 処理設備 位置 前記切羽 制御方法 前記スラリー処理施設 計量手段 運行 スラリー処理システム 走行位置情報 前記泥水供給量制御装置 泥量ＱＳ 用地 ＱＳ ロードセル 供給 前記排泥ポンプ 泥量ＱＳ スラリー返送量制御装置地上部 地上部 走行速度等 前記排出管 混合土砂 前記圧送ポンプ 泥量ＱＤ 排泥量ＱＤ トンネルＴ 前記スラリー管理システム 地山 各種使用機材 前記運行管理装置 進 該主排泥管 加泥材供給装置 圧力管理 前記主排泥管 公共用地等 タンク 管理方法 前記スラリー返送手段 発進立坑 当該運行管理装置 切羽圧力 泥水圧 分流器 回転数 式 前記複数 工法 排泥流速等 搬入等 当該土砂ホッパ 前記泥水供給手段 実施 一定量 図 進長 複数箇所 数 前記切替装置 運行位置 加泥材等 記載 水位データ シールドトンネル 当該 水位レベル 前記搬送手段 比重管理 粘性管理 ストローク数 切替装置切羽Ｈ 前記ダンプトラック 当該泥水 位置表示装置 レベル 方法 Ｋ 連絡用シャフトＫ 前記計量手段 含砂量管理等 ＧＰＳ衛星 形態 接続 スラリーポンプ等 手段 トンネル上部 合流器 圧送 対象地盤Ｇ 泥水槽 説明 制限 等 流量 検出 条件 運転 進機Ｋ 通信装置等 長距離シールド シールド径 進速度 規模 走行 前記ロードセル 泥土圧シールド工法 進能力 当該複数 搬送距離 雨水管路等 当該実施形態 前記レベル 交通公害 調整槽 分岐部 メンテナンス費用 構成要素 下流側 概念図 ポンプ圧送等 工事 公共用地 走行データ 当該合流器 工期 特徴 他 合計 所 遮断 リアルタイム 連絡用シャフトＴ 運行指令 トンネルＧ 管長さや管径等 当該ダンプトラック 礫等 摩耗等 低下等 騒音等 ローヘッドスクリーン等 増車等 水位 Ｓ 物流条件等 分岐配管 硬質地盤 データ 目的地 走行ルート 工事費 対象地盤H ＧＰＳ 各種設備 管内圧力 圧力容器 ＧＰＳ信号 都市部 先端部 前面 砂 課題 高速化施工 多量 設備 搬入 長期化 増大化 短縮 衛星 図面 同一 符号 近傍 貯溜量 ）～（ 圧 車 効果 沿道条件 入力データ 施工現場周辺 混合物 産業廃棄物 交通渋滞 受信信号 送信信号 塑性流動性 機械費 技術分野 技術 バインダー分 礫 コンクリート状 増大 作業効率 問題点 目的 該総泥水量 配管 データ通信機能 データ通信 混雑個所 通信 設計変更 山留め 部分 確保 増加 セグメント 種々 原因 一つ プラント 低減 材料 Global Positioning System 前者 後者 公知 各々 基 内部 後記 圧送量 停止 増減 最適 ＰＨＳ 趣旨 範囲 設置 公園 下水道\n",
      "\n",
      "===== # 25, Topic : 13, p : 33.5510 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 外側足場 内側足場 躯体 足場 昇降装置 内部空間 外側 内側 構造物 躯体Ｋ 力 構造 水平力 クライミング装置 型枠足場 フレーム 図 外側型枠 水平方向 クライミング操作 型枠 方向 スリップフォーム工法 閉鎖型足場 発明 方向性 施工法 垂直荷重 コンクリート 内側型枠 断面四角形 コンクリート壁 閉鎖型外側足場 特開平 号公報 車輪 枠 閉鎖型内側足場 力受け クライミングジャッキ 溝断面 断面円形 躯体表面 正面図 剛性フレーム 外側共 外側面 接部材昇降装置 中空構造物 説明 ー 数 図示 例 ブラケット 煙突 レール 表面 躯体側 進退装置 センターコア構造 断面四角形状 クライミングロッド クライミング過程 工法 接部材 橋脚 鉄筋 課題 該躯体 ｂ 状態 所定 位置 チェーンブロック等 コアー等 煙突等 レール等 コンンクリート壁 平面図 側面図 油圧ジャッキ 前記問題点 技術分野 技術 複数分割 分割 側面 ワイヤーロープ 引き上げ方式 コンンクリート 建物 台数 手段 実施 形態 形状 要所 形式 押し上げ つて 効果 図面 符号\n",
      "\n",
      "===== # 26, Topic : 13, p : 33.8143 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : シールドジャッキ装置 ピストンロッド 抵抗ジャッキ装置 シールド機本体 進機 セグメント端部 端部 ジャッキ ピストンロッド先端 調整バルブ装置 抵抗値 開閉弁 セグメント 方向 所定値 セグメント端面 ガーダー部 開閉力 図 フランジ部 ジャッキ本体 力 抵抗値Ｆ 進方向 シリンダー 抵抗ジャッキ本体 油圧室 油圧力 実施形態 ｂ 油圧回路 端面 曲線施工 値 固定金具 弁 説明 スプレッダー 構成説明図 テール部 変更方向 先端 当該シールド機本体 抵抗 発明 伸長 前記ピストンロッド 所定 圧油供給手段 シールドジャッキ装置等 手動ハンドル 圧油供給装置 推進力 推進 スプリング ジャッキ等 弁座 付勢力 圧力 ｃ 方向側 ピストンロッド伸長 固定 カッターヘッド カッターヘッド駆動装置 先端側 凹状段部 排土装置 周方向 フード部 主要部 先端部 段部 推進用 圧油供給源 方向変更 前記抵抗値 圧油 部分 シールドスキンプレート 進方向変更 構成要素 推進方向 所定方向 球面継手 作動 相互固定 伸長方向 進 構成 シールドスキンプレート内側 切羽側 環状 ｄ 推進操作 推進量 方向変更手段 装置群 開閉力可変弁 軸方向 止水機能 当該フランジ部 テールシール 前記シリンダー 空間 複数 推力 ボルト ハウジング 板 ｅ 状態 方向制御 直進方向 掘削方向 調整 工作業 油圧力 相互 曲率 凹状段部形成側 実施 形態 側 引き抜き力 方法 特徴 位置 課題 スチール製セグメント 推進状態 ハンドル軸 手段 ネジ穴 伸長動作開始点 具体的構成 チェーンブロック 具体的構成要素 チェーンブロック等 ３つ 機器 場所 後端 エレクター 内側 箇所 直し 他方 伸長移動 支点 時点 停止 ～ 番号 ２つ 作用 動作 表面 断面図 貫通ネジ穴 切羽部分 係止 配置状態 技術 機能 周 作業 鋼殻部分 ねじ穴 開放状態 技術分野 問題点 配置例 保護下 掘削 均等荷重 配列位置 相関関係 直進 圧縮度 技術的課題 外部 内部 全面 後部 一般 等間隔 適性 形状 構造 使用 ルート 他 目的 １つ 一端 他端 意味 逆 内方 凹状 厚み 態様 ６つ 最大 次 繰り返し 経過 効果 図面 符号\n",
      "\n",
      "===== # 27, Topic : 13, p : 41.9110 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 法枠ブロック 底面壁 面壁 連結用底面壁部 法面 側方 係合凹部 法枠ブロック同士 図 係合穴 前記底面壁 実施形態 盛土 側端部 控え壁 前記係 法枠本体 アンカーピン 発明 側方方向 法面勾配 目用 面 当該法枠ブロック 前記連結用底面壁部 法枠ブロック裏面 端部 請求項 連結 盛土層 前記アンカーピン 係 矩形状底面壁 端面 凹部 上下方向 前記 上下 目 後方 安定性 盛土補強土工法 矩形状 該前面壁 突出片 上面 地盤 ｂ 係合 側 記法枠ブロック 法面全面 ｃ 階段状 係合固定 前記側端部 ジオテキスタイル 土砂 空孔 ａ 該底面壁 前端部 ブロック部材 自立安定性 斜視図 補強材 壁 網目状 前記空孔 転圧 前記控え壁 型枠 圧 位置 該連結用底面壁部 方向 傾斜面 側端 当該法枠本体 ｃ－ｃ線 アンカー 下端面 内側面 前記補強材 直近後方 安定化 施工 上記発明 ｅ－ｅ線矢視 IV－IV線矢視 基礎地盤Ｇ 直方体状 格子状 裁頭円錐状 記法枠本体 平面部 勾配 幅 断面図 軽量化 － 前方 ブロック分 穴 バットレス部 水平方向 上方 作業能率 前記階段状 縦断面図 盛土層間 両側端部 列 フック 字状 側面図 平面矩形状 上端面 支持面 控え壁上面 一体化 先端上面 領域 実施 形態 略矩形状 位置調整 水抜き用 前記突出片 説明 状態 同一 下面 特徴 他方 階段状部分 タイヤローラ タイヤローラー 該アンカーピン 幅方向 千鳥状 前記先端 プレキャストコンクリート製 相対移動 中央部 盛土補強工法 裁頭円柱状 係止具 根張り 基礎地盤 コ字状 後方直近 挿入係合し 幅方向中央 最深部 複数 効果 一対 課題 重心 任意 記載 直上 互い 植林 配列 ２つ １つ 前記ジオテキスタイル 孔 地盤低下 ～（ｃ 先端 同士 該係合凹部 上記 相対位置 垂直後方 直近領域 コスト削減 多段 作業 張力 開口 植生 目的 該控 上層 下層 緩み 直下 中央 前述 手順 造成 転圧作業 転圧付与 符号 平面半円形状 傾斜部分 横幅 半円柱状 正面視 材料コスト削減 傾斜角度 一体 水平 部材 鋼製 号公報参照 技術分野 技術 工法 特開平 こぼれだし 景観向上 傾倒モーメント 添付図面 盛り立て 角度 煉瓦積み 景観 図面 側部 面積 虞 メッシュ 手段 自重 ～ 奥行 程度 間隔 同形 工場 トラック 実線 紙面 配置 腐食 素材 耐食性 ステンレス 位置決め 図示 敷設 所期 植物 構成 相違 前面 薄肉 水捌け 小径 両者 隙間 要旨 範囲 変形 搬送 設置 広範囲\n",
      "\n",
      "===== # 28, Topic : 7, p : 23.1904 %\n",
      "Topic words : 地震, 架構, 降伏点鋼, 剛性, モーメント, 溶接等, 鋼材, 鉛直荷重, 柱, 記載\n",
      "Input : 連層耐震壁 並列連層耐震壁 前記連層耐震壁 前記並列連層耐震壁 建物 扁平梁 桁行方向 層数 梁間方向 連結梁 桁行方向Ｙ 前記建物 梁成 梁 層 前記扁平梁 請求項 梁幅 前記連結梁 方向 鉄筋コンクリート 梁間方向Ｘ 構造 ラーメン架構 前記連層耐震壁相互 前記 図 耐震性 連層耐震壁相互 ダンパ 上記並列連層耐震壁 地震応答変形 相互 階高 地震力 変形 地震エネルギー 前記ラーメン架構 記載 面外方向 前記ダンパ 実施 形態 下層階 耐震効果 鉛直方向 エネルギー吸収能力 階高寸法 耐震性能 耐震要素 発明 構面内 基準階 制震ダンパ 幅内部 断面寸法 鉄筋コンクリート造建物 鉄筋コンクリート造 制震効果 梁間方向Ｙ 平面形状 Ｄ 効果 鉄骨鉄筋コンクリート造 柱 建 スリット入り鋼板 Ａ－Ａ断面図 内部 断面形状 ｂ 上記 建程度 建～ 平面図 板状建物 一体 変形状態 剛性 該ダンパ 定着 相対変位 模式図 水平変位δ 階段室等 鉄骨鉄筋コンクリート建物 空間内部 程度 参照 特徴 断面図 基準階平面図 鉄骨鉄筋コンクリート ダンパＸ 定着部 ダンパ 複数箇所 エレベータシャフト 説明 上層 課題 任意 増加 耐力 損傷 スラブ 主筋 せん断変形 鉄骨 強度 図面 例 下面 面 両側 Ｔ コ Ｈ ユニットバス等 細部構造等 オイルダンパ等 水平剛性 上記事情 技術分野 技術 回転角 バリアフリー対応 単独 構面 上限 目的 手段 他 レベル Ｌ ａ 趣旨 範囲 種々 改良 設計 変更 各種 符号\n",
      "\n",
      "===== # 29, Topic : 6, p : 14.6896 %\n",
      "Topic words : 上部, ｄ, 実施形態, ｃ, 作業, 側面図, ｂ, 号公報, 両側, 上面\n",
      "Input : 収容壁部 図 テナントエリア 冷暖房機器 天井面 設備機器類 ブレース 請求項 開口部 設置スペース 字状 建物 テナントビル 実施形態 外周部 発明 スパン ユニット分 フレーム 照明器具 配管ダクト類 コスト 前記収容壁部 形態 上記実施形態 外周側 設備機器設置スペース 梁 ユニットごと 設置工事 ユニット テナント変更等 平面図 天井伏図 耐震性 架構 窓 レースウエイ 前記天井面 ラーメン架構 天井 コアエリア 内部空間 追加変更工事 ハ バルコニー 下階 上記 図示例 電灯設備工事 ～図 柱 ペリメータ負荷処理用 インテリア負荷処理用 配線用 有効面積比 位置 位置変更 構造的安定性 通線工事 テナント 配線 テナント変更 合成床版 桁行方向 有効天井高 側 空間 立面図 スラブ 実施 改変工事等 断面図 工事 梁伏図 間仕切り壁 腰壁 廊下 断面 盤 躯体コスト 口 冷暖房負荷 デッドスペース 動線計画 建設コスト テナントビル等 天井高 部材断面 内部 通風 Ｈ形鋼 ユニット分つまり 保守点検 基本 所要 側部 床 外壁 出入口 位置変更等 冷暖房 ペリメータ負荷 インテリア負荷 スペース スパンごとつまりテナントエリア 直射日光 屋外ユニット等 屋外ユニット 日射負荷 電気配線用 電気配線類 説明 各階 下面 採光 屋内ユニット 分電盤 削減 スパン分 階高 鉄骨ラーメン構造 構造設計 設計的変更 住宅用アルミサッシ等 端子盤等 建具工事費 梁下 採算性 耐久性 快適性 利便性 融通性 可変性 鉄骨製作コスト ケーブル等 際等 ルーバ等 予備スペース 例 外周フレーム コスト削減 Ｘ状 基準階 前記ブレース 性能 機能 課題 外部 用途 符号 境界 着脱 増減 動力盤 サッシ 庇 通常 山形 支障 向上 柱空間 点検扉 幅方向中央 立体駐車場棟 通風口 排気口 斜めブレース 技術分野 技術 ボルト締結 スラブ厚 ＯＡフロア 作業員 軽量パネル パイプシャフト 外気 熱交換 上部壁面 給気 新鮮外気 汎用品 効果 遮蔽効果 要請 方策 実状 手段 事務所 建て 付属 溶接 併用 おき 取り付け 外観 内観 自体 ＥＰＳ 一体 ウオールスルータイプ 吹出口 下部 吸込口 入口 換気 制御 独立 使い勝手 外側 夏期 低減 一般 Ｋ 所望 規模 犠牲 格別 図面\n",
      "\n",
      "===== # 30, Topic : 13, p : 20.9745 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 風力発電装置 図 回転軸 屋根面 回転羽根 風車 面 請求項 発電機 角部 発電 実施 形態 発電効率 防風カバー 風 平面図 発明 発電エネルギー 回転 壁面 建物 ｂ 設置面 回転効率 屋根 頂部 面側 一方向 軸受け板 方向 延長線 断面図 他方 ～ 風向き パドルタイプ 特徴 回転方向 風力 孔 サボニウスタイプ 効果 防波堤 回転エネルギー 時計回り 右回り 前記風車 該延長線 一直線状 一般住宅 角部付近 説明 課題 該風車 箇所 屋上 バネ 上部 下部 鍔 外壁部 正面図 上記 手段 図面 構成 位置 上側 前記 斜面 状態 軸受け板間 ｂ側 下側 前記長孔 電気エネルギー 技術分野 技術 高層建物 構造物 半円形 目的 台数 一定 同士 年間 内側 符号\n",
      "\n",
      "===== # 31, Topic : 13, p : 32.8408 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 壁 壁用ＰＣ板 壁用プレキャストコンクリート板 図 マリオン壁 上階 下階 立ち上げ壁 接合ピン 金物 断面図 上部 ボルト 背面図 間仕切壁 バルコニー 面内方向 面外方向 体 上部プレート Ａ－Ａ線断面図 Ｂ－Ｂ線断面図 Ｃ－Ｃ線断面図 Ｄ－Ｄ線断面図 請求項 発明 床 板 小口部 実施 形態 要部 前記壁用ＰＣ板 該壁用プレキャストコンクリート板 断面凹形 止め板 孔 受け金物 ファスナー プレキャストコンクリート板 凹溝 構築方法 引きボルト 位置合わせ 操作溝 先端部 受け体 下部 Ｅ－Ｅ線断面図 Ｆ－Ｆ線断面図 Ｇ－Ｇ線断面図 Ｈ－Ｈ線断面図 Ｉ－Ｉ線断面図 位置 背板 固定板 上側小口部 斜視図 ～図 下面 層間変位 段部 モルタル カバー 前記接合ピン 取り付け誤差 収納孔 説明 課題 上記 見栄え 該受 特徴 外壁 インサートパイプ フルプレキャストコンクリート板 下側 開口部 構築 操作 手段 該上部プレート 右 左 構成 ナット 調整 表面 固定 前記側壁 手前側 技術分野 技術 該ファスナー 側壁 目的 凹溝内 一対 前方 反対 上下 底面 頭部 ａ ｂ 現場 各階 突出 クレーン 効果 図面 符号\n",
      "\n",
      "===== # 32, Topic : 13, p : 25.6339 %\n",
      "Topic words : 部材, 力, 形状, 方向, 一端, 移動, 間隔, 斜視図, 上下, 内側\n",
      "Input : 梁部材 梁 鉄筋挿通孔 充填材 柱頭 ＰＣａ梁部材 柱部材 ＰＣａ柱部材 軸鉄筋 枝付き梁部材 中核体 柱部材軸鉄筋 注入孔 梁鉄筋 注入 ＰＣａ部材 部材 方法 目地空間 挿通孔 ＰＣａ柱部材柱頭 ＰＣａ枝付き梁部材 梁部材上面 接合 目地モルタル ＰＣａ 接合方法 梁部材接合 止め材 柱 枝付き梁部材建て入れ 材 鉄筋 梁軸鉄筋 柱部材柱頭 ＰＣａ柱 梁部材下面 部材水平鉄筋 梁部材接合方法 左右梁部材 ＰＣａ柱部材柱脚 左ＰＣａ梁部材 右ＰＣａ梁部材 発明 管 注入口 シース管 前記柱部材柱頭 プレキャスト鉄筋コンクリート梁部材 梁部材建て入れ シース管内 プレキャスト鉄筋コンクリート 充填 プレキャスト鉄筋コンクリート柱部材柱頭 慣用方法 完全充填 下階ＰＣａ柱部材柱頭 梁上面 梁下 充填材注入 鉛直方向 上端部 柱部材辺長 軸鉄筋上端部 プレキャスト鉄筋コンクリート柱 パネルゾーン 利点 流動性 問題点 せい高 柱頭空間 上方 水平方向 鉄筋接合作業 目地 練りモルタル 作業 コンクリート孔 鉄筋挿通孔群 発明方法 梁端面 鉄筋量 鉄筋定着 上昇速度 硬化充填材 部材間接合力 前記中核体 充填材上昇速度 上階 中核体下面 目地空間周縁 図示 位置 下面 入れ 下 注入方向 前記中核体下面 前記挿通孔 目地モルタル施工 直線状 図 縦断図 差 ヘッド差 該充填材 注入作業 該中核体 説明 側面 距離 ばらつき 柱頭部位 柱頭外周 建築軸組 間隔空間 上記 硬化 工程 接合状態 鉄筋コンクリート造建築 上記目地空間周囲 前記露出上端部 方向 端部 柱頭周縁内側 シース管下端 周縁 シース管内壁 改良方法 連通孔 コルゲート管 煩雑 部分 略称 課題 施工 間隙 空気 ボイド 図面 要部 露出端部 ライナー 内壁 態様 省力効果 せん断補強筋 細部配筋 施工工種 工程短縮効果大 コンクリート 挿通鉄筋 凹凸面 利用分野 間隔 Ｔ字形 一体 応力集中 部位 一体構造 状態 程度位 係合性 凹凸 型枠 上方開口 ポンプ負荷 実施状況 産業 技術 略記 相互 工法 通常 十字形 構成 セメントグラウト 構築 置換 ごと 手段 特徴 都合 形成 真っ直ぐ 領域 中央 同一 各種 板 帯状 排出 本 一つ つぎ 上述 符号\n"
     ]
    }
   ],
   "source": [
    "trainer.lasy_predict(ntm_model, dataloader_test, bow_vocab, num_example=50, n_top_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "55ee831d6af1ae9706634b6e60f3ce072bf89ecf7e73a290d2e6ee7cdf841cc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
